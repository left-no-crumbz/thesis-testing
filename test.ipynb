{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a7a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ASTForAudioClassification, ASTFeatureExtractor\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Audio\n",
    "import torchaudio\n",
    "\n",
    "# Load the feature extractor and model\n",
    "model_path = \"./runs/ast_classifier/checkpoint-best\"  # Update this path if your best model is saved elsewhere\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", num_mel_bins=64, max_length=507)\n",
    "model = ASTForAudioClassification.from_pretrained(model_path)\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_audio(file_path, threshold=0.5):\n",
    "    # Load and preprocess the audio\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    \n",
    "    # Convert to mono if needed\n",
    "    if len(waveform.shape) > 1 and waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Resample if needed (to 16kHz)\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Convert to numpy array and normalize\n",
    "    waveform = waveform.numpy().squeeze()\n",
    "    \n",
    "    # Extract features\n",
    "    inputs = feature_extractor(\n",
    "        waveform, \n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "        max_length=507\n",
    "    )\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        confidence, predicted_class = torch.max(probabilities, 1)\n",
    "        \n",
    "    # Convert to human-readable output\n",
    "    label = \"Fake\" if predicted_class.item() == 0 else \"Real\"\n",
    "    confidence = confidence.item() * 100  # Convert to percentage\n",
    "    \n",
    "    return {\n",
    "        \"prediction\": label,\n",
    "        \"confidence\": f\"{confidence:.2f}%\",\n",
    "        \"probabilities\": {\n",
    "            \"Fake\": f\"{probabilities[0][0].item()*100:.2f}%\",\n",
    "            \"Real\": f\"{probabilities[0][1].item()*100:.2f}%\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8875ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "audio_file = \"path/to/your/audio_file.wav\"  # Replace with your audio file path\n",
    "result = predict_audio(audio_file)\n",
    "print(f\"Prediction: {result['prediction']}\")\n",
    "print(f\"Confidence: {result['confidence']}\")\n",
    "print(\"Probabilities:\")\n",
    "print(f\"  - Fake: {result['probabilities']['Fake']}\")\n",
    "print(f\"  - Real: {result['probabilities']['Real']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
