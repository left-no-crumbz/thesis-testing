{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27471c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import ASTFeatureExtractor\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ac471d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b741a51391b4500a4a60c51866ddd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/13956 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c610bef088c9458fb61fe7d06ba7e4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2826 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f079b78b177d4479bf7e5819d147a144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1088 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"audiofolder\", data_dir=\"./for-2seconds\")\n",
    "\n",
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model, num_mel_bins=64, max_length=507)\n",
    "\n",
    "\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "SAMPLING_RATE = feature_extractor.sampling_rate\n",
    "num_labels = len(np.unique(dataset[\"train\"][\"label\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f317f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 13956\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 2826\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 1088\n",
      "    })\n",
      "})\n",
      "dataset train features: {'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "dataset test features: {'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "dataset validation features: {'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "model_input_name: input_values\n",
      "SAMPLING_RATE: 16000\n",
      "dataset[\"train\"][0]: {'audio': {'path': 'C:\\\\Users\\\\Crumbz\\\\Desktop\\\\4TH-YEAR-1ST-SEM\\\\THESIS\\\\thesis-testing\\\\for-2seconds\\\\training\\\\fake\\\\file10005.mp3.wav_16k.wav_norm.wav_mono.wav_silence.wav_2sec.wav', 'array': array([ 0.10552979,  0.11013794,  0.00952148, ..., -0.20837402,\n",
      "       -0.25552368, -0.24255371]), 'sampling_rate': 16000}, 'label': 0}\n",
      "num_labels: 2\n",
      "dataset columns: ['audio', 'label']\n"
     ]
    }
   ],
   "source": [
    "print('dataset:', dataset)\n",
    "print('dataset train features:', dataset['train'].features)\n",
    "print('dataset test features:', dataset['test'].features)\n",
    "print('dataset validation features:', dataset['validation'].features)\n",
    "print('model_input_name:', model_input_name)\n",
    "print('SAMPLING_RATE:', SAMPLING_RATE)\n",
    "print('dataset[\"train\"][0]:', dataset['train'][0])\n",
    "print('num_labels:', num_labels)\n",
    "print('dataset columns:', dataset['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18162b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  -1.6385198831558228\n",
      "std:  3.336308717727661\n"
     ]
    }
   ],
   "source": [
    "# calculate values for normalization\n",
    "feature_extractor.do_normalize = False\n",
    "\n",
    "# Initialize running statistics\n",
    "n = 0\n",
    "mean = 0.0\n",
    "M2 = 0.0  # For running variance calculation\n",
    "\n",
    "def preprocess_audio(batch):\n",
    "    wavs = [audio[\"array\"] for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\", return_attention_mask=True, max_length=507)\n",
    "    return {\n",
    "        model_input_name: inputs.get(model_input_name),\n",
    "        \"labels\": torch.tensor(batch[\"label\"])\n",
    "    }\n",
    "\n",
    "dataset = dataset.rename_column(\"audio\", \"input_values\")\n",
    "dataset[\"train\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "\n",
    "# Process in batches to save memory\n",
    "for batch in dataset[\"train\"]:\n",
    "    \n",
    "    audio_input = batch[model_input_name]\n",
    "    batch_size = audio_input.shape[0]\n",
    "\n",
    "    # Calculate batch statistics\n",
    "    batch_mean = torch.mean(audio_input)\n",
    "    batch_variance = torch.var(audio_input, unbiased=False)  # Use N instead of N-1 for population variance\n",
    "    \n",
    "    # Update running statistics using Welford's online algorithm\n",
    "    delta = batch_mean - mean\n",
    "    mean += delta * batch_size / (n + batch_size)\n",
    "    M2 += batch_variance * batch_size + delta ** 2 * n * batch_size / (n + batch_size)\n",
    "    n += batch_size\n",
    "\n",
    "# Calculate final statistics\n",
    "feature_extractor.mean = mean.item()\n",
    "feature_extractor.std = torch.sqrt(M2 / n).item()  # Population standard deviation\n",
    "feature_extractor.do_normalize = True\n",
    "\n",
    "print('mean: ', feature_extractor.mean)\n",
    "print('std: ', feature_extractor.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af852b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianSNR, GainTransition, Gain, ClippingDistortion, TimeStretch, PitchShift\n",
    "\n",
    "audio_augmentations = Compose([\n",
    "    AddGaussianSNR(min_snr_db=10, max_snr_db=20),\n",
    "    Gain(min_gain_db=-6, max_gain_db=6),\n",
    "    GainTransition(min_gain_db=-6, max_gain_db=6, min_duration=0.01, max_duration=0.3, duration_unit=\"fraction\"),\n",
    "    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=30, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4),\n",
    "], p=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd7f4b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_values', 'label'],\n",
      "        num_rows: 13956\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_values', 'label'],\n",
      "        num_rows: 2826\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_values', 'label'],\n",
      "        num_rows: 1088\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def preprocess_audio_with_transforms(batch):\n",
    "    # we apply augmentations on each waveform\n",
    "    wavs = [audio_augmentations(audio[\"array\"], sample_rate=SAMPLING_RATE) for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\", return_attention_mask=True, max_length=507)\n",
    "    return {\n",
    "        model_input_name: inputs.get(model_input_name),\n",
    "        \"labels\": torch.tensor(batch[\"label\"])\n",
    "    }\n",
    "\n",
    "print(dataset)\n",
    "# Cast the audio column to the appropriate feature type and rename it\n",
    "dataset = dataset.cast_column(\"input_values\", Audio(sampling_rate=feature_extractor.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88736686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with augmentations on the training set\n",
    "dataset[\"train\"].set_transform(preprocess_audio_with_transforms, output_all_columns=False)\n",
    "# w/o augmentations on the test set\n",
    "dataset[\"test\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "dataset[\"validation\"].set_transform(preprocess_audio, output_all_columns=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dd71c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_values': {'path': 'C:\\\\Users\\\\Crumbz\\\\Desktop\\\\4TH-YEAR-1ST-SEM\\\\THESIS\\\\thesis-testing\\\\for-2seconds\\\\training\\\\fake\\\\file10005.mp3.wav_16k.wav_norm.wav_mono.wav_silence.wav_2sec.wav', 'array': array([ 0.10552979,  0.11013794,  0.00952148, ..., -0.20837402,\n",
      "       -0.25552368, -0.24255371]), 'sampling_rate': 16000}, 'label': 0}\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "temp_ds = dataset['train'].with_format(None)  # This removes any applied transforms\n",
    "temp_ds_test = dataset['test'].with_format(None)  # This removes any applied transforms\n",
    "\n",
    "print(temp_ds[0])\n",
    "unique_labels = sorted(set(temp_ds[\"label\"] + temp_ds_test[\"label\"]))\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "feb51aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- audio_spectrogram_transformer.embeddings.position_embeddings: found shape torch.Size([1, 1214, 768]) in the checkpoint and torch.Size([1, 252, 768]) in the model instantiated\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ASTConfig, ASTForAudioClassification\n",
    "\n",
    "# Load configuration from the pretrained model\n",
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "\n",
    "# Update configuration with the number of labels in our dataset\n",
    "config.num_mel_bins = 64  # Make sure this matches your feature extractor\n",
    "config.max_length = 507   # Or whatever your sequence length is\n",
    "config.num_labels = num_labels\n",
    "config.label2id = {\"fake\": 0, \"real\": 1}\n",
    "config.id2label = {0: \"fake\", 1: \"real\"}\n",
    "\n",
    "# Initialize the model with the updated configuration\n",
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    pretrained_model,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59a04121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Configure training run with TrainingArguments class\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./runs/ast_classifier\",\n",
    "    logging_dir=\"./logs/ast_classifier\",\n",
    "    report_to=\"tensorboard\",\n",
    "    learning_rate=5e-5,\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,  # Add this line\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    remove_unused_columns=False,  # Add this line\n",
    "    fp16=True,  # Enable mixed precision training if your GPU supports it\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d070a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "AVERAGE = \"macro\" if config.num_labels > 2 else \"binary\"\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    metrics = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "    metrics.update(precision.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a8de353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Crumbz\\AppData\\Local\\Temp\\ipykernel_1912\\2139010736.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, DataCollatorWithPadding\n",
    "\n",
    "\n",
    "# Initialize the data collator\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=feature_extractor,  # Your feature extractor acts as the tokenizer\n",
    "    padding=True,\n",
    "    max_length=config.max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "\n",
    "# Setup the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,  # Use the metrics function from above\n",
    "    tokenizer=feature_extractor,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aac110b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Crumbz\\Desktop\\4TH-YEAR-1ST-SEM\\THESIS\\thesis-testing\\.venv\\Lib\\site-packages\\audiomentations\\core\\transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input shape: torch.Size([507, 64])\n",
      "Forward pass successful!\n"
     ]
    }
   ],
   "source": [
    "# Add this before training to verify shapes\n",
    "sample = next(iter(dataset[\"train\"]))\n",
    "print(f\"Sample input shape: {sample['input_values'].shape}\")\n",
    "\n",
    "# Forward pass to check for errors\n",
    "with torch.no_grad():\n",
    "    output = model(sample[\"input_values\"].unsqueeze(0))\n",
    "print(\"Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "300ab2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config: ASTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ASTForAudioClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"frequency_stride\": 10,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"fake\",\n",
      "    \"1\": \"real\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"fake\": 0,\n",
      "    \"real\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_length\": 507,\n",
      "  \"model_type\": \"audio-spectrogram-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_mel_bins\": 64,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"time_stride\": 10,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.1\"\n",
      "}\n",
      "\n",
      "Sample input shape: torch.Size([507, 64])\n",
      "Model's expected input shape: 507\n",
      "Feature extractor config: ASTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"ASTFeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"max_length\": 507,\n",
      "  \"mean\": -1.6385198831558228,\n",
      "  \"num_mel_bins\": 64,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000,\n",
      "  \"std\": 3.336308717727661\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model config: {model.config}\")\n",
    "print(f\"Sample input shape: {sample['input_values'].shape}\")\n",
    "print(f\"Model's expected input shape: {model.config.max_length}\")\n",
    "print(f\"Feature extractor config: {feature_extractor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b32f0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3490' max='3490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3490/3490 : < :, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3490, training_loss=0.0, metrics={'train_runtime': 1.9453, 'train_samples_per_second': 14348.103, 'train_steps_per_second': 1794.027, 'total_flos': 4.64354803663276e+17, 'train_loss': 0.0, 'epoch': 2.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True) # Set to False if you want to train from scratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
