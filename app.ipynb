{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27471c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + \"D:\\\\ProgramFiles\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.44.35207\\\\bin\\\\Hostx64\\\\x64\"\n",
    "os.environ[\"CUDA_HOME\"] = \"D:\\\\ProgramFiles\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.9\"\n",
    "from datasets import load_dataset\n",
    "from transformers import ASTFeatureExtractor\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.profiler\n",
    "import numpy as np\n",
    "from datasets import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059084f1",
   "metadata": {},
   "source": [
    "Make sure to install Pytorch and CUDA Toolkit to make this work\n",
    "\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3821aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.0\n",
      "System: Windows 11\n",
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "CUDA device count: 1\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"System: {platform.system()} {platform.release()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a34e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import is_speech_available\n",
    "from transformers.audio_utils import mel_filter_bank, spectrogram, window_function\n",
    "\n",
    "if is_speech_available():\n",
    "    import torchaudio.compliance.kaldi as ta_kaldi\n",
    "\n",
    "# based on the following literature that uses the Hamming window:\n",
    "    # https://arxiv.org/pdf/2505.15136\n",
    "    # https://arxiv.org/pdf/2409.05924\n",
    "    # https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11007653\n",
    "class ASTFeatureExtractorHamming(ASTFeatureExtractor):\n",
    "    \"\"\"\n",
    "    Custom AST Feature Extractor that uses Hamming window instead of Hann/Hanning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Override the window for numpy-based processing (when torchaudio is not available)\n",
    "        if not is_speech_available():\n",
    "            # Recalculate mel filters and window with hamming\n",
    "            mel_filters = mel_filter_bank(\n",
    "                num_frequency_bins=257,\n",
    "                num_mel_filters=self.num_mel_bins,\n",
    "                min_frequency=20,\n",
    "                max_frequency=self.sampling_rate // 2,\n",
    "                sampling_rate=self.sampling_rate,\n",
    "                norm=None,\n",
    "                mel_scale=\"kaldi\",\n",
    "                triangularize_in_mel_space=True,\n",
    "            )\n",
    "            self.mel_filters = mel_filters\n",
    "            # Use hamming window instead of hann\n",
    "            self.window = window_function(400, \"hamming\", periodic=False)\n",
    "    \n",
    "    def _extract_fbank_features(self, waveform: np.ndarray, max_length: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Override to use hamming window type in torchaudio.compliance.kaldi.fbank\n",
    "        \"\"\"\n",
    "        if is_speech_available():\n",
    "            waveform = torch.from_numpy(waveform).unsqueeze(0)\n",
    "            fbank = ta_kaldi.fbank(\n",
    "                waveform,\n",
    "                sample_frequency=self.sampling_rate,\n",
    "                window_type=\"hamming\",  # Changed from \"hanning\" to \"hamming\"\n",
    "                num_mel_bins=self.num_mel_bins,\n",
    "            )\n",
    "        else:\n",
    "            # Use numpy implementation with hamming window\n",
    "            waveform = np.squeeze(waveform)\n",
    "            fbank = spectrogram(\n",
    "                waveform,\n",
    "                self.window,  # This is now hamming window from __init__\n",
    "                frame_length=400, # this follows the 25 ms frame length used in the paper (16000mhz * 0.025 = 400)\n",
    "                hop_length=160, # this follows the hop length used in the paper (16000mhz * 0.01 = 160)\n",
    "                fft_length=512,\n",
    "                power=2.0,\n",
    "                center=False,\n",
    "                preemphasis=0.97,\n",
    "                mel_filters=self.mel_filters,\n",
    "                log_mel=\"log\",\n",
    "                mel_floor=1.192092955078125e-07,\n",
    "                remove_dc_offset=True,\n",
    "            ).T\n",
    "            fbank = torch.from_numpy(fbank)\n",
    "\n",
    "        n_frames = fbank.shape[0]\n",
    "        difference = max_length - n_frames\n",
    "\n",
    "        # pad or truncate, depending on difference\n",
    "        if difference > 0:\n",
    "            pad_module = torch.nn.ZeroPad2d((0, 0, 0, difference))\n",
    "            fbank = pad_module(fbank)\n",
    "        elif difference < 0:\n",
    "            fbank = fbank[0:max_length, :]\n",
    "\n",
    "        fbank = fbank.numpy()\n",
    "        return fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6bce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Dropout\n",
    "from transformers.models.audio_spectrogram_transformer.modeling_audio_spectrogram_transformer import ASTEmbeddings\n",
    "\n",
    "class ASTEmbeddingsWithPatchout(ASTEmbeddings):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.patchout_prob = config.patchout_prob\n",
    "        self.patchout_strategy = config.patchout_strategy\n",
    "        self.patchout_dropout = Dropout(self.patchout_prob)\n",
    "\n",
    "    def forward(self, input_values: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = input_values.shape[0]\n",
    "        embeddings = self.patch_embeddings(input_values)  # Shape: (batch_size, num_patches, hidden_size)\n",
    "\n",
    "        # Apply Patchout during training\n",
    "        if self.training:\n",
    "            if self.patchout_strategy == \"unstructured\":\n",
    "                embeddings = self.patchout_dropout(embeddings)\n",
    "            elif self.patchout_strategy == \"structured\":\n",
    "                embeddings = self.structured_patchout(embeddings)\n",
    "\n",
    "        # Add CLS and distillation tokens + positional embeddings\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        distillation_tokens = self.distillation_token.expand(batch_size, -1, -1)\n",
    "        embeddings = torch.cat((cls_tokens, distillation_tokens, embeddings), dim=1)\n",
    "        embeddings = embeddings + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def structured_patchout(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply structured Patchout by dropping entire rows/columns of patches.\n",
    "        \"\"\"\n",
    "        batch_size, num_patches, hidden_size = embeddings.shape\n",
    "        freq_out_dim, time_out_dim = self.get_shape(self.config)\n",
    "        \n",
    "        # Reshape to 2D grid: (batch_size, freq_out_dim, time_out_dim, hidden_size)\n",
    "        embeddings = embeddings.view(batch_size, freq_out_dim, time_out_dim, hidden_size)\n",
    "\n",
    "        # Randomly drop frequency columns or time rows\n",
    "        if self.patchout_prob > 0:\n",
    "            # Drop frequency columns\n",
    "            freq_mask = torch.rand(freq_out_dim, device=embeddings.device) > self.patchout_prob\n",
    "            embeddings = embeddings * freq_mask.view(1, freq_out_dim, 1, 1)\n",
    "\n",
    "            # Drop time rows\n",
    "            time_mask = torch.rand(time_out_dim, device=embeddings.device) > self.patchout_prob\n",
    "            embeddings = embeddings * time_mask.view(1, 1, time_out_dim, 1)\n",
    "\n",
    "        # Flatten back to (batch_size, num_patches, hidden_size)\n",
    "        embeddings = embeddings.view(batch_size, num_patches, hidden_size)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a973f6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfb717ca3634c179d766935b237c39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/253914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797cecddc3844929b698821086ee06f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31738 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b95a16960a46f89919014bb08ea4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31742 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "NUM_PROC = (os.cpu_count() - 1) * 4\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=\"./dataset\", num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3b8ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset))\n",
    "print(type(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12cae748",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "\n",
    "NUM_MEL_BINS = 128 # based on https://arxiv.org/pdf/2409.05924\n",
    "MAX_SEQUENCE_LENGTH = 507\n",
    "\n",
    "global feature_extractor\n",
    "\n",
    "feature_extractor = ASTFeatureExtractorHamming.from_pretrained(\n",
    "        pretrained_model, \n",
    "        num_mel_bins=NUM_MEL_BINS, \n",
    "        max_length=MAX_SEQUENCE_LENGTH\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ac471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom AST Feature Extractor with Hamming window created successfully!\n",
      "Sampling rate: 16000\n",
      "Mel bins: 128\n"
     ]
    }
   ],
   "source": [
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "SAMPLING_RATE = feature_extractor.sampling_rate\n",
    "print(\"Custom AST Feature Extractor with Hamming window created successfully!\")\n",
    "print(f\"Sampling rate: {feature_extractor.sampling_rate}\")\n",
    "print(f\"Mel bins: {feature_extractor.num_mel_bins}\")\n",
    "\n",
    "num_labels = len(np.unique(dataset[\"train\"][\"label\"]))\n",
    "# num_labels = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f317f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 253914\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 31738\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 31742\n",
      "    })\n",
      "})\n",
      "dataset train features: {'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "dataset test features: {'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "dataset validation features: {'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "model_input_name: input_values\n",
      "SAMPLING_RATE: 16000\n",
      "dataset[\"train\"][0]: {'audio': {'path': 'D:\\\\ProgramFiles\\\\dev\\\\repos\\\\thesis-testing\\\\dataset\\\\train\\\\fake\\\\0.wav', 'array': array([ 8.55924794e-04,  5.84704467e-05,  7.75483320e-04, ...,\n",
      "       -1.02691360e-04, -2.82649242e-04,  0.00000000e+00], shape=(29105,)), 'sampling_rate': 16000}, 'label': 0}\n",
      "num_labels: 2\n",
      "dataset columns: ['audio', 'label']\n"
     ]
    }
   ],
   "source": [
    "print('dataset:', dataset)\n",
    "print('dataset train features:', dataset['train'].features)\n",
    "print('dataset test features:', dataset['test'].features)\n",
    "print('dataset validation features:', dataset['validation'].features)\n",
    "print('model_input_name:', model_input_name)\n",
    "print('SAMPLING_RATE:', SAMPLING_RATE)\n",
    "print('dataset[\"train\"][0]:', dataset['train'][0])\n",
    "\n",
    "# For when using IterableDataset\n",
    "# for example in dataset['train']:\n",
    "#     print('dataset[\"train\"][0]:', example)\n",
    "#     break\n",
    "\n",
    "print('num_labels:', num_labels)\n",
    "print('dataset columns:', dataset['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18162b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_values', 'label'],\n",
      "        num_rows: 253914\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_values', 'label'],\n",
      "        num_rows: 31738\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_values', 'label'],\n",
      "        num_rows: 31742\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# calculate values for normalization\n",
    "feature_extractor.do_normalize = False\n",
    "\n",
    "def preprocess_audio(batch):\n",
    "    wavs = [audio[\"array\"] for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\", return_attention_mask=True, max_length=507)\n",
    "    \n",
    "    # print(\"wavs:\", wavs)\n",
    "    # print(\"inputs:\", inputs)\n",
    "    \n",
    "    return {\n",
    "        model_input_name: inputs.get(model_input_name),\n",
    "        \"labels\": torch.tensor(batch[\"label\"])\n",
    "    }\n",
    "\n",
    "dataset = dataset.rename_column(\"audio\", \"input_values\")\n",
    "# this can't be if we're going to use an iterable dataset:\n",
    "dataset[\"train\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "\n",
    "# dataset[\"train\"] = dataset[\"train\"].map(\n",
    "#     preprocess_audio,\n",
    "#     batched=True,\n",
    "# )\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5604b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "def compute_dataset_statistics_optimized(dataset, model_input_name, batch_size=None, device=None):\n",
    "    \"\"\"\n",
    "    Optimized computation of dataset mean and standard deviation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset to compute statistics for\n",
    "        model_input_name: The key name for model input in batch\n",
    "        batch_size: Batch size (auto-optimized if None)\n",
    "        device: Device to use ('cuda', 'cpu', or None for auto-detection)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (mean, std)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. OPTIMIZATION: Auto-detect optimal batch size and device\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    if batch_size is None:\n",
    "        # Start with a larger batch size for statistics computation\n",
    "        if device == 'cuda':\n",
    "            # Try to use more GPU memory for faster processing\n",
    "            batch_size = min(512, len(dataset) // 10)  # Use 10% of dataset or 512, whichever is smaller\n",
    "        else:\n",
    "            batch_size = min(256, len(dataset) // 20)  # Conservative for CPU\n",
    "    \n",
    "    print(f\"Using device: {device}, batch_size: {batch_size}\")\n",
    "    \n",
    "    # 2. OPTIMIZATION: Configure DataLoader for maximum performance\n",
    "    dataloader_kwargs = {\n",
    "        'batch_size': batch_size,\n",
    "        'shuffle': False,  # No need to shuffle for statistics\n",
    "        'drop_last': False,  # Process all data\n",
    "        'pin_memory': device == 'cuda',  # Only pin memory if using GPU\n",
    "        'num_workers': 0,  # Start with 0 to avoid multiprocessing issues\n",
    "    }\n",
    "    \n",
    "    # Try to enable multiprocessing if it works\n",
    "    try:\n",
    "        test_loader = DataLoader(dataset, batch_size=2, num_workers=2)\n",
    "        next(iter(test_loader))  # Test if multiprocessing works\n",
    "        dataloader_kwargs['num_workers'] = min(4, os.cpu_count() - 1)  # Conservative worker count\n",
    "        print(f\"Multiprocessing enabled with {dataloader_kwargs['num_workers']} workers\")\n",
    "        del test_loader\n",
    "    except:\n",
    "        print(\"Multiprocessing failed, using single process\")\n",
    "    \n",
    "    dataloader = DataLoader(dataset, **dataloader_kwargs)\n",
    "    \n",
    "    # 3. OPTIMIZATION: Use appropriate tensor dtypes and device placement\n",
    "    if device == 'cuda':\n",
    "        sum_dtype = torch.float64  # Higher precision for accumulation\n",
    "        working_dtype = torch.float32  # Working precision\n",
    "    else:\n",
    "        sum_dtype = torch.float64\n",
    "        working_dtype = torch.float32\n",
    "    \n",
    "    # Initialize accumulators on the target device with appropriate dtype\n",
    "    total_sum = torch.tensor(0.0, dtype=sum_dtype, device=device)\n",
    "    total_squared_sum = torch.tensor(0.0, dtype=sum_dtype, device=device)\n",
    "    total_count = 0\n",
    "    \n",
    "    # 4. OPTIMIZATION: Process with memory-efficient operations\n",
    "    progress_bar = tqdm(dataloader, desc=\"Computing normalization stats\", unit=\"batch\")\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Get batch data\n",
    "            batch_data = batch[model_input_name]\n",
    "            \n",
    "            # 5. OPTIMIZATION: Efficient tensor conversion and device transfer\n",
    "            if not isinstance(batch_data, torch.Tensor):\n",
    "                batch_data = torch.tensor(batch_data, dtype=working_dtype)\n",
    "            else:\n",
    "                batch_data = batch_data.to(dtype=working_dtype)\n",
    "            \n",
    "            # Move to target device if needed\n",
    "            if batch_data.device != torch.device(device):\n",
    "                batch_data = batch_data.to(device, non_blocking=True)\n",
    "            \n",
    "            # 6. OPTIMIZATION: Efficient flattening and computation\n",
    "            # Use view instead of flatten when possible (more memory efficient)\n",
    "            flat_batch = batch_data.view(-1)\n",
    "            \n",
    "            # Compute statistics using vectorized operations\n",
    "            batch_sum = flat_batch.sum(dtype=sum_dtype)\n",
    "            batch_squared_sum = flat_batch.pow(2).sum(dtype=sum_dtype)\n",
    "            batch_count = flat_batch.numel()\n",
    "            \n",
    "            # Update accumulators\n",
    "            total_sum += batch_sum\n",
    "            total_squared_sum += batch_squared_sum\n",
    "            total_count += batch_count\n",
    "            \n",
    "            # 7. OPTIMIZATION: Memory cleanup for large datasets\n",
    "            if batch_idx % 100 == 0:  # Periodic cleanup\n",
    "                if device == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            \n",
    "            # Update progress bar with current estimates\n",
    "            if batch_idx % 10 == 0:\n",
    "                current_mean = (total_sum / total_count).item()\n",
    "                progress_bar.set_postfix({\n",
    "                    'current_mean': f'{current_mean:.4f}',\n",
    "                    'processed': f'{total_count:,}'\n",
    "                })\n",
    "    \n",
    "    # 8. OPTIMIZATION: Numerically stable standard deviation calculation\n",
    "    # Use the corrected formula to avoid numerical instability\n",
    "    mean = total_sum / total_count\n",
    "    variance = (total_squared_sum / total_count) - (mean * mean)\n",
    "    \n",
    "    # Clamp variance to avoid negative values due to floating point errors\n",
    "    variance = torch.clamp(variance, min=0.0)\n",
    "    std = torch.sqrt(variance)\n",
    "    \n",
    "    # Convert back to Python scalars\n",
    "    final_mean = mean.item()\n",
    "    final_std = std.item()\n",
    "    \n",
    "    # Final cleanup\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return final_mean, final_std\n",
    "\n",
    "def compute_dataset_statistics_chunked(dataset, model_input_name, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Alternative approach: Process dataset in chunks without DataLoader for maximum memory efficiency.\n",
    "    Use this if the DataLoader approach still has issues.\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset in chunks of {chunk_size}\")\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    sum_dtype = torch.float64\n",
    "    \n",
    "    total_sum = torch.tensor(0.0, dtype=sum_dtype, device=device)\n",
    "    total_squared_sum = torch.tensor(0.0, dtype=sum_dtype, device=device)\n",
    "    total_count = 0\n",
    "    \n",
    "    dataset_size = len(dataset)\n",
    "    num_chunks = (dataset_size + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for chunk_idx in tqdm(range(num_chunks), desc=\"Processing chunks\"):\n",
    "            start_idx = chunk_idx * chunk_size\n",
    "            end_idx = min(start_idx + chunk_size, dataset_size)\n",
    "            \n",
    "            # Process items in current chunk\n",
    "            chunk_tensors = []\n",
    "            for idx in range(start_idx, end_idx):\n",
    "                item = dataset[idx][model_input_name]\n",
    "                if not isinstance(item, torch.Tensor):\n",
    "                    item = torch.tensor(item, dtype=torch.float32)\n",
    "                chunk_tensors.append(item.flatten())\n",
    "            \n",
    "            # Concatenate chunk data\n",
    "            if chunk_tensors:\n",
    "                chunk_data = torch.cat(chunk_tensors).to(device)\n",
    "                \n",
    "                # Compute statistics\n",
    "                chunk_sum = chunk_data.sum(dtype=sum_dtype)\n",
    "                chunk_squared_sum = chunk_data.pow(2).sum(dtype=sum_dtype)\n",
    "                chunk_count = chunk_data.numel()\n",
    "                \n",
    "                total_sum += chunk_sum\n",
    "                total_squared_sum += chunk_squared_sum\n",
    "                total_count += chunk_count\n",
    "                \n",
    "                # Cleanup\n",
    "                del chunk_data, chunk_tensors\n",
    "                if device == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    mean = total_sum / total_count\n",
    "    variance = (total_squared_sum / total_count) - (mean * mean)\n",
    "    variance = torch.clamp(variance, min=0.0)\n",
    "    std = torch.sqrt(variance)\n",
    "    \n",
    "    return mean.item(), std.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed6897e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized - Mean: 0.31605425901883943, Std: 0.45787811188377187\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Optimized DataLoader approach (recommended)\n",
    "try:\n",
    "    # mean, std = compute_dataset_statistics_optimized(\n",
    "    #     dataset=dataset[\"train\"],\n",
    "    #     model_input_name=model_input_name,\n",
    "    #     batch_size=256,  # Start with this, increase if you have more memory\n",
    "    #     device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # )\n",
    "    # Testing purposes\n",
    "    mean = 0.31605425901883943\n",
    "    std = 0.45787811188377187\n",
    "    \n",
    "    print(f'Optimized - Mean: {mean}, Std: {std}')\n",
    "    \n",
    "    feature_extractor.mean = mean\n",
    "    feature_extractor.std = std\n",
    "except Exception as e:\n",
    "    print(f\"DataLoader approach failed: {e}\")\n",
    "    print(\"Trying chunked approach...\")\n",
    "    \n",
    "    # Method 2: Fallback chunked approach\n",
    "    mean, std = compute_dataset_statistics_chunked(\n",
    "        dataset=dataset[\"train\"],\n",
    "        model_input_name=model_input_name,\n",
    "        chunk_size=500\n",
    "    )\n",
    "    print(f'Chunked - Mean: {mean}, Std: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af852b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, BandPassFilter, Mp3Compression, RoomSimulator, Gain, ClippingDistortion\n",
    "import math  # Needed for azimuth/elevation parameters\n",
    "\n",
    "audio_augmentations = Compose([\n",
    "    # 1. Noise injection (both papers)\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    \n",
    "    # 2. Bandpass filtering (continuous-learning.pdf)\n",
    "    BandPassFilter(min_center_freq=300, max_center_freq=3000, \n",
    "                   min_bandwidth_fraction=0.1, max_bandwidth_fraction=0.3, p=0.5),\n",
    "    \n",
    "    # 3. MP3 compression (hybrid-audio.pdf)\n",
    "    Mp3Compression(min_bitrate=16, max_bitrate=32, p=0.5),\n",
    "    \n",
    "    # 4. Room simulation - CORRECTED (continuous-learning.pdf)\n",
    "    RoomSimulator(\n",
    "        min_size_x=5.0, max_size_x=15.0,\n",
    "        min_size_y=5.0, max_size_y=15.0,\n",
    "        min_size_z=2.4, max_size_z=4.0,\n",
    "        min_absorption_value=0.1, max_absorption_value=0.8,\n",
    "        min_mic_distance=1.0, max_mic_distance=5.0,  # Far-field simulation\n",
    "        min_mic_azimuth=-math.pi, max_mic_azimuth=math.pi,\n",
    "        min_mic_elevation=-math.pi/4, max_mic_elevation=math.pi/4,\n",
    "        calculation_mode=\"absorption\",\n",
    "        p=0.5\n",
    "    ),\n",
    "    \n",
    "    # 5. Gain variation (hybrid-audio.pdf)\n",
    "    Gain(min_gain_db=-6, max_gain_db=6, p=0.3),\n",
    "    \n",
    "    # 6. Clipping distortion (both papers)\n",
    "    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=10, p=0.3),\n",
    "], p=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd7f4b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset train features: {'input_values': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "dataset test features: {'input_values': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "dataset validation features: {'input_values': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_audio_with_transforms(batch):\n",
    "    # we apply augmentations on each waveform\n",
    "    wavs = [audio_augmentations(audio[\"array\"], sample_rate=SAMPLING_RATE) for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\", return_attention_mask=True, max_length=507)\n",
    "    return {\n",
    "        model_input_name: inputs.get(model_input_name),\n",
    "        \"labels\": torch.tensor(batch[\"label\"])\n",
    "    }\n",
    "\n",
    "print('dataset train features:', dataset['train'].features)\n",
    "print('dataset test features:', dataset['test'].features)\n",
    "print('dataset validation features:', dataset['validation'].features)\n",
    "# Cast the audio column to the appropriate feature type and rename it\n",
    "dataset = dataset.cast_column(\"input_values\", Audio(sampling_rate=feature_extractor.sampling_rate))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88736686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with augmentations on the training set\n",
    "dataset[\"train\"].set_transform(preprocess_audio_with_transforms, output_all_columns=False)\n",
    "# w/o augmentations on the test set\n",
    "dataset[\"test\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "dataset[\"validation\"].set_transform(preprocess_audio, output_all_columns=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dd71c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_values': {'path': 'D:\\\\ProgramFiles\\\\dev\\\\repos\\\\thesis-testing\\\\dataset\\\\train\\\\fake\\\\0.wav', 'array': array([ 8.55924794e-04,  5.84704467e-05,  7.75483320e-04, ...,\n",
      "       -1.02691360e-04, -2.82649242e-04,  0.00000000e+00], shape=(29105,)), 'sampling_rate': 16000}, 'label': 0}\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "temp_ds = dataset['train'].with_format(None)  # This removes any applied transforms\n",
    "temp_ds_test = dataset['test'].with_format(None)  # This removes any applied transforms\n",
    "\n",
    "print(temp_ds[0])\n",
    "unique_labels = sorted(set(temp_ds[\"label\"] + temp_ds_test[\"label\"]))\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feb51aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- audio_spectrogram_transformer.embeddings.position_embeddings: found shape torch.Size([1, 1214, 768]) in the checkpoint and torch.Size([1, 250, 768]) in the model instantiated\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASTEmbeddings(\n",
      "  (patch_embeddings): ASTPatchEmbeddings(\n",
      "    (projection): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import ASTConfig, ASTForAudioClassification\n",
    "\n",
    "# Load configuration from the pretrained model\n",
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "\n",
    "# Update configuration with the number of labels in our dataset\n",
    "config.num_mel_bins = NUM_MEL_BINS  # Make sure this matches your feature extractor\n",
    "config.max_length = MAX_SEQUENCE_LENGTH   # Or whatever your sequence length is\n",
    "config.num_labels = num_labels\n",
    "config.label2id = {\"fake\": 0, \"real\": 1}\n",
    "config.id2label = {0: \"fake\", 1: \"real\"}\n",
    "\n",
    "# setting dropout to prevent overfitting. based on https://www.mdpi.com/2073-431X/13/10/256\n",
    "config.hidden_dropout_prob = 0.10\n",
    "\n",
    "# having a patch size of 16, and time and frequency stride of 16, allows us to have NO \n",
    "# overlaps. this prevents difficulties if the file has real and fake audio \n",
    "# (based on https://arxiv.org/pdf/2409.05924)\n",
    "config.patch_size = 16\n",
    "config.frequency_stride = 16\n",
    "config.time_stride = 16\n",
    "\n",
    "config.patchout_prob = 0.5  # Probability of dropping a patch\n",
    "config.patchout_strategy = \"structured\"  # \"unstructured\" or \"structured\"\n",
    "\n",
    "# Initialize the model with the updated configuration\n",
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    pretrained_model,\n",
    "    config=config,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "print(model.audio_spectrogram_transformer.embeddings)  # See what this submodule is\n",
    "\n",
    "model.audio_spectrogram_transformer.embeddings.embeddings = ASTEmbeddingsWithPatchout(config)\n",
    "\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29ef2a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "structured\n",
      "Output shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(config.patchout_prob)\n",
    "print(config.patchout_strategy)\n",
    "sample_input = torch.randn(1, 128, 507)  # Batch of spectrograms (batch_size, num_mel_bins, max_length)\n",
    "output = model(sample_input)\n",
    "print(\"Output shape:\", output.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59a04121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight decay:  3.9690415546402594e-05\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "# weight decay formula based on https://arxiv.org/abs/1711.05101:\n",
    "    # weight decay = ynorm(sqr(batch_size/(dataset_size * num_train_epochs)))\n",
    "    # where ynorm is between 0.025 and 0.05 https://towardsdatascience.com/weight-decay-and-its-peculiar-effects-66e0aee3e7b8/\n",
    "\n",
    "DATASET_SIZE = 317_394\n",
    "YNORM = 0.025\n",
    "BATCH_SIZE = 16\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "WEIGHT_DECAY = YNORM * np.sqrt(BATCH_SIZE / (DATASET_SIZE * NUM_TRAIN_EPOCHS))\n",
    "\n",
    "print(\"Weight decay: \", WEIGHT_DECAY)\n",
    "\n",
    "# Configure training run with TrainingArguments class\n",
    "training_args = TrainingArguments(\n",
    "    optim=\"adamw_torch_fused\", # based on https://huggingface.co/docs/transformers/v4.35.2/en/perf_train_gpu_one#optimizer-choice\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    output_dir=\"./runs/ast_classifier\",\n",
    "    logging_dir=\"./logs/ast_classifier\",\n",
    "    report_to=\"tensorboard\",\n",
    "    learning_rate=2e-5, # based on https://arxiv.org/pdf/2505.15136\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # Start with 8, can increase if memory allows\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    dataloader_num_workers=0, # based on https://huggingface.co/docs/transformers/v4.35.2/en/perf_train_gpu_one#data-preloading\n",
    "    dataloader_pin_memory=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\", # based on https://www.mdpi.com/2073-431X/13/10/256 and https://arxiv.org/pdf/2505.15136\n",
    "    logging_steps=20,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    "    lr_scheduler_type=\"cosine\", # based on https://arxiv.org/pdf/2505.15136\n",
    "    weight_decay=WEIGHT_DECAY, # based on https://arxiv.org/abs/1711.05101 and https://towardsdatascience.com/weight-decay-and-its-peculiar-effects-66e0aee3e7b8/\n",
    "    torch_compile=True,\n",
    "    torch_compile_backend=\"aot_eager\",\n",
    "    torch_empty_cache_steps=4,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d070a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "AVERAGE = \"macro\" if config.num_labels > 2 else \"binary\"\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    metrics = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "    metrics.update(precision.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a8de353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crumbz\\AppData\\Local\\Temp\\ipykernel_12940\\2623151492.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- audio_spectrogram_transformer.embeddings.position_embeddings: found shape torch.Size([1, 1214, 768]) in the checkpoint and torch.Size([1, 250, 768]) in the model instantiated\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, DataCollatorWithPadding\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize the data collator\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=feature_extractor,  # Your feature extractor acts as the tokenizer\n",
    "    padding=True,\n",
    "    max_length=config.max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "def init_model():\n",
    "    return ASTForAudioClassification.from_pretrained(\n",
    "    pretrained_model,\n",
    "    config=config,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=init_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    "    data_collator=data_collator,\n",
    "    # if no improvements happened to validation loss within 5 epochs, stop training.\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.00)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aac110b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramFiles\\dev\\repos\\thesis-testing\\.venv\\Lib\\site-packages\\audiomentations\\core\\transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input shape: torch.Size([507, 128])\n",
      "Forward pass successful!\n"
     ]
    }
   ],
   "source": [
    "# Add this before training to verify shapes\n",
    "sample = next(iter(dataset[\"train\"]))\n",
    "print(f\"Sample input shape: {sample['input_values'].shape}\")\n",
    "\n",
    "# Forward pass to check for errors\n",
    "with torch.no_grad():\n",
    "    output = model(sample[\"input_values\"].unsqueeze(0))\n",
    "print(\"Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "300ab2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config: ASTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ASTForAudioClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"frequency_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"fake\",\n",
      "    \"1\": \"real\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"fake\": 0,\n",
      "    \"real\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_length\": 507,\n",
      "  \"model_type\": \"audio-spectrogram-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_mel_bins\": 128,\n",
      "  \"patch_size\": 16,\n",
      "  \"patchout_prob\": 0.5,\n",
      "  \"patchout_strategy\": \"structured\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"time_stride\": 16,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.54.1\"\n",
      "}\n",
      "\n",
      "Sample input shape: torch.Size([507, 128])\n",
      "Model's expected input shape: 507\n",
      "Feature extractor config: ASTFeatureExtractorHamming {\n",
      "  \"do_normalize\": false,\n",
      "  \"feature_extractor_type\": \"ASTFeatureExtractorHamming\",\n",
      "  \"feature_size\": 1,\n",
      "  \"max_length\": 507,\n",
      "  \"mean\": 0.31605425901883943,\n",
      "  \"num_mel_bins\": 128,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000,\n",
      "  \"std\": 0.45787811188377187\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model config: {model.config}\")\n",
    "print(f\"Sample input shape: {sample['input_values'].shape}\")\n",
    "print(f\"Model's expected input shape: {model.config.max_length}\")\n",
    "print(f\"Feature extractor config: {feature_extractor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0103e3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce RTX 2060\n",
      "GPU Memory Total: 6.44 GB\n",
      "GPU Memory Allocated: 0.00 GB\n",
      "GPU Memory Cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "print(f\"Using device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "     print(f\"GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76028015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading scaler: [Errno 2] No such file or directory: 'runs\\\\ast_classifier\\\\checkpoint-63480\\\\scaler.pt'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.serialization import safe_globals\n",
    "import numpy as np\n",
    "from accelerate.utils import get_grad_scaler\n",
    "\n",
    "# Get the correct scaler type based on your hardware\n",
    "scaler = get_grad_scaler()\n",
    "\n",
    "CHECKPOINT_NUM = 63480\n",
    "# Load the scaler state dict\n",
    "with safe_globals([np.core.multiarray.scalar, np.dtype, np.dtypes.Float64DType]):\n",
    "    try:\n",
    "        scaler_state = torch.load(fr\"runs\\ast_classifier\\checkpoint-{CHECKPOINT_NUM}\\scaler.pt\", weights_only=True)\n",
    "        print(\"Scaler state type:\", type(scaler_state))\n",
    "        print(\"Scaler state contents:\", scaler_state)\n",
    "        print(\"Trainer accelerator scaler: \", trainer.accelerator.scaler)\n",
    "        if not trainer.accelerator.scaler:\n",
    "            print(\"Trainer accelerator scaler is None. Setting up the scaler\")\n",
    "            scaler.load_state_dict(scaler_state)\n",
    "            trainer.accelerator.scaler = scaler\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error loading scaler: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6c1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-03 12:45:52,791] A new study created in memory with name: no-name-fa005678-f308-4232-9234-a4621dc7a5cc\n",
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- audio_spectrogram_transformer.embeddings.position_embeddings: found shape torch.Size([1, 1214, 768]) in the checkpoint and torch.Size([1, 250, 768]) in the model instantiated\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cudagraphs', 'inductor', 'onnxrt', 'openxla', 'tvm']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='634800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   102/634800 01:32 < 163:14:34, 1.08 it/s, Epoch 0.00/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramFiles\\dev\\repos\\thesis-testing\\.venv\\Lib\\site-packages\\audiomentations\\core\\transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import os\n",
    "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"  # For internal stack traces\n",
    "os.environ[\"TORCH_LOGS\"] = \"+dynamo\"     # Additional context\n",
    "\n",
    "print(torch._dynamo.list_backends())\n",
    "\n",
    "def compute_objective(metrics: dict[str, float]) -> list[float]:\n",
    "    return metrics[\"eval_loss\"], metrics[\"eval_accuracy\"]\n",
    "\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16]),\n",
    "        \"per_device_eval_batch_size\": trial.suggest_categorical(\"per_device_eval_batch_size\", [16, 32]),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [5, 10, 15, 20]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 5e-5),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "        \"warmup_steps\": trial.suggest_categorical(\"warmup_steps\", [0, 500, 1000]),\n",
    "        \"lr_scheduler_type\": trial.suggest_categorical(\"lr_scheduler_type\", [\"linear\", \"cosine\", \"cosine_with_restarts\"]), \n",
    "    }\n",
    "\n",
    "best_trials = trainer.hyperparameter_search(\n",
    "    direction=[\"minimize\", \"maximize\"],\n",
    "    backend=\"optuna\",\n",
    "    hp_space=hp_space,\n",
    "    n_trials=20,\n",
    "    compute_objective=compute_objective,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678d5074",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=~/ray_results/audio_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b32f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.serialization.safe_globals([np.core.multiarray.scalar, np.dtype, np.dtypes.Float64DType]):\n",
    "    trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b94859",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "     print(f\"GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODEL EVALUATION AND PREDICTION\n",
    "\n",
    "import os\n",
    "import json\n",
    "from transformers import ASTForAudioClassification\n",
    "\n",
    "# Define the checkpoint path\n",
    "checkpoint_path = \"runs/ast_classifier/checkpoint-158700\"\n",
    "\n",
    "# Load the model and feature extractor\n",
    "model = ASTForAudioClassification.from_pretrained(checkpoint_path)\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Look for training history\n",
    "trainer_state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "if os.path.exists(trainer_state_path):\n",
    "    with open(trainer_state_path, \"r\") as f:\n",
    "        trainer_state = json.load(f)\n",
    "    print(\"\\nTraining metrics from trainer_state.json:\")\n",
    "    print(json.dumps(trainer_state, indent=2))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "import librosa\n",
    "def predict_audio(file_path, model, feature_extractor, device=\"cuda\"):\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(file_path, sr=feature_extractor.sampling_rate)\n",
    "\n",
    "    # Preprocess the audio\n",
    "    inputs = feature_extractor(\n",
    "        audio,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    print(f\"Raw logits: {logits}\")\n",
    "    print(f\"Raw probabilities: {probabilities}\")\n",
    "\n",
    "    # Get predicted class (0 for fake, 1 for real)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "    print(f\"P(fake): {probabilities[0][0].item():.4f}\")\n",
    "    print(f\"P(real): {probabilities[0][1].item():.4f}\")\n",
    "\n",
    "    # Map class index to label\n",
    "    label = \"fake\" if predicted_class == 0 else \"real\"\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"confidence\": confidence,\n",
    "        \"probabilities\": {\n",
    "            \"fake\": probabilities[0][0].item(),\n",
    "            \"real\": probabilities[0][1].item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "audio_file_path = \"\" # Adjust the path as needed\n",
    "result = predict_audio(audio_file_path, model, feature_extractor, device)\n",
    "\n",
    "print(f\"Prediction: {result['label']}\")\n",
    "print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "print(f\"Probabilities - Fake: {result['probabilities']['fake']:.4f}, Real: {result['probabilities']['real']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835cc84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### AUDIO CUTTING SCRIPT\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def cut_audio(input_path, output_dir=\"./cut\", clip_duration=5):\n",
    "     # Ensure output directory exists\n",
    "     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "     # Get file name and extension\n",
    "     base_name = os.path.basename(input_path)\n",
    "     name, ext = os.path.splitext(base_name)\n",
    "\n",
    "     # Load audio\n",
    "     audio, sr = librosa.load(input_path, sr=None)\n",
    "     total_duration = librosa.get_duration(y=audio, sr=sr)\n",
    "     clip_samples = int(clip_duration * sr)\n",
    "     num_clips = int(total_duration // clip_duration) + (1 if total_duration % clip_duration > 0 else 0)\n",
    "\n",
    "     for i in range(num_clips):\n",
    "          start_sample = i * clip_samples\n",
    "          end_sample = min((i + 1) * clip_samples, len(audio))\n",
    "          clip_audio = audio[start_sample:end_sample]\n",
    "          out_path = os.path.join(output_dir, f\"{name}({i+1}){ext}\")\n",
    "          sf.write(out_path, clip_audio, sr)\n",
    "          print(f\"Saved: {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     if len(sys.argv) < 2:\n",
    "          print(\"Usage: python cut_audio.py <path_to_audio_file>\")\n",
    "          sys.exit(1)\n",
    "     audio_path = \"\"  # Adjust the path as needed\n",
    "     if not os.path.exists(audio_path):\n",
    "          print(f\"File not found: {audio_path}\")\n",
    "          sys.exit(1)\n",
    "     cut_audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SEGMENTED AUDIO PREDICTION SCRIPT\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Directory containing segmented audio clips\n",
    "segmented_dir = \"./cut\"\n",
    "\n",
    "# Collect all audio files and group by base name (without segment index)\n",
    "audio_groups = defaultdict(list)\n",
    "for fname in os.listdir(segmented_dir):\n",
    "     if fname.lower().endswith(('.wav', '.mp3', '.flac', '.ogg')):\n",
    "          # Extract base name (e.g., \"audio(1).wav\" -> \"audio\")\n",
    "          base = fname.split('(')[0]\n",
    "          audio_groups[base].append(os.path.join(segmented_dir, fname))\n",
    "\n",
    "# Function to predict for a single audio file\n",
    "def predict_single(file_path):\n",
    "     return predict_audio(file_path, model, feature_extractor, device)\n",
    "\n",
    "# Aggregate predictions for each group\n",
    "results = {}\n",
    "for base, files in audio_groups.items():\n",
    "     fake_count = 0\n",
    "     real_count = 0\n",
    "     for fpath in files:\n",
    "          pred = predict_single(fpath)\n",
    "          if pred[\"label\"] == \"fake\":\n",
    "               fake_count += 1\n",
    "          else:\n",
    "               real_count += 1\n",
    "     total = fake_count + real_count\n",
    "     results[base] = {\n",
    "          \"fake_ratio\": fake_count / total if total > 0 else 0,\n",
    "          \"real_ratio\": real_count / total if total > 0 else 0,\n",
    "          \"fake_count\": fake_count,\n",
    "          \"real_count\": real_count,\n",
    "          \"total\": total\n",
    "     }\n",
    "\n",
    "# Print summary\n",
    "for base, stats in results.items():\n",
    "     print(f\"{base}: Fake {stats['fake_count']}/{stats['total']} ({stats['fake_ratio']:.2f}), \"\n",
    "            f\"Real {stats['real_count']}/{stats['total']} ({stats['real_ratio']:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
