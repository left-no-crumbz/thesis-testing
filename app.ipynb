{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27471c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import ASTFeatureExtractor\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.profiler\n",
    "import numpy as np\n",
    "from datasets import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059084f1",
   "metadata": {},
   "source": [
    "Make sure to install Pytorch and CUDA Toolkit to make this work\n",
    "\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3821aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.0\n",
      "System: Windows 11\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "CUDA device count: 1\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"System: {platform.system()} {platform.release()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a34e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import is_speech_available\n",
    "from transformers.audio_utils import mel_filter_bank, spectrogram, window_function\n",
    "\n",
    "if is_speech_available():\n",
    "    import torchaudio.compliance.kaldi as ta_kaldi\n",
    "\n",
    "# based on the following literature that uses the Hamming window:\n",
    "    # https://arxiv.org/pdf/2505.15136\n",
    "    # https://arxiv.org/pdf/2409.05924\n",
    "    # https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11007653\n",
    "class ASTFeatureExtractorHamming(ASTFeatureExtractor):\n",
    "    \"\"\"\n",
    "    Custom AST Feature Extractor that uses Hamming window instead of Hann/Hanning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Override the window for numpy-based processing (when torchaudio is not available)\n",
    "        if not is_speech_available():\n",
    "            # Recalculate mel filters and window with hamming\n",
    "            mel_filters = mel_filter_bank(\n",
    "                num_frequency_bins=257,\n",
    "                num_mel_filters=self.num_mel_bins,\n",
    "                min_frequency=20,\n",
    "                max_frequency=self.sampling_rate // 2,\n",
    "                sampling_rate=self.sampling_rate,\n",
    "                norm=None,\n",
    "                mel_scale=\"kaldi\",\n",
    "                triangularize_in_mel_space=True,\n",
    "            )\n",
    "            self.mel_filters = mel_filters\n",
    "            # Use hamming window instead of hann\n",
    "            self.window = window_function(400, \"hamming\", periodic=False)\n",
    "    \n",
    "    def _extract_fbank_features(self, waveform: np.ndarray, max_length: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Override to use hamming window type in torchaudio.compliance.kaldi.fbank\n",
    "        \"\"\"\n",
    "        if is_speech_available():\n",
    "            waveform = torch.from_numpy(waveform).unsqueeze(0)\n",
    "            fbank = ta_kaldi.fbank(\n",
    "                waveform,\n",
    "                sample_frequency=self.sampling_rate,\n",
    "                window_type=\"hamming\",  # Changed from \"hanning\" to \"hamming\"\n",
    "                num_mel_bins=self.num_mel_bins,\n",
    "            )\n",
    "        else:\n",
    "            # Use numpy implementation with hamming window\n",
    "            waveform = np.squeeze(waveform)\n",
    "            fbank = spectrogram(\n",
    "                waveform,\n",
    "                self.window,  # This is now hamming window from __init__\n",
    "                frame_length=400, # this follows the 25 ms frame length used in the paper (16000mhz * 0.025 = 400)\n",
    "                hop_length=160, # this follows the hop length used in the paper (16000mhz * 0.01 = 160)\n",
    "                fft_length=512,\n",
    "                power=2.0,\n",
    "                center=False,\n",
    "                preemphasis=0.97,\n",
    "                mel_filters=self.mel_filters,\n",
    "                log_mel=\"log\",\n",
    "                mel_floor=1.192092955078125e-07,\n",
    "                remove_dc_offset=True,\n",
    "            ).T\n",
    "            fbank = torch.from_numpy(fbank)\n",
    "\n",
    "        n_frames = fbank.shape[0]\n",
    "        difference = max_length - n_frames\n",
    "\n",
    "        # pad or truncate, depending on difference\n",
    "        if difference > 0:\n",
    "            pad_module = torch.nn.ZeroPad2d((0, 0, 0, difference))\n",
    "            fbank = pad_module(fbank)\n",
    "        elif difference < 0:\n",
    "            fbank = fbank[0:max_length, :]\n",
    "\n",
    "        fbank = fbank.numpy()\n",
    "        return fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6bce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Dropout\n",
    "from transformers.models.audio_spectrogram_transformer.modeling_audio_spectrogram_transformer import ASTEmbeddings\n",
    "\n",
    "class ASTEmbeddingsWithPatchout(ASTEmbeddings):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.patchout_prob = config.patchout_prob\n",
    "        self.patchout_strategy = config.patchout_strategy\n",
    "        self.patchout_dropout = Dropout(self.patchout_prob)\n",
    "\n",
    "    def forward(self, input_values: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = input_values.shape[0]\n",
    "        embeddings = self.patch_embeddings(input_values)  # Shape: (batch_size, num_patches, hidden_size)\n",
    "\n",
    "        # Apply Patchout during training\n",
    "        if self.training:\n",
    "            if self.patchout_strategy == \"unstructured\":\n",
    "                embeddings = self.patchout_dropout(embeddings)\n",
    "            elif self.patchout_strategy == \"structured\":\n",
    "                embeddings = self.structured_patchout(embeddings)\n",
    "\n",
    "        # Add CLS and distillation tokens + positional embeddings\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        distillation_tokens = self.distillation_token.expand(batch_size, -1, -1)\n",
    "        embeddings = torch.cat((cls_tokens, distillation_tokens, embeddings), dim=1)\n",
    "        embeddings = embeddings + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def structured_patchout(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply structured Patchout by dropping entire rows/columns of patches.\n",
    "        \"\"\"\n",
    "        batch_size, num_patches, hidden_size = embeddings.shape\n",
    "        freq_out_dim, time_out_dim = self.get_shape(self.config)\n",
    "        \n",
    "        # Reshape to 2D grid: (batch_size, freq_out_dim, time_out_dim, hidden_size)\n",
    "        embeddings = embeddings.view(batch_size, freq_out_dim, time_out_dim, hidden_size)\n",
    "\n",
    "        # Randomly drop frequency columns or time rows\n",
    "        if self.patchout_prob > 0:\n",
    "            # Drop frequency columns\n",
    "            freq_mask = torch.rand(freq_out_dim, device=embeddings.device) > self.patchout_prob\n",
    "            embeddings = embeddings * freq_mask.view(1, freq_out_dim, 1, 1)\n",
    "\n",
    "            # Drop time rows\n",
    "            time_mask = torch.rand(time_out_dim, device=embeddings.device) > self.patchout_prob\n",
    "            embeddings = embeddings * time_mask.view(1, 1, time_out_dim, 1)\n",
    "\n",
    "        # Flatten back to (batch_size, num_patches, hidden_size)\n",
    "        embeddings = embeddings.view(batch_size, num_patches, hidden_size)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a973f6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5beba87995b24468ac53eee0a187bed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/276532 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6643fbe138479db8b6eba9645d70e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34567 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a98ccfc0e14e4b9ac8d0cc0949f5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34567 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "NUM_PROC = (os.cpu_count() - 1) * 4\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=\"./dataset-oversampled-15\", num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3b8ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset))\n",
    "print(type(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12cae748",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "\n",
    "NUM_MEL_BINS = 128 # based on https://arxiv.org/pdf/2409.05924\n",
    "MAX_SEQUENCE_LENGTH = 1024\n",
    "\n",
    "global feature_extractor\n",
    "\n",
    "feature_extractor = ASTFeatureExtractorHamming.from_pretrained(\n",
    "        pretrained_model, \n",
    "        num_mel_bins=NUM_MEL_BINS, \n",
    "        max_length=MAX_SEQUENCE_LENGTH\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ac471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom AST Feature Extractor with Hamming window created successfully!\n",
      "Sampling rate: 16000\n",
      "Mel bins: 128\n"
     ]
    }
   ],
   "source": [
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "SAMPLING_RATE = feature_extractor.sampling_rate\n",
    "print(\"Custom AST Feature Extractor with Hamming window created successfully!\")\n",
    "print(f\"Sampling rate: {feature_extractor.sampling_rate}\")\n",
    "print(f\"Mel bins: {feature_extractor.num_mel_bins}\")\n",
    "\n",
    "num_labels = len(np.unique(dataset[\"train\"][\"label\"]))\n",
    "# num_labels = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f317f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 276532\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 34567\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 34567\n",
      "    })\n",
      "})\n",
      "dataset train features: {'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "dataset test features: {'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "dataset validation features: {'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "model_input_name: input_values\n",
      "SAMPLING_RATE: 16000\n",
      "dataset[\"train\"][0]: {'audio': {'path': 'C:\\\\Users\\\\crumbz\\\\Downloads\\\\thesis-testing\\\\dataset-oversampled-15\\\\train\\\\fake\\\\1.wav', 'array': array([-0.00036084,  0.00093773, -0.00047797, ..., -0.00141395,\n",
      "       -0.00243979,  0.        ], shape=(177537,)), 'sampling_rate': 16000}, 'label': 0}\n",
      "num_labels: 2\n",
      "dataset columns: ['audio', 'label']\n"
     ]
    }
   ],
   "source": [
    "print('dataset:', dataset)\n",
    "print('dataset train features:', dataset['train'].features)\n",
    "print('dataset test features:', dataset['test'].features)\n",
    "print('dataset validation features:', dataset['validation'].features)\n",
    "print('model_input_name:', model_input_name)\n",
    "print('SAMPLING_RATE:', SAMPLING_RATE)\n",
    "print('dataset[\"train\"][0]:', dataset['train'][0])\n",
    "\n",
    "# For when using IterableDataset\n",
    "# for example in dataset['train']:\n",
    "#     print('dataset[\"train\"][0]:', example)\n",
    "#     break\n",
    "\n",
    "print('num_labels:', num_labels)\n",
    "print('dataset columns:', dataset['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18162b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_values', 'label'],\n",
      "        num_rows: 276532\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_values', 'label'],\n",
      "        num_rows: 34567\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_values', 'label'],\n",
      "        num_rows: 34567\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# calculate values for normalization\n",
    "feature_extractor.do_normalize = False\n",
    "\n",
    "def preprocess_audio(batch):\n",
    "    wavs = [audio[\"array\"] for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\", return_attention_mask=True, max_length=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    # print(\"wavs:\", wavs)\n",
    "    # print(\"inputs:\", inputs)\n",
    "    \n",
    "    return {\n",
    "        model_input_name: inputs.get(model_input_name),\n",
    "        \"labels\": torch.tensor(batch[\"label\"])\n",
    "    }\n",
    "\n",
    "dataset = dataset.rename_column(\"audio\", \"input_values\")\n",
    "# this can't be if we're going to use an iterable dataset:\n",
    "dataset[\"train\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "\n",
    "# dataset[\"train\"] = dataset[\"train\"].map(\n",
    "#     preprocess_audio,\n",
    "#     batched=True,\n",
    "# )\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5604b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "def compute_dataset_statistics_optimized(dataset, model_input_name, batch_size=None, device=None):\n",
    "    \"\"\"\n",
    "    Optimized computation of dataset mean and standard deviation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset to compute statistics for\n",
    "        model_input_name: The key name for model input in batch\n",
    "        batch_size: Batch size (auto-optimized if None)\n",
    "        device: Device to use ('cuda', 'cpu', or None for auto-detection)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (mean, std)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. OPTIMIZATION: Auto-detect optimal batch size and device\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    if batch_size is None:\n",
    "        # Start with a larger batch size for statistics computation\n",
    "        if device == 'cuda':\n",
    "            # Try to use more GPU memory for faster processing\n",
    "            batch_size = min(512, len(dataset) // 10)  # Use 10% of dataset or 512, whichever is smaller\n",
    "        else:\n",
    "            batch_size = min(256, len(dataset) // 20)  # Conservative for CPU\n",
    "    \n",
    "    print(f\"Using device: {device}, batch_size: {batch_size}\")\n",
    "    \n",
    "    # 2. OPTIMIZATION: Configure DataLoader for maximum performance\n",
    "    dataloader_kwargs = {\n",
    "        'batch_size': batch_size,\n",
    "        'shuffle': False,  # No need to shuffle for statistics\n",
    "        'drop_last': False,  # Process all data\n",
    "        'pin_memory': device == 'cuda',  # Only pin memory if using GPU\n",
    "        'num_workers': 0,  # Start with 0 to avoid multiprocessing issues\n",
    "    }\n",
    "    \n",
    "    # Try to enable multiprocessing if it works\n",
    "    try:\n",
    "        test_loader = DataLoader(dataset, batch_size=2, num_workers=2)\n",
    "        next(iter(test_loader))  # Test if multiprocessing works\n",
    "        dataloader_kwargs['num_workers'] = min(4, os.cpu_count() - 1)  # Conservative worker count\n",
    "        print(f\"Multiprocessing enabled with {dataloader_kwargs['num_workers']} workers\")\n",
    "        del test_loader\n",
    "    except:\n",
    "        print(\"Multiprocessing failed, using single process\")\n",
    "    \n",
    "    dataloader = DataLoader(dataset, **dataloader_kwargs)\n",
    "    \n",
    "    # 3. OPTIMIZATION: Use appropriate tensor dtypes and device placement\n",
    "    if device == 'cuda':\n",
    "        sum_dtype = torch.float64  # Higher precision for accumulation\n",
    "        working_dtype = torch.float32  # Working precision\n",
    "    else:\n",
    "        sum_dtype = torch.float64\n",
    "        working_dtype = torch.float32\n",
    "    \n",
    "    # Initialize accumulators on the target device with appropriate dtype\n",
    "    total_sum = torch.tensor(0.0, dtype=sum_dtype, device=device)\n",
    "    total_squared_sum = torch.tensor(0.0, dtype=sum_dtype, device=device)\n",
    "    total_count = 0\n",
    "    \n",
    "    # 4. OPTIMIZATION: Process with memory-efficient operations\n",
    "    progress_bar = tqdm(dataloader, desc=\"Computing normalization stats\", unit=\"batch\")\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Get batch data\n",
    "            batch_data = batch[model_input_name]\n",
    "            \n",
    "            # 5. OPTIMIZATION: Efficient tensor conversion and device transfer\n",
    "            if not isinstance(batch_data, torch.Tensor):\n",
    "                batch_data = torch.tensor(batch_data, dtype=working_dtype)\n",
    "            else:\n",
    "                batch_data = batch_data.to(dtype=working_dtype)\n",
    "            \n",
    "            # Move to target device if needed\n",
    "            if batch_data.device != torch.device(device):\n",
    "                batch_data = batch_data.to(device, non_blocking=True)\n",
    "            \n",
    "            # 6. OPTIMIZATION: Efficient flattening and computation\n",
    "            # Use view instead of flatten when possible (more memory efficient)\n",
    "            flat_batch = batch_data.view(-1)\n",
    "            \n",
    "            # Compute statistics using vectorized operations\n",
    "            batch_sum = flat_batch.sum(dtype=sum_dtype)\n",
    "            batch_squared_sum = flat_batch.pow(2).sum(dtype=sum_dtype)\n",
    "            batch_count = flat_batch.numel()\n",
    "            \n",
    "            # Update accumulators\n",
    "            total_sum += batch_sum\n",
    "            total_squared_sum += batch_squared_sum\n",
    "            total_count += batch_count\n",
    "            \n",
    "            # 7. OPTIMIZATION: Memory cleanup for large datasets\n",
    "            if batch_idx % 100 == 0:  # Periodic cleanup\n",
    "                if device == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            \n",
    "            # Update progress bar with current estimates\n",
    "            if batch_idx % 10 == 0:\n",
    "                current_mean = (total_sum / total_count).item()\n",
    "                progress_bar.set_postfix({\n",
    "                    'current_mean': f'{current_mean:.4f}',\n",
    "                    'processed': f'{total_count:,}'\n",
    "                })\n",
    "    \n",
    "    # 8. OPTIMIZATION: Numerically stable standard deviation calculation\n",
    "    # Use the corrected formula to avoid numerical instability\n",
    "    mean = total_sum / total_count\n",
    "    variance = (total_squared_sum / total_count) - (mean * mean)\n",
    "    \n",
    "    # Clamp variance to avoid negative values due to floating point errors\n",
    "    variance = torch.clamp(variance, min=0.0)\n",
    "    std = torch.sqrt(variance)\n",
    "    \n",
    "    # Convert back to Python scalars\n",
    "    final_mean = mean.item()\n",
    "    final_std = std.item()\n",
    "    \n",
    "    # Final cleanup\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return final_mean, final_std\n",
    "\n",
    "def compute_dataset_statistics_chunked(dataset, model_input_name, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Alternative approach: Process dataset in chunks without DataLoader for maximum memory efficiency.\n",
    "    Use this if the DataLoader approach still has issues.\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset in chunks of {chunk_size}\")\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    sum_dtype = torch.float64\n",
    "    \n",
    "    total_sum = torch.tensor(0.0, dtype=sum_dtype, device=device)\n",
    "    total_squared_sum = torch.tensor(0.0, dtype=sum_dtype, device=device)\n",
    "    total_count = 0\n",
    "    \n",
    "    dataset_size = len(dataset)\n",
    "    num_chunks = (dataset_size + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for chunk_idx in tqdm(range(num_chunks), desc=\"Processing chunks\"):\n",
    "            start_idx = chunk_idx * chunk_size\n",
    "            end_idx = min(start_idx + chunk_size, dataset_size)\n",
    "            \n",
    "            # Process items in current chunk\n",
    "            chunk_tensors = []\n",
    "            for idx in range(start_idx, end_idx):\n",
    "                item = dataset[idx][model_input_name]\n",
    "                if not isinstance(item, torch.Tensor):\n",
    "                    item = torch.tensor(item, dtype=torch.float32)\n",
    "                chunk_tensors.append(item.flatten())\n",
    "            \n",
    "            # Concatenate chunk data\n",
    "            if chunk_tensors:\n",
    "                chunk_data = torch.cat(chunk_tensors).to(device)\n",
    "                \n",
    "                # Compute statistics\n",
    "                chunk_sum = chunk_data.sum(dtype=sum_dtype)\n",
    "                chunk_squared_sum = chunk_data.pow(2).sum(dtype=sum_dtype)\n",
    "                chunk_count = chunk_data.numel()\n",
    "                \n",
    "                total_sum += chunk_sum\n",
    "                total_squared_sum += chunk_squared_sum\n",
    "                total_count += chunk_count\n",
    "                \n",
    "                # Cleanup\n",
    "                del chunk_data, chunk_tensors\n",
    "                if device == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    mean = total_sum / total_count\n",
    "    variance = (total_squared_sum / total_count) - (mean * mean)\n",
    "    variance = torch.clamp(variance, min=0.0)\n",
    "    std = torch.sqrt(variance)\n",
    "    \n",
    "    return mean.item(), std.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed6897e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized - Mean: -7.460440170265478, Std: 4.897325661819559\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Optimized DataLoader approach (recommended)\n",
    "try:\n",
    "    # mean, std = compute_dataset_statistics_optimized(\n",
    "    #     dataset=dataset[\"train\"],\n",
    "    #     model_input_name=model_input_name,\n",
    "    #     batch_size=512,  # Start with this, increase if you have more memory\n",
    "    #     device='cuda'\n",
    "    # )\n",
    "    \n",
    "    # Testing purposes og dataset values\n",
    "    # mean = 0.31605425901883943\n",
    "    # std = 0.45787811188377187\n",
    "    \n",
    "    # dataset balanced. calculate again just to be sure tho\n",
    "    # mean = -7.5272675213621705\n",
    "    # std = 4.944255594530362\n",
    "    \n",
    "    # dataset oversampled 15\n",
    "    mean = -7.460440170265478\n",
    "    std = 4.897325661819559\n",
    "    \n",
    "    print(f'Optimized - Mean: {mean}, Std: {std}')\n",
    "    \n",
    "    feature_extractor.mean = mean\n",
    "    feature_extractor.std = std\n",
    "    feature_extractor.do_normalize = True\n",
    "except Exception as e:\n",
    "    print(f\"DataLoader approach failed: {e}\")\n",
    "    print(\"Trying chunked approach...\")\n",
    "    \n",
    "    # Method 2: Fallback chunked approach\n",
    "    mean, std = compute_dataset_statistics_chunked(\n",
    "        dataset=dataset[\"train\"],\n",
    "        model_input_name=model_input_name,\n",
    "        chunk_size=500\n",
    "    )\n",
    "    print(f'Chunked - Mean: {mean}, Std: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af852b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, BandPassFilter, Mp3Compression, RoomSimulator, Gain, ClippingDistortion\n",
    "import math  # Needed for azimuth/elevation parameters\n",
    "\n",
    "audio_augmentations = Compose([\n",
    "    # 1. Noise injection (both papers)\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    \n",
    "    # 2. Bandpass filtering (continuous-learning.pdf)\n",
    "    BandPassFilter(min_center_freq=300, max_center_freq=3000, \n",
    "                   min_bandwidth_fraction=0.1, max_bandwidth_fraction=0.3, p=0.5),\n",
    "    \n",
    "    # 3. MP3 compression (hybrid-audio.pdf)\n",
    "    Mp3Compression(min_bitrate=16, max_bitrate=32, p=0.5),\n",
    "    \n",
    "    # 4. Room simulation - CORRECTED (continuous-learning.pdf)\n",
    "    RoomSimulator(\n",
    "        min_size_x=5.0, max_size_x=15.0,\n",
    "        min_size_y=5.0, max_size_y=15.0,\n",
    "        min_size_z=2.4, max_size_z=4.0,\n",
    "        min_absorption_value=0.1, max_absorption_value=0.8,\n",
    "        min_mic_distance=1.0, max_mic_distance=5.0,  # Far-field simulation\n",
    "        min_mic_azimuth=-math.pi, max_mic_azimuth=math.pi,\n",
    "        min_mic_elevation=-math.pi/4, max_mic_elevation=math.pi/4,\n",
    "        calculation_mode=\"absorption\",\n",
    "        p=0.5\n",
    "    ),\n",
    "    \n",
    "    # 5. Gain variation (hybrid-audio.pdf)\n",
    "    Gain(min_gain_db=-6, max_gain_db=6, p=0.3),\n",
    "    \n",
    "    # 6. Clipping distortion (both papers)\n",
    "    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=10, p=0.3),\n",
    "], p=0.4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd7f4b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset train features: {'input_values': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "dataset test features: {'input_values': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "dataset validation features: {'input_values': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_audio_with_transforms(batch):\n",
    "    # we apply augmentations on each waveform\n",
    "    wavs = [audio_augmentations(audio[\"array\"], sample_rate=SAMPLING_RATE) for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\", return_attention_mask=True, max_length=MAX_SEQUENCE_LENGTH)\n",
    "    return {\n",
    "        model_input_name: inputs.get(model_input_name),\n",
    "        \"labels\": torch.tensor(batch[\"label\"])\n",
    "    }\n",
    "\n",
    "print('dataset train features:', dataset['train'].features)\n",
    "print('dataset test features:', dataset['test'].features)\n",
    "print('dataset validation features:', dataset['validation'].features)\n",
    "# Cast the audio column to the appropriate feature type and rename it\n",
    "dataset = dataset.cast_column(\"input_values\", Audio(sampling_rate=feature_extractor.sampling_rate))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88736686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with augmentations on the training set\n",
    "dataset[\"train\"].set_transform(preprocess_audio_with_transforms, output_all_columns=False)\n",
    "# w/o augmentations on the test set\n",
    "dataset[\"test\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "dataset[\"validation\"].set_transform(preprocess_audio, output_all_columns=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dd71c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_values': {'path': 'C:\\\\Users\\\\crumbz\\\\Downloads\\\\thesis-testing\\\\dataset-oversampled-15\\\\train\\\\fake\\\\1.wav', 'array': array([-0.00036084,  0.00093773, -0.00047797, ..., -0.00141395,\n",
      "       -0.00243979,  0.        ], shape=(177537,)), 'sampling_rate': 16000}, 'label': 0}\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "temp_ds = dataset['train'].with_format(None)  # This removes any applied transforms\n",
    "temp_ds_test = dataset['test'].with_format(None)  # This removes any applied transforms\n",
    "\n",
    "print(temp_ds[0])\n",
    "unique_labels = sorted(set(temp_ds[\"label\"] + temp_ds_test[\"label\"]))\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feb51aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASTEmbeddings(\n",
      "  (patch_embeddings): ASTPatchEmbeddings(\n",
      "    (projection): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import ASTConfig, ASTForAudioClassification\n",
    "\n",
    "# Load configuration from the pretrained model\n",
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "\n",
    "# Update configuration with the number of labels in our dataset\n",
    "config.num_mel_bins = NUM_MEL_BINS  # Make sure this matches your feature extractor\n",
    "config.max_length = MAX_SEQUENCE_LENGTH   # Or whatever your sequence length is\n",
    "config.num_labels = num_labels\n",
    "config.label2id = {\"fake\": 0, \"real\": 1}\n",
    "config.id2label = {0: \"fake\", 1: \"real\"}\n",
    "\n",
    "\n",
    "# setting dropout to prevent overfitting. based on https://www.mdpi.com/2073-431X/13/10/256\n",
    "config.hidden_dropout_prob = 0.10\n",
    "\n",
    "# having a patch size of 16, and time and frequency stride of 16, allows us to have NO \n",
    "# overlaps. this prevents difficulties if the file has real and fake audio \n",
    "# (based on https://arxiv.org/pdf/2409.05924)\n",
    "\n",
    "# commenting these out for now\n",
    "# config.patch_size = 16\n",
    "# config.frequency_stride = 16\n",
    "# config.time_stride = 16\n",
    "\n",
    "config.patchout_prob = 0.5  # Probability of dropping a patch\n",
    "config.patchout_strategy = \"structured\"  # \"unstructured\" or \"structured\"\n",
    "\n",
    "# Initialize the model with the updated configuration\n",
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    pretrained_model,\n",
    "    config=config,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "print(model.audio_spectrogram_transformer.embeddings)  # See what this submodule is\n",
    "\n",
    "model.audio_spectrogram_transformer.embeddings.embeddings = ASTEmbeddingsWithPatchout(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29ef2a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "structured\n",
      "Output shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(config.patchout_prob)\n",
    "print(config.patchout_strategy)\n",
    "sample_input = torch.randn(1, 128, MAX_SEQUENCE_LENGTH)  # Batch of spectrograms (batch_size, num_mel_bins, max_length)\n",
    "output = model(sample_input)\n",
    "print(\"Output shape:\", output.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59a04121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight decay:  2.7068620883749747e-05\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "# weight decay formula based on https://arxiv.org/abs/1711.05101:\n",
    "    # weight decay = ynorm(sqr(batch_size/(dataset_size * num_train_epochs)))\n",
    "    # where ynorm is between 0.025 and 0.05 https://towardsdatascience.com/weight-decay-and-its-peculiar-effects-66e0aee3e7b8/\n",
    "\n",
    "DATASET_SIZE = 341_199 \n",
    "YNORM = 0.025\n",
    "BATCH_SIZE = 8\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "WEIGHT_DECAY = YNORM * np.sqrt(BATCH_SIZE / (DATASET_SIZE * NUM_TRAIN_EPOCHS))\n",
    "\n",
    "print(\"Weight decay: \", WEIGHT_DECAY)\n",
    "\n",
    "# Configure training run with TrainingArguments class\n",
    "training_args = TrainingArguments(\n",
    "    optim=\"adamw_torch_fused\", # based on https://huggingface.co/docs/transformers/v4.35.2/en/perf_train_gpu_one#optimizer-choice\n",
    "    # gradient_checkpointing=True,\n",
    "    output_dir=\"./runs/ast_classifier\",\n",
    "    logging_dir=\"./logs/ast_classifier\",\n",
    "    report_to=\"tensorboard\",\n",
    "    learning_rate=2e-5, # based on https://arxiv.org/pdf/2505.15136\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # Start with 8, can increase if memory allows\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    dataloader_num_workers=0, # based on https://huggingface.co/docs/transformers/v4.35.2/en/perf_train_gpu_one#data-preloading\n",
    "    dataloader_pin_memory=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\", # based on https://www.mdpi.com/2073-431X/13/10/256 and https://arxiv.org/pdf/2505.15136\n",
    "    logging_steps=20,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    # warmup_ratio=0.05,\n",
    "    # label_smoothing_factor=0.05,\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    "    lr_scheduler_type=\"cosine\", # based on https://arxiv.org/pdf/2505.15136\n",
    "    weight_decay=WEIGHT_DECAY, # based on https://arxiv.org/abs/1711.05101 and https://towardsdatascience.com/weight-decay-and-its-peculiar-effects-66e0aee3e7b8/\n",
    "    #  orch_compile=True,\n",
    "    # torch_compile_backend=\"eager\",\n",
    "    # torch_empty_cache_steps=4,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d070a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "roc_auc = evaluate.load(\"roc_auc\")\n",
    "\n",
    "AVERAGE = \"macro\" if config.num_labels > 2 else \"binary\"\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    # Stable softmax for prob of positive class (index 1)\n",
    "    logits_max = np.max(logits, axis=1, keepdims=True)\n",
    "    exp = np.exp(logits - logits_max)\n",
    "    probs = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "    prob_pos = probs[:, 1] if probs.shape[1] > 1 else probs[:, 0]\n",
    "\n",
    "    metrics = {}\n",
    "    metrics.update(accuracy.compute(predictions=preds, references=labels))\n",
    "    metrics.update(precision.compute(predictions=preds, references=labels, average=AVERAGE))\n",
    "    metrics.update(recall.compute(predictions=preds, references=labels, average=AVERAGE))\n",
    "    metrics.update(f1.compute(predictions=preds, references=labels, average=AVERAGE))\n",
    "    # roc_auc for binary\n",
    "    if config.num_labels == 2:\n",
    "        metrics.update(roc_auc.compute(prediction_scores=prob_pos, references=labels))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a8de353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crumbz\\AppData\\Local\\Temp\\ipykernel_11236\\3154806320.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, DataCollatorWithPadding\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize the data collator\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=feature_extractor,  # Your feature extractor acts as the tokenizer\n",
    "    padding=True,\n",
    "    max_length=config.max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "def init_model():\n",
    "    model = ASTForAudioClassification.from_pretrained(\n",
    "        pretrained_model,\n",
    "        config=config,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    # actually use patchout\n",
    "    model.audio_spectrogram_transformer.embeddings.embeddings = ASTEmbeddingsWithPatchout(config)\n",
    "    return model\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=init_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    "    data_collator=data_collator,\n",
    "    # if no improvements happened to validation loss within 5 epochs, stop training.\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.00)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aac110b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input shape: torch.Size([1024, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Forward pass to check for errors\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mForward pass successful!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crumbz\\Downloads\\thesis-testing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crumbz\\Downloads\\thesis-testing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crumbz\\Downloads\\thesis-testing\\.venv\\Lib\\site-packages\\transformers\\models\\audio_spectrogram_transformer\\modeling_audio_spectrogram_transformer.py:553\u001b[39m, in \u001b[36mASTForAudioClassification.forward\u001b[39m\u001b[34m(self, input_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[33;03minput_values (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`):\u001b[39;00m\n\u001b[32m    540\u001b[39m \u001b[33;03m    Float values mel features extracted from the raw audio waveform. Raw audio waveform can be obtained by\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    549\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    551\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maudio_spectrogram_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m pooled_output = outputs[\u001b[32m1\u001b[39m]\n\u001b[32m    562\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.classifier(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crumbz\\Downloads\\thesis-testing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crumbz\\Downloads\\thesis-testing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crumbz\\Downloads\\thesis-testing\\.venv\\Lib\\site-packages\\transformers\\models\\audio_spectrogram_transformer\\modeling_audio_spectrogram_transformer.py:472\u001b[39m, in \u001b[36mASTModel.forward\u001b[39m\u001b[34m(self, input_values, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    469\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    470\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m embedding_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m encoder_outputs = \u001b[38;5;28mself\u001b[39m.encoder(\n\u001b[32m    475\u001b[39m     embedding_output,\n\u001b[32m    476\u001b[39m     head_mask=head_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    479\u001b[39m     return_dict=return_dict,\n\u001b[32m    480\u001b[39m )\n\u001b[32m    481\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crumbz\\Downloads\\thesis-testing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crumbz\\Downloads\\thesis-testing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crumbz\\Downloads\\thesis-testing\\.venv\\Lib\\site-packages\\transformers\\models\\audio_spectrogram_transformer\\modeling_audio_spectrogram_transformer.py:64\u001b[39m, in \u001b[36mASTEmbeddings.forward\u001b[39m\u001b[34m(self, input_values)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_values: torch.Tensor) -> torch.Tensor:\n\u001b[32m     63\u001b[39m     batch_size = input_values.shape[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     cls_tokens = \u001b[38;5;28mself\u001b[39m.cls_token.expand(batch_size, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m     distillation_tokens = \u001b[38;5;28mself\u001b[39m.distillation_token.expand(batch_size, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crumbz\\Downloads\\thesis-testing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crumbz\\Downloads\\thesis-testing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crumbz\\Downloads\\thesis-testing\\.venv\\Lib\\site-packages\\transformers\\models\\audio_spectrogram_transformer\\modeling_audio_spectrogram_transformer.py:95\u001b[39m, in \u001b[36mASTPatchEmbeddings.forward\u001b[39m\u001b[34m(self, input_values)\u001b[39m\n\u001b[32m     93\u001b[39m input_values = input_values.unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m     94\u001b[39m input_values = input_values.transpose(\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m.flatten(\u001b[32m2\u001b[39m).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crumbz\\Downloads\\thesis-testing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crumbz\\Downloads\\thesis-testing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crumbz\\Downloads\\thesis-testing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crumbz\\Downloads\\thesis-testing\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "# Add this before training to verify shapes\n",
    "sample = next(iter(dataset[\"train\"]))\n",
    "print(f\"Sample input shape: {sample['input_values'].shape}\")\n",
    "\n",
    "# Forward pass to check for errors\n",
    "with torch.no_grad():\n",
    "    output = model(sample[\"input_values\"].unsqueeze(0))\n",
    "print(\"Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5bc2418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val sample: <class 'torch.Tensor'> torch.Size([1024, 128]) label: tensor(0)\n",
      "Forward OK; logits shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity checks\n",
    "sample_val = next(iter(dataset[\"validation\"]))\n",
    "x = sample_val[\"input_values\"]  # tensor [time, freq] or [seq_len, feat] depending on extractor\n",
    "y = sample_val[\"labels\"]\n",
    "print(\"Val sample:\", type(x), getattr(x, \"shape\", None), \"label:\", y)\n",
    "\n",
    "# Move model and input to the same device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = init_model().to(device)  # Move model to device first\n",
    "\n",
    "# Forward pass smoke test\n",
    "with torch.no_grad():\n",
    "    x = x.unsqueeze(0).to(device)  # Add batch dim and move to same device\n",
    "    out = model(x)\n",
    "print(\"Forward OK; logits shape:\", out.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "300ab2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config: ASTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ASTForAudioClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"frequency_stride\": 10,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"fake\",\n",
      "    \"1\": \"real\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"fake\": 0,\n",
      "    \"real\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_length\": 1024,\n",
      "  \"model_type\": \"audio-spectrogram-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_mel_bins\": 128,\n",
      "  \"patch_size\": 16,\n",
      "  \"patchout_prob\": 0.5,\n",
      "  \"patchout_strategy\": \"structured\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"time_stride\": 10,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.55.0\"\n",
      "}\n",
      "\n",
      "Sample input shape: torch.Size([1024, 128])\n",
      "Model's expected input shape: 1024\n",
      "Feature extractor config: ASTFeatureExtractorHamming {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"ASTFeatureExtractorHamming\",\n",
      "  \"feature_size\": 1,\n",
      "  \"max_length\": 1024,\n",
      "  \"mean\": -7.460440170265478,\n",
      "  \"num_mel_bins\": 128,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000,\n",
      "  \"std\": 4.897325661819559\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model config: {model.config}\")\n",
    "print(f\"Sample input shape: {sample['input_values'].shape}\")\n",
    "print(f\"Model's expected input shape: {model.config.max_length}\")\n",
    "print(f\"Feature extractor config: {feature_extractor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0103e3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce RTX 2060\n",
      "GPU Memory Total: 6.44 GB\n",
      "GPU Memory Allocated: 0.71 GB\n",
      "GPU Memory Cached: 0.84 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "print(f\"Using device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "     print(f\"GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76028015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading scaler: [Errno 2] No such file or directory: 'runs\\\\ast_classifier\\\\checkpoint-63480\\\\scaler.pt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crumbz\\AppData\\Local\\Temp\\ipykernel_11236\\1730644306.py:11: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
      "  with safe_globals([np.core.multiarray.scalar, np.dtype, np.dtypes.Float64DType]):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.serialization import safe_globals\n",
    "import numpy as np\n",
    "from accelerate.utils import get_grad_scaler\n",
    "\n",
    "# Get the correct scaler type based on your hardware\n",
    "scaler = get_grad_scaler()\n",
    "\n",
    "CHECKPOINT_NUM = 63480\n",
    "# Load the scaler state dict\n",
    "with safe_globals([np.core.multiarray.scalar, np.dtype, np.dtypes.Float64DType]):\n",
    "    try:\n",
    "        scaler_state = torch.load(fr\"runs\\ast_classifier\\checkpoint-{CHECKPOINT_NUM}\\scaler.pt\", weights_only=True)\n",
    "        print(\"Scaler state type:\", type(scaler_state))\n",
    "        print(\"Scaler state contents:\", scaler_state)\n",
    "        print(\"Trainer accelerator scaler: \", trainer.accelerator.scaler)\n",
    "        if not trainer.accelerator.scaler:\n",
    "            print(\"Trainer accelerator scaler is None. Setting up the scaler\")\n",
    "            scaler.load_state_dict(scaler_state)\n",
    "            trainer.accelerator.scaler = scaler\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error loading scaler: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import os\n",
    "# os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"  # For internal stack traces\n",
    "# os.environ[\"TORCH_LOGS\"] = \"+dynamo\"     # Additional context\n",
    "\n",
    "# print(torch._dynamo.list_backends())\n",
    "\n",
    "# def compute_objective(metrics: dict[str, float]) -> list[float]:\n",
    "#     return metrics[\"eval_loss\"], metrics[\"eval_accuracy\"]\n",
    "\n",
    "# def hp_space(trial):\n",
    "#     return {\n",
    "#         \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16]),\n",
    "#         \"per_device_eval_batch_size\": trial.suggest_categorical(\"per_device_eval_batch_size\", [4, 8, 16]),\n",
    "#         \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [5]),\n",
    "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 5e-5),\n",
    "#         \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "#         \"warmup_steps\": trial.suggest_categorical(\"warmup_steps\", [0, 500, 1000]),\n",
    "#         \"lr_scheduler_type\": trial.suggest_categorical(\"lr_scheduler_type\", [\"linear\", \"cosine\", \"cosine_with_restarts\"]), \n",
    "#     }\n",
    "\n",
    "# best_trials = trainer.hyperparameter_search(\n",
    "#     direction=[\"minimize\", \"maximize\"],\n",
    "#     backend=\"optuna\",\n",
    "    \n",
    "#     hp_space=hp_space,\n",
    "#     n_trials=1,\n",
    "#     compute_objective=compute_objective,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7bea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b32f0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='691340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    67/691340 03:15 < 577:13:48, 0.33 it/s, Epoch 0.00/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.serialization.safe_globals([np.core.multiarray.scalar, np.dtype, np.dtypes.Float64DType]):\n",
    "    trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ebb607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training/eval:\n",
    "save_dir = trainer.state.best_model_checkpoint or training_args.output_dir\n",
    "feature_extractor.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b94859",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "     print(f\"GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available\")     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
