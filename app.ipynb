{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27471c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import ASTFeatureExtractor\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.profiler\n",
    "import numpy as np\n",
    "from datasets import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3821aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"System: {platform.system()} {platform.release()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import is_speech_available\n",
    "from transformers.audio_utils import mel_filter_bank, spectrogram, window_function\n",
    "\n",
    "if is_speech_available():\n",
    "    import torchaudio.compliance.kaldi as ta_kaldi\n",
    "\n",
    "# based on the following literature that uses the Hamming window:\n",
    "    # https://arxiv.org/pdf/2505.15136\n",
    "    # https://arxiv.org/pdf/2409.05924\n",
    "    # https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11007653\n",
    "class ASTFeatureExtractorHamming(ASTFeatureExtractor):\n",
    "    \"\"\"\n",
    "    Custom AST Feature Extractor that uses Hamming window instead of Hann/Hanning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Override the window for numpy-based processing (when torchaudio is not available)\n",
    "        if not is_speech_available():\n",
    "            # Recalculate mel filters and window with hamming\n",
    "            mel_filters = mel_filter_bank(\n",
    "                num_frequency_bins=257,\n",
    "                num_mel_filters=self.num_mel_bins,\n",
    "                min_frequency=20,\n",
    "                max_frequency=self.sampling_rate // 2,\n",
    "                sampling_rate=self.sampling_rate,\n",
    "                norm=None,\n",
    "                mel_scale=\"kaldi\",\n",
    "                triangularize_in_mel_space=True,\n",
    "            )\n",
    "            self.mel_filters = mel_filters\n",
    "            # Use hamming window instead of hann\n",
    "            self.window = window_function(400, \"hamming\", periodic=False)\n",
    "    \n",
    "    def _extract_fbank_features(self, waveform: np.ndarray, max_length: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Override to use hamming window type in torchaudio.compliance.kaldi.fbank\n",
    "        \"\"\"\n",
    "        if is_speech_available():\n",
    "            waveform = torch.from_numpy(waveform).unsqueeze(0)\n",
    "            fbank = ta_kaldi.fbank(\n",
    "                waveform,\n",
    "                sample_frequency=self.sampling_rate,\n",
    "                window_type=\"hamming\",  # Changed from \"hanning\" to \"hamming\"\n",
    "                num_mel_bins=self.num_mel_bins,\n",
    "            )\n",
    "        else:\n",
    "            # Use numpy implementation with hamming window\n",
    "            waveform = np.squeeze(waveform)\n",
    "            fbank = spectrogram(\n",
    "                waveform,\n",
    "                self.window,  # This is now hamming window from __init__\n",
    "                frame_length=400, # this follows the 25 ms frame length used in the paper (16000mhz * 0.025 = 400)\n",
    "                hop_length=160, # this follows the hop length used in the paper (16000mhz * 0.01 = 160)\n",
    "                fft_length=512,\n",
    "                power=2.0,\n",
    "                center=False,\n",
    "                preemphasis=0.97,\n",
    "                mel_filters=self.mel_filters,\n",
    "                log_mel=\"log\",\n",
    "                mel_floor=1.192092955078125e-07,\n",
    "                remove_dc_offset=True,\n",
    "            ).T\n",
    "            fbank = torch.from_numpy(fbank)\n",
    "\n",
    "        n_frames = fbank.shape[0]\n",
    "        difference = max_length - n_frames\n",
    "\n",
    "        # pad or truncate, depending on difference\n",
    "        if difference > 0:\n",
    "            pad_module = torch.nn.ZeroPad2d((0, 0, 0, difference))\n",
    "            fbank = pad_module(fbank)\n",
    "        elif difference < 0:\n",
    "            fbank = fbank[0:max_length, :]\n",
    "\n",
    "        fbank = fbank.numpy()\n",
    "        return fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "NUM_PROC = os.cpu_count() - 1\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=\"./dataset\", num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b8ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dataset))\n",
    "print(type(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cae748",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "\n",
    "NUM_MEL_BINS = 128 # based on https://arxiv.org/pdf/2409.05924\n",
    "MAX_SEQUENCE_LENGTH = 507\n",
    "\n",
    "global feature_extractor\n",
    "\n",
    "feature_extractor = ASTFeatureExtractorHamming.from_pretrained(\n",
    "        pretrained_model, \n",
    "        num_mel_bins=NUM_MEL_BINS, \n",
    "        max_length=MAX_SEQUENCE_LENGTH\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ac471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "SAMPLING_RATE = feature_extractor.sampling_rate\n",
    "print(\"Custom AST Feature Extractor with Hamming window created successfully!\")\n",
    "print(f\"Sampling rate: {feature_extractor.sampling_rate}\")\n",
    "print(f\"Mel bins: {feature_extractor.num_mel_bins}\")\n",
    "\n",
    "num_labels = len(np.unique(dataset[\"train\"][\"label\"]))\n",
    "# num_labels = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f317f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dataset:', dataset)\n",
    "print('dataset train features:', dataset['train'].features)\n",
    "print('dataset test features:', dataset['test'].features)\n",
    "print('dataset validation features:', dataset['validation'].features)\n",
    "print('model_input_name:', model_input_name)\n",
    "print('SAMPLING_RATE:', SAMPLING_RATE)\n",
    "print('dataset[\"train\"][0]:', dataset['train'][0])\n",
    "\n",
    "# For when using IterableDataset\n",
    "# for example in dataset['train']:\n",
    "#     print('dataset[\"train\"][0]:', example)\n",
    "#     break\n",
    "\n",
    "print('num_labels:', num_labels)\n",
    "print('dataset columns:', dataset['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18162b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate values for normalization\n",
    "feature_extractor.do_normalize = False\n",
    "\n",
    "# Initialize running statistics\n",
    "n = 0\n",
    "mean = 0.0\n",
    "M2 = 0.0  # For running variance calculation\n",
    "\n",
    "def preprocess_audio(batch):\n",
    "    wavs = [audio[\"array\"] for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\", return_attention_mask=True, max_length=507)\n",
    "    \n",
    "    print(\"wavs:\", wavs)\n",
    "    print(\"inputs:\", inputs)\n",
    "    \n",
    "    return {\n",
    "        model_input_name: inputs.get(model_input_name),\n",
    "        \"labels\": torch.tensor(batch[\"label\"])\n",
    "    }\n",
    "\n",
    "dataset = dataset.rename_column(\"audio\", \"input_values\")\n",
    "# this can't be if we're going to use an iterable dataset:\n",
    "dataset[\"train\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "\n",
    "# dataset[\"train\"] = dataset[\"train\"].map(\n",
    "#     preprocess_audio,\n",
    "#     batched=True,\n",
    "# )\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5604b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "def compute_dataset_statistics_optimized(dataset, model_input_name, batch_size=None, device=None):\n",
    "    \"\"\"\n",
    "    Optimized computation of dataset mean and standard deviation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset to compute statistics for\n",
    "        model_input_name: The key name for model input in batch\n",
    "        batch_size: Batch size (auto-optimized if None)\n",
    "        device: Device to use ('cuda', 'cpu', or None for auto-detection)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (mean, std)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. OPTIMIZATION: Auto-detect optimal batch size and device\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    if batch_size is None:\n",
    "        # Start with a larger batch size for statistics computation\n",
    "        if device == 'cuda':\n",
    "            # Try to use more GPU memory for faster processing\n",
    "            batch_size = min(512, len(dataset) // 10)  # Use 10% of dataset or 512, whichever is smaller\n",
    "        else:\n",
    "            batch_size = min(256, len(dataset) // 20)  # Conservative for CPU\n",
    "    \n",
    "    print(f\"Using device: {device}, batch_size: {batch_size}\")\n",
    "    \n",
    "    # 2. OPTIMIZATION: Configure DataLoader for maximum performance\n",
    "    dataloader_kwargs = {\n",
    "        'batch_size': batch_size,\n",
    "        'shuffle': False,  # No need to shuffle for statistics\n",
    "        'drop_last': False,  # Process all data\n",
    "        'pin_memory': device == 'cuda',  # Only pin memory if using GPU\n",
    "        'num_workers': 0,  # Start with 0 to avoid multiprocessing issues\n",
    "    }\n",
    "    \n",
    "    # Try to enable multiprocessing if it works\n",
    "    try:\n",
    "        test_loader = DataLoader(dataset, batch_size=2, num_workers=2)\n",
    "        next(iter(test_loader))  # Test if multiprocessing works\n",
    "        dataloader_kwargs['num_workers'] = min(4, os.cpu_count() - 1)  # Conservative worker count\n",
    "        print(f\"Multiprocessing enabled with {dataloader_kwargs['num_workers']} workers\")\n",
    "        del test_loader\n",
    "    except:\n",
    "        print(\"Multiprocessing failed, using single process\")\n",
    "    \n",
    "    dataloader = DataLoader(dataset, **dataloader_kwargs)\n",
    "    \n",
    "    # 3. OPTIMIZATION: Use appropriate tensor dtypes and device placement\n",
    "    if device == 'cuda':\n",
    "        sum_dtype = torch.float64  # Higher precision for accumulation\n",
    "        working_dtype = torch.float32  # Working precision\n",
    "    else:\n",
    "        sum_dtype = torch.float64\n",
    "        working_dtype = torch.float32\n",
    "    \n",
    "    # Initialize accumulators on the target device with appropriate dtype\n",
    "    total_sum = torch.tensor(0.0, dtype=sum_dtype, device=device)\n",
    "    total_squared_sum = torch.tensor(0.0, dtype=sum_dtype, device=device)\n",
    "    total_count = 0\n",
    "    \n",
    "    # 4. OPTIMIZATION: Process with memory-efficient operations\n",
    "    progress_bar = tqdm(dataloader, desc=\"Computing normalization stats\", unit=\"batch\")\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Get batch data\n",
    "            batch_data = batch[model_input_name]\n",
    "            \n",
    "            # 5. OPTIMIZATION: Efficient tensor conversion and device transfer\n",
    "            if not isinstance(batch_data, torch.Tensor):\n",
    "                batch_data = torch.tensor(batch_data, dtype=working_dtype)\n",
    "            else:\n",
    "                batch_data = batch_data.to(dtype=working_dtype)\n",
    "            \n",
    "            # Move to target device if needed\n",
    "            if batch_data.device != torch.device(device):\n",
    "                batch_data = batch_data.to(device, non_blocking=True)\n",
    "            \n",
    "            # 6. OPTIMIZATION: Efficient flattening and computation\n",
    "            # Use view instead of flatten when possible (more memory efficient)\n",
    "            flat_batch = batch_data.view(-1)\n",
    "            \n",
    "            # Compute statistics using vectorized operations\n",
    "            batch_sum = flat_batch.sum(dtype=sum_dtype)\n",
    "            batch_squared_sum = flat_batch.pow(2).sum(dtype=sum_dtype)\n",
    "            batch_count = flat_batch.numel()\n",
    "            \n",
    "            # Update accumulators\n",
    "            total_sum += batch_sum\n",
    "            total_squared_sum += batch_squared_sum\n",
    "            total_count += batch_count\n",
    "            \n",
    "            # 7. OPTIMIZATION: Memory cleanup for large datasets\n",
    "            if batch_idx % 100 == 0:  # Periodic cleanup\n",
    "                if device == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            \n",
    "            # Update progress bar with current estimates\n",
    "            if batch_idx % 10 == 0:\n",
    "                current_mean = (total_sum / total_count).item()\n",
    "                progress_bar.set_postfix({\n",
    "                    'current_mean': f'{current_mean:.4f}',\n",
    "                    'processed': f'{total_count:,}'\n",
    "                })\n",
    "    \n",
    "    # 8. OPTIMIZATION: Numerically stable standard deviation calculation\n",
    "    # Use the corrected formula to avoid numerical instability\n",
    "    mean = total_sum / total_count\n",
    "    variance = (total_squared_sum / total_count) - (mean * mean)\n",
    "    \n",
    "    # Clamp variance to avoid negative values due to floating point errors\n",
    "    variance = torch.clamp(variance, min=0.0)\n",
    "    std = torch.sqrt(variance)\n",
    "    \n",
    "    # Convert back to Python scalars\n",
    "    final_mean = mean.item()\n",
    "    final_std = std.item()\n",
    "    \n",
    "    # Final cleanup\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return final_mean, final_std\n",
    "\n",
    "def compute_dataset_statistics_chunked(dataset, model_input_name, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Alternative approach: Process dataset in chunks without DataLoader for maximum memory efficiency.\n",
    "    Use this if the DataLoader approach still has issues.\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset in chunks of {chunk_size}\")\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    sum_dtype = torch.float64\n",
    "    \n",
    "    total_sum = torch.tensor(0.0, dtype=sum_dtype, device=device)\n",
    "    total_squared_sum = torch.tensor(0.0, dtype=sum_dtype, device=device)\n",
    "    total_count = 0\n",
    "    \n",
    "    dataset_size = len(dataset)\n",
    "    num_chunks = (dataset_size + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for chunk_idx in tqdm(range(num_chunks), desc=\"Processing chunks\"):\n",
    "            start_idx = chunk_idx * chunk_size\n",
    "            end_idx = min(start_idx + chunk_size, dataset_size)\n",
    "            \n",
    "            # Process items in current chunk\n",
    "            chunk_tensors = []\n",
    "            for idx in range(start_idx, end_idx):\n",
    "                item = dataset[idx][model_input_name]\n",
    "                if not isinstance(item, torch.Tensor):\n",
    "                    item = torch.tensor(item, dtype=torch.float32)\n",
    "                chunk_tensors.append(item.flatten())\n",
    "            \n",
    "            # Concatenate chunk data\n",
    "            if chunk_tensors:\n",
    "                chunk_data = torch.cat(chunk_tensors).to(device)\n",
    "                \n",
    "                # Compute statistics\n",
    "                chunk_sum = chunk_data.sum(dtype=sum_dtype)\n",
    "                chunk_squared_sum = chunk_data.pow(2).sum(dtype=sum_dtype)\n",
    "                chunk_count = chunk_data.numel()\n",
    "                \n",
    "                total_sum += chunk_sum\n",
    "                total_squared_sum += chunk_squared_sum\n",
    "                total_count += chunk_count\n",
    "                \n",
    "                # Cleanup\n",
    "                del chunk_data, chunk_tensors\n",
    "                if device == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    mean = total_sum / total_count\n",
    "    variance = (total_squared_sum / total_count) - (mean * mean)\n",
    "    variance = torch.clamp(variance, min=0.0)\n",
    "    std = torch.sqrt(variance)\n",
    "    \n",
    "    return mean.item(), std.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6897e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Optimized DataLoader approach (recommended)\n",
    "try:\n",
    "    mean, std = compute_dataset_statistics_optimized(\n",
    "        dataset=dataset[\"train\"],\n",
    "        model_input_name=model_input_name,\n",
    "        batch_size=256,  # Start with this, increase if you have more memory\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    print(f'Optimized - Mean: {mean}, Std: {std}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"DataLoader approach failed: {e}\")\n",
    "    print(\"Trying chunked approach...\")\n",
    "    \n",
    "    # Method 2: Fallback chunked approach\n",
    "    mean, std = compute_dataset_statistics_chunked(\n",
    "        dataset=dataset[\"train\"],\n",
    "        model_input_name=model_input_name,\n",
    "        chunk_size=500\n",
    "    )\n",
    "    print(f'Chunked - Mean: {mean}, Std: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af852b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Might have to change this\n",
    "\n",
    "from audiomentations import Compose, AddGaussianSNR, GainTransition, Gain, ClippingDistortion, TimeStretch, PitchShift\n",
    "\n",
    "audio_augmentations = Compose([\n",
    "    AddGaussianSNR(min_snr_db=10, max_snr_db=20),\n",
    "    Gain(min_gain_db=-6, max_gain_db=6),\n",
    "    GainTransition(min_gain_db=-6, max_gain_db=6, min_duration=0.01, max_duration=0.3, duration_unit=\"fraction\"),\n",
    "    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=30, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4),\n",
    "], p=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio_with_transforms(batch):\n",
    "    # we apply augmentations on each waveform\n",
    "    wavs = [audio_augmentations(audio[\"array\"], sample_rate=SAMPLING_RATE) for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\", return_attention_mask=True, max_length=507)\n",
    "    return {\n",
    "        model_input_name: inputs.get(model_input_name),\n",
    "        \"labels\": torch.tensor(batch[\"label\"])\n",
    "    }\n",
    "\n",
    "print('dataset train features:', dataset['train'].features)\n",
    "print('dataset test features:', dataset['test'].features)\n",
    "print('dataset validation features:', dataset['validation'].features)\n",
    "# Cast the audio column to the appropriate feature type and rename it\n",
    "dataset = dataset.cast_column(\"input_values\", Audio(sampling_rate=feature_extractor.sampling_rate))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88736686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with augmentations on the training set\n",
    "dataset[\"train\"].set_transform(preprocess_audio_with_transforms, output_all_columns=False)\n",
    "# w/o augmentations on the test set\n",
    "dataset[\"test\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "dataset[\"validation\"].set_transform(preprocess_audio, output_all_columns=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd71c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ds = dataset['train'].with_format(None)  # This removes any applied transforms\n",
    "temp_ds_test = dataset['test'].with_format(None)  # This removes any applied transforms\n",
    "\n",
    "print(temp_ds[0])\n",
    "unique_labels = sorted(set(temp_ds[\"label\"] + temp_ds_test[\"label\"]))\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb51aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ASTConfig, ASTForAudioClassification\n",
    "\n",
    "# Load configuration from the pretrained model\n",
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "\n",
    "# Update configuration with the number of labels in our dataset\n",
    "config.num_mel_bins = NUM_MEL_BINS  # Make sure this matches your feature extractor\n",
    "config.max_length = MAX_SEQUENCE_LENGTH   # Or whatever your sequence length is\n",
    "config.num_labels = num_labels\n",
    "config.label2id = {\"fake\": 0, \"real\": 1}\n",
    "config.id2label = {0: \"fake\", 1: \"real\"}\n",
    "\n",
    "# setting dropout to prevent overfitting. based on https://www.mdpi.com/2073-431X/13/10/256\n",
    "config.hidden_dropout_prob = 0.10\n",
    "\n",
    "# having a patch size of 16, and time and frequency stride of 16, allows us to have NO \n",
    "# overlaps. this prevents difficulties if the file has real and fake audio \n",
    "# (based on https://arxiv.org/pdf/2409.05924)\n",
    "config.patch_size = 16\n",
    "config.frequency_stride = 16\n",
    "config.time_stride = 16\n",
    "\n",
    "# Initialize the model with the updated configuration\n",
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    pretrained_model,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a04121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "# weight decay formula based on https://arxiv.org/abs/1711.05101:\n",
    "    # weight decay = ynorm(sqr(batch_size/(dataset_size * num_train_epochs)))\n",
    "    # where ynorm is between 0.025 and 0.05 https://towardsdatascience.com/weight-decay-and-its-peculiar-effects-66e0aee3e7b8/\n",
    "\n",
    "DATASET_SIZE = 317_394\n",
    "YNORM = 0.025\n",
    "BATCH_SIZE = 8\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "WEIGHT_DECAY = YNORM * np.sqrt(BATCH_SIZE / (DATASET_SIZE * NUM_TRAIN_EPOCHS))\n",
    "\n",
    "print(\"Weight decay: \", WEIGHT_DECAY)\n",
    "\n",
    "# Configure training run with TrainingArguments class\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./runs/ast_classifier\",\n",
    "    logging_dir=\"./logs/ast_classifier\",\n",
    "    report_to=\"tensorboard\",\n",
    "    learning_rate=2e-5, # based on https://arxiv.org/pdf/2505.15136\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # Start with 8, can increase if memory allows\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=2,\n",
    "    dataloader_num_workers=0,  # Changed from 4 to 0 for Windows\n",
    "    dataloader_pin_memory=False,  # Disable pin_memory on Windows\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\", # based on https://www.mdpi.com/2073-431X/13/10/256 and https://arxiv.org/pdf/2505.15136\n",
    "    logging_steps=20,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    "    lr_scheduler_type=\"cosine\", # based on https://arxiv.org/pdf/2505.15136\n",
    "    weight_decay=WEIGHT_DECAY, # based on https://arxiv.org/abs/1711.05101 and https://towardsdatascience.com/weight-decay-and-its-peculiar-effects-66e0aee3e7b8/\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "AVERAGE = \"macro\" if config.num_labels > 2 else \"binary\"\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    metrics = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "    metrics.update(precision.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8de353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorWithPadding\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize the data collator\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=feature_extractor,  # Your feature extractor acts as the tokenizer\n",
    "    padding=True,\n",
    "    max_length=config.max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    "    data_collator=data_collator,\n",
    "    # if no improvements happened to validation loss within 5 epochs, stop training.\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.00)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac110b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this before training to verify shapes\n",
    "sample = next(iter(dataset[\"train\"]))\n",
    "print(f\"Sample input shape: {sample['input_values'].shape}\")\n",
    "\n",
    "# Forward pass to check for errors\n",
    "with torch.no_grad():\n",
    "    output = model(sample[\"input_values\"].unsqueeze(0))\n",
    "print(\"Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300ab2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model config: {model.config}\")\n",
    "print(f\"Sample input shape: {sample['input_values'].shape}\")\n",
    "print(f\"Model's expected input shape: {model.config.max_length}\")\n",
    "print(f\"Feature extractor config: {feature_extractor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0103e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "print(f\"Using device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "     print(f\"GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76028015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.serialization import safe_globals\n",
    "import numpy as np\n",
    "from accelerate.utils import get_grad_scaler\n",
    "\n",
    "# Get the correct scaler type based on your hardware\n",
    "scaler = get_grad_scaler()\n",
    "\n",
    "CHECKPOINT_NUM = 63480\n",
    "# Load the scaler state dict\n",
    "with safe_globals([np.core.multiarray.scalar, np.dtype, np.dtypes.Float64DType]):\n",
    "    try:\n",
    "        scaler_state = torch.load(fr\"runs\\ast_classifier\\checkpoint-{CHECKPOINT_NUM}\\scaler.pt\", weights_only=True)\n",
    "        print(\"Scaler state type:\", type(scaler_state))\n",
    "        print(\"Scaler state contents:\", scaler_state)\n",
    "        print(\"Trainer accelerator scaler: \", trainer.accelerator.scaler)\n",
    "        if not trainer.accelerator.scaler:\n",
    "            print(\"Trainer accelerator scaler is None. Setting up the scaler\")\n",
    "            scaler.load_state_dict(scaler_state)\n",
    "            trainer.accelerator.scaler = scaler\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading scaler: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b32f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.serialization.safe_globals([np.core.multiarray.scalar, np.dtype, np.dtypes.Float64DType]):\n",
    "    trainer.train(resume_from_checkpoint=r\"runs\\ast_classifier\\checkpoint-63480\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b94859",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "     print(f\"GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODEL EVALUATION AND PREDICTION\n",
    "\n",
    "import os\n",
    "import json\n",
    "from transformers import ASTForAudioClassification\n",
    "\n",
    "# Define the checkpoint path\n",
    "checkpoint_path = \"runs/ast_classifier/checkpoint-158700\"\n",
    "\n",
    "# Load the model and feature extractor\n",
    "model = ASTForAudioClassification.from_pretrained(checkpoint_path)\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Look for training history\n",
    "trainer_state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "if os.path.exists(trainer_state_path):\n",
    "    with open(trainer_state_path, \"r\") as f:\n",
    "        trainer_state = json.load(f)\n",
    "    print(\"\\nTraining metrics from trainer_state.json:\")\n",
    "    print(json.dumps(trainer_state, indent=2))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "import librosa\n",
    "def predict_audio(file_path, model, feature_extractor, device=\"cuda\"):\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(file_path, sr=feature_extractor.sampling_rate)\n",
    "\n",
    "    # Preprocess the audio\n",
    "    inputs = feature_extractor(\n",
    "        audio,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    print(f\"Raw logits: {logits}\")\n",
    "    print(f\"Raw probabilities: {probabilities}\")\n",
    "\n",
    "    # Get predicted class (0 for fake, 1 for real)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "    print(f\"P(fake): {probabilities[0][0].item():.4f}\")\n",
    "    print(f\"P(real): {probabilities[0][1].item():.4f}\")\n",
    "\n",
    "    # Map class index to label\n",
    "    label = \"fake\" if predicted_class == 0 else \"real\"\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"confidence\": confidence,\n",
    "        \"probabilities\": {\n",
    "            \"fake\": probabilities[0][0].item(),\n",
    "            \"real\": probabilities[0][1].item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "audio_file_path = \"\" # Adjust the path as needed\n",
    "result = predict_audio(audio_file_path, model, feature_extractor, device)\n",
    "\n",
    "print(f\"Prediction: {result['label']}\")\n",
    "print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "print(f\"Probabilities - Fake: {result['probabilities']['fake']:.4f}, Real: {result['probabilities']['real']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835cc84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### AUDIO CUTTING SCRIPT\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def cut_audio(input_path, output_dir=\"./cut\", clip_duration=5):\n",
    "     # Ensure output directory exists\n",
    "     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "     # Get file name and extension\n",
    "     base_name = os.path.basename(input_path)\n",
    "     name, ext = os.path.splitext(base_name)\n",
    "\n",
    "     # Load audio\n",
    "     audio, sr = librosa.load(input_path, sr=None)\n",
    "     total_duration = librosa.get_duration(y=audio, sr=sr)\n",
    "     clip_samples = int(clip_duration * sr)\n",
    "     num_clips = int(total_duration // clip_duration) + (1 if total_duration % clip_duration > 0 else 0)\n",
    "\n",
    "     for i in range(num_clips):\n",
    "          start_sample = i * clip_samples\n",
    "          end_sample = min((i + 1) * clip_samples, len(audio))\n",
    "          clip_audio = audio[start_sample:end_sample]\n",
    "          out_path = os.path.join(output_dir, f\"{name}({i+1}){ext}\")\n",
    "          sf.write(out_path, clip_audio, sr)\n",
    "          print(f\"Saved: {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     if len(sys.argv) < 2:\n",
    "          print(\"Usage: python cut_audio.py <path_to_audio_file>\")\n",
    "          sys.exit(1)\n",
    "     audio_path = \"\"  # Adjust the path as needed\n",
    "     if not os.path.exists(audio_path):\n",
    "          print(f\"File not found: {audio_path}\")\n",
    "          sys.exit(1)\n",
    "     cut_audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SEGMENTED AUDIO PREDICTION SCRIPT\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Directory containing segmented audio clips\n",
    "segmented_dir = \"./cut\"\n",
    "\n",
    "# Collect all audio files and group by base name (without segment index)\n",
    "audio_groups = defaultdict(list)\n",
    "for fname in os.listdir(segmented_dir):\n",
    "     if fname.lower().endswith(('.wav', '.mp3', '.flac', '.ogg')):\n",
    "          # Extract base name (e.g., \"audio(1).wav\" -> \"audio\")\n",
    "          base = fname.split('(')[0]\n",
    "          audio_groups[base].append(os.path.join(segmented_dir, fname))\n",
    "\n",
    "# Function to predict for a single audio file\n",
    "def predict_single(file_path):\n",
    "     return predict_audio(file_path, model, feature_extractor, device)\n",
    "\n",
    "# Aggregate predictions for each group\n",
    "results = {}\n",
    "for base, files in audio_groups.items():\n",
    "     fake_count = 0\n",
    "     real_count = 0\n",
    "     for fpath in files:\n",
    "          pred = predict_single(fpath)\n",
    "          if pred[\"label\"] == \"fake\":\n",
    "               fake_count += 1\n",
    "          else:\n",
    "               real_count += 1\n",
    "     total = fake_count + real_count\n",
    "     results[base] = {\n",
    "          \"fake_ratio\": fake_count / total if total > 0 else 0,\n",
    "          \"real_ratio\": real_count / total if total > 0 else 0,\n",
    "          \"fake_count\": fake_count,\n",
    "          \"real_count\": real_count,\n",
    "          \"total\": total\n",
    "     }\n",
    "\n",
    "# Print summary\n",
    "for base, stats in results.items():\n",
    "     print(f\"{base}: Fake {stats['fake_count']}/{stats['total']} ({stats['fake_ratio']:.2f}), \"\n",
    "            f\"Real {stats['real_count']}/{stats['total']} ({stats['real_ratio']:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
