{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27471c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import ASTFeatureExtractor\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.profiler\n",
    "import numpy as np\n",
    "from datasets import Audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3821aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.7\n",
      "System: Windows 11\n",
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "CUDA device count: 1\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"System: {platform.system()} {platform.release()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ac471d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f220dd453e649bdb48420a350bdd411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/253906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d62ca36bc7466cb9aa27f6d794f59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31738 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5038ea126b364d82b32f3b2b7713ca5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31742 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65190e0a215b49a2ba9501f95b8cfc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/253906 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec307ff59984473fae177fb035ce12a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing checksums:  72%|#######1  | 182069/253906 [00:05<00:01, 36412.44it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5183cd2ad8544adeadd624e03e33c20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/31738 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74799abd4152492295ca56836543c6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/31742 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4504e75ba84166b31f0813b0e47843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738082e605384cf9b9b484473a8cbdb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c80a09f1614f54a3770bd5295965c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"audiofolder\", data_dir=\"./dataset\")\n",
    "\n",
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model, num_mel_bins=64, max_length=507)\n",
    "\n",
    "\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "SAMPLING_RATE = feature_extractor.sampling_rate\n",
    "num_labels = len(np.unique(dataset[\"train\"][\"label\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99f317f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 253906\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 31738\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 31742\n",
      "    })\n",
      "})\n",
      "dataset train features: {'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "dataset test features: {'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "dataset validation features: {'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "model_input_name: input_values\n",
      "SAMPLING_RATE: 16000\n",
      "dataset[\"train\"][0]: {'audio': {'path': 'D:\\\\Documents\\\\Github\\\\thesis-testing\\\\dataset\\\\train\\\\fake\\\\0.wav', 'array': array([ 8.55924794e-04,  5.84704467e-05,  7.75483320e-04, ...,\n",
      "       -1.02691360e-04, -2.82649242e-04,  0.00000000e+00]), 'sampling_rate': 16000}, 'label': 0}\n",
      "num_labels: 2\n",
      "dataset columns: ['audio', 'label']\n"
     ]
    }
   ],
   "source": [
    "print('dataset:', dataset)\n",
    "print('dataset train features:', dataset['train'].features)\n",
    "print('dataset test features:', dataset['test'].features)\n",
    "print('dataset validation features:', dataset['validation'].features)\n",
    "print('model_input_name:', model_input_name)\n",
    "print('SAMPLING_RATE:', SAMPLING_RATE)\n",
    "print('dataset[\"train\"][0]:', dataset['train'][0])\n",
    "print('num_labels:', num_labels)\n",
    "print('dataset columns:', dataset['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18162b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  -7.9208784103393555\n",
      "std:  5.350068092346191\n"
     ]
    }
   ],
   "source": [
    "# calculate values for normalization\n",
    "feature_extractor.do_normalize = False\n",
    "\n",
    "# Initialize running statistics\n",
    "n = 0\n",
    "mean = 0.0\n",
    "M2 = 0.0  # For running variance calculation\n",
    "\n",
    "def preprocess_audio(batch):\n",
    "    wavs = [audio[\"array\"] for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\", return_attention_mask=True, max_length=507)\n",
    "    return {\n",
    "        model_input_name: inputs.get(model_input_name),\n",
    "        \"labels\": torch.tensor(batch[\"label\"])\n",
    "    }\n",
    "\n",
    "dataset = dataset.rename_column(\"audio\", \"input_values\")\n",
    "dataset[\"train\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "\n",
    "# Process in batches to save memory\n",
    "for batch in dataset[\"train\"]:\n",
    "\n",
    "    audio_input = batch[model_input_name]\n",
    "    batch_size = audio_input.shape[0]\n",
    "\n",
    "    # Calculate batch statistics\n",
    "    batch_mean = torch.mean(audio_input)\n",
    "    batch_variance = torch.var(audio_input, unbiased=False)  # Use N instead of N-1 for population variance\n",
    "\n",
    "    # Update running statistics using Welford's online algorithm\n",
    "    delta = batch_mean - mean\n",
    "    mean += delta * batch_size / (n + batch_size)\n",
    "    M2 += batch_variance * batch_size + delta ** 2 * n * batch_size / (n + batch_size)\n",
    "    n += batch_size\n",
    "\n",
    "# Calculate final statistics\n",
    "feature_extractor.mean = mean.item()\n",
    "feature_extractor.std = torch.sqrt(M2 / n).item()  # Population standard deviation\n",
    "feature_extractor.do_normalize = True\n",
    "\n",
    "print('mean: ', feature_extractor.mean)\n",
    "print('std: ', feature_extractor.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af852b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianSNR, GainTransition, Gain, ClippingDistortion, TimeStretch, PitchShift\n",
    "\n",
    "audio_augmentations = Compose([\n",
    "    AddGaussianSNR(min_snr_db=10, max_snr_db=20),\n",
    "    Gain(min_gain_db=-6, max_gain_db=6),\n",
    "    GainTransition(min_gain_db=-6, max_gain_db=6, min_duration=0.01, max_duration=0.3, duration_unit=\"fraction\"),\n",
    "    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=30, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4),\n",
    "], p=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd7f4b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_values', 'label'],\n",
      "        num_rows: 253906\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_values', 'label'],\n",
      "        num_rows: 31738\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_values', 'label'],\n",
      "        num_rows: 31742\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def preprocess_audio_with_transforms(batch):\n",
    "    # we apply augmentations on each waveform\n",
    "    wavs = [audio_augmentations(audio[\"array\"], sample_rate=SAMPLING_RATE) for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\", return_attention_mask=True, max_length=507)\n",
    "    return {\n",
    "        model_input_name: inputs.get(model_input_name),\n",
    "        \"labels\": torch.tensor(batch[\"label\"])\n",
    "    }\n",
    "\n",
    "print(dataset)\n",
    "# Cast the audio column to the appropriate feature type and rename it\n",
    "dataset = dataset.cast_column(\"input_values\", Audio(sampling_rate=feature_extractor.sampling_rate))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88736686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with augmentations on the training set\n",
    "dataset[\"train\"].set_transform(preprocess_audio_with_transforms, output_all_columns=False)\n",
    "# w/o augmentations on the test set\n",
    "dataset[\"test\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "dataset[\"validation\"].set_transform(preprocess_audio, output_all_columns=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dd71c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_values': {'path': 'D:\\\\Documents\\\\Github\\\\thesis-testing\\\\dataset\\\\train\\\\fake\\\\0.wav', 'array': array([ 8.55924794e-04,  5.84704467e-05,  7.75483320e-04, ...,\n",
      "       -1.02691360e-04, -2.82649242e-04,  0.00000000e+00]), 'sampling_rate': 16000}, 'label': 0}\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "temp_ds = dataset['train'].with_format(None)  # This removes any applied transforms\n",
    "temp_ds_test = dataset['test'].with_format(None)  # This removes any applied transforms\n",
    "\n",
    "print(temp_ds[0])\n",
    "unique_labels = sorted(set(temp_ds[\"label\"] + temp_ds_test[\"label\"]))\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feb51aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- audio_spectrogram_transformer.embeddings.position_embeddings: found shape torch.Size([1, 1214, 768]) in the checkpoint and torch.Size([1, 252, 768]) in the model instantiated\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ASTConfig, ASTForAudioClassification\n",
    "\n",
    "# Load configuration from the pretrained model\n",
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "\n",
    "# Update configuration with the number of labels in our dataset\n",
    "config.num_mel_bins = 64  # Make sure this matches your feature extractor\n",
    "config.max_length = 507   # Or whatever your sequence length is\n",
    "config.num_labels = num_labels\n",
    "config.label2id = {\"fake\": 0, \"real\": 1}\n",
    "config.id2label = {0: \"fake\", 1: \"real\"}\n",
    "\n",
    "# Initialize the model with the updated configuration\n",
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    pretrained_model,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59a04121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Configure training run with TrainingArguments class\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./runs/ast_classifier\",\n",
    "    logging_dir=\"./logs/ast_classifier\",\n",
    "    report_to=\"tensorboard\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,  # Start with 8, can increase if memory allows\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    dataloader_num_workers=0,  # Changed from 4 to 0 for Windows\n",
    "    dataloader_pin_memory=False,  # Disable pin_memory on Windows\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_steps=20,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d070a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "AVERAGE = \"macro\" if config.num_labels > 2 else \"binary\"\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    metrics = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "    metrics.update(precision.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a8de353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kace\\AppData\\Local\\Temp\\ipykernel_14296\\3091343712.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, DataCollatorWithPadding\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize the data collator\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=feature_extractor,  # Your feature extractor acts as the tokenizer\n",
    "    padding=True,\n",
    "    max_length=config.max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aac110b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\Github\\thesis-testing\\venv\\Lib\\site-packages\\audiomentations\\core\\transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input shape: torch.Size([507, 64])\n",
      "Forward pass successful!\n"
     ]
    }
   ],
   "source": [
    "# Add this before training to verify shapes\n",
    "sample = next(iter(dataset[\"train\"]))\n",
    "print(f\"Sample input shape: {sample['input_values'].shape}\")\n",
    "\n",
    "# Forward pass to check for errors\n",
    "with torch.no_grad():\n",
    "    output = model(sample[\"input_values\"].unsqueeze(0))\n",
    "print(\"Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "300ab2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config: ASTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ASTForAudioClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"frequency_stride\": 10,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"fake\",\n",
      "    \"1\": \"real\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"fake\": 0,\n",
      "    \"real\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_length\": 507,\n",
      "  \"model_type\": \"audio-spectrogram-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_mel_bins\": 64,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"time_stride\": 10,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.1\"\n",
      "}\n",
      "\n",
      "Sample input shape: torch.Size([507, 64])\n",
      "Model's expected input shape: 507\n",
      "Feature extractor config: ASTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"ASTFeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"max_length\": 507,\n",
      "  \"mean\": -7.9208784103393555,\n",
      "  \"num_mel_bins\": 64,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000,\n",
      "  \"std\": 5.350068092346191\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model config: {model.config}\")\n",
    "print(f\"Sample input shape: {sample['input_values'].shape}\")\n",
    "print(f\"Model's expected input shape: {model.config.max_length}\")\n",
    "print(f\"Feature extractor config: {feature_extractor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0103e3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "GPU Memory Total: 6.44 GB\n",
      "GPU Memory Allocated: 0.00 GB\n",
      "GPU Memory Cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "print(f\"Using device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "     print(f\"GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b32f0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158700' max='158700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158700/158700 29:43:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.070400</td>\n",
       "      <td>0.017584</td>\n",
       "      <td>0.994739</td>\n",
       "      <td>0.990321</td>\n",
       "      <td>0.999244</td>\n",
       "      <td>0.994762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>0.004508</td>\n",
       "      <td>0.998834</td>\n",
       "      <td>0.998740</td>\n",
       "      <td>0.998929</td>\n",
       "      <td>0.998834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.998110</td>\n",
       "      <td>0.996795</td>\n",
       "      <td>0.999433</td>\n",
       "      <td>0.998112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.999181</td>\n",
       "      <td>0.998615</td>\n",
       "      <td>0.999748</td>\n",
       "      <td>0.999181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.999401</td>\n",
       "      <td>0.999559</td>\n",
       "      <td>0.999244</td>\n",
       "      <td>0.999401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>0.999401</td>\n",
       "      <td>0.999559</td>\n",
       "      <td>0.999244</td>\n",
       "      <td>0.999401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.999779</td>\n",
       "      <td>0.999748</td>\n",
       "      <td>0.999811</td>\n",
       "      <td>0.999779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>0.999370</td>\n",
       "      <td>0.999622</td>\n",
       "      <td>0.999118</td>\n",
       "      <td>0.999370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.999779</td>\n",
       "      <td>0.999748</td>\n",
       "      <td>0.999811</td>\n",
       "      <td>0.999779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.999874</td>\n",
       "      <td>0.999811</td>\n",
       "      <td>0.999937</td>\n",
       "      <td>0.999874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "d:\\Documents\\Github\\thesis-testing\\venv\\Lib\\site-packages\\audiomentations\\core\\transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "d:\\Documents\\Github\\thesis-testing\\venv\\Lib\\site-packages\\audiomentations\\core\\transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "d:\\Documents\\Github\\thesis-testing\\venv\\Lib\\site-packages\\audiomentations\\core\\transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "NaN or Inf found in input tensor.\n",
      "d:\\Documents\\Github\\thesis-testing\\venv\\Lib\\site-packages\\audiomentations\\core\\transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "d:\\Documents\\Github\\thesis-testing\\venv\\Lib\\site-packages\\audiomentations\\core\\transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "NaN or Inf found in input tensor.\n",
      "d:\\Documents\\Github\\thesis-testing\\venv\\Lib\\site-packages\\audiomentations\\core\\transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "d:\\Documents\\Github\\thesis-testing\\venv\\Lib\\site-packages\\audiomentations\\core\\transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "NaN or Inf found in input tensor.\n",
      "d:\\Documents\\Github\\thesis-testing\\venv\\Lib\\site-packages\\audiomentations\\core\\transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "d:\\Documents\\Github\\thesis-testing\\venv\\Lib\\site-packages\\audiomentations\\core\\transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=158700, training_loss=0.02415744743147359, metrics={'train_runtime': 107030.1665, 'train_samples_per_second': 23.723, 'train_steps_per_second': 1.483, 'total_flos': 4.2240782021685215e+19, 'train_loss': 0.02415744743147359, 'epoch': 10.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=False) #Set to False when no checkpoint is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03b94859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Total: 6.44 GB\n",
      "GPU Memory Allocated: 1.06 GB\n",
      "GPU Memory Cached: 2.63 GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "     print(f\"GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "624e63f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training metrics from trainer_state.json:\n",
      "{\n",
      "  \"best_global_step\": 158700,\n",
      "  \"best_metric\": 0.9998739839959675,\n",
      "  \"best_model_checkpoint\": \"./runs/ast_classifier\\\\checkpoint-158700\",\n",
      "  \"epoch\": 10.0,\n",
      "  \"eval_steps\": 500,\n",
      "  \"global_step\": 158700,\n",
      "  \"is_hyper_param_search\": false,\n",
      "  \"is_local_process_zero\": true,\n",
      "  \"is_world_process_zero\": true,\n",
      "  \"log_history\": [\n",
      "    {\n",
      "      \"epoch\": 0.0012602791518321309,\n",
      "      \"grad_norm\": 6.989597320556641,\n",
      "      \"learning_rate\": 4.9994328922495275e-05,\n",
      "      \"loss\": 0.876,\n",
      "      \"step\": 20\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.0025205583036642617,\n",
      "      \"grad_norm\": 8.377490043640137,\n",
      "      \"learning_rate\": 4.9988027725267804e-05,\n",
      "      \"loss\": 0.3318,\n",
      "      \"step\": 40\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.0037808374554963923,\n",
      "      \"grad_norm\": 5.075488090515137,\n",
      "      \"learning_rate\": 4.9981726528040326e-05,\n",
      "      \"loss\": 0.2956,\n",
      "      \"step\": 60\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.005041116607328523,\n",
      "      \"grad_norm\": 5.285743236541748,\n",
      "      \"learning_rate\": 4.9975425330812855e-05,\n",
      "      \"loss\": 0.241,\n",
      "      \"step\": 80\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.006301395759160654,\n",
      "      \"grad_norm\": 7.702948093414307,\n",
      "      \"learning_rate\": 4.9969124133585385e-05,\n",
      "      \"loss\": 0.2624,\n",
      "      \"step\": 100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.007561674910992785,\n",
      "      \"grad_norm\": 6.596869468688965,\n",
      "      \"learning_rate\": 4.9962822936357914e-05,\n",
      "      \"loss\": 0.314,\n",
      "      \"step\": 120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.008821954062824915,\n",
      "      \"grad_norm\": 15.443524360656738,\n",
      "      \"learning_rate\": 4.9956521739130436e-05,\n",
      "      \"loss\": 0.2968,\n",
      "      \"step\": 140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.010082233214657047,\n",
      "      \"grad_norm\": 8.308904647827148,\n",
      "      \"learning_rate\": 4.9950220541902965e-05,\n",
      "      \"loss\": 0.2266,\n",
      "      \"step\": 160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.011342512366489177,\n",
      "      \"grad_norm\": 3.349510908126831,\n",
      "      \"learning_rate\": 4.994391934467549e-05,\n",
      "      \"loss\": 0.1855,\n",
      "      \"step\": 180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.012602791518321308,\n",
      "      \"grad_norm\": 6.7464070320129395,\n",
      "      \"learning_rate\": 4.9937618147448016e-05,\n",
      "      \"loss\": 0.1803,\n",
      "      \"step\": 200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.01386307067015344,\n",
      "      \"grad_norm\": 3.4514272212982178,\n",
      "      \"learning_rate\": 4.9931316950220545e-05,\n",
      "      \"loss\": 0.237,\n",
      "      \"step\": 220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.01512334982198557,\n",
      "      \"grad_norm\": 0.6459407210350037,\n",
      "      \"learning_rate\": 4.992501575299307e-05,\n",
      "      \"loss\": 0.1398,\n",
      "      \"step\": 240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.0163836289738177,\n",
      "      \"grad_norm\": 8.305119514465332,\n",
      "      \"learning_rate\": 4.99187145557656e-05,\n",
      "      \"loss\": 0.2348,\n",
      "      \"step\": 260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.01764390812564983,\n",
      "      \"grad_norm\": 4.1274237632751465,\n",
      "      \"learning_rate\": 4.99127284183995e-05,\n",
      "      \"loss\": 0.2163,\n",
      "      \"step\": 280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.018904187277481964,\n",
      "      \"grad_norm\": 7.720673084259033,\n",
      "      \"learning_rate\": 4.9906427221172026e-05,\n",
      "      \"loss\": 0.1509,\n",
      "      \"step\": 300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.020164466429314094,\n",
      "      \"grad_norm\": 2.0075721740722656,\n",
      "      \"learning_rate\": 4.9900126023944555e-05,\n",
      "      \"loss\": 0.2224,\n",
      "      \"step\": 320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.021424745581146223,\n",
      "      \"grad_norm\": 8.844006538391113,\n",
      "      \"learning_rate\": 4.989382482671708e-05,\n",
      "      \"loss\": 0.1721,\n",
      "      \"step\": 340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.022685024732978353,\n",
      "      \"grad_norm\": 15.150880813598633,\n",
      "      \"learning_rate\": 4.988752362948961e-05,\n",
      "      \"loss\": 0.1947,\n",
      "      \"step\": 360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.023945303884810486,\n",
      "      \"grad_norm\": 9.433160781860352,\n",
      "      \"learning_rate\": 4.988122243226213e-05,\n",
      "      \"loss\": 0.1144,\n",
      "      \"step\": 380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.025205583036642616,\n",
      "      \"grad_norm\": 11.340153694152832,\n",
      "      \"learning_rate\": 4.987492123503466e-05,\n",
      "      \"loss\": 0.1905,\n",
      "      \"step\": 400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.026465862188474746,\n",
      "      \"grad_norm\": 6.826876163482666,\n",
      "      \"learning_rate\": 4.986862003780719e-05,\n",
      "      \"loss\": 0.1578,\n",
      "      \"step\": 420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.02772614134030688,\n",
      "      \"grad_norm\": 5.8792548179626465,\n",
      "      \"learning_rate\": 4.9862318840579716e-05,\n",
      "      \"loss\": 0.2176,\n",
      "      \"step\": 440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.02898642049213901,\n",
      "      \"grad_norm\": 6.929354190826416,\n",
      "      \"learning_rate\": 4.985601764335224e-05,\n",
      "      \"loss\": 0.1827,\n",
      "      \"step\": 460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.03024669964397114,\n",
      "      \"grad_norm\": 15.013358116149902,\n",
      "      \"learning_rate\": 4.984971644612477e-05,\n",
      "      \"loss\": 0.2186,\n",
      "      \"step\": 480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.03150697879580327,\n",
      "      \"grad_norm\": 0.21512790024280548,\n",
      "      \"learning_rate\": 4.984341524889729e-05,\n",
      "      \"loss\": 0.1394,\n",
      "      \"step\": 500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.0327672579476354,\n",
      "      \"grad_norm\": 2.0837864875793457,\n",
      "      \"learning_rate\": 4.983711405166982e-05,\n",
      "      \"loss\": 0.2079,\n",
      "      \"step\": 520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.034027537099467535,\n",
      "      \"grad_norm\": 3.964289665222168,\n",
      "      \"learning_rate\": 4.983081285444235e-05,\n",
      "      \"loss\": 0.138,\n",
      "      \"step\": 540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.03528781625129966,\n",
      "      \"grad_norm\": 0.5255091786384583,\n",
      "      \"learning_rate\": 4.982451165721487e-05,\n",
      "      \"loss\": 0.2078,\n",
      "      \"step\": 560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.036548095403131795,\n",
      "      \"grad_norm\": 2.970526933670044,\n",
      "      \"learning_rate\": 4.98182104599874e-05,\n",
      "      \"loss\": 0.1499,\n",
      "      \"step\": 580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.03780837455496393,\n",
      "      \"grad_norm\": 0.49460670351982117,\n",
      "      \"learning_rate\": 4.981190926275993e-05,\n",
      "      \"loss\": 0.129,\n",
      "      \"step\": 600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.039068653706796054,\n",
      "      \"grad_norm\": 0.2887594997882843,\n",
      "      \"learning_rate\": 4.980560806553246e-05,\n",
      "      \"loss\": 0.1137,\n",
      "      \"step\": 620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.04032893285862819,\n",
      "      \"grad_norm\": 1.452006220817566,\n",
      "      \"learning_rate\": 4.979962192816636e-05,\n",
      "      \"loss\": 0.2342,\n",
      "      \"step\": 640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.041589212010460314,\n",
      "      \"grad_norm\": 17.370786666870117,\n",
      "      \"learning_rate\": 4.979332073093888e-05,\n",
      "      \"loss\": 0.183,\n",
      "      \"step\": 660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.04284949116229245,\n",
      "      \"grad_norm\": 2.181570053100586,\n",
      "      \"learning_rate\": 4.978701953371141e-05,\n",
      "      \"loss\": 0.1766,\n",
      "      \"step\": 680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.04410977031412458,\n",
      "      \"grad_norm\": 2.349905014038086,\n",
      "      \"learning_rate\": 4.978071833648393e-05,\n",
      "      \"loss\": 0.1256,\n",
      "      \"step\": 700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.045370049465956706,\n",
      "      \"grad_norm\": 10.130622863769531,\n",
      "      \"learning_rate\": 4.977441713925646e-05,\n",
      "      \"loss\": 0.1339,\n",
      "      \"step\": 720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.04663032861778884,\n",
      "      \"grad_norm\": 2.881906270980835,\n",
      "      \"learning_rate\": 4.976811594202899e-05,\n",
      "      \"loss\": 0.0878,\n",
      "      \"step\": 740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.04789060776962097,\n",
      "      \"grad_norm\": 0.1317240595817566,\n",
      "      \"learning_rate\": 4.976181474480152e-05,\n",
      "      \"loss\": 0.1578,\n",
      "      \"step\": 760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.0491508869214531,\n",
      "      \"grad_norm\": 3.102935552597046,\n",
      "      \"learning_rate\": 4.975551354757404e-05,\n",
      "      \"loss\": 0.2321,\n",
      "      \"step\": 780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.05041116607328523,\n",
      "      \"grad_norm\": 6.004142761230469,\n",
      "      \"learning_rate\": 4.974921235034657e-05,\n",
      "      \"loss\": 0.1462,\n",
      "      \"step\": 800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.051671445225117366,\n",
      "      \"grad_norm\": 25.375802993774414,\n",
      "      \"learning_rate\": 4.974291115311909e-05,\n",
      "      \"loss\": 0.1657,\n",
      "      \"step\": 820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.05293172437694949,\n",
      "      \"grad_norm\": 2.289499521255493,\n",
      "      \"learning_rate\": 4.973660995589162e-05,\n",
      "      \"loss\": 0.1493,\n",
      "      \"step\": 840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.054192003528781625,\n",
      "      \"grad_norm\": 10.308645248413086,\n",
      "      \"learning_rate\": 4.9730308758664144e-05,\n",
      "      \"loss\": 0.1476,\n",
      "      \"step\": 860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.05545228268061376,\n",
      "      \"grad_norm\": 0.22552619874477386,\n",
      "      \"learning_rate\": 4.972400756143667e-05,\n",
      "      \"loss\": 0.1317,\n",
      "      \"step\": 880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.056712561832445885,\n",
      "      \"grad_norm\": 4.566976070404053,\n",
      "      \"learning_rate\": 4.97177063642092e-05,\n",
      "      \"loss\": 0.1368,\n",
      "      \"step\": 900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.05797284098427802,\n",
      "      \"grad_norm\": 6.290228366851807,\n",
      "      \"learning_rate\": 4.971140516698173e-05,\n",
      "      \"loss\": 0.1187,\n",
      "      \"step\": 920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.05923312013611015,\n",
      "      \"grad_norm\": 0.02109752967953682,\n",
      "      \"learning_rate\": 4.970510396975426e-05,\n",
      "      \"loss\": 0.1893,\n",
      "      \"step\": 940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.06049339928794228,\n",
      "      \"grad_norm\": 10.23307991027832,\n",
      "      \"learning_rate\": 4.969880277252678e-05,\n",
      "      \"loss\": 0.1512,\n",
      "      \"step\": 960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.06175367843977441,\n",
      "      \"grad_norm\": 1.7625316381454468,\n",
      "      \"learning_rate\": 4.969250157529931e-05,\n",
      "      \"loss\": 0.0871,\n",
      "      \"step\": 980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.06301395759160654,\n",
      "      \"grad_norm\": 1.4187413454055786,\n",
      "      \"learning_rate\": 4.968620037807183e-05,\n",
      "      \"loss\": 0.1224,\n",
      "      \"step\": 1000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.06427423674343867,\n",
      "      \"grad_norm\": 0.9880316853523254,\n",
      "      \"learning_rate\": 4.967989918084436e-05,\n",
      "      \"loss\": 0.1918,\n",
      "      \"step\": 1020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.0655345158952708,\n",
      "      \"grad_norm\": 7.1914143562316895,\n",
      "      \"learning_rate\": 4.9673597983616885e-05,\n",
      "      \"loss\": 0.1343,\n",
      "      \"step\": 1040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.06679479504710294,\n",
      "      \"grad_norm\": 10.46216106414795,\n",
      "      \"learning_rate\": 4.9667296786389414e-05,\n",
      "      \"loss\": 0.1123,\n",
      "      \"step\": 1060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.06805507419893507,\n",
      "      \"grad_norm\": 0.11846970021724701,\n",
      "      \"learning_rate\": 4.966099558916194e-05,\n",
      "      \"loss\": 0.0815,\n",
      "      \"step\": 1080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.06931535335076719,\n",
      "      \"grad_norm\": 5.556836128234863,\n",
      "      \"learning_rate\": 4.965469439193447e-05,\n",
      "      \"loss\": 0.1691,\n",
      "      \"step\": 1100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.07057563250259932,\n",
      "      \"grad_norm\": 5.078402042388916,\n",
      "      \"learning_rate\": 4.9648393194707e-05,\n",
      "      \"loss\": 0.1701,\n",
      "      \"step\": 1120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.07183591165443146,\n",
      "      \"grad_norm\": 1.717098355293274,\n",
      "      \"learning_rate\": 4.964209199747952e-05,\n",
      "      \"loss\": 0.1062,\n",
      "      \"step\": 1140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.07309619080626359,\n",
      "      \"grad_norm\": 6.543384552001953,\n",
      "      \"learning_rate\": 4.963579080025205e-05,\n",
      "      \"loss\": 0.1332,\n",
      "      \"step\": 1160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.07435646995809572,\n",
      "      \"grad_norm\": 3.905756711959839,\n",
      "      \"learning_rate\": 4.9629489603024574e-05,\n",
      "      \"loss\": 0.156,\n",
      "      \"step\": 1180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.07561674910992786,\n",
      "      \"grad_norm\": 0.6382389664649963,\n",
      "      \"learning_rate\": 4.9623188405797103e-05,\n",
      "      \"loss\": 0.135,\n",
      "      \"step\": 1200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.07687702826175997,\n",
      "      \"grad_norm\": 16.810667037963867,\n",
      "      \"learning_rate\": 4.9616887208569626e-05,\n",
      "      \"loss\": 0.1569,\n",
      "      \"step\": 1220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.07813730741359211,\n",
      "      \"grad_norm\": 13.580602645874023,\n",
      "      \"learning_rate\": 4.961058601134216e-05,\n",
      "      \"loss\": 0.1719,\n",
      "      \"step\": 1240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.07939758656542424,\n",
      "      \"grad_norm\": 4.477736473083496,\n",
      "      \"learning_rate\": 4.9604284814114684e-05,\n",
      "      \"loss\": 0.0797,\n",
      "      \"step\": 1260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.08065786571725637,\n",
      "      \"grad_norm\": 1.508083462715149,\n",
      "      \"learning_rate\": 4.959798361688721e-05,\n",
      "      \"loss\": 0.0961,\n",
      "      \"step\": 1280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.08191814486908851,\n",
      "      \"grad_norm\": 0.2638581693172455,\n",
      "      \"learning_rate\": 4.9591682419659735e-05,\n",
      "      \"loss\": 0.1518,\n",
      "      \"step\": 1300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.08317842402092063,\n",
      "      \"grad_norm\": 17.83453941345215,\n",
      "      \"learning_rate\": 4.9585381222432264e-05,\n",
      "      \"loss\": 0.1003,\n",
      "      \"step\": 1320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.08443870317275276,\n",
      "      \"grad_norm\": 3.8921172618865967,\n",
      "      \"learning_rate\": 4.957908002520479e-05,\n",
      "      \"loss\": 0.1042,\n",
      "      \"step\": 1340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.0856989823245849,\n",
      "      \"grad_norm\": 13.159822463989258,\n",
      "      \"learning_rate\": 4.9572778827977315e-05,\n",
      "      \"loss\": 0.1016,\n",
      "      \"step\": 1360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.08695926147641703,\n",
      "      \"grad_norm\": 2.969879150390625,\n",
      "      \"learning_rate\": 4.9566477630749844e-05,\n",
      "      \"loss\": 0.1114,\n",
      "      \"step\": 1380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.08821954062824916,\n",
      "      \"grad_norm\": 0.4552854597568512,\n",
      "      \"learning_rate\": 4.9560176433522373e-05,\n",
      "      \"loss\": 0.1057,\n",
      "      \"step\": 1400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.0894798197800813,\n",
      "      \"grad_norm\": 0.0465676374733448,\n",
      "      \"learning_rate\": 4.95538752362949e-05,\n",
      "      \"loss\": 0.0918,\n",
      "      \"step\": 1420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.09074009893191341,\n",
      "      \"grad_norm\": 1.4001010656356812,\n",
      "      \"learning_rate\": 4.9547574039067425e-05,\n",
      "      \"loss\": 0.109,\n",
      "      \"step\": 1440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.09200037808374555,\n",
      "      \"grad_norm\": 2.7700254917144775,\n",
      "      \"learning_rate\": 4.9541272841839954e-05,\n",
      "      \"loss\": 0.1168,\n",
      "      \"step\": 1460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.09326065723557768,\n",
      "      \"grad_norm\": 0.8324950933456421,\n",
      "      \"learning_rate\": 4.9534971644612476e-05,\n",
      "      \"loss\": 0.08,\n",
      "      \"step\": 1480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.09452093638740981,\n",
      "      \"grad_norm\": 0.03056248649954796,\n",
      "      \"learning_rate\": 4.9528670447385005e-05,\n",
      "      \"loss\": 0.1411,\n",
      "      \"step\": 1500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.09578121553924195,\n",
      "      \"grad_norm\": 10.701178550720215,\n",
      "      \"learning_rate\": 4.952236925015753e-05,\n",
      "      \"loss\": 0.0972,\n",
      "      \"step\": 1520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.09704149469107408,\n",
      "      \"grad_norm\": 1.0490021705627441,\n",
      "      \"learning_rate\": 4.9516068052930056e-05,\n",
      "      \"loss\": 0.2526,\n",
      "      \"step\": 1540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.0983017738429062,\n",
      "      \"grad_norm\": 5.741815090179443,\n",
      "      \"learning_rate\": 4.9509766855702585e-05,\n",
      "      \"loss\": 0.0951,\n",
      "      \"step\": 1560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.09956205299473833,\n",
      "      \"grad_norm\": 5.312826156616211,\n",
      "      \"learning_rate\": 4.9503465658475115e-05,\n",
      "      \"loss\": 0.1851,\n",
      "      \"step\": 1580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.10082233214657046,\n",
      "      \"grad_norm\": 5.071767807006836,\n",
      "      \"learning_rate\": 4.9497164461247644e-05,\n",
      "      \"loss\": 0.1041,\n",
      "      \"step\": 1600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1020826112984026,\n",
      "      \"grad_norm\": 7.3789963722229,\n",
      "      \"learning_rate\": 4.9490863264020166e-05,\n",
      "      \"loss\": 0.1039,\n",
      "      \"step\": 1620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.10334289045023473,\n",
      "      \"grad_norm\": 0.29307031631469727,\n",
      "      \"learning_rate\": 4.9484562066792695e-05,\n",
      "      \"loss\": 0.0641,\n",
      "      \"step\": 1640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.10460316960206686,\n",
      "      \"grad_norm\": 3.9864253997802734,\n",
      "      \"learning_rate\": 4.947826086956522e-05,\n",
      "      \"loss\": 0.2278,\n",
      "      \"step\": 1660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.10586344875389898,\n",
      "      \"grad_norm\": 0.9057971835136414,\n",
      "      \"learning_rate\": 4.9471959672337746e-05,\n",
      "      \"loss\": 0.0903,\n",
      "      \"step\": 1680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.10712372790573112,\n",
      "      \"grad_norm\": 13.566811561584473,\n",
      "      \"learning_rate\": 4.946565847511027e-05,\n",
      "      \"loss\": 0.0645,\n",
      "      \"step\": 1700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.10838400705756325,\n",
      "      \"grad_norm\": 11.374462127685547,\n",
      "      \"learning_rate\": 4.94593572778828e-05,\n",
      "      \"loss\": 0.1052,\n",
      "      \"step\": 1720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.10964428620939538,\n",
      "      \"grad_norm\": 1.1447986364364624,\n",
      "      \"learning_rate\": 4.9453056080655327e-05,\n",
      "      \"loss\": 0.1456,\n",
      "      \"step\": 1740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.11090456536122752,\n",
      "      \"grad_norm\": 14.858553886413574,\n",
      "      \"learning_rate\": 4.9446754883427856e-05,\n",
      "      \"loss\": 0.0811,\n",
      "      \"step\": 1760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.11216484451305964,\n",
      "      \"grad_norm\": 8.170421600341797,\n",
      "      \"learning_rate\": 4.9440453686200385e-05,\n",
      "      \"loss\": 0.1417,\n",
      "      \"step\": 1780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.11342512366489177,\n",
      "      \"grad_norm\": 6.996744632720947,\n",
      "      \"learning_rate\": 4.943415248897291e-05,\n",
      "      \"loss\": 0.1062,\n",
      "      \"step\": 1800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1146854028167239,\n",
      "      \"grad_norm\": 4.2230987548828125,\n",
      "      \"learning_rate\": 4.9427851291745436e-05,\n",
      "      \"loss\": 0.0621,\n",
      "      \"step\": 1820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.11594568196855604,\n",
      "      \"grad_norm\": 10.492480278015137,\n",
      "      \"learning_rate\": 4.942155009451796e-05,\n",
      "      \"loss\": 0.1157,\n",
      "      \"step\": 1840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.11720596112038817,\n",
      "      \"grad_norm\": 6.939258098602295,\n",
      "      \"learning_rate\": 4.941524889729049e-05,\n",
      "      \"loss\": 0.1313,\n",
      "      \"step\": 1860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1184662402722203,\n",
      "      \"grad_norm\": 1.3652544021606445,\n",
      "      \"learning_rate\": 4.9408947700063016e-05,\n",
      "      \"loss\": 0.0918,\n",
      "      \"step\": 1880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.11972651942405242,\n",
      "      \"grad_norm\": 2.2391390800476074,\n",
      "      \"learning_rate\": 4.9402646502835545e-05,\n",
      "      \"loss\": 0.245,\n",
      "      \"step\": 1900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.12098679857588455,\n",
      "      \"grad_norm\": 1.7204068899154663,\n",
      "      \"learning_rate\": 4.939634530560807e-05,\n",
      "      \"loss\": 0.1094,\n",
      "      \"step\": 1920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.12224707772771669,\n",
      "      \"grad_norm\": 8.844012260437012,\n",
      "      \"learning_rate\": 4.9390044108380597e-05,\n",
      "      \"loss\": 0.1331,\n",
      "      \"step\": 1940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.12350735687954882,\n",
      "      \"grad_norm\": 7.058777332305908,\n",
      "      \"learning_rate\": 4.938374291115312e-05,\n",
      "      \"loss\": 0.1206,\n",
      "      \"step\": 1960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.12476763603138095,\n",
      "      \"grad_norm\": 1.3873555660247803,\n",
      "      \"learning_rate\": 4.937744171392565e-05,\n",
      "      \"loss\": 0.0636,\n",
      "      \"step\": 1980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.12602791518321307,\n",
      "      \"grad_norm\": 4.6648030281066895,\n",
      "      \"learning_rate\": 4.937114051669818e-05,\n",
      "      \"loss\": 0.1099,\n",
      "      \"step\": 2000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1272881943350452,\n",
      "      \"grad_norm\": 3.165059804916382,\n",
      "      \"learning_rate\": 4.93648393194707e-05,\n",
      "      \"loss\": 0.0847,\n",
      "      \"step\": 2020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.12854847348687734,\n",
      "      \"grad_norm\": 0.09908688068389893,\n",
      "      \"learning_rate\": 4.935853812224323e-05,\n",
      "      \"loss\": 0.1233,\n",
      "      \"step\": 2040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.12980875263870947,\n",
      "      \"grad_norm\": 0.03348054364323616,\n",
      "      \"learning_rate\": 4.935223692501576e-05,\n",
      "      \"loss\": 0.1046,\n",
      "      \"step\": 2060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1310690317905416,\n",
      "      \"grad_norm\": 8.576385498046875,\n",
      "      \"learning_rate\": 4.9345935727788286e-05,\n",
      "      \"loss\": 0.0681,\n",
      "      \"step\": 2080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.13232931094237374,\n",
      "      \"grad_norm\": 2.502655029296875,\n",
      "      \"learning_rate\": 4.933963453056081e-05,\n",
      "      \"loss\": 0.1233,\n",
      "      \"step\": 2100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.13358959009420587,\n",
      "      \"grad_norm\": 14.68161678314209,\n",
      "      \"learning_rate\": 4.933333333333334e-05,\n",
      "      \"loss\": 0.1221,\n",
      "      \"step\": 2120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.134849869246038,\n",
      "      \"grad_norm\": 26.16400909423828,\n",
      "      \"learning_rate\": 4.932703213610586e-05,\n",
      "      \"loss\": 0.0923,\n",
      "      \"step\": 2140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.13611014839787014,\n",
      "      \"grad_norm\": 4.1791300773620605,\n",
      "      \"learning_rate\": 4.932073093887839e-05,\n",
      "      \"loss\": 0.0686,\n",
      "      \"step\": 2160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.13737042754970225,\n",
      "      \"grad_norm\": 3.398982524871826,\n",
      "      \"learning_rate\": 4.931442974165091e-05,\n",
      "      \"loss\": 0.0564,\n",
      "      \"step\": 2180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.13863070670153438,\n",
      "      \"grad_norm\": 6.565633773803711,\n",
      "      \"learning_rate\": 4.930812854442344e-05,\n",
      "      \"loss\": 0.1567,\n",
      "      \"step\": 2200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1398909858533665,\n",
      "      \"grad_norm\": 0.5728756189346313,\n",
      "      \"learning_rate\": 4.930182734719597e-05,\n",
      "      \"loss\": 0.0813,\n",
      "      \"step\": 2220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.14115126500519865,\n",
      "      \"grad_norm\": 0.07849208265542984,\n",
      "      \"learning_rate\": 4.92955261499685e-05,\n",
      "      \"loss\": 0.0811,\n",
      "      \"step\": 2240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.14241154415703078,\n",
      "      \"grad_norm\": 2.162595272064209,\n",
      "      \"learning_rate\": 4.928922495274103e-05,\n",
      "      \"loss\": 0.1494,\n",
      "      \"step\": 2260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1436718233088629,\n",
      "      \"grad_norm\": 0.08075916022062302,\n",
      "      \"learning_rate\": 4.928292375551355e-05,\n",
      "      \"loss\": 0.1121,\n",
      "      \"step\": 2280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.14493210246069504,\n",
      "      \"grad_norm\": 8.234757423400879,\n",
      "      \"learning_rate\": 4.927662255828608e-05,\n",
      "      \"loss\": 0.0714,\n",
      "      \"step\": 2300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.14619238161252718,\n",
      "      \"grad_norm\": 6.582774639129639,\n",
      "      \"learning_rate\": 4.92703213610586e-05,\n",
      "      \"loss\": 0.1712,\n",
      "      \"step\": 2320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1474526607643593,\n",
      "      \"grad_norm\": 14.272419929504395,\n",
      "      \"learning_rate\": 4.926402016383113e-05,\n",
      "      \"loss\": 0.1121,\n",
      "      \"step\": 2340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.14871293991619144,\n",
      "      \"grad_norm\": 0.09195776283740997,\n",
      "      \"learning_rate\": 4.925771896660365e-05,\n",
      "      \"loss\": 0.0535,\n",
      "      \"step\": 2360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.14997321906802358,\n",
      "      \"grad_norm\": 4.577816963195801,\n",
      "      \"learning_rate\": 4.925141776937619e-05,\n",
      "      \"loss\": 0.0609,\n",
      "      \"step\": 2380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1512334982198557,\n",
      "      \"grad_norm\": 20.52442169189453,\n",
      "      \"learning_rate\": 4.924511657214871e-05,\n",
      "      \"loss\": 0.1383,\n",
      "      \"step\": 2400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.15249377737168782,\n",
      "      \"grad_norm\": 9.646416664123535,\n",
      "      \"learning_rate\": 4.923881537492124e-05,\n",
      "      \"loss\": 0.122,\n",
      "      \"step\": 2420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.15375405652351995,\n",
      "      \"grad_norm\": 9.623043060302734,\n",
      "      \"learning_rate\": 4.923251417769376e-05,\n",
      "      \"loss\": 0.0733,\n",
      "      \"step\": 2440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.15501433567535208,\n",
      "      \"grad_norm\": 0.4035397469997406,\n",
      "      \"learning_rate\": 4.922621298046629e-05,\n",
      "      \"loss\": 0.0711,\n",
      "      \"step\": 2460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.15627461482718422,\n",
      "      \"grad_norm\": 3.660886287689209,\n",
      "      \"learning_rate\": 4.921991178323882e-05,\n",
      "      \"loss\": 0.1097,\n",
      "      \"step\": 2480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.15753489397901635,\n",
      "      \"grad_norm\": 2.636598587036133,\n",
      "      \"learning_rate\": 4.921361058601134e-05,\n",
      "      \"loss\": 0.1173,\n",
      "      \"step\": 2500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.15879517313084848,\n",
      "      \"grad_norm\": 0.16535820066928864,\n",
      "      \"learning_rate\": 4.920730938878387e-05,\n",
      "      \"loss\": 0.0683,\n",
      "      \"step\": 2520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.16005545228268062,\n",
      "      \"grad_norm\": 0.008578252047300339,\n",
      "      \"learning_rate\": 4.92010081915564e-05,\n",
      "      \"loss\": 0.0395,\n",
      "      \"step\": 2540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.16131573143451275,\n",
      "      \"grad_norm\": 1.4561532735824585,\n",
      "      \"learning_rate\": 4.919470699432893e-05,\n",
      "      \"loss\": 0.0737,\n",
      "      \"step\": 2560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.16257601058634488,\n",
      "      \"grad_norm\": 0.08327748626470566,\n",
      "      \"learning_rate\": 4.918840579710145e-05,\n",
      "      \"loss\": 0.0959,\n",
      "      \"step\": 2580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.16383628973817702,\n",
      "      \"grad_norm\": 12.289834976196289,\n",
      "      \"learning_rate\": 4.918210459987398e-05,\n",
      "      \"loss\": 0.111,\n",
      "      \"step\": 2600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.16509656889000915,\n",
      "      \"grad_norm\": 10.90804386138916,\n",
      "      \"learning_rate\": 4.91758034026465e-05,\n",
      "      \"loss\": 0.1577,\n",
      "      \"step\": 2620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.16635684804184125,\n",
      "      \"grad_norm\": 3.9782464504241943,\n",
      "      \"learning_rate\": 4.916950220541903e-05,\n",
      "      \"loss\": 0.103,\n",
      "      \"step\": 2640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1676171271936734,\n",
      "      \"grad_norm\": 8.513080596923828,\n",
      "      \"learning_rate\": 4.9163201008191554e-05,\n",
      "      \"loss\": 0.0804,\n",
      "      \"step\": 2660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.16887740634550552,\n",
      "      \"grad_norm\": 1.7775752544403076,\n",
      "      \"learning_rate\": 4.915689981096408e-05,\n",
      "      \"loss\": 0.0939,\n",
      "      \"step\": 2680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.17013768549733765,\n",
      "      \"grad_norm\": 0.3609244227409363,\n",
      "      \"learning_rate\": 4.915059861373661e-05,\n",
      "      \"loss\": 0.1037,\n",
      "      \"step\": 2700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1713979646491698,\n",
      "      \"grad_norm\": 0.018686022609472275,\n",
      "      \"learning_rate\": 4.914429741650914e-05,\n",
      "      \"loss\": 0.0422,\n",
      "      \"step\": 2720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.17265824380100192,\n",
      "      \"grad_norm\": 7.193293571472168,\n",
      "      \"learning_rate\": 4.913799621928167e-05,\n",
      "      \"loss\": 0.1489,\n",
      "      \"step\": 2740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.17391852295283405,\n",
      "      \"grad_norm\": 9.708684921264648,\n",
      "      \"learning_rate\": 4.913169502205419e-05,\n",
      "      \"loss\": 0.0965,\n",
      "      \"step\": 2760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1751788021046662,\n",
      "      \"grad_norm\": 7.701924800872803,\n",
      "      \"learning_rate\": 4.912570888468809e-05,\n",
      "      \"loss\": 0.1562,\n",
      "      \"step\": 2780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.17643908125649832,\n",
      "      \"grad_norm\": 0.0325886532664299,\n",
      "      \"learning_rate\": 4.911940768746062e-05,\n",
      "      \"loss\": 0.0842,\n",
      "      \"step\": 2800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.17769936040833045,\n",
      "      \"grad_norm\": 8.095756530761719,\n",
      "      \"learning_rate\": 4.9113106490233145e-05,\n",
      "      \"loss\": 0.0702,\n",
      "      \"step\": 2820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1789596395601626,\n",
      "      \"grad_norm\": 9.416025161743164,\n",
      "      \"learning_rate\": 4.9106805293005674e-05,\n",
      "      \"loss\": 0.0862,\n",
      "      \"step\": 2840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.18021991871199472,\n",
      "      \"grad_norm\": 22.37908172607422,\n",
      "      \"learning_rate\": 4.91005040957782e-05,\n",
      "      \"loss\": 0.0743,\n",
      "      \"step\": 2860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.18148019786382683,\n",
      "      \"grad_norm\": 3.0986886024475098,\n",
      "      \"learning_rate\": 4.909420289855073e-05,\n",
      "      \"loss\": 0.1371,\n",
      "      \"step\": 2880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.18274047701565896,\n",
      "      \"grad_norm\": 0.2059469372034073,\n",
      "      \"learning_rate\": 4.9087901701323254e-05,\n",
      "      \"loss\": 0.0409,\n",
      "      \"step\": 2900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1840007561674911,\n",
      "      \"grad_norm\": 0.06335426867008209,\n",
      "      \"learning_rate\": 4.908160050409578e-05,\n",
      "      \"loss\": 0.0875,\n",
      "      \"step\": 2920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.18526103531932323,\n",
      "      \"grad_norm\": 0.847908079624176,\n",
      "      \"learning_rate\": 4.9075299306868305e-05,\n",
      "      \"loss\": 0.0535,\n",
      "      \"step\": 2940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.18652131447115536,\n",
      "      \"grad_norm\": 18.8087158203125,\n",
      "      \"learning_rate\": 4.9068998109640834e-05,\n",
      "      \"loss\": 0.0985,\n",
      "      \"step\": 2960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1877815936229875,\n",
      "      \"grad_norm\": 4.6115522384643555,\n",
      "      \"learning_rate\": 4.9062696912413356e-05,\n",
      "      \"loss\": 0.0746,\n",
      "      \"step\": 2980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.18904187277481962,\n",
      "      \"grad_norm\": 17.42864990234375,\n",
      "      \"learning_rate\": 4.9056395715185886e-05,\n",
      "      \"loss\": 0.0526,\n",
      "      \"step\": 3000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.19030215192665176,\n",
      "      \"grad_norm\": 0.9937516450881958,\n",
      "      \"learning_rate\": 4.9050094517958415e-05,\n",
      "      \"loss\": 0.1098,\n",
      "      \"step\": 3020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1915624310784839,\n",
      "      \"grad_norm\": 10.881094932556152,\n",
      "      \"learning_rate\": 4.9043793320730944e-05,\n",
      "      \"loss\": 0.0853,\n",
      "      \"step\": 3040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.19282271023031602,\n",
      "      \"grad_norm\": 0.30733954906463623,\n",
      "      \"learning_rate\": 4.903749212350347e-05,\n",
      "      \"loss\": 0.0785,\n",
      "      \"step\": 3060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.19408298938214816,\n",
      "      \"grad_norm\": 0.03962111100554466,\n",
      "      \"learning_rate\": 4.9031190926275995e-05,\n",
      "      \"loss\": 0.0354,\n",
      "      \"step\": 3080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.19534326853398026,\n",
      "      \"grad_norm\": 0.40762317180633545,\n",
      "      \"learning_rate\": 4.9024889729048524e-05,\n",
      "      \"loss\": 0.0651,\n",
      "      \"step\": 3100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1966035476858124,\n",
      "      \"grad_norm\": 0.3119143843650818,\n",
      "      \"learning_rate\": 4.9018588531821046e-05,\n",
      "      \"loss\": 0.0812,\n",
      "      \"step\": 3120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.19786382683764453,\n",
      "      \"grad_norm\": 0.9222527146339417,\n",
      "      \"learning_rate\": 4.9012287334593575e-05,\n",
      "      \"loss\": 0.1269,\n",
      "      \"step\": 3140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.19912410598947666,\n",
      "      \"grad_norm\": 4.678102016448975,\n",
      "      \"learning_rate\": 4.90059861373661e-05,\n",
      "      \"loss\": 0.114,\n",
      "      \"step\": 3160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2003843851413088,\n",
      "      \"grad_norm\": 9.511985778808594,\n",
      "      \"learning_rate\": 4.8999684940138627e-05,\n",
      "      \"loss\": 0.084,\n",
      "      \"step\": 3180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.20164466429314093,\n",
      "      \"grad_norm\": 0.09082119911909103,\n",
      "      \"learning_rate\": 4.8993383742911156e-05,\n",
      "      \"loss\": 0.087,\n",
      "      \"step\": 3200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.20290494344497306,\n",
      "      \"grad_norm\": 0.13513746857643127,\n",
      "      \"learning_rate\": 4.8987082545683685e-05,\n",
      "      \"loss\": 0.067,\n",
      "      \"step\": 3220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2041652225968052,\n",
      "      \"grad_norm\": 0.14876201748847961,\n",
      "      \"learning_rate\": 4.8980781348456214e-05,\n",
      "      \"loss\": 0.105,\n",
      "      \"step\": 3240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.20542550174863733,\n",
      "      \"grad_norm\": 0.40683281421661377,\n",
      "      \"learning_rate\": 4.8974480151228736e-05,\n",
      "      \"loss\": 0.0667,\n",
      "      \"step\": 3260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.20668578090046946,\n",
      "      \"grad_norm\": 6.51578950881958,\n",
      "      \"learning_rate\": 4.8968178954001265e-05,\n",
      "      \"loss\": 0.0812,\n",
      "      \"step\": 3280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2079460600523016,\n",
      "      \"grad_norm\": 0.9617591500282288,\n",
      "      \"learning_rate\": 4.896187775677379e-05,\n",
      "      \"loss\": 0.0502,\n",
      "      \"step\": 3300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.20920633920413373,\n",
      "      \"grad_norm\": 2.5645816326141357,\n",
      "      \"learning_rate\": 4.8955576559546316e-05,\n",
      "      \"loss\": 0.1461,\n",
      "      \"step\": 3320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.21046661835596583,\n",
      "      \"grad_norm\": 10.626169204711914,\n",
      "      \"learning_rate\": 4.894927536231884e-05,\n",
      "      \"loss\": 0.1688,\n",
      "      \"step\": 3340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.21172689750779797,\n",
      "      \"grad_norm\": 0.281276136636734,\n",
      "      \"learning_rate\": 4.894297416509137e-05,\n",
      "      \"loss\": 0.0623,\n",
      "      \"step\": 3360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2129871766596301,\n",
      "      \"grad_norm\": 0.5136945247650146,\n",
      "      \"learning_rate\": 4.89366729678639e-05,\n",
      "      \"loss\": 0.0809,\n",
      "      \"step\": 3380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.21424745581146223,\n",
      "      \"grad_norm\": 0.08504756540060043,\n",
      "      \"learning_rate\": 4.8930371770636426e-05,\n",
      "      \"loss\": 0.0664,\n",
      "      \"step\": 3400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.21550773496329437,\n",
      "      \"grad_norm\": 6.036271572113037,\n",
      "      \"learning_rate\": 4.892407057340895e-05,\n",
      "      \"loss\": 0.0349,\n",
      "      \"step\": 3420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2167680141151265,\n",
      "      \"grad_norm\": 11.342411994934082,\n",
      "      \"learning_rate\": 4.891776937618148e-05,\n",
      "      \"loss\": 0.1008,\n",
      "      \"step\": 3440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.21802829326695863,\n",
      "      \"grad_norm\": 0.03489483520388603,\n",
      "      \"learning_rate\": 4.8911468178954006e-05,\n",
      "      \"loss\": 0.1086,\n",
      "      \"step\": 3460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.21928857241879077,\n",
      "      \"grad_norm\": 1.2552622556686401,\n",
      "      \"learning_rate\": 4.890516698172653e-05,\n",
      "      \"loss\": 0.0726,\n",
      "      \"step\": 3480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2205488515706229,\n",
      "      \"grad_norm\": 0.6766469478607178,\n",
      "      \"learning_rate\": 4.889886578449906e-05,\n",
      "      \"loss\": 0.038,\n",
      "      \"step\": 3500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.22180913072245503,\n",
      "      \"grad_norm\": 0.005569393280893564,\n",
      "      \"learning_rate\": 4.8892564587271586e-05,\n",
      "      \"loss\": 0.1114,\n",
      "      \"step\": 3520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.22306940987428717,\n",
      "      \"grad_norm\": 1.3573334217071533,\n",
      "      \"learning_rate\": 4.8886263390044115e-05,\n",
      "      \"loss\": 0.1352,\n",
      "      \"step\": 3540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.22432968902611927,\n",
      "      \"grad_norm\": 0.26459094882011414,\n",
      "      \"learning_rate\": 4.887996219281664e-05,\n",
      "      \"loss\": 0.108,\n",
      "      \"step\": 3560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2255899681779514,\n",
      "      \"grad_norm\": 3.080416202545166,\n",
      "      \"learning_rate\": 4.887366099558917e-05,\n",
      "      \"loss\": 0.0604,\n",
      "      \"step\": 3580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.22685024732978354,\n",
      "      \"grad_norm\": 4.319312572479248,\n",
      "      \"learning_rate\": 4.886735979836169e-05,\n",
      "      \"loss\": 0.0667,\n",
      "      \"step\": 3600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.22811052648161567,\n",
      "      \"grad_norm\": 10.186842918395996,\n",
      "      \"learning_rate\": 4.886105860113422e-05,\n",
      "      \"loss\": 0.0876,\n",
      "      \"step\": 3620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2293708056334478,\n",
      "      \"grad_norm\": 3.509760856628418,\n",
      "      \"learning_rate\": 4.885475740390674e-05,\n",
      "      \"loss\": 0.0663,\n",
      "      \"step\": 3640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.23063108478527994,\n",
      "      \"grad_norm\": 5.997052192687988,\n",
      "      \"learning_rate\": 4.884845620667927e-05,\n",
      "      \"loss\": 0.0787,\n",
      "      \"step\": 3660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.23189136393711207,\n",
      "      \"grad_norm\": 10.178759574890137,\n",
      "      \"learning_rate\": 4.88421550094518e-05,\n",
      "      \"loss\": 0.1241,\n",
      "      \"step\": 3680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2331516430889442,\n",
      "      \"grad_norm\": 0.08598197251558304,\n",
      "      \"learning_rate\": 4.883585381222433e-05,\n",
      "      \"loss\": 0.0507,\n",
      "      \"step\": 3700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.23441192224077634,\n",
      "      \"grad_norm\": 8.652010917663574,\n",
      "      \"learning_rate\": 4.8829552614996856e-05,\n",
      "      \"loss\": 0.0526,\n",
      "      \"step\": 3720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.23567220139260847,\n",
      "      \"grad_norm\": 9.9122896194458,\n",
      "      \"learning_rate\": 4.882325141776938e-05,\n",
      "      \"loss\": 0.1353,\n",
      "      \"step\": 3740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2369324805444406,\n",
      "      \"grad_norm\": 0.4833053946495056,\n",
      "      \"learning_rate\": 4.881695022054191e-05,\n",
      "      \"loss\": 0.0707,\n",
      "      \"step\": 3760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.23819275969627274,\n",
      "      \"grad_norm\": 0.6267245411872864,\n",
      "      \"learning_rate\": 4.881064902331443e-05,\n",
      "      \"loss\": 0.0657,\n",
      "      \"step\": 3780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.23945303884810484,\n",
      "      \"grad_norm\": 0.8531333208084106,\n",
      "      \"learning_rate\": 4.880434782608696e-05,\n",
      "      \"loss\": 0.0924,\n",
      "      \"step\": 3800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.24071331799993698,\n",
      "      \"grad_norm\": 8.249244689941406,\n",
      "      \"learning_rate\": 4.879804662885948e-05,\n",
      "      \"loss\": 0.0652,\n",
      "      \"step\": 3820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2419735971517691,\n",
      "      \"grad_norm\": 0.057210225611925125,\n",
      "      \"learning_rate\": 4.879174543163201e-05,\n",
      "      \"loss\": 0.0684,\n",
      "      \"step\": 3840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.24323387630360124,\n",
      "      \"grad_norm\": 0.10741905122995377,\n",
      "      \"learning_rate\": 4.878544423440454e-05,\n",
      "      \"loss\": 0.0654,\n",
      "      \"step\": 3860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.24449415545543338,\n",
      "      \"grad_norm\": 6.829278945922852,\n",
      "      \"learning_rate\": 4.877914303717707e-05,\n",
      "      \"loss\": 0.0683,\n",
      "      \"step\": 3880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2457544346072655,\n",
      "      \"grad_norm\": 7.199930191040039,\n",
      "      \"learning_rate\": 4.877284183994959e-05,\n",
      "      \"loss\": 0.0604,\n",
      "      \"step\": 3900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.24701471375909764,\n",
      "      \"grad_norm\": 9.276254653930664,\n",
      "      \"learning_rate\": 4.876654064272212e-05,\n",
      "      \"loss\": 0.0588,\n",
      "      \"step\": 3920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.24827499291092978,\n",
      "      \"grad_norm\": 2.363115072250366,\n",
      "      \"learning_rate\": 4.876023944549465e-05,\n",
      "      \"loss\": 0.0696,\n",
      "      \"step\": 3940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2495352720627619,\n",
      "      \"grad_norm\": 5.391358852386475,\n",
      "      \"learning_rate\": 4.875393824826717e-05,\n",
      "      \"loss\": 0.1258,\n",
      "      \"step\": 3960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.250795551214594,\n",
      "      \"grad_norm\": 4.07792854309082,\n",
      "      \"learning_rate\": 4.87476370510397e-05,\n",
      "      \"loss\": 0.0736,\n",
      "      \"step\": 3980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.25205583036642615,\n",
      "      \"grad_norm\": 0.24931778013706207,\n",
      "      \"learning_rate\": 4.874133585381222e-05,\n",
      "      \"loss\": 0.061,\n",
      "      \"step\": 4000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2533161095182583,\n",
      "      \"grad_norm\": 3.2774548530578613,\n",
      "      \"learning_rate\": 4.873503465658476e-05,\n",
      "      \"loss\": 0.0701,\n",
      "      \"step\": 4020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2545763886700904,\n",
      "      \"grad_norm\": 0.08145177364349365,\n",
      "      \"learning_rate\": 4.872873345935728e-05,\n",
      "      \"loss\": 0.098,\n",
      "      \"step\": 4040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.25583666782192255,\n",
      "      \"grad_norm\": 0.0390741229057312,\n",
      "      \"learning_rate\": 4.872243226212981e-05,\n",
      "      \"loss\": 0.0816,\n",
      "      \"step\": 4060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2570969469737547,\n",
      "      \"grad_norm\": 0.2862966060638428,\n",
      "      \"learning_rate\": 4.871613106490233e-05,\n",
      "      \"loss\": 0.0731,\n",
      "      \"step\": 4080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2583572261255868,\n",
      "      \"grad_norm\": 0.07577396929264069,\n",
      "      \"learning_rate\": 4.870982986767486e-05,\n",
      "      \"loss\": 0.0635,\n",
      "      \"step\": 4100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.25961750527741895,\n",
      "      \"grad_norm\": 3.003730297088623,\n",
      "      \"learning_rate\": 4.870352867044738e-05,\n",
      "      \"loss\": 0.0552,\n",
      "      \"step\": 4120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2608777844292511,\n",
      "      \"grad_norm\": 0.06244244426488876,\n",
      "      \"learning_rate\": 4.869722747321991e-05,\n",
      "      \"loss\": 0.0911,\n",
      "      \"step\": 4140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2621380635810832,\n",
      "      \"grad_norm\": 0.010716745629906654,\n",
      "      \"learning_rate\": 4.869092627599244e-05,\n",
      "      \"loss\": 0.0626,\n",
      "      \"step\": 4160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.26339834273291535,\n",
      "      \"grad_norm\": 0.06118674576282501,\n",
      "      \"learning_rate\": 4.868462507876497e-05,\n",
      "      \"loss\": 0.0942,\n",
      "      \"step\": 4180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2646586218847475,\n",
      "      \"grad_norm\": 0.19392894208431244,\n",
      "      \"learning_rate\": 4.86783238815375e-05,\n",
      "      \"loss\": 0.0487,\n",
      "      \"step\": 4200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2659189010365796,\n",
      "      \"grad_norm\": 0.013496046885848045,\n",
      "      \"learning_rate\": 4.867202268431002e-05,\n",
      "      \"loss\": 0.1115,\n",
      "      \"step\": 4220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.26717918018841175,\n",
      "      \"grad_norm\": 0.474960595369339,\n",
      "      \"learning_rate\": 4.866572148708255e-05,\n",
      "      \"loss\": 0.0387,\n",
      "      \"step\": 4240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2684394593402439,\n",
      "      \"grad_norm\": 0.021874049678444862,\n",
      "      \"learning_rate\": 4.865942028985507e-05,\n",
      "      \"loss\": 0.0538,\n",
      "      \"step\": 4260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.269699738492076,\n",
      "      \"grad_norm\": 0.5417795181274414,\n",
      "      \"learning_rate\": 4.86531190926276e-05,\n",
      "      \"loss\": 0.0486,\n",
      "      \"step\": 4280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.27096001764390815,\n",
      "      \"grad_norm\": 0.09475312381982803,\n",
      "      \"learning_rate\": 4.8646817895400124e-05,\n",
      "      \"loss\": 0.0923,\n",
      "      \"step\": 4300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2722202967957403,\n",
      "      \"grad_norm\": 0.02228853851556778,\n",
      "      \"learning_rate\": 4.864051669817265e-05,\n",
      "      \"loss\": 0.0592,\n",
      "      \"step\": 4320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2734805759475724,\n",
      "      \"grad_norm\": 4.779207229614258,\n",
      "      \"learning_rate\": 4.863421550094518e-05,\n",
      "      \"loss\": 0.106,\n",
      "      \"step\": 4340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2747408550994045,\n",
      "      \"grad_norm\": 1.8848077058792114,\n",
      "      \"learning_rate\": 4.862791430371771e-05,\n",
      "      \"loss\": 0.0517,\n",
      "      \"step\": 4360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2760011342512366,\n",
      "      \"grad_norm\": 17.94026756286621,\n",
      "      \"learning_rate\": 4.862161310649024e-05,\n",
      "      \"loss\": 0.1,\n",
      "      \"step\": 4380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.27726141340306876,\n",
      "      \"grad_norm\": 1.1176104545593262,\n",
      "      \"learning_rate\": 4.861531190926276e-05,\n",
      "      \"loss\": 0.1291,\n",
      "      \"step\": 4400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2785216925549009,\n",
      "      \"grad_norm\": 0.04149693623185158,\n",
      "      \"learning_rate\": 4.860901071203529e-05,\n",
      "      \"loss\": 0.0736,\n",
      "      \"step\": 4420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.279781971706733,\n",
      "      \"grad_norm\": 0.2609304189682007,\n",
      "      \"learning_rate\": 4.8602709514807814e-05,\n",
      "      \"loss\": 0.0411,\n",
      "      \"step\": 4440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.28104225085856516,\n",
      "      \"grad_norm\": 0.007296055555343628,\n",
      "      \"learning_rate\": 4.859640831758034e-05,\n",
      "      \"loss\": 0.0773,\n",
      "      \"step\": 4460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2823025300103973,\n",
      "      \"grad_norm\": 0.060768190771341324,\n",
      "      \"learning_rate\": 4.8590107120352865e-05,\n",
      "      \"loss\": 0.0729,\n",
      "      \"step\": 4480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2835628091622294,\n",
      "      \"grad_norm\": 0.12336280941963196,\n",
      "      \"learning_rate\": 4.8583805923125394e-05,\n",
      "      \"loss\": 0.0835,\n",
      "      \"step\": 4500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.28482308831406156,\n",
      "      \"grad_norm\": 0.912050187587738,\n",
      "      \"learning_rate\": 4.857750472589792e-05,\n",
      "      \"loss\": 0.0651,\n",
      "      \"step\": 4520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2860833674658937,\n",
      "      \"grad_norm\": 6.081547737121582,\n",
      "      \"learning_rate\": 4.857120352867045e-05,\n",
      "      \"loss\": 0.0276,\n",
      "      \"step\": 4540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2873436466177258,\n",
      "      \"grad_norm\": 0.9108909964561462,\n",
      "      \"learning_rate\": 4.8564902331442974e-05,\n",
      "      \"loss\": 0.0458,\n",
      "      \"step\": 4560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.28860392576955796,\n",
      "      \"grad_norm\": 27.99903678894043,\n",
      "      \"learning_rate\": 4.8558601134215503e-05,\n",
      "      \"loss\": 0.0779,\n",
      "      \"step\": 4580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2898642049213901,\n",
      "      \"grad_norm\": 3.75490403175354,\n",
      "      \"learning_rate\": 4.855229993698803e-05,\n",
      "      \"loss\": 0.0711,\n",
      "      \"step\": 4600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2911244840732222,\n",
      "      \"grad_norm\": 5.748169898986816,\n",
      "      \"learning_rate\": 4.8545998739760555e-05,\n",
      "      \"loss\": 0.046,\n",
      "      \"step\": 4620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.29238476322505436,\n",
      "      \"grad_norm\": 0.6794788837432861,\n",
      "      \"learning_rate\": 4.8539697542533084e-05,\n",
      "      \"loss\": 0.0876,\n",
      "      \"step\": 4640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2936450423768865,\n",
      "      \"grad_norm\": 14.038543701171875,\n",
      "      \"learning_rate\": 4.853339634530561e-05,\n",
      "      \"loss\": 0.0653,\n",
      "      \"step\": 4660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2949053215287186,\n",
      "      \"grad_norm\": 3.020444869995117,\n",
      "      \"learning_rate\": 4.852709514807814e-05,\n",
      "      \"loss\": 0.0878,\n",
      "      \"step\": 4680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.29616560068055076,\n",
      "      \"grad_norm\": 0.048804979771375656,\n",
      "      \"learning_rate\": 4.8520793950850664e-05,\n",
      "      \"loss\": 0.082,\n",
      "      \"step\": 4700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2974258798323829,\n",
      "      \"grad_norm\": 0.036679577082395554,\n",
      "      \"learning_rate\": 4.851449275362319e-05,\n",
      "      \"loss\": 0.0652,\n",
      "      \"step\": 4720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.298686158984215,\n",
      "      \"grad_norm\": 0.037396401166915894,\n",
      "      \"learning_rate\": 4.8508191556395715e-05,\n",
      "      \"loss\": 0.062,\n",
      "      \"step\": 4740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.29994643813604716,\n",
      "      \"grad_norm\": 0.006891618017107248,\n",
      "      \"learning_rate\": 4.8501890359168244e-05,\n",
      "      \"loss\": 0.0363,\n",
      "      \"step\": 4760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3012067172878793,\n",
      "      \"grad_norm\": 22.74394416809082,\n",
      "      \"learning_rate\": 4.849558916194077e-05,\n",
      "      \"loss\": 0.0992,\n",
      "      \"step\": 4780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3024669964397114,\n",
      "      \"grad_norm\": 0.22906720638275146,\n",
      "      \"learning_rate\": 4.8489287964713296e-05,\n",
      "      \"loss\": 0.1023,\n",
      "      \"step\": 4800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3037272755915435,\n",
      "      \"grad_norm\": 2.814680814743042,\n",
      "      \"learning_rate\": 4.8482986767485825e-05,\n",
      "      \"loss\": 0.0501,\n",
      "      \"step\": 4820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.30498755474337563,\n",
      "      \"grad_norm\": 0.05007238686084747,\n",
      "      \"learning_rate\": 4.8476685570258354e-05,\n",
      "      \"loss\": 0.0698,\n",
      "      \"step\": 4840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.30624783389520777,\n",
      "      \"grad_norm\": 0.02168477699160576,\n",
      "      \"learning_rate\": 4.847038437303088e-05,\n",
      "      \"loss\": 0.0444,\n",
      "      \"step\": 4860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3075081130470399,\n",
      "      \"grad_norm\": 0.9878990650177002,\n",
      "      \"learning_rate\": 4.8464083175803405e-05,\n",
      "      \"loss\": 0.1121,\n",
      "      \"step\": 4880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.30876839219887203,\n",
      "      \"grad_norm\": 8.425019264221191,\n",
      "      \"learning_rate\": 4.8457781978575934e-05,\n",
      "      \"loss\": 0.0695,\n",
      "      \"step\": 4900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.31002867135070417,\n",
      "      \"grad_norm\": 0.005453509744256735,\n",
      "      \"learning_rate\": 4.8451480781348456e-05,\n",
      "      \"loss\": 0.0367,\n",
      "      \"step\": 4920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3112889505025363,\n",
      "      \"grad_norm\": 0.07321517169475555,\n",
      "      \"learning_rate\": 4.8445179584120986e-05,\n",
      "      \"loss\": 0.0671,\n",
      "      \"step\": 4940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.31254922965436843,\n",
      "      \"grad_norm\": 0.4786430299282074,\n",
      "      \"learning_rate\": 4.843887838689351e-05,\n",
      "      \"loss\": 0.0446,\n",
      "      \"step\": 4960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.31380950880620057,\n",
      "      \"grad_norm\": 2.6093287467956543,\n",
      "      \"learning_rate\": 4.843257718966604e-05,\n",
      "      \"loss\": 0.1021,\n",
      "      \"step\": 4980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3150697879580327,\n",
      "      \"grad_norm\": 2.24580454826355,\n",
      "      \"learning_rate\": 4.8426275992438566e-05,\n",
      "      \"loss\": 0.0322,\n",
      "      \"step\": 5000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.31633006710986483,\n",
      "      \"grad_norm\": Infinity,\n",
      "      \"learning_rate\": 4.8419974795211095e-05,\n",
      "      \"loss\": 0.0916,\n",
      "      \"step\": 5020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.31759034626169697,\n",
      "      \"grad_norm\": 0.4215964078903198,\n",
      "      \"learning_rate\": 4.8413988657844996e-05,\n",
      "      \"loss\": 0.1043,\n",
      "      \"step\": 5040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3188506254135291,\n",
      "      \"grad_norm\": 28.5858154296875,\n",
      "      \"learning_rate\": 4.840768746061752e-05,\n",
      "      \"loss\": 0.0308,\n",
      "      \"step\": 5060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.32011090456536123,\n",
      "      \"grad_norm\": 0.7405111789703369,\n",
      "      \"learning_rate\": 4.840138626339005e-05,\n",
      "      \"loss\": 0.0583,\n",
      "      \"step\": 5080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.32137118371719336,\n",
      "      \"grad_norm\": 0.008328747935593128,\n",
      "      \"learning_rate\": 4.839508506616257e-05,\n",
      "      \"loss\": 0.0853,\n",
      "      \"step\": 5100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3226314628690255,\n",
      "      \"grad_norm\": 1.1864200830459595,\n",
      "      \"learning_rate\": 4.83887838689351e-05,\n",
      "      \"loss\": 0.0358,\n",
      "      \"step\": 5120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.32389174202085763,\n",
      "      \"grad_norm\": 0.017595704644918442,\n",
      "      \"learning_rate\": 4.838248267170763e-05,\n",
      "      \"loss\": 0.07,\n",
      "      \"step\": 5140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.32515202117268976,\n",
      "      \"grad_norm\": 0.00422492204234004,\n",
      "      \"learning_rate\": 4.8376181474480156e-05,\n",
      "      \"loss\": 0.0405,\n",
      "      \"step\": 5160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3264123003245219,\n",
      "      \"grad_norm\": 0.020326124504208565,\n",
      "      \"learning_rate\": 4.8369880277252686e-05,\n",
      "      \"loss\": 0.0946,\n",
      "      \"step\": 5180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.32767257947635403,\n",
      "      \"grad_norm\": 4.035846710205078,\n",
      "      \"learning_rate\": 4.836357908002521e-05,\n",
      "      \"loss\": 0.0513,\n",
      "      \"step\": 5200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.32893285862818616,\n",
      "      \"grad_norm\": 0.05415407195687294,\n",
      "      \"learning_rate\": 4.835727788279774e-05,\n",
      "      \"loss\": 0.0617,\n",
      "      \"step\": 5220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3301931377800183,\n",
      "      \"grad_norm\": 1.4442684650421143,\n",
      "      \"learning_rate\": 4.835097668557026e-05,\n",
      "      \"loss\": 0.0287,\n",
      "      \"step\": 5240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.33145341693185043,\n",
      "      \"grad_norm\": 2.300769567489624,\n",
      "      \"learning_rate\": 4.834467548834279e-05,\n",
      "      \"loss\": 0.0763,\n",
      "      \"step\": 5260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3327136960836825,\n",
      "      \"grad_norm\": 0.5624440312385559,\n",
      "      \"learning_rate\": 4.833837429111531e-05,\n",
      "      \"loss\": 0.0143,\n",
      "      \"step\": 5280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.33397397523551464,\n",
      "      \"grad_norm\": 29.746469497680664,\n",
      "      \"learning_rate\": 4.833207309388784e-05,\n",
      "      \"loss\": 0.0691,\n",
      "      \"step\": 5300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3352342543873468,\n",
      "      \"grad_norm\": 8.538153648376465,\n",
      "      \"learning_rate\": 4.832577189666037e-05,\n",
      "      \"loss\": 0.1027,\n",
      "      \"step\": 5320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3364945335391789,\n",
      "      \"grad_norm\": 5.830715179443359,\n",
      "      \"learning_rate\": 4.83194706994329e-05,\n",
      "      \"loss\": 0.0734,\n",
      "      \"step\": 5340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.33775481269101104,\n",
      "      \"grad_norm\": 0.983144998550415,\n",
      "      \"learning_rate\": 4.831316950220542e-05,\n",
      "      \"loss\": 0.1079,\n",
      "      \"step\": 5360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3390150918428432,\n",
      "      \"grad_norm\": 0.023094583302736282,\n",
      "      \"learning_rate\": 4.830686830497795e-05,\n",
      "      \"loss\": 0.0506,\n",
      "      \"step\": 5380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3402753709946753,\n",
      "      \"grad_norm\": 0.07861457020044327,\n",
      "      \"learning_rate\": 4.830056710775048e-05,\n",
      "      \"loss\": 0.1235,\n",
      "      \"step\": 5400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.34153565014650744,\n",
      "      \"grad_norm\": 0.27915143966674805,\n",
      "      \"learning_rate\": 4.8294265910523e-05,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 5420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3427959292983396,\n",
      "      \"grad_norm\": 0.008731074631214142,\n",
      "      \"learning_rate\": 4.828796471329553e-05,\n",
      "      \"loss\": 0.0354,\n",
      "      \"step\": 5440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3440562084501717,\n",
      "      \"grad_norm\": 7.119776248931885,\n",
      "      \"learning_rate\": 4.828166351606805e-05,\n",
      "      \"loss\": 0.0048,\n",
      "      \"step\": 5460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.34531648760200384,\n",
      "      \"grad_norm\": 0.005988416727632284,\n",
      "      \"learning_rate\": 4.827536231884058e-05,\n",
      "      \"loss\": 0.1281,\n",
      "      \"step\": 5480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.346576766753836,\n",
      "      \"grad_norm\": 9.287652015686035,\n",
      "      \"learning_rate\": 4.826906112161311e-05,\n",
      "      \"loss\": 0.0948,\n",
      "      \"step\": 5500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3478370459056681,\n",
      "      \"grad_norm\": 0.08609241992235184,\n",
      "      \"learning_rate\": 4.826275992438564e-05,\n",
      "      \"loss\": 0.0895,\n",
      "      \"step\": 5520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.34909732505750024,\n",
      "      \"grad_norm\": 0.029376449063420296,\n",
      "      \"learning_rate\": 4.825645872715816e-05,\n",
      "      \"loss\": 0.0238,\n",
      "      \"step\": 5540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3503576042093324,\n",
      "      \"grad_norm\": 0.010847410187125206,\n",
      "      \"learning_rate\": 4.825015752993069e-05,\n",
      "      \"loss\": 0.071,\n",
      "      \"step\": 5560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3516178833611645,\n",
      "      \"grad_norm\": 0.004605160094797611,\n",
      "      \"learning_rate\": 4.824385633270321e-05,\n",
      "      \"loss\": 0.0849,\n",
      "      \"step\": 5580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.35287816251299664,\n",
      "      \"grad_norm\": 0.4633597135543823,\n",
      "      \"learning_rate\": 4.823755513547574e-05,\n",
      "      \"loss\": 0.067,\n",
      "      \"step\": 5600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3541384416648288,\n",
      "      \"grad_norm\": 5.967082500457764,\n",
      "      \"learning_rate\": 4.823125393824827e-05,\n",
      "      \"loss\": 0.0287,\n",
      "      \"step\": 5620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3553987208166609,\n",
      "      \"grad_norm\": 0.029177255928516388,\n",
      "      \"learning_rate\": 4.822495274102079e-05,\n",
      "      \"loss\": 0.0655,\n",
      "      \"step\": 5640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.35665899996849304,\n",
      "      \"grad_norm\": 0.01053645834326744,\n",
      "      \"learning_rate\": 4.821865154379333e-05,\n",
      "      \"loss\": 0.082,\n",
      "      \"step\": 5660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3579192791203252,\n",
      "      \"grad_norm\": 4.384653091430664,\n",
      "      \"learning_rate\": 4.821235034656585e-05,\n",
      "      \"loss\": 0.1208,\n",
      "      \"step\": 5680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3591795582721573,\n",
      "      \"grad_norm\": 0.03878080099821091,\n",
      "      \"learning_rate\": 4.820604914933838e-05,\n",
      "      \"loss\": 0.0521,\n",
      "      \"step\": 5700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.36043983742398944,\n",
      "      \"grad_norm\": 1.2655575275421143,\n",
      "      \"learning_rate\": 4.81997479521109e-05,\n",
      "      \"loss\": 0.0641,\n",
      "      \"step\": 5720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3617001165758215,\n",
      "      \"grad_norm\": 12.886695861816406,\n",
      "      \"learning_rate\": 4.819344675488343e-05,\n",
      "      \"loss\": 0.1027,\n",
      "      \"step\": 5740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.36296039572765365,\n",
      "      \"grad_norm\": 7.41424560546875,\n",
      "      \"learning_rate\": 4.818714555765595e-05,\n",
      "      \"loss\": 0.0975,\n",
      "      \"step\": 5760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3642206748794858,\n",
      "      \"grad_norm\": 20.990440368652344,\n",
      "      \"learning_rate\": 4.818084436042848e-05,\n",
      "      \"loss\": 0.0331,\n",
      "      \"step\": 5780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3654809540313179,\n",
      "      \"grad_norm\": 0.07634319365024567,\n",
      "      \"learning_rate\": 4.817454316320101e-05,\n",
      "      \"loss\": 0.0487,\n",
      "      \"step\": 5800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.36674123318315005,\n",
      "      \"grad_norm\": 21.137258529663086,\n",
      "      \"learning_rate\": 4.816824196597354e-05,\n",
      "      \"loss\": 0.0232,\n",
      "      \"step\": 5820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3680015123349822,\n",
      "      \"grad_norm\": 1.4136638641357422,\n",
      "      \"learning_rate\": 4.816194076874607e-05,\n",
      "      \"loss\": 0.0971,\n",
      "      \"step\": 5840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3692617914868143,\n",
      "      \"grad_norm\": 0.009461907669901848,\n",
      "      \"learning_rate\": 4.815563957151859e-05,\n",
      "      \"loss\": 0.1266,\n",
      "      \"step\": 5860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.37052207063864645,\n",
      "      \"grad_norm\": 0.10471153259277344,\n",
      "      \"learning_rate\": 4.814933837429112e-05,\n",
      "      \"loss\": 0.0689,\n",
      "      \"step\": 5880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3717823497904786,\n",
      "      \"grad_norm\": 0.008850392885506153,\n",
      "      \"learning_rate\": 4.814303717706364e-05,\n",
      "      \"loss\": 0.0523,\n",
      "      \"step\": 5900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3730426289423107,\n",
      "      \"grad_norm\": 3.804810047149658,\n",
      "      \"learning_rate\": 4.813673597983617e-05,\n",
      "      \"loss\": 0.0538,\n",
      "      \"step\": 5920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.37430290809414285,\n",
      "      \"grad_norm\": 0.022092550992965698,\n",
      "      \"learning_rate\": 4.8130434782608694e-05,\n",
      "      \"loss\": 0.0478,\n",
      "      \"step\": 5940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.375563187245975,\n",
      "      \"grad_norm\": 5.392937183380127,\n",
      "      \"learning_rate\": 4.812413358538122e-05,\n",
      "      \"loss\": 0.0352,\n",
      "      \"step\": 5960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3768234663978071,\n",
      "      \"grad_norm\": 11.14411449432373,\n",
      "      \"learning_rate\": 4.811783238815375e-05,\n",
      "      \"loss\": 0.0658,\n",
      "      \"step\": 5980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.37808374554963925,\n",
      "      \"grad_norm\": 0.17671313881874084,\n",
      "      \"learning_rate\": 4.811153119092628e-05,\n",
      "      \"loss\": 0.0876,\n",
      "      \"step\": 6000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3793440247014714,\n",
      "      \"grad_norm\": 2.671680450439453,\n",
      "      \"learning_rate\": 4.8105229993698804e-05,\n",
      "      \"loss\": 0.1283,\n",
      "      \"step\": 6020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3806043038533035,\n",
      "      \"grad_norm\": 0.0901724323630333,\n",
      "      \"learning_rate\": 4.809892879647133e-05,\n",
      "      \"loss\": 0.0614,\n",
      "      \"step\": 6040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.38186458300513565,\n",
      "      \"grad_norm\": 0.9181809425354004,\n",
      "      \"learning_rate\": 4.809262759924386e-05,\n",
      "      \"loss\": 0.0749,\n",
      "      \"step\": 6060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3831248621569678,\n",
      "      \"grad_norm\": 0.3178388476371765,\n",
      "      \"learning_rate\": 4.8086326402016384e-05,\n",
      "      \"loss\": 0.0838,\n",
      "      \"step\": 6080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3843851413087999,\n",
      "      \"grad_norm\": 15.42479133605957,\n",
      "      \"learning_rate\": 4.808002520478891e-05,\n",
      "      \"loss\": 0.0312,\n",
      "      \"step\": 6100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.38564542046063205,\n",
      "      \"grad_norm\": 10.418185234069824,\n",
      "      \"learning_rate\": 4.8073724007561435e-05,\n",
      "      \"loss\": 0.0767,\n",
      "      \"step\": 6120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3869056996124642,\n",
      "      \"grad_norm\": 0.02852483093738556,\n",
      "      \"learning_rate\": 4.8067422810333964e-05,\n",
      "      \"loss\": 0.0397,\n",
      "      \"step\": 6140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3881659787642963,\n",
      "      \"grad_norm\": 0.011686439625918865,\n",
      "      \"learning_rate\": 4.806112161310649e-05,\n",
      "      \"loss\": 0.0239,\n",
      "      \"step\": 6160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.38942625791612845,\n",
      "      \"grad_norm\": 5.806576251983643,\n",
      "      \"learning_rate\": 4.805482041587902e-05,\n",
      "      \"loss\": 0.0418,\n",
      "      \"step\": 6180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3906865370679605,\n",
      "      \"grad_norm\": 0.12820367515087128,\n",
      "      \"learning_rate\": 4.8048519218651545e-05,\n",
      "      \"loss\": 0.0633,\n",
      "      \"step\": 6200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.39194681621979266,\n",
      "      \"grad_norm\": 0.9826383590698242,\n",
      "      \"learning_rate\": 4.8042218021424074e-05,\n",
      "      \"loss\": 0.0814,\n",
      "      \"step\": 6220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3932070953716248,\n",
      "      \"grad_norm\": 5.5526123046875,\n",
      "      \"learning_rate\": 4.8035916824196596e-05,\n",
      "      \"loss\": 0.0398,\n",
      "      \"step\": 6240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3944673745234569,\n",
      "      \"grad_norm\": 0.04736286401748657,\n",
      "      \"learning_rate\": 4.8029615626969125e-05,\n",
      "      \"loss\": 0.0914,\n",
      "      \"step\": 6260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.39572765367528906,\n",
      "      \"grad_norm\": 0.054580386728048325,\n",
      "      \"learning_rate\": 4.8023314429741654e-05,\n",
      "      \"loss\": 0.1409,\n",
      "      \"step\": 6280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3969879328271212,\n",
      "      \"grad_norm\": 3.5771002769470215,\n",
      "      \"learning_rate\": 4.801701323251418e-05,\n",
      "      \"loss\": 0.0497,\n",
      "      \"step\": 6300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3982482119789533,\n",
      "      \"grad_norm\": 3.7855098247528076,\n",
      "      \"learning_rate\": 4.801071203528671e-05,\n",
      "      \"loss\": 0.0648,\n",
      "      \"step\": 6320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.39950849113078546,\n",
      "      \"grad_norm\": 0.03240585699677467,\n",
      "      \"learning_rate\": 4.8004410838059234e-05,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 6340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4007687702826176,\n",
      "      \"grad_norm\": 0.2811816334724426,\n",
      "      \"learning_rate\": 4.799810964083176e-05,\n",
      "      \"loss\": 0.1138,\n",
      "      \"step\": 6360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4020290494344497,\n",
      "      \"grad_norm\": 0.5347787737846375,\n",
      "      \"learning_rate\": 4.7991808443604286e-05,\n",
      "      \"loss\": 0.1637,\n",
      "      \"step\": 6380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.40328932858628186,\n",
      "      \"grad_norm\": 0.1288652867078781,\n",
      "      \"learning_rate\": 4.7985507246376815e-05,\n",
      "      \"loss\": 0.1005,\n",
      "      \"step\": 6400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.404549607738114,\n",
      "      \"grad_norm\": 5.344554424285889,\n",
      "      \"learning_rate\": 4.797920604914934e-05,\n",
      "      \"loss\": 0.0864,\n",
      "      \"step\": 6420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4058098868899461,\n",
      "      \"grad_norm\": 2.683236837387085,\n",
      "      \"learning_rate\": 4.7972904851921866e-05,\n",
      "      \"loss\": 0.1089,\n",
      "      \"step\": 6440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.40707016604177826,\n",
      "      \"grad_norm\": 3.6816325187683105,\n",
      "      \"learning_rate\": 4.7966603654694395e-05,\n",
      "      \"loss\": 0.0965,\n",
      "      \"step\": 6460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4083304451936104,\n",
      "      \"grad_norm\": 0.12433401495218277,\n",
      "      \"learning_rate\": 4.7960302457466924e-05,\n",
      "      \"loss\": 0.0295,\n",
      "      \"step\": 6480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4095907243454425,\n",
      "      \"grad_norm\": 0.7101385593414307,\n",
      "      \"learning_rate\": 4.7954001260239446e-05,\n",
      "      \"loss\": 0.0385,\n",
      "      \"step\": 6500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.41085100349727466,\n",
      "      \"grad_norm\": 35.154075622558594,\n",
      "      \"learning_rate\": 4.7947700063011975e-05,\n",
      "      \"loss\": 0.0862,\n",
      "      \"step\": 6520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4121112826491068,\n",
      "      \"grad_norm\": 0.34908753633499146,\n",
      "      \"learning_rate\": 4.7941398865784504e-05,\n",
      "      \"loss\": 0.1191,\n",
      "      \"step\": 6540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4133715618009389,\n",
      "      \"grad_norm\": 0.0495530404150486,\n",
      "      \"learning_rate\": 4.7935097668557027e-05,\n",
      "      \"loss\": 0.0532,\n",
      "      \"step\": 6560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.41463184095277106,\n",
      "      \"grad_norm\": 11.168868064880371,\n",
      "      \"learning_rate\": 4.7928796471329556e-05,\n",
      "      \"loss\": 0.0536,\n",
      "      \"step\": 6580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4158921201046032,\n",
      "      \"grad_norm\": 8.737963676452637,\n",
      "      \"learning_rate\": 4.792249527410208e-05,\n",
      "      \"loss\": 0.0547,\n",
      "      \"step\": 6600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4171523992564353,\n",
      "      \"grad_norm\": 6.544421672821045,\n",
      "      \"learning_rate\": 4.791619407687461e-05,\n",
      "      \"loss\": 0.079,\n",
      "      \"step\": 6620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.41841267840826746,\n",
      "      \"grad_norm\": 0.24705322086811066,\n",
      "      \"learning_rate\": 4.7909892879647136e-05,\n",
      "      \"loss\": 0.0736,\n",
      "      \"step\": 6640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.41967295756009954,\n",
      "      \"grad_norm\": 0.9115923047065735,\n",
      "      \"learning_rate\": 4.7903591682419665e-05,\n",
      "      \"loss\": 0.0566,\n",
      "      \"step\": 6660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.42093323671193167,\n",
      "      \"grad_norm\": 0.026121681556105614,\n",
      "      \"learning_rate\": 4.789729048519219e-05,\n",
      "      \"loss\": 0.0336,\n",
      "      \"step\": 6680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4221935158637638,\n",
      "      \"grad_norm\": 12.612021446228027,\n",
      "      \"learning_rate\": 4.7890989287964716e-05,\n",
      "      \"loss\": 0.1119,\n",
      "      \"step\": 6700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.42345379501559594,\n",
      "      \"grad_norm\": 5.031988143920898,\n",
      "      \"learning_rate\": 4.788468809073724e-05,\n",
      "      \"loss\": 0.0677,\n",
      "      \"step\": 6720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.42471407416742807,\n",
      "      \"grad_norm\": 0.3515089452266693,\n",
      "      \"learning_rate\": 4.787838689350977e-05,\n",
      "      \"loss\": 0.0462,\n",
      "      \"step\": 6740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4259743533192602,\n",
      "      \"grad_norm\": 0.7474532723426819,\n",
      "      \"learning_rate\": 4.78720856962823e-05,\n",
      "      \"loss\": 0.0513,\n",
      "      \"step\": 6760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.42723463247109233,\n",
      "      \"grad_norm\": 10.441596031188965,\n",
      "      \"learning_rate\": 4.786578449905482e-05,\n",
      "      \"loss\": 0.098,\n",
      "      \"step\": 6780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.42849491162292447,\n",
      "      \"grad_norm\": 0.12322792410850525,\n",
      "      \"learning_rate\": 4.7859483301827355e-05,\n",
      "      \"loss\": 0.0954,\n",
      "      \"step\": 6800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4297551907747566,\n",
      "      \"grad_norm\": 4.334676265716553,\n",
      "      \"learning_rate\": 4.785318210459988e-05,\n",
      "      \"loss\": 0.0856,\n",
      "      \"step\": 6820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.43101546992658873,\n",
      "      \"grad_norm\": 0.013606999069452286,\n",
      "      \"learning_rate\": 4.7846880907372406e-05,\n",
      "      \"loss\": 0.0414,\n",
      "      \"step\": 6840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.43227574907842087,\n",
      "      \"grad_norm\": 0.14364632964134216,\n",
      "      \"learning_rate\": 4.784057971014493e-05,\n",
      "      \"loss\": 0.0829,\n",
      "      \"step\": 6860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.433536028230253,\n",
      "      \"grad_norm\": 8.30142879486084,\n",
      "      \"learning_rate\": 4.783427851291746e-05,\n",
      "      \"loss\": 0.0422,\n",
      "      \"step\": 6880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.43479630738208513,\n",
      "      \"grad_norm\": 0.009500360116362572,\n",
      "      \"learning_rate\": 4.782797731568998e-05,\n",
      "      \"loss\": 0.1269,\n",
      "      \"step\": 6900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.43605658653391727,\n",
      "      \"grad_norm\": 0.006720248609781265,\n",
      "      \"learning_rate\": 4.782167611846251e-05,\n",
      "      \"loss\": 0.0565,\n",
      "      \"step\": 6920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4373168656857494,\n",
      "      \"grad_norm\": 7.899814128875732,\n",
      "      \"learning_rate\": 4.781537492123504e-05,\n",
      "      \"loss\": 0.0699,\n",
      "      \"step\": 6940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.43857714483758153,\n",
      "      \"grad_norm\": 1.1490803956985474,\n",
      "      \"learning_rate\": 4.780907372400757e-05,\n",
      "      \"loss\": 0.0448,\n",
      "      \"step\": 6960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.43983742398941367,\n",
      "      \"grad_norm\": 1.6489124298095703,\n",
      "      \"learning_rate\": 4.7802772526780096e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 6980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4410977031412458,\n",
      "      \"grad_norm\": 0.11773212999105453,\n",
      "      \"learning_rate\": 4.779647132955262e-05,\n",
      "      \"loss\": 0.06,\n",
      "      \"step\": 7000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.44235798229307793,\n",
      "      \"grad_norm\": 0.007044790778309107,\n",
      "      \"learning_rate\": 4.779017013232515e-05,\n",
      "      \"loss\": 0.017,\n",
      "      \"step\": 7020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.44361826144491007,\n",
      "      \"grad_norm\": 0.1582648754119873,\n",
      "      \"learning_rate\": 4.778386893509767e-05,\n",
      "      \"loss\": 0.0925,\n",
      "      \"step\": 7040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4448785405967422,\n",
      "      \"grad_norm\": 0.2813369631767273,\n",
      "      \"learning_rate\": 4.77775677378702e-05,\n",
      "      \"loss\": 0.0544,\n",
      "      \"step\": 7060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.44613881974857433,\n",
      "      \"grad_norm\": 0.3722339868545532,\n",
      "      \"learning_rate\": 4.777126654064272e-05,\n",
      "      \"loss\": 0.0938,\n",
      "      \"step\": 7080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.44739909890040647,\n",
      "      \"grad_norm\": 0.029589178040623665,\n",
      "      \"learning_rate\": 4.776496534341525e-05,\n",
      "      \"loss\": 0.0672,\n",
      "      \"step\": 7100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.44865937805223854,\n",
      "      \"grad_norm\": 0.0147646339610219,\n",
      "      \"learning_rate\": 4.775866414618778e-05,\n",
      "      \"loss\": 0.034,\n",
      "      \"step\": 7120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4499196572040707,\n",
      "      \"grad_norm\": 0.6518011689186096,\n",
      "      \"learning_rate\": 4.775236294896031e-05,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 7140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4511799363559028,\n",
      "      \"grad_norm\": 0.362802654504776,\n",
      "      \"learning_rate\": 4.774606175173283e-05,\n",
      "      \"loss\": 0.0754,\n",
      "      \"step\": 7160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.45244021550773494,\n",
      "      \"grad_norm\": 0.1761714071035385,\n",
      "      \"learning_rate\": 4.774007561436673e-05,\n",
      "      \"loss\": 0.1177,\n",
      "      \"step\": 7180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4537004946595671,\n",
      "      \"grad_norm\": 0.12442658096551895,\n",
      "      \"learning_rate\": 4.773377441713926e-05,\n",
      "      \"loss\": 0.0471,\n",
      "      \"step\": 7200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4549607738113992,\n",
      "      \"grad_norm\": 4.507923603057861,\n",
      "      \"learning_rate\": 4.772747321991178e-05,\n",
      "      \"loss\": 0.077,\n",
      "      \"step\": 7220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.45622105296323134,\n",
      "      \"grad_norm\": 2.559037685394287,\n",
      "      \"learning_rate\": 4.772117202268431e-05,\n",
      "      \"loss\": 0.039,\n",
      "      \"step\": 7240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4574813321150635,\n",
      "      \"grad_norm\": 1.5014766454696655,\n",
      "      \"learning_rate\": 4.7714870825456834e-05,\n",
      "      \"loss\": 0.0881,\n",
      "      \"step\": 7260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4587416112668956,\n",
      "      \"grad_norm\": 0.03132971003651619,\n",
      "      \"learning_rate\": 4.770856962822936e-05,\n",
      "      \"loss\": 0.0168,\n",
      "      \"step\": 7280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.46000189041872774,\n",
      "      \"grad_norm\": 5.73601770401001,\n",
      "      \"learning_rate\": 4.77022684310019e-05,\n",
      "      \"loss\": 0.0744,\n",
      "      \"step\": 7300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4612621695705599,\n",
      "      \"grad_norm\": 5.387157440185547,\n",
      "      \"learning_rate\": 4.769596723377442e-05,\n",
      "      \"loss\": 0.0899,\n",
      "      \"step\": 7320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.462522448722392,\n",
      "      \"grad_norm\": 0.19253386557102203,\n",
      "      \"learning_rate\": 4.768966603654695e-05,\n",
      "      \"loss\": 0.0659,\n",
      "      \"step\": 7340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.46378272787422414,\n",
      "      \"grad_norm\": 15.3233642578125,\n",
      "      \"learning_rate\": 4.768336483931947e-05,\n",
      "      \"loss\": 0.0881,\n",
      "      \"step\": 7360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4650430070260563,\n",
      "      \"grad_norm\": 0.11351698637008667,\n",
      "      \"learning_rate\": 4.7677063642092e-05,\n",
      "      \"loss\": 0.0389,\n",
      "      \"step\": 7380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4663032861778884,\n",
      "      \"grad_norm\": 0.02517307922244072,\n",
      "      \"learning_rate\": 4.767076244486452e-05,\n",
      "      \"loss\": 0.0548,\n",
      "      \"step\": 7400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.46756356532972054,\n",
      "      \"grad_norm\": 0.14158496260643005,\n",
      "      \"learning_rate\": 4.766446124763705e-05,\n",
      "      \"loss\": 0.101,\n",
      "      \"step\": 7420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4688238444815527,\n",
      "      \"grad_norm\": 0.23429475724697113,\n",
      "      \"learning_rate\": 4.765816005040958e-05,\n",
      "      \"loss\": 0.0679,\n",
      "      \"step\": 7440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4700841236333848,\n",
      "      \"grad_norm\": 7.254276752471924,\n",
      "      \"learning_rate\": 4.765185885318211e-05,\n",
      "      \"loss\": 0.0583,\n",
      "      \"step\": 7460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.47134440278521694,\n",
      "      \"grad_norm\": 0.02139163576066494,\n",
      "      \"learning_rate\": 4.764555765595463e-05,\n",
      "      \"loss\": 0.0741,\n",
      "      \"step\": 7480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4726046819370491,\n",
      "      \"grad_norm\": 0.03919967636466026,\n",
      "      \"learning_rate\": 4.763925645872716e-05,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 7500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4738649610888812,\n",
      "      \"grad_norm\": 0.00711747957393527,\n",
      "      \"learning_rate\": 4.763295526149969e-05,\n",
      "      \"loss\": 0.0533,\n",
      "      \"step\": 7520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.47512524024071334,\n",
      "      \"grad_norm\": 0.013619364239275455,\n",
      "      \"learning_rate\": 4.762665406427221e-05,\n",
      "      \"loss\": 0.0238,\n",
      "      \"step\": 7540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4763855193925455,\n",
      "      \"grad_norm\": 0.0981348305940628,\n",
      "      \"learning_rate\": 4.762035286704474e-05,\n",
      "      \"loss\": 0.0291,\n",
      "      \"step\": 7560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.47764579854437755,\n",
      "      \"grad_norm\": 13.3329496383667,\n",
      "      \"learning_rate\": 4.7614051669817264e-05,\n",
      "      \"loss\": 0.1227,\n",
      "      \"step\": 7580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4789060776962097,\n",
      "      \"grad_norm\": 6.204606533050537,\n",
      "      \"learning_rate\": 4.760775047258979e-05,\n",
      "      \"loss\": 0.1007,\n",
      "      \"step\": 7600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4801663568480418,\n",
      "      \"grad_norm\": 0.17252813279628754,\n",
      "      \"learning_rate\": 4.760144927536232e-05,\n",
      "      \"loss\": 0.0964,\n",
      "      \"step\": 7620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.48142663599987395,\n",
      "      \"grad_norm\": 0.13233627378940582,\n",
      "      \"learning_rate\": 4.759514807813485e-05,\n",
      "      \"loss\": 0.0575,\n",
      "      \"step\": 7640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4826869151517061,\n",
      "      \"grad_norm\": 0.05481228232383728,\n",
      "      \"learning_rate\": 4.7588846880907374e-05,\n",
      "      \"loss\": 0.0575,\n",
      "      \"step\": 7660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4839471943035382,\n",
      "      \"grad_norm\": 0.06554854661226273,\n",
      "      \"learning_rate\": 4.75825456836799e-05,\n",
      "      \"loss\": 0.0432,\n",
      "      \"step\": 7680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.48520747345537035,\n",
      "      \"grad_norm\": 0.7494663596153259,\n",
      "      \"learning_rate\": 4.7576244486452425e-05,\n",
      "      \"loss\": 0.0737,\n",
      "      \"step\": 7700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4864677526072025,\n",
      "      \"grad_norm\": 0.3292778432369232,\n",
      "      \"learning_rate\": 4.7569943289224954e-05,\n",
      "      \"loss\": 0.1206,\n",
      "      \"step\": 7720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4877280317590346,\n",
      "      \"grad_norm\": 0.00791325606405735,\n",
      "      \"learning_rate\": 4.756364209199748e-05,\n",
      "      \"loss\": 0.0734,\n",
      "      \"step\": 7740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.48898831091086675,\n",
      "      \"grad_norm\": 0.0018078666180372238,\n",
      "      \"learning_rate\": 4.7557340894770005e-05,\n",
      "      \"loss\": 0.0338,\n",
      "      \"step\": 7760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4902485900626989,\n",
      "      \"grad_norm\": 0.09755639731884003,\n",
      "      \"learning_rate\": 4.7551039697542534e-05,\n",
      "      \"loss\": 0.0934,\n",
      "      \"step\": 7780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.491508869214531,\n",
      "      \"grad_norm\": 2.8784987926483154,\n",
      "      \"learning_rate\": 4.754473850031506e-05,\n",
      "      \"loss\": 0.0336,\n",
      "      \"step\": 7800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.49276914836636315,\n",
      "      \"grad_norm\": 1.1302769184112549,\n",
      "      \"learning_rate\": 4.753843730308759e-05,\n",
      "      \"loss\": 0.0644,\n",
      "      \"step\": 7820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4940294275181953,\n",
      "      \"grad_norm\": 0.6806050539016724,\n",
      "      \"learning_rate\": 4.7532136105860115e-05,\n",
      "      \"loss\": 0.0912,\n",
      "      \"step\": 7840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4952897066700274,\n",
      "      \"grad_norm\": 4.485098361968994,\n",
      "      \"learning_rate\": 4.7525834908632644e-05,\n",
      "      \"loss\": 0.0406,\n",
      "      \"step\": 7860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.49654998582185955,\n",
      "      \"grad_norm\": 0.030626319348812103,\n",
      "      \"learning_rate\": 4.7519533711405166e-05,\n",
      "      \"loss\": 0.0446,\n",
      "      \"step\": 7880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4978102649736917,\n",
      "      \"grad_norm\": 0.0641264095902443,\n",
      "      \"learning_rate\": 4.7513232514177695e-05,\n",
      "      \"loss\": 0.0542,\n",
      "      \"step\": 7900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4990705441255238,\n",
      "      \"grad_norm\": 0.34544381499290466,\n",
      "      \"learning_rate\": 4.750693131695022e-05,\n",
      "      \"loss\": 0.0493,\n",
      "      \"step\": 7920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5003308232773559,\n",
      "      \"grad_norm\": 8.016695976257324,\n",
      "      \"learning_rate\": 4.750063011972275e-05,\n",
      "      \"loss\": 0.0893,\n",
      "      \"step\": 7940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.501591102429188,\n",
      "      \"grad_norm\": 3.9718120098114014,\n",
      "      \"learning_rate\": 4.7494328922495275e-05,\n",
      "      \"loss\": 0.0607,\n",
      "      \"step\": 7960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5028513815810202,\n",
      "      \"grad_norm\": 0.15318521857261658,\n",
      "      \"learning_rate\": 4.7488027725267804e-05,\n",
      "      \"loss\": 0.0682,\n",
      "      \"step\": 7980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5041116607328523,\n",
      "      \"grad_norm\": 0.5462860465049744,\n",
      "      \"learning_rate\": 4.7481726528040333e-05,\n",
      "      \"loss\": 0.0602,\n",
      "      \"step\": 8000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5053719398846844,\n",
      "      \"grad_norm\": 2.5283362865448,\n",
      "      \"learning_rate\": 4.7475425330812856e-05,\n",
      "      \"loss\": 0.1163,\n",
      "      \"step\": 8020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5066322190365166,\n",
      "      \"grad_norm\": 11.373705863952637,\n",
      "      \"learning_rate\": 4.7469124133585385e-05,\n",
      "      \"loss\": 0.085,\n",
      "      \"step\": 8040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5078924981883487,\n",
      "      \"grad_norm\": 0.28745532035827637,\n",
      "      \"learning_rate\": 4.746282293635791e-05,\n",
      "      \"loss\": 0.078,\n",
      "      \"step\": 8060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5091527773401808,\n",
      "      \"grad_norm\": 0.38193434476852417,\n",
      "      \"learning_rate\": 4.7456521739130436e-05,\n",
      "      \"loss\": 0.0695,\n",
      "      \"step\": 8080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.510413056492013,\n",
      "      \"grad_norm\": 0.0944240540266037,\n",
      "      \"learning_rate\": 4.7450220541902965e-05,\n",
      "      \"loss\": 0.0393,\n",
      "      \"step\": 8100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5116733356438451,\n",
      "      \"grad_norm\": 0.019357629120349884,\n",
      "      \"learning_rate\": 4.7443919344675494e-05,\n",
      "      \"loss\": 0.0357,\n",
      "      \"step\": 8120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5129336147956772,\n",
      "      \"grad_norm\": 0.007685549091547728,\n",
      "      \"learning_rate\": 4.7437618147448016e-05,\n",
      "      \"loss\": 0.0678,\n",
      "      \"step\": 8140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5141938939475094,\n",
      "      \"grad_norm\": 0.01392384059727192,\n",
      "      \"learning_rate\": 4.7431316950220545e-05,\n",
      "      \"loss\": 0.051,\n",
      "      \"step\": 8160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5154541730993415,\n",
      "      \"grad_norm\": 4.006877899169922,\n",
      "      \"learning_rate\": 4.742501575299307e-05,\n",
      "      \"loss\": 0.0486,\n",
      "      \"step\": 8180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5167144522511736,\n",
      "      \"grad_norm\": 1.0302454233169556,\n",
      "      \"learning_rate\": 4.74187145557656e-05,\n",
      "      \"loss\": 0.0554,\n",
      "      \"step\": 8200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5179747314030058,\n",
      "      \"grad_norm\": 0.021631283685564995,\n",
      "      \"learning_rate\": 4.7412413358538126e-05,\n",
      "      \"loss\": 0.0693,\n",
      "      \"step\": 8220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5192350105548379,\n",
      "      \"grad_norm\": 0.02248368225991726,\n",
      "      \"learning_rate\": 4.740611216131065e-05,\n",
      "      \"loss\": 0.027,\n",
      "      \"step\": 8240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.52049528970667,\n",
      "      \"grad_norm\": 0.006706688553094864,\n",
      "      \"learning_rate\": 4.739981096408318e-05,\n",
      "      \"loss\": 0.0517,\n",
      "      \"step\": 8260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5217555688585022,\n",
      "      \"grad_norm\": 7.472154140472412,\n",
      "      \"learning_rate\": 4.7393509766855706e-05,\n",
      "      \"loss\": 0.0424,\n",
      "      \"step\": 8280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5230158480103343,\n",
      "      \"grad_norm\": 12.79321002960205,\n",
      "      \"learning_rate\": 4.7387208569628235e-05,\n",
      "      \"loss\": 0.053,\n",
      "      \"step\": 8300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5242761271621664,\n",
      "      \"grad_norm\": 22.917665481567383,\n",
      "      \"learning_rate\": 4.738090737240076e-05,\n",
      "      \"loss\": 0.0869,\n",
      "      \"step\": 8320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5255364063139986,\n",
      "      \"grad_norm\": 0.3459901213645935,\n",
      "      \"learning_rate\": 4.7374606175173286e-05,\n",
      "      \"loss\": 0.0613,\n",
      "      \"step\": 8340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5267966854658307,\n",
      "      \"grad_norm\": 0.00478943157941103,\n",
      "      \"learning_rate\": 4.736830497794581e-05,\n",
      "      \"loss\": 0.0607,\n",
      "      \"step\": 8360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5280569646176628,\n",
      "      \"grad_norm\": 0.01940474472939968,\n",
      "      \"learning_rate\": 4.736200378071834e-05,\n",
      "      \"loss\": 0.0746,\n",
      "      \"step\": 8380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.529317243769495,\n",
      "      \"grad_norm\": 0.05525878071784973,\n",
      "      \"learning_rate\": 4.735570258349086e-05,\n",
      "      \"loss\": 0.0495,\n",
      "      \"step\": 8400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5305775229213271,\n",
      "      \"grad_norm\": 0.03653648868203163,\n",
      "      \"learning_rate\": 4.734940138626339e-05,\n",
      "      \"loss\": 0.0581,\n",
      "      \"step\": 8420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5318378020731592,\n",
      "      \"grad_norm\": 0.057477060705423355,\n",
      "      \"learning_rate\": 4.7343100189035925e-05,\n",
      "      \"loss\": 0.0524,\n",
      "      \"step\": 8440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5330980812249914,\n",
      "      \"grad_norm\": 0.5844238996505737,\n",
      "      \"learning_rate\": 4.733679899180845e-05,\n",
      "      \"loss\": 0.0484,\n",
      "      \"step\": 8460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5343583603768235,\n",
      "      \"grad_norm\": 0.4349050521850586,\n",
      "      \"learning_rate\": 4.7330497794580976e-05,\n",
      "      \"loss\": 0.0519,\n",
      "      \"step\": 8480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5356186395286556,\n",
      "      \"grad_norm\": 0.01164469588547945,\n",
      "      \"learning_rate\": 4.73241965973535e-05,\n",
      "      \"loss\": 0.0488,\n",
      "      \"step\": 8500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5368789186804878,\n",
      "      \"grad_norm\": 5.370800971984863,\n",
      "      \"learning_rate\": 4.731789540012603e-05,\n",
      "      \"loss\": 0.0666,\n",
      "      \"step\": 8520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5381391978323199,\n",
      "      \"grad_norm\": 18.417421340942383,\n",
      "      \"learning_rate\": 4.731159420289855e-05,\n",
      "      \"loss\": 0.0523,\n",
      "      \"step\": 8540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.539399476984152,\n",
      "      \"grad_norm\": 0.03376423940062523,\n",
      "      \"learning_rate\": 4.730529300567108e-05,\n",
      "      \"loss\": 0.0575,\n",
      "      \"step\": 8560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5406597561359842,\n",
      "      \"grad_norm\": 0.02202434279024601,\n",
      "      \"learning_rate\": 4.729899180844361e-05,\n",
      "      \"loss\": 0.0183,\n",
      "      \"step\": 8580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5419200352878163,\n",
      "      \"grad_norm\": 0.21275347471237183,\n",
      "      \"learning_rate\": 4.729269061121614e-05,\n",
      "      \"loss\": 0.0862,\n",
      "      \"step\": 8600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5431803144396484,\n",
      "      \"grad_norm\": 0.014504500664770603,\n",
      "      \"learning_rate\": 4.728638941398866e-05,\n",
      "      \"loss\": 0.0405,\n",
      "      \"step\": 8620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5444405935914806,\n",
      "      \"grad_norm\": 0.05086156725883484,\n",
      "      \"learning_rate\": 4.728008821676119e-05,\n",
      "      \"loss\": 0.0503,\n",
      "      \"step\": 8640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5457008727433127,\n",
      "      \"grad_norm\": 4.509451866149902,\n",
      "      \"learning_rate\": 4.727378701953372e-05,\n",
      "      \"loss\": 0.1317,\n",
      "      \"step\": 8660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5469611518951448,\n",
      "      \"grad_norm\": 1.5080945491790771,\n",
      "      \"learning_rate\": 4.726748582230624e-05,\n",
      "      \"loss\": 0.0537,\n",
      "      \"step\": 8680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.548221431046977,\n",
      "      \"grad_norm\": 6.184338092803955,\n",
      "      \"learning_rate\": 4.726118462507877e-05,\n",
      "      \"loss\": 0.0589,\n",
      "      \"step\": 8700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.549481710198809,\n",
      "      \"grad_norm\": 1.1082597970962524,\n",
      "      \"learning_rate\": 4.725488342785129e-05,\n",
      "      \"loss\": 0.0719,\n",
      "      \"step\": 8720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5507419893506411,\n",
      "      \"grad_norm\": 8.860063552856445,\n",
      "      \"learning_rate\": 4.724858223062382e-05,\n",
      "      \"loss\": 0.0166,\n",
      "      \"step\": 8740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5520022685024732,\n",
      "      \"grad_norm\": 0.04776636138558388,\n",
      "      \"learning_rate\": 4.724228103339635e-05,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 8760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5532625476543054,\n",
      "      \"grad_norm\": 21.467695236206055,\n",
      "      \"learning_rate\": 4.723597983616888e-05,\n",
      "      \"loss\": 0.0599,\n",
      "      \"step\": 8780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5545228268061375,\n",
      "      \"grad_norm\": 0.7957943081855774,\n",
      "      \"learning_rate\": 4.72296786389414e-05,\n",
      "      \"loss\": 0.0739,\n",
      "      \"step\": 8800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5557831059579696,\n",
      "      \"grad_norm\": 0.932508111000061,\n",
      "      \"learning_rate\": 4.722337744171393e-05,\n",
      "      \"loss\": 0.0776,\n",
      "      \"step\": 8820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5570433851098018,\n",
      "      \"grad_norm\": 0.1015951856970787,\n",
      "      \"learning_rate\": 4.721707624448645e-05,\n",
      "      \"loss\": 0.05,\n",
      "      \"step\": 8840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5583036642616339,\n",
      "      \"grad_norm\": 1.7589293718338013,\n",
      "      \"learning_rate\": 4.721077504725898e-05,\n",
      "      \"loss\": 0.0696,\n",
      "      \"step\": 8860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.559563943413466,\n",
      "      \"grad_norm\": 3.0802931785583496,\n",
      "      \"learning_rate\": 4.720447385003151e-05,\n",
      "      \"loss\": 0.098,\n",
      "      \"step\": 8880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5608242225652982,\n",
      "      \"grad_norm\": 5.768102169036865,\n",
      "      \"learning_rate\": 4.719817265280403e-05,\n",
      "      \"loss\": 0.0569,\n",
      "      \"step\": 8900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5620845017171303,\n",
      "      \"grad_norm\": 0.14110685884952545,\n",
      "      \"learning_rate\": 4.719187145557656e-05,\n",
      "      \"loss\": 0.0668,\n",
      "      \"step\": 8920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5633447808689624,\n",
      "      \"grad_norm\": 0.18338707089424133,\n",
      "      \"learning_rate\": 4.718557025834909e-05,\n",
      "      \"loss\": 0.0495,\n",
      "      \"step\": 8940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5646050600207946,\n",
      "      \"grad_norm\": 0.03896163031458855,\n",
      "      \"learning_rate\": 4.717926906112162e-05,\n",
      "      \"loss\": 0.0415,\n",
      "      \"step\": 8960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5658653391726267,\n",
      "      \"grad_norm\": 7.763710021972656,\n",
      "      \"learning_rate\": 4.717296786389414e-05,\n",
      "      \"loss\": 0.0943,\n",
      "      \"step\": 8980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5671256183244588,\n",
      "      \"grad_norm\": 0.10052556544542313,\n",
      "      \"learning_rate\": 4.716666666666667e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 9000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.568385897476291,\n",
      "      \"grad_norm\": 0.05857083946466446,\n",
      "      \"learning_rate\": 4.716036546943919e-05,\n",
      "      \"loss\": 0.0344,\n",
      "      \"step\": 9020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5696461766281231,\n",
      "      \"grad_norm\": 1.5027986764907837,\n",
      "      \"learning_rate\": 4.715406427221172e-05,\n",
      "      \"loss\": 0.0513,\n",
      "      \"step\": 9040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5709064557799552,\n",
      "      \"grad_norm\": 0.03255071863532066,\n",
      "      \"learning_rate\": 4.7147763074984244e-05,\n",
      "      \"loss\": 0.0305,\n",
      "      \"step\": 9060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5721667349317874,\n",
      "      \"grad_norm\": 0.03729468211531639,\n",
      "      \"learning_rate\": 4.714146187775678e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 9080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5734270140836195,\n",
      "      \"grad_norm\": 4.705999851226807,\n",
      "      \"learning_rate\": 4.713516068052931e-05,\n",
      "      \"loss\": 0.0615,\n",
      "      \"step\": 9100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5746872932354516,\n",
      "      \"grad_norm\": 18.30361557006836,\n",
      "      \"learning_rate\": 4.712885948330183e-05,\n",
      "      \"loss\": 0.0837,\n",
      "      \"step\": 9120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5759475723872838,\n",
      "      \"grad_norm\": 0.20344896614551544,\n",
      "      \"learning_rate\": 4.712255828607436e-05,\n",
      "      \"loss\": 0.0257,\n",
      "      \"step\": 9140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5772078515391159,\n",
      "      \"grad_norm\": 0.010054238140583038,\n",
      "      \"learning_rate\": 4.711625708884688e-05,\n",
      "      \"loss\": 0.0121,\n",
      "      \"step\": 9160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.578468130690948,\n",
      "      \"grad_norm\": 0.05223524197936058,\n",
      "      \"learning_rate\": 4.710995589161941e-05,\n",
      "      \"loss\": 0.1162,\n",
      "      \"step\": 9180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5797284098427802,\n",
      "      \"grad_norm\": 13.206975936889648,\n",
      "      \"learning_rate\": 4.7103654694391934e-05,\n",
      "      \"loss\": 0.0739,\n",
      "      \"step\": 9200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5809886889946123,\n",
      "      \"grad_norm\": 0.11169709265232086,\n",
      "      \"learning_rate\": 4.709735349716446e-05,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 9220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5822489681464444,\n",
      "      \"grad_norm\": 7.997443199157715,\n",
      "      \"learning_rate\": 4.709105229993699e-05,\n",
      "      \"loss\": 0.057,\n",
      "      \"step\": 9240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5835092472982766,\n",
      "      \"grad_norm\": 0.01452275738120079,\n",
      "      \"learning_rate\": 4.708475110270952e-05,\n",
      "      \"loss\": 0.0475,\n",
      "      \"step\": 9260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5847695264501087,\n",
      "      \"grad_norm\": 2.3395731449127197,\n",
      "      \"learning_rate\": 4.707844990548204e-05,\n",
      "      \"loss\": 0.0541,\n",
      "      \"step\": 9280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5860298056019408,\n",
      "      \"grad_norm\": 7.500533580780029,\n",
      "      \"learning_rate\": 4.707214870825457e-05,\n",
      "      \"loss\": 0.0649,\n",
      "      \"step\": 9300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.587290084753773,\n",
      "      \"grad_norm\": 16.248388290405273,\n",
      "      \"learning_rate\": 4.7065847511027094e-05,\n",
      "      \"loss\": 0.0774,\n",
      "      \"step\": 9320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5885503639056051,\n",
      "      \"grad_norm\": 0.030033044517040253,\n",
      "      \"learning_rate\": 4.705954631379962e-05,\n",
      "      \"loss\": 0.0549,\n",
      "      \"step\": 9340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5898106430574372,\n",
      "      \"grad_norm\": 8.600911140441895,\n",
      "      \"learning_rate\": 4.705324511657215e-05,\n",
      "      \"loss\": 0.0732,\n",
      "      \"step\": 9360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5910709222092694,\n",
      "      \"grad_norm\": 0.24025855958461761,\n",
      "      \"learning_rate\": 4.7046943919344675e-05,\n",
      "      \"loss\": 0.0319,\n",
      "      \"step\": 9380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5923312013611015,\n",
      "      \"grad_norm\": 0.11729554086923599,\n",
      "      \"learning_rate\": 4.7040642722117204e-05,\n",
      "      \"loss\": 0.0735,\n",
      "      \"step\": 9400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5935914805129336,\n",
      "      \"grad_norm\": 0.09965415298938751,\n",
      "      \"learning_rate\": 4.703434152488973e-05,\n",
      "      \"loss\": 0.0659,\n",
      "      \"step\": 9420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5948517596647658,\n",
      "      \"grad_norm\": 0.009900685399770737,\n",
      "      \"learning_rate\": 4.702804032766226e-05,\n",
      "      \"loss\": 0.068,\n",
      "      \"step\": 9440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5961120388165979,\n",
      "      \"grad_norm\": 0.04463949427008629,\n",
      "      \"learning_rate\": 4.7021739130434784e-05,\n",
      "      \"loss\": 0.0408,\n",
      "      \"step\": 9460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.59737231796843,\n",
      "      \"grad_norm\": 1.4867448806762695,\n",
      "      \"learning_rate\": 4.701543793320731e-05,\n",
      "      \"loss\": 0.0503,\n",
      "      \"step\": 9480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5986325971202622,\n",
      "      \"grad_norm\": 0.252226859331131,\n",
      "      \"learning_rate\": 4.7009136735979835e-05,\n",
      "      \"loss\": 0.0146,\n",
      "      \"step\": 9500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5998928762720943,\n",
      "      \"grad_norm\": 6.96367883682251,\n",
      "      \"learning_rate\": 4.7002835538752364e-05,\n",
      "      \"loss\": 0.0817,\n",
      "      \"step\": 9520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6011531554239264,\n",
      "      \"grad_norm\": 0.48152613639831543,\n",
      "      \"learning_rate\": 4.6996534341524887e-05,\n",
      "      \"loss\": 0.0292,\n",
      "      \"step\": 9540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6024134345757586,\n",
      "      \"grad_norm\": 0.043939393013715744,\n",
      "      \"learning_rate\": 4.6990233144297416e-05,\n",
      "      \"loss\": 0.0755,\n",
      "      \"step\": 9560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6036737137275907,\n",
      "      \"grad_norm\": 7.3504462242126465,\n",
      "      \"learning_rate\": 4.6983931947069945e-05,\n",
      "      \"loss\": 0.1489,\n",
      "      \"step\": 9580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6049339928794228,\n",
      "      \"grad_norm\": 1.3450202941894531,\n",
      "      \"learning_rate\": 4.6977630749842474e-05,\n",
      "      \"loss\": 0.0548,\n",
      "      \"step\": 9600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.606194272031255,\n",
      "      \"grad_norm\": 6.9827880859375,\n",
      "      \"learning_rate\": 4.6971644612476375e-05,\n",
      "      \"loss\": 0.1004,\n",
      "      \"step\": 9620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.607454551183087,\n",
      "      \"grad_norm\": 1.351224422454834,\n",
      "      \"learning_rate\": 4.69653434152489e-05,\n",
      "      \"loss\": 0.0949,\n",
      "      \"step\": 9640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6087148303349191,\n",
      "      \"grad_norm\": 1.3382591009140015,\n",
      "      \"learning_rate\": 4.6959042218021426e-05,\n",
      "      \"loss\": 0.0856,\n",
      "      \"step\": 9660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6099751094867513,\n",
      "      \"grad_norm\": 0.41752761602401733,\n",
      "      \"learning_rate\": 4.6952741020793955e-05,\n",
      "      \"loss\": 0.0585,\n",
      "      \"step\": 9680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6112353886385834,\n",
      "      \"grad_norm\": 6.4015936851501465,\n",
      "      \"learning_rate\": 4.694643982356648e-05,\n",
      "      \"loss\": 0.046,\n",
      "      \"step\": 9700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6124956677904155,\n",
      "      \"grad_norm\": 5.5087971687316895,\n",
      "      \"learning_rate\": 4.6940138626339006e-05,\n",
      "      \"loss\": 0.0483,\n",
      "      \"step\": 9720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6137559469422477,\n",
      "      \"grad_norm\": 0.023594103753566742,\n",
      "      \"learning_rate\": 4.6933837429111535e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 9740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6150162260940798,\n",
      "      \"grad_norm\": 0.024492047727108,\n",
      "      \"learning_rate\": 4.6927536231884064e-05,\n",
      "      \"loss\": 0.003,\n",
      "      \"step\": 9760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6162765052459119,\n",
      "      \"grad_norm\": 0.545230507850647,\n",
      "      \"learning_rate\": 4.6921235034656587e-05,\n",
      "      \"loss\": 0.1103,\n",
      "      \"step\": 9780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6175367843977441,\n",
      "      \"grad_norm\": 4.520772933959961,\n",
      "      \"learning_rate\": 4.6914933837429116e-05,\n",
      "      \"loss\": 0.0937,\n",
      "      \"step\": 9800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6187970635495762,\n",
      "      \"grad_norm\": 3.2158925533294678,\n",
      "      \"learning_rate\": 4.690863264020164e-05,\n",
      "      \"loss\": 0.1062,\n",
      "      \"step\": 9820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6200573427014083,\n",
      "      \"grad_norm\": 0.5150173306465149,\n",
      "      \"learning_rate\": 4.690233144297417e-05,\n",
      "      \"loss\": 0.0626,\n",
      "      \"step\": 9840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6213176218532405,\n",
      "      \"grad_norm\": 0.9272637367248535,\n",
      "      \"learning_rate\": 4.689603024574669e-05,\n",
      "      \"loss\": 0.0406,\n",
      "      \"step\": 9860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6225779010050726,\n",
      "      \"grad_norm\": 3.202035665512085,\n",
      "      \"learning_rate\": 4.688972904851922e-05,\n",
      "      \"loss\": 0.088,\n",
      "      \"step\": 9880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6238381801569047,\n",
      "      \"grad_norm\": 0.6597269773483276,\n",
      "      \"learning_rate\": 4.688342785129175e-05,\n",
      "      \"loss\": 0.0453,\n",
      "      \"step\": 9900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6250984593087369,\n",
      "      \"grad_norm\": 0.06567664444446564,\n",
      "      \"learning_rate\": 4.6877126654064276e-05,\n",
      "      \"loss\": 0.0299,\n",
      "      \"step\": 9920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.626358738460569,\n",
      "      \"grad_norm\": 0.025329861789941788,\n",
      "      \"learning_rate\": 4.6870825456836805e-05,\n",
      "      \"loss\": 0.051,\n",
      "      \"step\": 9940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6276190176124011,\n",
      "      \"grad_norm\": 4.6089677810668945,\n",
      "      \"learning_rate\": 4.686452425960933e-05,\n",
      "      \"loss\": 0.081,\n",
      "      \"step\": 9960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6288792967642333,\n",
      "      \"grad_norm\": 10.677755355834961,\n",
      "      \"learning_rate\": 4.6858223062381857e-05,\n",
      "      \"loss\": 0.0163,\n",
      "      \"step\": 9980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6301395759160654,\n",
      "      \"grad_norm\": 18.910966873168945,\n",
      "      \"learning_rate\": 4.685192186515438e-05,\n",
      "      \"loss\": 0.087,\n",
      "      \"step\": 10000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6313998550678975,\n",
      "      \"grad_norm\": 0.015598587691783905,\n",
      "      \"learning_rate\": 4.684562066792691e-05,\n",
      "      \"loss\": 0.0682,\n",
      "      \"step\": 10020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6326601342197297,\n",
      "      \"grad_norm\": 0.00917043350636959,\n",
      "      \"learning_rate\": 4.683931947069943e-05,\n",
      "      \"loss\": 0.0548,\n",
      "      \"step\": 10040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6339204133715618,\n",
      "      \"grad_norm\": 0.03983205929398537,\n",
      "      \"learning_rate\": 4.683301827347196e-05,\n",
      "      \"loss\": 0.0905,\n",
      "      \"step\": 10060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6351806925233939,\n",
      "      \"grad_norm\": 0.17905883491039276,\n",
      "      \"learning_rate\": 4.682671707624449e-05,\n",
      "      \"loss\": 0.0468,\n",
      "      \"step\": 10080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6364409716752261,\n",
      "      \"grad_norm\": 5.7014875411987305,\n",
      "      \"learning_rate\": 4.682041587901702e-05,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 10100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6377012508270582,\n",
      "      \"grad_norm\": 11.169157028198242,\n",
      "      \"learning_rate\": 4.6814114681789546e-05,\n",
      "      \"loss\": 0.049,\n",
      "      \"step\": 10120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6389615299788903,\n",
      "      \"grad_norm\": 10.5001859664917,\n",
      "      \"learning_rate\": 4.680781348456207e-05,\n",
      "      \"loss\": 0.0542,\n",
      "      \"step\": 10140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6402218091307225,\n",
      "      \"grad_norm\": 6.660096168518066,\n",
      "      \"learning_rate\": 4.68015122873346e-05,\n",
      "      \"loss\": 0.1043,\n",
      "      \"step\": 10160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6414820882825546,\n",
      "      \"grad_norm\": 10.984694480895996,\n",
      "      \"learning_rate\": 4.679521109010712e-05,\n",
      "      \"loss\": 0.0454,\n",
      "      \"step\": 10180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6427423674343867,\n",
      "      \"grad_norm\": 0.22249539196491241,\n",
      "      \"learning_rate\": 4.678890989287965e-05,\n",
      "      \"loss\": 0.0862,\n",
      "      \"step\": 10200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6440026465862189,\n",
      "      \"grad_norm\": 0.05440720543265343,\n",
      "      \"learning_rate\": 4.678260869565218e-05,\n",
      "      \"loss\": 0.0414,\n",
      "      \"step\": 10220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.645262925738051,\n",
      "      \"grad_norm\": 0.0068922266364097595,\n",
      "      \"learning_rate\": 4.677630749842471e-05,\n",
      "      \"loss\": 0.0328,\n",
      "      \"step\": 10240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6465232048898831,\n",
      "      \"grad_norm\": 1.9765515327453613,\n",
      "      \"learning_rate\": 4.677000630119723e-05,\n",
      "      \"loss\": 0.0558,\n",
      "      \"step\": 10260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6477834840417153,\n",
      "      \"grad_norm\": 0.02246333658695221,\n",
      "      \"learning_rate\": 4.676370510396976e-05,\n",
      "      \"loss\": 0.0479,\n",
      "      \"step\": 10280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6490437631935474,\n",
      "      \"grad_norm\": 0.06727202236652374,\n",
      "      \"learning_rate\": 4.675740390674228e-05,\n",
      "      \"loss\": 0.0341,\n",
      "      \"step\": 10300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6503040423453795,\n",
      "      \"grad_norm\": 0.07682961970567703,\n",
      "      \"learning_rate\": 4.675110270951481e-05,\n",
      "      \"loss\": 0.0857,\n",
      "      \"step\": 10320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6515643214972117,\n",
      "      \"grad_norm\": 0.03389522805809975,\n",
      "      \"learning_rate\": 4.674480151228734e-05,\n",
      "      \"loss\": 0.033,\n",
      "      \"step\": 10340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6528246006490438,\n",
      "      \"grad_norm\": 0.0033075925894081593,\n",
      "      \"learning_rate\": 4.673850031505986e-05,\n",
      "      \"loss\": 0.0371,\n",
      "      \"step\": 10360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6540848798008759,\n",
      "      \"grad_norm\": 15.016427040100098,\n",
      "      \"learning_rate\": 4.673219911783239e-05,\n",
      "      \"loss\": 0.0502,\n",
      "      \"step\": 10380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6553451589527081,\n",
      "      \"grad_norm\": 1.639092206954956,\n",
      "      \"learning_rate\": 4.672589792060492e-05,\n",
      "      \"loss\": 0.1324,\n",
      "      \"step\": 10400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6566054381045402,\n",
      "      \"grad_norm\": 0.345129132270813,\n",
      "      \"learning_rate\": 4.671959672337745e-05,\n",
      "      \"loss\": 0.0557,\n",
      "      \"step\": 10420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6578657172563723,\n",
      "      \"grad_norm\": 12.89963150024414,\n",
      "      \"learning_rate\": 4.671329552614997e-05,\n",
      "      \"loss\": 0.0382,\n",
      "      \"step\": 10440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6591259964082045,\n",
      "      \"grad_norm\": 0.9810912013053894,\n",
      "      \"learning_rate\": 4.67069943289225e-05,\n",
      "      \"loss\": 0.0813,\n",
      "      \"step\": 10460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6603862755600366,\n",
      "      \"grad_norm\": 0.8368459939956665,\n",
      "      \"learning_rate\": 4.670069313169502e-05,\n",
      "      \"loss\": 0.0844,\n",
      "      \"step\": 10480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6616465547118687,\n",
      "      \"grad_norm\": 3.9312736988067627,\n",
      "      \"learning_rate\": 4.669439193446755e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 10500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6629068338637009,\n",
      "      \"grad_norm\": 1.2338343858718872,\n",
      "      \"learning_rate\": 4.668809073724007e-05,\n",
      "      \"loss\": 0.0518,\n",
      "      \"step\": 10520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.664167113015533,\n",
      "      \"grad_norm\": 0.08009921759366989,\n",
      "      \"learning_rate\": 4.66817895400126e-05,\n",
      "      \"loss\": 0.0465,\n",
      "      \"step\": 10540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.665427392167365,\n",
      "      \"grad_norm\": 0.03726644068956375,\n",
      "      \"learning_rate\": 4.667548834278513e-05,\n",
      "      \"loss\": 0.0407,\n",
      "      \"step\": 10560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6666876713191972,\n",
      "      \"grad_norm\": 0.004477078095078468,\n",
      "      \"learning_rate\": 4.666918714555766e-05,\n",
      "      \"loss\": 0.029,\n",
      "      \"step\": 10580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6679479504710293,\n",
      "      \"grad_norm\": 10.58732795715332,\n",
      "      \"learning_rate\": 4.666288594833019e-05,\n",
      "      \"loss\": 0.0922,\n",
      "      \"step\": 10600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6692082296228614,\n",
      "      \"grad_norm\": 0.016991283744573593,\n",
      "      \"learning_rate\": 4.665658475110271e-05,\n",
      "      \"loss\": 0.0399,\n",
      "      \"step\": 10620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6704685087746936,\n",
      "      \"grad_norm\": 0.4353739619255066,\n",
      "      \"learning_rate\": 4.665028355387524e-05,\n",
      "      \"loss\": 0.0609,\n",
      "      \"step\": 10640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6717287879265257,\n",
      "      \"grad_norm\": 0.013446642085909843,\n",
      "      \"learning_rate\": 4.664398235664776e-05,\n",
      "      \"loss\": 0.0878,\n",
      "      \"step\": 10660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6729890670783578,\n",
      "      \"grad_norm\": 0.4312681555747986,\n",
      "      \"learning_rate\": 4.663768115942029e-05,\n",
      "      \"loss\": 0.0189,\n",
      "      \"step\": 10680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.67424934623019,\n",
      "      \"grad_norm\": 10.08935260772705,\n",
      "      \"learning_rate\": 4.6631379962192814e-05,\n",
      "      \"loss\": 0.0223,\n",
      "      \"step\": 10700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6755096253820221,\n",
      "      \"grad_norm\": 5.938556671142578,\n",
      "      \"learning_rate\": 4.662507876496535e-05,\n",
      "      \"loss\": 0.0466,\n",
      "      \"step\": 10720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6767699045338542,\n",
      "      \"grad_norm\": 0.008982207626104355,\n",
      "      \"learning_rate\": 4.661877756773787e-05,\n",
      "      \"loss\": 0.0538,\n",
      "      \"step\": 10740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6780301836856863,\n",
      "      \"grad_norm\": 4.630053520202637,\n",
      "      \"learning_rate\": 4.66124763705104e-05,\n",
      "      \"loss\": 0.0188,\n",
      "      \"step\": 10760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6792904628375185,\n",
      "      \"grad_norm\": 0.3670445382595062,\n",
      "      \"learning_rate\": 4.660617517328292e-05,\n",
      "      \"loss\": 0.0363,\n",
      "      \"step\": 10780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6805507419893506,\n",
      "      \"grad_norm\": 6.031826972961426,\n",
      "      \"learning_rate\": 4.659987397605545e-05,\n",
      "      \"loss\": 0.0394,\n",
      "      \"step\": 10800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6818110211411827,\n",
      "      \"grad_norm\": 0.6417878866195679,\n",
      "      \"learning_rate\": 4.659357277882798e-05,\n",
      "      \"loss\": 0.0449,\n",
      "      \"step\": 10820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6830713002930149,\n",
      "      \"grad_norm\": 0.028982531279325485,\n",
      "      \"learning_rate\": 4.6587271581600504e-05,\n",
      "      \"loss\": 0.0129,\n",
      "      \"step\": 10840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.684331579444847,\n",
      "      \"grad_norm\": 7.713030815124512,\n",
      "      \"learning_rate\": 4.658097038437303e-05,\n",
      "      \"loss\": 0.1001,\n",
      "      \"step\": 10860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6855918585966791,\n",
      "      \"grad_norm\": 0.11448197811841965,\n",
      "      \"learning_rate\": 4.657466918714556e-05,\n",
      "      \"loss\": 0.0365,\n",
      "      \"step\": 10880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6868521377485113,\n",
      "      \"grad_norm\": 0.5243691802024841,\n",
      "      \"learning_rate\": 4.656836798991809e-05,\n",
      "      \"loss\": 0.0365,\n",
      "      \"step\": 10900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6881124169003434,\n",
      "      \"grad_norm\": 16.064189910888672,\n",
      "      \"learning_rate\": 4.656206679269061e-05,\n",
      "      \"loss\": 0.0576,\n",
      "      \"step\": 10920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6893726960521755,\n",
      "      \"grad_norm\": 0.18956880271434784,\n",
      "      \"learning_rate\": 4.655576559546314e-05,\n",
      "      \"loss\": 0.0527,\n",
      "      \"step\": 10940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6906329752040077,\n",
      "      \"grad_norm\": 2.1533379554748535,\n",
      "      \"learning_rate\": 4.6549464398235664e-05,\n",
      "      \"loss\": 0.0844,\n",
      "      \"step\": 10960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6918932543558398,\n",
      "      \"grad_norm\": 2.069467067718506,\n",
      "      \"learning_rate\": 4.654316320100819e-05,\n",
      "      \"loss\": 0.0927,\n",
      "      \"step\": 10980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.693153533507672,\n",
      "      \"grad_norm\": 0.04041127488017082,\n",
      "      \"learning_rate\": 4.6536862003780716e-05,\n",
      "      \"loss\": 0.0841,\n",
      "      \"step\": 11000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6944138126595041,\n",
      "      \"grad_norm\": 0.04150478169322014,\n",
      "      \"learning_rate\": 4.6530560806553245e-05,\n",
      "      \"loss\": 0.0511,\n",
      "      \"step\": 11020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6956740918113362,\n",
      "      \"grad_norm\": 0.31597667932510376,\n",
      "      \"learning_rate\": 4.6524259609325774e-05,\n",
      "      \"loss\": 0.0734,\n",
      "      \"step\": 11040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6969343709631683,\n",
      "      \"grad_norm\": 0.11543092876672745,\n",
      "      \"learning_rate\": 4.65179584120983e-05,\n",
      "      \"loss\": 0.1334,\n",
      "      \"step\": 11060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6981946501150005,\n",
      "      \"grad_norm\": 0.025885391980409622,\n",
      "      \"learning_rate\": 4.651165721487083e-05,\n",
      "      \"loss\": 0.0583,\n",
      "      \"step\": 11080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6994549292668326,\n",
      "      \"grad_norm\": 8.234251976013184,\n",
      "      \"learning_rate\": 4.6505356017643354e-05,\n",
      "      \"loss\": 0.0786,\n",
      "      \"step\": 11100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7007152084186647,\n",
      "      \"grad_norm\": 5.520215034484863,\n",
      "      \"learning_rate\": 4.649905482041588e-05,\n",
      "      \"loss\": 0.0784,\n",
      "      \"step\": 11120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7019754875704969,\n",
      "      \"grad_norm\": 7.327229976654053,\n",
      "      \"learning_rate\": 4.6492753623188405e-05,\n",
      "      \"loss\": 0.0427,\n",
      "      \"step\": 11140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.703235766722329,\n",
      "      \"grad_norm\": 0.22041615843772888,\n",
      "      \"learning_rate\": 4.6486452425960934e-05,\n",
      "      \"loss\": 0.0306,\n",
      "      \"step\": 11160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7044960458741611,\n",
      "      \"grad_norm\": 0.10333330929279327,\n",
      "      \"learning_rate\": 4.648015122873346e-05,\n",
      "      \"loss\": 0.0314,\n",
      "      \"step\": 11180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7057563250259933,\n",
      "      \"grad_norm\": 0.031361594796180725,\n",
      "      \"learning_rate\": 4.6473850031505986e-05,\n",
      "      \"loss\": 0.0863,\n",
      "      \"step\": 11200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7070166041778254,\n",
      "      \"grad_norm\": 0.9413063526153564,\n",
      "      \"learning_rate\": 4.6467548834278515e-05,\n",
      "      \"loss\": 0.0329,\n",
      "      \"step\": 11220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7082768833296575,\n",
      "      \"grad_norm\": 0.3530305027961731,\n",
      "      \"learning_rate\": 4.6461247637051044e-05,\n",
      "      \"loss\": 0.0722,\n",
      "      \"step\": 11240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7095371624814897,\n",
      "      \"grad_norm\": 0.01965460740029812,\n",
      "      \"learning_rate\": 4.645494643982357e-05,\n",
      "      \"loss\": 0.0595,\n",
      "      \"step\": 11260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7107974416333218,\n",
      "      \"grad_norm\": 0.03730672970414162,\n",
      "      \"learning_rate\": 4.6448645242596095e-05,\n",
      "      \"loss\": 0.0433,\n",
      "      \"step\": 11280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.712057720785154,\n",
      "      \"grad_norm\": 0.8470544815063477,\n",
      "      \"learning_rate\": 4.6442344045368624e-05,\n",
      "      \"loss\": 0.1069,\n",
      "      \"step\": 11300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7133179999369861,\n",
      "      \"grad_norm\": 0.06204388290643692,\n",
      "      \"learning_rate\": 4.6436042848141146e-05,\n",
      "      \"loss\": 0.0407,\n",
      "      \"step\": 11320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7145782790888182,\n",
      "      \"grad_norm\": 0.16059546172618866,\n",
      "      \"learning_rate\": 4.6429741650913675e-05,\n",
      "      \"loss\": 0.0553,\n",
      "      \"step\": 11340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7158385582406503,\n",
      "      \"grad_norm\": 0.2121727019548416,\n",
      "      \"learning_rate\": 4.6423440453686204e-05,\n",
      "      \"loss\": 0.1317,\n",
      "      \"step\": 11360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7170988373924825,\n",
      "      \"grad_norm\": 0.36792629957199097,\n",
      "      \"learning_rate\": 4.6417139256458733e-05,\n",
      "      \"loss\": 0.0612,\n",
      "      \"step\": 11380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7183591165443146,\n",
      "      \"grad_norm\": 0.019063593819737434,\n",
      "      \"learning_rate\": 4.6410838059231256e-05,\n",
      "      \"loss\": 0.0194,\n",
      "      \"step\": 11400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7196193956961467,\n",
      "      \"grad_norm\": 0.2822956144809723,\n",
      "      \"learning_rate\": 4.6404536862003785e-05,\n",
      "      \"loss\": 0.0669,\n",
      "      \"step\": 11420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7208796748479789,\n",
      "      \"grad_norm\": 0.046441081911325455,\n",
      "      \"learning_rate\": 4.639823566477631e-05,\n",
      "      \"loss\": 0.0723,\n",
      "      \"step\": 11440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.722139953999811,\n",
      "      \"grad_norm\": 0.6101183891296387,\n",
      "      \"learning_rate\": 4.6391934467548836e-05,\n",
      "      \"loss\": 0.0467,\n",
      "      \"step\": 11460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.723400233151643,\n",
      "      \"grad_norm\": 3.5299510955810547,\n",
      "      \"learning_rate\": 4.6385633270321365e-05,\n",
      "      \"loss\": 0.0623,\n",
      "      \"step\": 11480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7246605123034752,\n",
      "      \"grad_norm\": 0.627066433429718,\n",
      "      \"learning_rate\": 4.637933207309389e-05,\n",
      "      \"loss\": 0.132,\n",
      "      \"step\": 11500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7259207914553073,\n",
      "      \"grad_norm\": 4.505162715911865,\n",
      "      \"learning_rate\": 4.6373030875866416e-05,\n",
      "      \"loss\": 0.0509,\n",
      "      \"step\": 11520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7271810706071394,\n",
      "      \"grad_norm\": 0.03866271674633026,\n",
      "      \"learning_rate\": 4.6366729678638945e-05,\n",
      "      \"loss\": 0.0406,\n",
      "      \"step\": 11540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7284413497589716,\n",
      "      \"grad_norm\": 8.781719207763672,\n",
      "      \"learning_rate\": 4.6360428481411475e-05,\n",
      "      \"loss\": 0.05,\n",
      "      \"step\": 11560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7297016289108037,\n",
      "      \"grad_norm\": 6.080777168273926,\n",
      "      \"learning_rate\": 4.6354127284184e-05,\n",
      "      \"loss\": 0.0212,\n",
      "      \"step\": 11580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7309619080626358,\n",
      "      \"grad_norm\": 0.0722712054848671,\n",
      "      \"learning_rate\": 4.6347826086956526e-05,\n",
      "      \"loss\": 0.0303,\n",
      "      \"step\": 11600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.732222187214468,\n",
      "      \"grad_norm\": 0.06247757375240326,\n",
      "      \"learning_rate\": 4.634152488972905e-05,\n",
      "      \"loss\": 0.0763,\n",
      "      \"step\": 11620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7334824663663001,\n",
      "      \"grad_norm\": 0.035316966474056244,\n",
      "      \"learning_rate\": 4.633522369250158e-05,\n",
      "      \"loss\": 0.0528,\n",
      "      \"step\": 11640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7347427455181322,\n",
      "      \"grad_norm\": 4.768519401550293,\n",
      "      \"learning_rate\": 4.63289224952741e-05,\n",
      "      \"loss\": 0.0593,\n",
      "      \"step\": 11660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7360030246699644,\n",
      "      \"grad_norm\": 12.240018844604492,\n",
      "      \"learning_rate\": 4.632262129804663e-05,\n",
      "      \"loss\": 0.0601,\n",
      "      \"step\": 11680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7372633038217965,\n",
      "      \"grad_norm\": 3.7352490425109863,\n",
      "      \"learning_rate\": 4.631632010081916e-05,\n",
      "      \"loss\": 0.0737,\n",
      "      \"step\": 11700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7385235829736286,\n",
      "      \"grad_norm\": 0.30540746450424194,\n",
      "      \"learning_rate\": 4.6310018903591686e-05,\n",
      "      \"loss\": 0.0672,\n",
      "      \"step\": 11720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7397838621254608,\n",
      "      \"grad_norm\": 0.11987296491861343,\n",
      "      \"learning_rate\": 4.6303717706364216e-05,\n",
      "      \"loss\": 0.068,\n",
      "      \"step\": 11740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7410441412772929,\n",
      "      \"grad_norm\": 7.617638111114502,\n",
      "      \"learning_rate\": 4.629741650913674e-05,\n",
      "      \"loss\": 0.0728,\n",
      "      \"step\": 11760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.742304420429125,\n",
      "      \"grad_norm\": 1.0701923370361328,\n",
      "      \"learning_rate\": 4.629111531190927e-05,\n",
      "      \"loss\": 0.0397,\n",
      "      \"step\": 11780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7435646995809572,\n",
      "      \"grad_norm\": 0.0869077667593956,\n",
      "      \"learning_rate\": 4.628481411468179e-05,\n",
      "      \"loss\": 0.0931,\n",
      "      \"step\": 11800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7448249787327893,\n",
      "      \"grad_norm\": 6.27673864364624,\n",
      "      \"learning_rate\": 4.627851291745432e-05,\n",
      "      \"loss\": 0.0636,\n",
      "      \"step\": 11820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7460852578846214,\n",
      "      \"grad_norm\": 7.04542875289917,\n",
      "      \"learning_rate\": 4.627221172022684e-05,\n",
      "      \"loss\": 0.0418,\n",
      "      \"step\": 11840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7473455370364536,\n",
      "      \"grad_norm\": 5.902878761291504,\n",
      "      \"learning_rate\": 4.626591052299937e-05,\n",
      "      \"loss\": 0.0502,\n",
      "      \"step\": 11860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7486058161882857,\n",
      "      \"grad_norm\": 7.460384845733643,\n",
      "      \"learning_rate\": 4.62596093257719e-05,\n",
      "      \"loss\": 0.0595,\n",
      "      \"step\": 11880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7498660953401178,\n",
      "      \"grad_norm\": 0.05969784036278725,\n",
      "      \"learning_rate\": 4.625330812854443e-05,\n",
      "      \"loss\": 0.0323,\n",
      "      \"step\": 11900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.75112637449195,\n",
      "      \"grad_norm\": 5.101088523864746,\n",
      "      \"learning_rate\": 4.6247006931316957e-05,\n",
      "      \"loss\": 0.0652,\n",
      "      \"step\": 11920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7523866536437821,\n",
      "      \"grad_norm\": 1.0311685800552368,\n",
      "      \"learning_rate\": 4.624070573408948e-05,\n",
      "      \"loss\": 0.046,\n",
      "      \"step\": 11940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7536469327956142,\n",
      "      \"grad_norm\": 0.020406262949109077,\n",
      "      \"learning_rate\": 4.623440453686201e-05,\n",
      "      \"loss\": 0.0572,\n",
      "      \"step\": 11960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7549072119474464,\n",
      "      \"grad_norm\": 0.3709343671798706,\n",
      "      \"learning_rate\": 4.622810333963453e-05,\n",
      "      \"loss\": 0.0386,\n",
      "      \"step\": 11980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7561674910992785,\n",
      "      \"grad_norm\": 0.026701493188738823,\n",
      "      \"learning_rate\": 4.622180214240706e-05,\n",
      "      \"loss\": 0.0527,\n",
      "      \"step\": 12000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7574277702511106,\n",
      "      \"grad_norm\": 0.05703088641166687,\n",
      "      \"learning_rate\": 4.621550094517959e-05,\n",
      "      \"loss\": 0.0579,\n",
      "      \"step\": 12020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7586880494029428,\n",
      "      \"grad_norm\": 0.47735270857810974,\n",
      "      \"learning_rate\": 4.620919974795212e-05,\n",
      "      \"loss\": 0.041,\n",
      "      \"step\": 12040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7599483285547749,\n",
      "      \"grad_norm\": 4.390919208526611,\n",
      "      \"learning_rate\": 4.620289855072464e-05,\n",
      "      \"loss\": 0.0277,\n",
      "      \"step\": 12060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.761208607706607,\n",
      "      \"grad_norm\": 11.761675834655762,\n",
      "      \"learning_rate\": 4.619659735349717e-05,\n",
      "      \"loss\": 0.0472,\n",
      "      \"step\": 12080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7624688868584392,\n",
      "      \"grad_norm\": 18.884830474853516,\n",
      "      \"learning_rate\": 4.619029615626969e-05,\n",
      "      \"loss\": 0.0589,\n",
      "      \"step\": 12100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7637291660102713,\n",
      "      \"grad_norm\": 0.004782428033649921,\n",
      "      \"learning_rate\": 4.618399495904222e-05,\n",
      "      \"loss\": 0.0455,\n",
      "      \"step\": 12120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7649894451621034,\n",
      "      \"grad_norm\": 11.829168319702148,\n",
      "      \"learning_rate\": 4.617769376181474e-05,\n",
      "      \"loss\": 0.0663,\n",
      "      \"step\": 12140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7662497243139356,\n",
      "      \"grad_norm\": 0.0022753800731152296,\n",
      "      \"learning_rate\": 4.617139256458727e-05,\n",
      "      \"loss\": 0.051,\n",
      "      \"step\": 12160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7675100034657677,\n",
      "      \"grad_norm\": 3.3829281330108643,\n",
      "      \"learning_rate\": 4.61650913673598e-05,\n",
      "      \"loss\": 0.0433,\n",
      "      \"step\": 12180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7687702826175998,\n",
      "      \"grad_norm\": 0.054219651967287064,\n",
      "      \"learning_rate\": 4.615879017013233e-05,\n",
      "      \"loss\": 0.0791,\n",
      "      \"step\": 12200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.770030561769432,\n",
      "      \"grad_norm\": 0.020260939374566078,\n",
      "      \"learning_rate\": 4.615248897290486e-05,\n",
      "      \"loss\": 0.0472,\n",
      "      \"step\": 12220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7712908409212641,\n",
      "      \"grad_norm\": 15.2724609375,\n",
      "      \"learning_rate\": 4.614618777567738e-05,\n",
      "      \"loss\": 0.0841,\n",
      "      \"step\": 12240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7725511200730962,\n",
      "      \"grad_norm\": 2.338841676712036,\n",
      "      \"learning_rate\": 4.613988657844991e-05,\n",
      "      \"loss\": 0.0321,\n",
      "      \"step\": 12260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7738113992249284,\n",
      "      \"grad_norm\": 0.0009633751469664276,\n",
      "      \"learning_rate\": 4.613358538122243e-05,\n",
      "      \"loss\": 0.023,\n",
      "      \"step\": 12280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7750716783767605,\n",
      "      \"grad_norm\": 0.007826965302228928,\n",
      "      \"learning_rate\": 4.612728418399496e-05,\n",
      "      \"loss\": 0.0235,\n",
      "      \"step\": 12300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7763319575285926,\n",
      "      \"grad_norm\": 0.1502206176519394,\n",
      "      \"learning_rate\": 4.612098298676748e-05,\n",
      "      \"loss\": 0.0584,\n",
      "      \"step\": 12320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7775922366804248,\n",
      "      \"grad_norm\": 0.017896104604005814,\n",
      "      \"learning_rate\": 4.611468178954001e-05,\n",
      "      \"loss\": 0.0602,\n",
      "      \"step\": 12340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7788525158322569,\n",
      "      \"grad_norm\": 0.021995076909661293,\n",
      "      \"learning_rate\": 4.610838059231254e-05,\n",
      "      \"loss\": 0.0502,\n",
      "      \"step\": 12360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.780112794984089,\n",
      "      \"grad_norm\": 3.8075530529022217,\n",
      "      \"learning_rate\": 4.610207939508507e-05,\n",
      "      \"loss\": 0.0454,\n",
      "      \"step\": 12380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.781373074135921,\n",
      "      \"grad_norm\": 0.1006091833114624,\n",
      "      \"learning_rate\": 4.60957781978576e-05,\n",
      "      \"loss\": 0.0867,\n",
      "      \"step\": 12400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7826333532877532,\n",
      "      \"grad_norm\": 0.35601451992988586,\n",
      "      \"learning_rate\": 4.608947700063012e-05,\n",
      "      \"loss\": 0.0462,\n",
      "      \"step\": 12420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7838936324395853,\n",
      "      \"grad_norm\": 4.952212333679199,\n",
      "      \"learning_rate\": 4.608317580340265e-05,\n",
      "      \"loss\": 0.0426,\n",
      "      \"step\": 12440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7851539115914175,\n",
      "      \"grad_norm\": 0.37173572182655334,\n",
      "      \"learning_rate\": 4.607687460617517e-05,\n",
      "      \"loss\": 0.0565,\n",
      "      \"step\": 12460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7864141907432496,\n",
      "      \"grad_norm\": 12.584774017333984,\n",
      "      \"learning_rate\": 4.60705734089477e-05,\n",
      "      \"loss\": 0.0988,\n",
      "      \"step\": 12480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7876744698950817,\n",
      "      \"grad_norm\": 0.03209386020898819,\n",
      "      \"learning_rate\": 4.6064272211720224e-05,\n",
      "      \"loss\": 0.0371,\n",
      "      \"step\": 12500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7889347490469139,\n",
      "      \"grad_norm\": 0.4462853670120239,\n",
      "      \"learning_rate\": 4.605797101449276e-05,\n",
      "      \"loss\": 0.0325,\n",
      "      \"step\": 12520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.790195028198746,\n",
      "      \"grad_norm\": 0.29711487889289856,\n",
      "      \"learning_rate\": 4.605166981726528e-05,\n",
      "      \"loss\": 0.0754,\n",
      "      \"step\": 12540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7914553073505781,\n",
      "      \"grad_norm\": 7.49525260925293,\n",
      "      \"learning_rate\": 4.604536862003781e-05,\n",
      "      \"loss\": 0.0781,\n",
      "      \"step\": 12560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7927155865024103,\n",
      "      \"grad_norm\": 0.08306465297937393,\n",
      "      \"learning_rate\": 4.6039067422810334e-05,\n",
      "      \"loss\": 0.0356,\n",
      "      \"step\": 12580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7939758656542424,\n",
      "      \"grad_norm\": 0.016861533746123314,\n",
      "      \"learning_rate\": 4.603276622558286e-05,\n",
      "      \"loss\": 0.0602,\n",
      "      \"step\": 12600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7952361448060745,\n",
      "      \"grad_norm\": 0.07213447242975235,\n",
      "      \"learning_rate\": 4.602646502835539e-05,\n",
      "      \"loss\": 0.0415,\n",
      "      \"step\": 12620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7964964239579067,\n",
      "      \"grad_norm\": 12.50028133392334,\n",
      "      \"learning_rate\": 4.6020163831127914e-05,\n",
      "      \"loss\": 0.0622,\n",
      "      \"step\": 12640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7977567031097388,\n",
      "      \"grad_norm\": 4.112276554107666,\n",
      "      \"learning_rate\": 4.601386263390044e-05,\n",
      "      \"loss\": 0.0533,\n",
      "      \"step\": 12660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7990169822615709,\n",
      "      \"grad_norm\": 3.9365155696868896,\n",
      "      \"learning_rate\": 4.600756143667297e-05,\n",
      "      \"loss\": 0.0493,\n",
      "      \"step\": 12680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.800277261413403,\n",
      "      \"grad_norm\": 0.16658514738082886,\n",
      "      \"learning_rate\": 4.60012602394455e-05,\n",
      "      \"loss\": 0.0411,\n",
      "      \"step\": 12700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8015375405652352,\n",
      "      \"grad_norm\": 0.042304836213588715,\n",
      "      \"learning_rate\": 4.599495904221802e-05,\n",
      "      \"loss\": 0.0444,\n",
      "      \"step\": 12720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8027978197170673,\n",
      "      \"grad_norm\": 0.027872469276189804,\n",
      "      \"learning_rate\": 4.598865784499055e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 12740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8040580988688995,\n",
      "      \"grad_norm\": 0.02536632865667343,\n",
      "      \"learning_rate\": 4.5982356647763075e-05,\n",
      "      \"loss\": 0.0475,\n",
      "      \"step\": 12760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8053183780207316,\n",
      "      \"grad_norm\": 0.41440349817276,\n",
      "      \"learning_rate\": 4.5976055450535604e-05,\n",
      "      \"loss\": 0.0352,\n",
      "      \"step\": 12780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8065786571725637,\n",
      "      \"grad_norm\": 9.725079536437988,\n",
      "      \"learning_rate\": 4.5969754253308126e-05,\n",
      "      \"loss\": 0.1257,\n",
      "      \"step\": 12800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8078389363243959,\n",
      "      \"grad_norm\": 0.07715149223804474,\n",
      "      \"learning_rate\": 4.5963453056080655e-05,\n",
      "      \"loss\": 0.0848,\n",
      "      \"step\": 12820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.809099215476228,\n",
      "      \"grad_norm\": 3.977210521697998,\n",
      "      \"learning_rate\": 4.5957151858853184e-05,\n",
      "      \"loss\": 0.0489,\n",
      "      \"step\": 12840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8103594946280601,\n",
      "      \"grad_norm\": 0.009730048477649689,\n",
      "      \"learning_rate\": 4.595085066162571e-05,\n",
      "      \"loss\": 0.0602,\n",
      "      \"step\": 12860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8116197737798923,\n",
      "      \"grad_norm\": 6.136133193969727,\n",
      "      \"learning_rate\": 4.594454946439824e-05,\n",
      "      \"loss\": 0.0332,\n",
      "      \"step\": 12880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8128800529317244,\n",
      "      \"grad_norm\": 2.9298360347747803,\n",
      "      \"learning_rate\": 4.5938248267170764e-05,\n",
      "      \"loss\": 0.0462,\n",
      "      \"step\": 12900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8141403320835565,\n",
      "      \"grad_norm\": 0.10613580048084259,\n",
      "      \"learning_rate\": 4.593194706994329e-05,\n",
      "      \"loss\": 0.075,\n",
      "      \"step\": 12920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8154006112353887,\n",
      "      \"grad_norm\": 11.406299591064453,\n",
      "      \"learning_rate\": 4.5925645872715816e-05,\n",
      "      \"loss\": 0.0867,\n",
      "      \"step\": 12940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8166608903872208,\n",
      "      \"grad_norm\": 2.862962245941162,\n",
      "      \"learning_rate\": 4.5919344675488345e-05,\n",
      "      \"loss\": 0.0389,\n",
      "      \"step\": 12960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8179211695390529,\n",
      "      \"grad_norm\": 0.16617295145988464,\n",
      "      \"learning_rate\": 4.591304347826087e-05,\n",
      "      \"loss\": 0.0528,\n",
      "      \"step\": 12980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.819181448690885,\n",
      "      \"grad_norm\": 0.007607799489051104,\n",
      "      \"learning_rate\": 4.5906742281033396e-05,\n",
      "      \"loss\": 0.0458,\n",
      "      \"step\": 13000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8204417278427172,\n",
      "      \"grad_norm\": 0.20493116974830627,\n",
      "      \"learning_rate\": 4.5900441083805925e-05,\n",
      "      \"loss\": 0.0364,\n",
      "      \"step\": 13020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8217020069945493,\n",
      "      \"grad_norm\": 0.1789986491203308,\n",
      "      \"learning_rate\": 4.5894139886578454e-05,\n",
      "      \"loss\": 0.1159,\n",
      "      \"step\": 13040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8229622861463814,\n",
      "      \"grad_norm\": 0.03538699075579643,\n",
      "      \"learning_rate\": 4.588783868935098e-05,\n",
      "      \"loss\": 0.0414,\n",
      "      \"step\": 13060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8242225652982136,\n",
      "      \"grad_norm\": 0.10973663628101349,\n",
      "      \"learning_rate\": 4.5881537492123505e-05,\n",
      "      \"loss\": 0.0962,\n",
      "      \"step\": 13080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8254828444500457,\n",
      "      \"grad_norm\": 0.5249761939048767,\n",
      "      \"learning_rate\": 4.5875236294896034e-05,\n",
      "      \"loss\": 0.0571,\n",
      "      \"step\": 13100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8267431236018778,\n",
      "      \"grad_norm\": 0.9802560210227966,\n",
      "      \"learning_rate\": 4.586893509766856e-05,\n",
      "      \"loss\": 0.0666,\n",
      "      \"step\": 13120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.82800340275371,\n",
      "      \"grad_norm\": 0.03694966807961464,\n",
      "      \"learning_rate\": 4.5862633900441086e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 13140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8292636819055421,\n",
      "      \"grad_norm\": 3.2266082763671875,\n",
      "      \"learning_rate\": 4.5856332703213615e-05,\n",
      "      \"loss\": 0.0639,\n",
      "      \"step\": 13160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8305239610573742,\n",
      "      \"grad_norm\": 7.134260654449463,\n",
      "      \"learning_rate\": 4.5850031505986144e-05,\n",
      "      \"loss\": 0.0997,\n",
      "      \"step\": 13180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8317842402092064,\n",
      "      \"grad_norm\": 8.13552188873291,\n",
      "      \"learning_rate\": 4.5843730308758666e-05,\n",
      "      \"loss\": 0.0481,\n",
      "      \"step\": 13200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8330445193610385,\n",
      "      \"grad_norm\": 0.0017356649041175842,\n",
      "      \"learning_rate\": 4.5837429111531195e-05,\n",
      "      \"loss\": 0.0317,\n",
      "      \"step\": 13220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8343047985128706,\n",
      "      \"grad_norm\": 1.2933200597763062,\n",
      "      \"learning_rate\": 4.583112791430372e-05,\n",
      "      \"loss\": 0.0257,\n",
      "      \"step\": 13240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8355650776647028,\n",
      "      \"grad_norm\": 6.811389923095703,\n",
      "      \"learning_rate\": 4.5824826717076246e-05,\n",
      "      \"loss\": 0.0566,\n",
      "      \"step\": 13260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8368253568165349,\n",
      "      \"grad_norm\": 0.0187965240329504,\n",
      "      \"learning_rate\": 4.5818525519848775e-05,\n",
      "      \"loss\": 0.0281,\n",
      "      \"step\": 13280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.838085635968367,\n",
      "      \"grad_norm\": 0.33676350116729736,\n",
      "      \"learning_rate\": 4.58122243226213e-05,\n",
      "      \"loss\": 0.1241,\n",
      "      \"step\": 13300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8393459151201991,\n",
      "      \"grad_norm\": 0.1926390528678894,\n",
      "      \"learning_rate\": 4.580592312539383e-05,\n",
      "      \"loss\": 0.0464,\n",
      "      \"step\": 13320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8406061942720312,\n",
      "      \"grad_norm\": 0.010080737993121147,\n",
      "      \"learning_rate\": 4.5799621928166356e-05,\n",
      "      \"loss\": 0.0678,\n",
      "      \"step\": 13340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8418664734238633,\n",
      "      \"grad_norm\": 4.815958499908447,\n",
      "      \"learning_rate\": 4.5793320730938885e-05,\n",
      "      \"loss\": 0.0383,\n",
      "      \"step\": 13360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8431267525756955,\n",
      "      \"grad_norm\": 0.25534743070602417,\n",
      "      \"learning_rate\": 4.578701953371141e-05,\n",
      "      \"loss\": 0.0315,\n",
      "      \"step\": 13380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8443870317275276,\n",
      "      \"grad_norm\": 4.914029121398926,\n",
      "      \"learning_rate\": 4.5780718336483936e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 13400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8456473108793597,\n",
      "      \"grad_norm\": 0.006281738169491291,\n",
      "      \"learning_rate\": 4.577441713925646e-05,\n",
      "      \"loss\": 0.0362,\n",
      "      \"step\": 13420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8469075900311919,\n",
      "      \"grad_norm\": 0.00751557806506753,\n",
      "      \"learning_rate\": 4.576811594202899e-05,\n",
      "      \"loss\": 0.0864,\n",
      "      \"step\": 13440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.848167869183024,\n",
      "      \"grad_norm\": 0.5081413984298706,\n",
      "      \"learning_rate\": 4.576181474480151e-05,\n",
      "      \"loss\": 0.0522,\n",
      "      \"step\": 13460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8494281483348561,\n",
      "      \"grad_norm\": 0.03225969895720482,\n",
      "      \"learning_rate\": 4.575551354757404e-05,\n",
      "      \"loss\": 0.0434,\n",
      "      \"step\": 13480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8506884274866883,\n",
      "      \"grad_norm\": 5.991766929626465,\n",
      "      \"learning_rate\": 4.574921235034657e-05,\n",
      "      \"loss\": 0.0351,\n",
      "      \"step\": 13500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8519487066385204,\n",
      "      \"grad_norm\": 0.02163073420524597,\n",
      "      \"learning_rate\": 4.57429111531191e-05,\n",
      "      \"loss\": 0.0336,\n",
      "      \"step\": 13520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8532089857903525,\n",
      "      \"grad_norm\": 0.13857945799827576,\n",
      "      \"learning_rate\": 4.5736609955891626e-05,\n",
      "      \"loss\": 0.0296,\n",
      "      \"step\": 13540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8544692649421847,\n",
      "      \"grad_norm\": 0.21176107227802277,\n",
      "      \"learning_rate\": 4.573030875866415e-05,\n",
      "      \"loss\": 0.0222,\n",
      "      \"step\": 13560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8557295440940168,\n",
      "      \"grad_norm\": 1.5558005571365356,\n",
      "      \"learning_rate\": 4.572400756143668e-05,\n",
      "      \"loss\": 0.0544,\n",
      "      \"step\": 13580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8569898232458489,\n",
      "      \"grad_norm\": 1.9056116342544556,\n",
      "      \"learning_rate\": 4.57177063642092e-05,\n",
      "      \"loss\": 0.0605,\n",
      "      \"step\": 13600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8582501023976811,\n",
      "      \"grad_norm\": 0.37130260467529297,\n",
      "      \"learning_rate\": 4.571140516698173e-05,\n",
      "      \"loss\": 0.051,\n",
      "      \"step\": 13620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8595103815495132,\n",
      "      \"grad_norm\": 0.1013282835483551,\n",
      "      \"learning_rate\": 4.570510396975425e-05,\n",
      "      \"loss\": 0.0294,\n",
      "      \"step\": 13640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8607706607013453,\n",
      "      \"grad_norm\": 0.22244124114513397,\n",
      "      \"learning_rate\": 4.5698802772526786e-05,\n",
      "      \"loss\": 0.0692,\n",
      "      \"step\": 13660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8620309398531775,\n",
      "      \"grad_norm\": 0.01383582130074501,\n",
      "      \"learning_rate\": 4.569250157529931e-05,\n",
      "      \"loss\": 0.0648,\n",
      "      \"step\": 13680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8632912190050096,\n",
      "      \"grad_norm\": 0.0441494844853878,\n",
      "      \"learning_rate\": 4.568620037807184e-05,\n",
      "      \"loss\": 0.032,\n",
      "      \"step\": 13700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8645514981568417,\n",
      "      \"grad_norm\": 0.5956366062164307,\n",
      "      \"learning_rate\": 4.568021424070574e-05,\n",
      "      \"loss\": 0.0936,\n",
      "      \"step\": 13720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8658117773086739,\n",
      "      \"grad_norm\": 5.665151596069336,\n",
      "      \"learning_rate\": 4.567391304347826e-05,\n",
      "      \"loss\": 0.0723,\n",
      "      \"step\": 13740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.867072056460506,\n",
      "      \"grad_norm\": 0.19867146015167236,\n",
      "      \"learning_rate\": 4.566761184625079e-05,\n",
      "      \"loss\": 0.0546,\n",
      "      \"step\": 13760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8683323356123381,\n",
      "      \"grad_norm\": 0.04001861438155174,\n",
      "      \"learning_rate\": 4.566131064902331e-05,\n",
      "      \"loss\": 0.0319,\n",
      "      \"step\": 13780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8695926147641703,\n",
      "      \"grad_norm\": 37.413673400878906,\n",
      "      \"learning_rate\": 4.565500945179584e-05,\n",
      "      \"loss\": 0.0262,\n",
      "      \"step\": 13800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8708528939160024,\n",
      "      \"grad_norm\": 0.30064693093299866,\n",
      "      \"learning_rate\": 4.564870825456837e-05,\n",
      "      \"loss\": 0.0935,\n",
      "      \"step\": 13820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8721131730678345,\n",
      "      \"grad_norm\": 0.384244441986084,\n",
      "      \"learning_rate\": 4.56424070573409e-05,\n",
      "      \"loss\": 0.0522,\n",
      "      \"step\": 13840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8733734522196667,\n",
      "      \"grad_norm\": 0.08958962559700012,\n",
      "      \"learning_rate\": 4.563610586011343e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 13860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8746337313714988,\n",
      "      \"grad_norm\": 0.28250813484191895,\n",
      "      \"learning_rate\": 4.562980466288595e-05,\n",
      "      \"loss\": 0.0553,\n",
      "      \"step\": 13880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8758940105233309,\n",
      "      \"grad_norm\": 0.09056070446968079,\n",
      "      \"learning_rate\": 4.562350346565848e-05,\n",
      "      \"loss\": 0.0314,\n",
      "      \"step\": 13900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8771542896751631,\n",
      "      \"grad_norm\": 26.380615234375,\n",
      "      \"learning_rate\": 4.5617202268431e-05,\n",
      "      \"loss\": 0.0436,\n",
      "      \"step\": 13920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8784145688269952,\n",
      "      \"grad_norm\": 0.08093801885843277,\n",
      "      \"learning_rate\": 4.561090107120353e-05,\n",
      "      \"loss\": 0.1267,\n",
      "      \"step\": 13940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8796748479788273,\n",
      "      \"grad_norm\": 16.631704330444336,\n",
      "      \"learning_rate\": 4.560459987397605e-05,\n",
      "      \"loss\": 0.0473,\n",
      "      \"step\": 13960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8809351271306595,\n",
      "      \"grad_norm\": 0.018150031566619873,\n",
      "      \"learning_rate\": 4.559829867674858e-05,\n",
      "      \"loss\": 0.0636,\n",
      "      \"step\": 13980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8821954062824916,\n",
      "      \"grad_norm\": 6.6134748458862305,\n",
      "      \"learning_rate\": 4.559199747952111e-05,\n",
      "      \"loss\": 0.0491,\n",
      "      \"step\": 14000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8834556854343237,\n",
      "      \"grad_norm\": 8.033160209655762,\n",
      "      \"learning_rate\": 4.558569628229364e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 14020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8847159645861559,\n",
      "      \"grad_norm\": 0.20980845391750336,\n",
      "      \"learning_rate\": 4.557939508506616e-05,\n",
      "      \"loss\": 0.0756,\n",
      "      \"step\": 14040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.885976243737988,\n",
      "      \"grad_norm\": 0.01684679090976715,\n",
      "      \"learning_rate\": 4.557309388783869e-05,\n",
      "      \"loss\": 0.0611,\n",
      "      \"step\": 14060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8872365228898201,\n",
      "      \"grad_norm\": 0.22333645820617676,\n",
      "      \"learning_rate\": 4.556679269061122e-05,\n",
      "      \"loss\": 0.0354,\n",
      "      \"step\": 14080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8884968020416523,\n",
      "      \"grad_norm\": 1.152116298675537,\n",
      "      \"learning_rate\": 4.556049149338374e-05,\n",
      "      \"loss\": 0.0529,\n",
      "      \"step\": 14100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8897570811934844,\n",
      "      \"grad_norm\": 0.031209710985422134,\n",
      "      \"learning_rate\": 4.555419029615627e-05,\n",
      "      \"loss\": 0.013,\n",
      "      \"step\": 14120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8910173603453165,\n",
      "      \"grad_norm\": 0.03626134246587753,\n",
      "      \"learning_rate\": 4.5547889098928794e-05,\n",
      "      \"loss\": 0.0572,\n",
      "      \"step\": 14140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8922776394971487,\n",
      "      \"grad_norm\": 0.21416959166526794,\n",
      "      \"learning_rate\": 4.554158790170133e-05,\n",
      "      \"loss\": 0.0526,\n",
      "      \"step\": 14160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8935379186489808,\n",
      "      \"grad_norm\": 7.680759429931641,\n",
      "      \"learning_rate\": 4.553528670447385e-05,\n",
      "      \"loss\": 0.0363,\n",
      "      \"step\": 14180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8947981978008129,\n",
      "      \"grad_norm\": 0.17808900773525238,\n",
      "      \"learning_rate\": 4.552898550724638e-05,\n",
      "      \"loss\": 0.0366,\n",
      "      \"step\": 14200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.896058476952645,\n",
      "      \"grad_norm\": 0.11218554526567459,\n",
      "      \"learning_rate\": 4.5522684310018904e-05,\n",
      "      \"loss\": 0.0369,\n",
      "      \"step\": 14220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8973187561044771,\n",
      "      \"grad_norm\": 0.02175402082502842,\n",
      "      \"learning_rate\": 4.551638311279143e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 14240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8985790352563092,\n",
      "      \"grad_norm\": 3.6512949466705322,\n",
      "      \"learning_rate\": 4.5510396975425334e-05,\n",
      "      \"loss\": 0.0413,\n",
      "      \"step\": 14260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8998393144081414,\n",
      "      \"grad_norm\": 0.03097127191722393,\n",
      "      \"learning_rate\": 4.5504095778197856e-05,\n",
      "      \"loss\": 0.0142,\n",
      "      \"step\": 14280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9010995935599735,\n",
      "      \"grad_norm\": 0.3022596836090088,\n",
      "      \"learning_rate\": 4.5497794580970385e-05,\n",
      "      \"loss\": 0.0816,\n",
      "      \"step\": 14300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9023598727118056,\n",
      "      \"grad_norm\": 32.11480712890625,\n",
      "      \"learning_rate\": 4.5491493383742914e-05,\n",
      "      \"loss\": 0.0753,\n",
      "      \"step\": 14320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9036201518636378,\n",
      "      \"grad_norm\": 0.03727547451853752,\n",
      "      \"learning_rate\": 4.548519218651544e-05,\n",
      "      \"loss\": 0.0409,\n",
      "      \"step\": 14340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9048804310154699,\n",
      "      \"grad_norm\": 0.01195797510445118,\n",
      "      \"learning_rate\": 4.5478890989287965e-05,\n",
      "      \"loss\": 0.0428,\n",
      "      \"step\": 14360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.906140710167302,\n",
      "      \"grad_norm\": 0.4462912380695343,\n",
      "      \"learning_rate\": 4.5472589792060494e-05,\n",
      "      \"loss\": 0.0176,\n",
      "      \"step\": 14380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9074009893191342,\n",
      "      \"grad_norm\": 0.01776699908077717,\n",
      "      \"learning_rate\": 4.546628859483302e-05,\n",
      "      \"loss\": 0.0342,\n",
      "      \"step\": 14400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9086612684709663,\n",
      "      \"grad_norm\": 0.004378767684102058,\n",
      "      \"learning_rate\": 4.5459987397605546e-05,\n",
      "      \"loss\": 0.0411,\n",
      "      \"step\": 14420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9099215476227984,\n",
      "      \"grad_norm\": 0.0035595460794866085,\n",
      "      \"learning_rate\": 4.5453686200378075e-05,\n",
      "      \"loss\": 0.0281,\n",
      "      \"step\": 14440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9111818267746306,\n",
      "      \"grad_norm\": 0.01823396608233452,\n",
      "      \"learning_rate\": 4.54473850031506e-05,\n",
      "      \"loss\": 0.1236,\n",
      "      \"step\": 14460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9124421059264627,\n",
      "      \"grad_norm\": 0.03809212148189545,\n",
      "      \"learning_rate\": 4.5441083805923126e-05,\n",
      "      \"loss\": 0.0603,\n",
      "      \"step\": 14480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9137023850782948,\n",
      "      \"grad_norm\": 0.10337182879447937,\n",
      "      \"learning_rate\": 4.5434782608695655e-05,\n",
      "      \"loss\": 0.0627,\n",
      "      \"step\": 14500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.914962664230127,\n",
      "      \"grad_norm\": 0.11767587810754776,\n",
      "      \"learning_rate\": 4.5428481411468184e-05,\n",
      "      \"loss\": 0.0511,\n",
      "      \"step\": 14520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9162229433819591,\n",
      "      \"grad_norm\": 0.009421146474778652,\n",
      "      \"learning_rate\": 4.5422180214240706e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 14540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9174832225337912,\n",
      "      \"grad_norm\": 5.52505350112915,\n",
      "      \"learning_rate\": 4.5415879017013235e-05,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 14560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9187435016856234,\n",
      "      \"grad_norm\": 6.214762210845947,\n",
      "      \"learning_rate\": 4.540957781978576e-05,\n",
      "      \"loss\": 0.095,\n",
      "      \"step\": 14580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9200037808374555,\n",
      "      \"grad_norm\": 0.4802316725254059,\n",
      "      \"learning_rate\": 4.540327662255829e-05,\n",
      "      \"loss\": 0.0682,\n",
      "      \"step\": 14600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9212640599892876,\n",
      "      \"grad_norm\": 0.006742214318364859,\n",
      "      \"learning_rate\": 4.5396975425330816e-05,\n",
      "      \"loss\": 0.0278,\n",
      "      \"step\": 14620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9225243391411198,\n",
      "      \"grad_norm\": 12.517877578735352,\n",
      "      \"learning_rate\": 4.5390674228103345e-05,\n",
      "      \"loss\": 0.0386,\n",
      "      \"step\": 14640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9237846182929519,\n",
      "      \"grad_norm\": 0.0018095184350386262,\n",
      "      \"learning_rate\": 4.5384373030875874e-05,\n",
      "      \"loss\": 0.0248,\n",
      "      \"step\": 14660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.925044897444784,\n",
      "      \"grad_norm\": 0.18025577068328857,\n",
      "      \"learning_rate\": 4.5378071833648396e-05,\n",
      "      \"loss\": 0.0552,\n",
      "      \"step\": 14680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9263051765966162,\n",
      "      \"grad_norm\": 5.885438442230225,\n",
      "      \"learning_rate\": 4.5371770636420925e-05,\n",
      "      \"loss\": 0.0281,\n",
      "      \"step\": 14700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9275654557484483,\n",
      "      \"grad_norm\": 0.01513284258544445,\n",
      "      \"learning_rate\": 4.536546943919345e-05,\n",
      "      \"loss\": 0.0287,\n",
      "      \"step\": 14720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9288257349002804,\n",
      "      \"grad_norm\": 0.032574430108070374,\n",
      "      \"learning_rate\": 4.5359168241965976e-05,\n",
      "      \"loss\": 0.0542,\n",
      "      \"step\": 14740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9300860140521126,\n",
      "      \"grad_norm\": 9.442314147949219,\n",
      "      \"learning_rate\": 4.53528670447385e-05,\n",
      "      \"loss\": 0.0435,\n",
      "      \"step\": 14760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9313462932039447,\n",
      "      \"grad_norm\": 0.009513228200376034,\n",
      "      \"learning_rate\": 4.534656584751103e-05,\n",
      "      \"loss\": 0.0582,\n",
      "      \"step\": 14780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9326065723557768,\n",
      "      \"grad_norm\": 5.242216110229492,\n",
      "      \"learning_rate\": 4.534026465028356e-05,\n",
      "      \"loss\": 0.0964,\n",
      "      \"step\": 14800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.933866851507609,\n",
      "      \"grad_norm\": 0.16402137279510498,\n",
      "      \"learning_rate\": 4.5333963453056086e-05,\n",
      "      \"loss\": 0.0316,\n",
      "      \"step\": 14820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9351271306594411,\n",
      "      \"grad_norm\": 6.770022392272949,\n",
      "      \"learning_rate\": 4.5327662255828615e-05,\n",
      "      \"loss\": 0.055,\n",
      "      \"step\": 14840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9363874098112732,\n",
      "      \"grad_norm\": 0.13546647131443024,\n",
      "      \"learning_rate\": 4.532136105860114e-05,\n",
      "      \"loss\": 0.0651,\n",
      "      \"step\": 14860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9376476889631054,\n",
      "      \"grad_norm\": 0.18411362171173096,\n",
      "      \"learning_rate\": 4.5315059861373666e-05,\n",
      "      \"loss\": 0.0928,\n",
      "      \"step\": 14880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9389079681149375,\n",
      "      \"grad_norm\": 0.43633517622947693,\n",
      "      \"learning_rate\": 4.530875866414619e-05,\n",
      "      \"loss\": 0.018,\n",
      "      \"step\": 14900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9401682472667696,\n",
      "      \"grad_norm\": 0.006674881558865309,\n",
      "      \"learning_rate\": 4.530245746691872e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 14920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9414285264186018,\n",
      "      \"grad_norm\": 0.005131285637617111,\n",
      "      \"learning_rate\": 4.529615626969124e-05,\n",
      "      \"loss\": 0.0178,\n",
      "      \"step\": 14940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9426888055704339,\n",
      "      \"grad_norm\": 0.4672951102256775,\n",
      "      \"learning_rate\": 4.528985507246377e-05,\n",
      "      \"loss\": 0.0803,\n",
      "      \"step\": 14960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.943949084722266,\n",
      "      \"grad_norm\": 0.0519232340157032,\n",
      "      \"learning_rate\": 4.52835538752363e-05,\n",
      "      \"loss\": 0.0492,\n",
      "      \"step\": 14980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9452093638740982,\n",
      "      \"grad_norm\": 3.421475648880005,\n",
      "      \"learning_rate\": 4.527725267800883e-05,\n",
      "      \"loss\": 0.032,\n",
      "      \"step\": 15000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9464696430259303,\n",
      "      \"grad_norm\": 0.06055670604109764,\n",
      "      \"learning_rate\": 4.527095148078135e-05,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 15020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9477299221777624,\n",
      "      \"grad_norm\": 0.0065307775512337685,\n",
      "      \"learning_rate\": 4.526465028355388e-05,\n",
      "      \"loss\": 0.0479,\n",
      "      \"step\": 15040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9489902013295946,\n",
      "      \"grad_norm\": 0.09027700871229172,\n",
      "      \"learning_rate\": 4.52583490863264e-05,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 15060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9502504804814267,\n",
      "      \"grad_norm\": 18.449466705322266,\n",
      "      \"learning_rate\": 4.525204788909893e-05,\n",
      "      \"loss\": 0.0678,\n",
      "      \"step\": 15080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9515107596332588,\n",
      "      \"grad_norm\": 4.737993240356445,\n",
      "      \"learning_rate\": 4.524574669187146e-05,\n",
      "      \"loss\": 0.0615,\n",
      "      \"step\": 15100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.952771038785091,\n",
      "      \"grad_norm\": 0.21949376165866852,\n",
      "      \"learning_rate\": 4.523944549464398e-05,\n",
      "      \"loss\": 0.0647,\n",
      "      \"step\": 15120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.954031317936923,\n",
      "      \"grad_norm\": 0.38775429129600525,\n",
      "      \"learning_rate\": 4.5233144297416516e-05,\n",
      "      \"loss\": 0.048,\n",
      "      \"step\": 15140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9552915970887551,\n",
      "      \"grad_norm\": 0.13441477715969086,\n",
      "      \"learning_rate\": 4.522684310018904e-05,\n",
      "      \"loss\": 0.0421,\n",
      "      \"step\": 15160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9565518762405872,\n",
      "      \"grad_norm\": 10.009575843811035,\n",
      "      \"learning_rate\": 4.522054190296157e-05,\n",
      "      \"loss\": 0.0435,\n",
      "      \"step\": 15180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9578121553924194,\n",
      "      \"grad_norm\": 9.855249404907227,\n",
      "      \"learning_rate\": 4.521424070573409e-05,\n",
      "      \"loss\": 0.0379,\n",
      "      \"step\": 15200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9590724345442515,\n",
      "      \"grad_norm\": 0.10159255564212799,\n",
      "      \"learning_rate\": 4.520793950850662e-05,\n",
      "      \"loss\": 0.0473,\n",
      "      \"step\": 15220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9603327136960836,\n",
      "      \"grad_norm\": 10.168527603149414,\n",
      "      \"learning_rate\": 4.520163831127914e-05,\n",
      "      \"loss\": 0.0631,\n",
      "      \"step\": 15240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9615929928479158,\n",
      "      \"grad_norm\": 0.017271438613533974,\n",
      "      \"learning_rate\": 4.519533711405167e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 15260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9628532719997479,\n",
      "      \"grad_norm\": 0.02852620556950569,\n",
      "      \"learning_rate\": 4.51890359168242e-05,\n",
      "      \"loss\": 0.0637,\n",
      "      \"step\": 15280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.96411355115158,\n",
      "      \"grad_norm\": 2.0553817749023438,\n",
      "      \"learning_rate\": 4.518273471959673e-05,\n",
      "      \"loss\": 0.0909,\n",
      "      \"step\": 15300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9653738303034122,\n",
      "      \"grad_norm\": 0.010779297910630703,\n",
      "      \"learning_rate\": 4.517643352236926e-05,\n",
      "      \"loss\": 0.0517,\n",
      "      \"step\": 15320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9666341094552443,\n",
      "      \"grad_norm\": 0.26144760847091675,\n",
      "      \"learning_rate\": 4.517013232514178e-05,\n",
      "      \"loss\": 0.0053,\n",
      "      \"step\": 15340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9678943886070764,\n",
      "      \"grad_norm\": 0.07389646023511887,\n",
      "      \"learning_rate\": 4.516383112791431e-05,\n",
      "      \"loss\": 0.0439,\n",
      "      \"step\": 15360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9691546677589086,\n",
      "      \"grad_norm\": 0.07015387713909149,\n",
      "      \"learning_rate\": 4.515752993068683e-05,\n",
      "      \"loss\": 0.0698,\n",
      "      \"step\": 15380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9704149469107407,\n",
      "      \"grad_norm\": 0.27314668893814087,\n",
      "      \"learning_rate\": 4.515122873345936e-05,\n",
      "      \"loss\": 0.0754,\n",
      "      \"step\": 15400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9716752260625728,\n",
      "      \"grad_norm\": 0.23208487033843994,\n",
      "      \"learning_rate\": 4.514492753623188e-05,\n",
      "      \"loss\": 0.0441,\n",
      "      \"step\": 15420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.972935505214405,\n",
      "      \"grad_norm\": 5.0150227546691895,\n",
      "      \"learning_rate\": 4.513862633900441e-05,\n",
      "      \"loss\": 0.0287,\n",
      "      \"step\": 15440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9741957843662371,\n",
      "      \"grad_norm\": 6.705011367797852,\n",
      "      \"learning_rate\": 4.513232514177694e-05,\n",
      "      \"loss\": 0.0351,\n",
      "      \"step\": 15460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9754560635180692,\n",
      "      \"grad_norm\": 0.007719460874795914,\n",
      "      \"learning_rate\": 4.512602394454947e-05,\n",
      "      \"loss\": 0.0632,\n",
      "      \"step\": 15480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9767163426699014,\n",
      "      \"grad_norm\": 1.7251144647598267,\n",
      "      \"learning_rate\": 4.511972274732199e-05,\n",
      "      \"loss\": 0.032,\n",
      "      \"step\": 15500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9779766218217335,\n",
      "      \"grad_norm\": 0.0026622938457876444,\n",
      "      \"learning_rate\": 4.511342155009452e-05,\n",
      "      \"loss\": 0.0299,\n",
      "      \"step\": 15520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9792369009735656,\n",
      "      \"grad_norm\": 0.19793950021266937,\n",
      "      \"learning_rate\": 4.510712035286705e-05,\n",
      "      \"loss\": 0.0335,\n",
      "      \"step\": 15540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9804971801253978,\n",
      "      \"grad_norm\": 21.468637466430664,\n",
      "      \"learning_rate\": 4.510081915563957e-05,\n",
      "      \"loss\": 0.0176,\n",
      "      \"step\": 15560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9817574592772299,\n",
      "      \"grad_norm\": 0.05236975848674774,\n",
      "      \"learning_rate\": 4.50945179584121e-05,\n",
      "      \"loss\": 0.0787,\n",
      "      \"step\": 15580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.983017738429062,\n",
      "      \"grad_norm\": 0.0033675755839794874,\n",
      "      \"learning_rate\": 4.508821676118462e-05,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 15600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9842780175808942,\n",
      "      \"grad_norm\": 0.059873394668102264,\n",
      "      \"learning_rate\": 4.508191556395715e-05,\n",
      "      \"loss\": 0.0459,\n",
      "      \"step\": 15620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9855382967327263,\n",
      "      \"grad_norm\": 0.008818383328616619,\n",
      "      \"learning_rate\": 4.507561436672968e-05,\n",
      "      \"loss\": 0.051,\n",
      "      \"step\": 15640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9867985758845584,\n",
      "      \"grad_norm\": 0.0393497534096241,\n",
      "      \"learning_rate\": 4.506931316950221e-05,\n",
      "      \"loss\": 0.0468,\n",
      "      \"step\": 15660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9880588550363906,\n",
      "      \"grad_norm\": 0.6247174143791199,\n",
      "      \"learning_rate\": 4.506301197227473e-05,\n",
      "      \"loss\": 0.0962,\n",
      "      \"step\": 15680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9893191341882227,\n",
      "      \"grad_norm\": 0.03290804475545883,\n",
      "      \"learning_rate\": 4.505671077504726e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 15700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9905794133400548,\n",
      "      \"grad_norm\": 0.07937852293252945,\n",
      "      \"learning_rate\": 4.5050409577819784e-05,\n",
      "      \"loss\": 0.016,\n",
      "      \"step\": 15720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.991839692491887,\n",
      "      \"grad_norm\": 0.022265609353780746,\n",
      "      \"learning_rate\": 4.504410838059231e-05,\n",
      "      \"loss\": 0.1092,\n",
      "      \"step\": 15740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9930999716437191,\n",
      "      \"grad_norm\": 0.041403912007808685,\n",
      "      \"learning_rate\": 4.503780718336484e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 15760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9943602507955512,\n",
      "      \"grad_norm\": 0.011079971678555012,\n",
      "      \"learning_rate\": 4.503150598613737e-05,\n",
      "      \"loss\": 0.0232,\n",
      "      \"step\": 15780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9956205299473834,\n",
      "      \"grad_norm\": 0.07691352814435959,\n",
      "      \"learning_rate\": 4.50252047889099e-05,\n",
      "      \"loss\": 0.0522,\n",
      "      \"step\": 15800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9968808090992155,\n",
      "      \"grad_norm\": 6.202030658721924,\n",
      "      \"learning_rate\": 4.501890359168242e-05,\n",
      "      \"loss\": 0.0277,\n",
      "      \"step\": 15820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9981410882510476,\n",
      "      \"grad_norm\": 7.493568420410156,\n",
      "      \"learning_rate\": 4.501260239445495e-05,\n",
      "      \"loss\": 0.0665,\n",
      "      \"step\": 15840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9994013674028798,\n",
      "      \"grad_norm\": 0.9115056395530701,\n",
      "      \"learning_rate\": 4.5006301197227474e-05,\n",
      "      \"loss\": 0.0704,\n",
      "      \"step\": 15860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0,\n",
      "      \"eval_accuracy\": 0.9947388318316426,\n",
      "      \"eval_f1\": 0.9947624274737338,\n",
      "      \"eval_loss\": 0.017583901062607765,\n",
      "      \"eval_precision\": 0.990320969151992,\n",
      "      \"eval_recall\": 0.999243903975805,\n",
      "      \"eval_runtime\": 570.5789,\n",
      "      \"eval_samples_per_second\": 55.631,\n",
      "      \"eval_steps_per_second\": 6.954,\n",
      "      \"step\": 15870\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.000630139575916,\n",
      "      \"grad_norm\": 3.4983179569244385,\n",
      "      \"learning_rate\": 4.5e-05,\n",
      "      \"loss\": 0.0406,\n",
      "      \"step\": 15880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0018904187277482,\n",
      "      \"grad_norm\": 0.08737634122371674,\n",
      "      \"learning_rate\": 4.4993698802772525e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 15900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0031506978795803,\n",
      "      \"grad_norm\": 0.013165079988539219,\n",
      "      \"learning_rate\": 4.4987397605545054e-05,\n",
      "      \"loss\": 0.1191,\n",
      "      \"step\": 15920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0044109770314125,\n",
      "      \"grad_norm\": 0.20767280459403992,\n",
      "      \"learning_rate\": 4.498109640831758e-05,\n",
      "      \"loss\": 0.0176,\n",
      "      \"step\": 15940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0056712561832446,\n",
      "      \"grad_norm\": 0.009862699545919895,\n",
      "      \"learning_rate\": 4.497479521109011e-05,\n",
      "      \"loss\": 0.0547,\n",
      "      \"step\": 15960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0069315353350767,\n",
      "      \"grad_norm\": 5.999934196472168,\n",
      "      \"learning_rate\": 4.496849401386264e-05,\n",
      "      \"loss\": 0.0396,\n",
      "      \"step\": 15980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0081918144869089,\n",
      "      \"grad_norm\": 0.031969789415597916,\n",
      "      \"learning_rate\": 4.4962192816635164e-05,\n",
      "      \"loss\": 0.0251,\n",
      "      \"step\": 16000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.009452093638741,\n",
      "      \"grad_norm\": 0.026533931493759155,\n",
      "      \"learning_rate\": 4.495589161940769e-05,\n",
      "      \"loss\": 0.041,\n",
      "      \"step\": 16020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0107123727905731,\n",
      "      \"grad_norm\": 29.731409072875977,\n",
      "      \"learning_rate\": 4.4949590422180215e-05,\n",
      "      \"loss\": 0.023,\n",
      "      \"step\": 16040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0119726519424053,\n",
      "      \"grad_norm\": 0.058299578726291656,\n",
      "      \"learning_rate\": 4.4943289224952744e-05,\n",
      "      \"loss\": 0.0654,\n",
      "      \"step\": 16060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0132329310942374,\n",
      "      \"grad_norm\": 4.927466869354248,\n",
      "      \"learning_rate\": 4.4936988027725266e-05,\n",
      "      \"loss\": 0.0611,\n",
      "      \"step\": 16080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0144932102460695,\n",
      "      \"grad_norm\": 0.11738558858633041,\n",
      "      \"learning_rate\": 4.4930686830497795e-05,\n",
      "      \"loss\": 0.0588,\n",
      "      \"step\": 16100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0157534893979017,\n",
      "      \"grad_norm\": 0.01715286262333393,\n",
      "      \"learning_rate\": 4.4924385633270324e-05,\n",
      "      \"loss\": 0.0297,\n",
      "      \"step\": 16120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0170137685497338,\n",
      "      \"grad_norm\": 0.04634368419647217,\n",
      "      \"learning_rate\": 4.491808443604285e-05,\n",
      "      \"loss\": 0.0401,\n",
      "      \"step\": 16140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.018274047701566,\n",
      "      \"grad_norm\": 0.014972743578255177,\n",
      "      \"learning_rate\": 4.4911783238815376e-05,\n",
      "      \"loss\": 0.1017,\n",
      "      \"step\": 16160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.019534326853398,\n",
      "      \"grad_norm\": 0.4166445732116699,\n",
      "      \"learning_rate\": 4.4905482041587905e-05,\n",
      "      \"loss\": 0.0782,\n",
      "      \"step\": 16180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0207946060052302,\n",
      "      \"grad_norm\": 0.1356208771467209,\n",
      "      \"learning_rate\": 4.4899180844360434e-05,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 16200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0220548851570623,\n",
      "      \"grad_norm\": 0.6465258002281189,\n",
      "      \"learning_rate\": 4.4892879647132956e-05,\n",
      "      \"loss\": 0.044,\n",
      "      \"step\": 16220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0233151643088945,\n",
      "      \"grad_norm\": 0.03986286744475365,\n",
      "      \"learning_rate\": 4.4886578449905485e-05,\n",
      "      \"loss\": 0.0418,\n",
      "      \"step\": 16240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0245754434607266,\n",
      "      \"grad_norm\": 0.2947201728820801,\n",
      "      \"learning_rate\": 4.488027725267801e-05,\n",
      "      \"loss\": 0.0507,\n",
      "      \"step\": 16260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0258357226125587,\n",
      "      \"grad_norm\": 0.1794825792312622,\n",
      "      \"learning_rate\": 4.4873976055450536e-05,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 16280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0270960017643909,\n",
      "      \"grad_norm\": 20.96284294128418,\n",
      "      \"learning_rate\": 4.4867674858223065e-05,\n",
      "      \"loss\": 0.029,\n",
      "      \"step\": 16300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.028356280916223,\n",
      "      \"grad_norm\": 0.07022979855537415,\n",
      "      \"learning_rate\": 4.4861373660995594e-05,\n",
      "      \"loss\": 0.0336,\n",
      "      \"step\": 16320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0296165600680551,\n",
      "      \"grad_norm\": 0.004247133620083332,\n",
      "      \"learning_rate\": 4.4855072463768117e-05,\n",
      "      \"loss\": 0.058,\n",
      "      \"step\": 16340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0308768392198873,\n",
      "      \"grad_norm\": 0.10413554310798645,\n",
      "      \"learning_rate\": 4.4848771266540646e-05,\n",
      "      \"loss\": 0.027,\n",
      "      \"step\": 16360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0321371183717194,\n",
      "      \"grad_norm\": 0.4249631464481354,\n",
      "      \"learning_rate\": 4.484247006931317e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 16380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0333973975235515,\n",
      "      \"grad_norm\": 0.001179825863800943,\n",
      "      \"learning_rate\": 4.48361688720857e-05,\n",
      "      \"loss\": 0.0221,\n",
      "      \"step\": 16400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0346576766753837,\n",
      "      \"grad_norm\": 0.01969126984477043,\n",
      "      \"learning_rate\": 4.482986767485822e-05,\n",
      "      \"loss\": 0.0611,\n",
      "      \"step\": 16420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0359179558272158,\n",
      "      \"grad_norm\": 1.2300159931182861,\n",
      "      \"learning_rate\": 4.4823566477630755e-05,\n",
      "      \"loss\": 0.0619,\n",
      "      \"step\": 16440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.037178234979048,\n",
      "      \"grad_norm\": 0.21795184910297394,\n",
      "      \"learning_rate\": 4.4817265280403284e-05,\n",
      "      \"loss\": 0.0311,\n",
      "      \"step\": 16460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.03843851413088,\n",
      "      \"grad_norm\": 0.21412497758865356,\n",
      "      \"learning_rate\": 4.4810964083175806e-05,\n",
      "      \"loss\": 0.0211,\n",
      "      \"step\": 16480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0396987932827122,\n",
      "      \"grad_norm\": 0.010966790840029716,\n",
      "      \"learning_rate\": 4.4804662885948335e-05,\n",
      "      \"loss\": 0.0018,\n",
      "      \"step\": 16500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0409590724345443,\n",
      "      \"grad_norm\": 39.18833923339844,\n",
      "      \"learning_rate\": 4.479836168872086e-05,\n",
      "      \"loss\": 0.0894,\n",
      "      \"step\": 16520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0422193515863765,\n",
      "      \"grad_norm\": 0.035712383687496185,\n",
      "      \"learning_rate\": 4.4792060491493387e-05,\n",
      "      \"loss\": 0.0159,\n",
      "      \"step\": 16540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0434796307382086,\n",
      "      \"grad_norm\": 7.147198677062988,\n",
      "      \"learning_rate\": 4.478575929426591e-05,\n",
      "      \"loss\": 0.0431,\n",
      "      \"step\": 16560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0447399098900407,\n",
      "      \"grad_norm\": 0.12302301079034805,\n",
      "      \"learning_rate\": 4.477945809703844e-05,\n",
      "      \"loss\": 0.0851,\n",
      "      \"step\": 16580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0460001890418729,\n",
      "      \"grad_norm\": 0.006254569161683321,\n",
      "      \"learning_rate\": 4.477315689981097e-05,\n",
      "      \"loss\": 0.0218,\n",
      "      \"step\": 16600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.047260468193705,\n",
      "      \"grad_norm\": 4.247878551483154,\n",
      "      \"learning_rate\": 4.4766855702583496e-05,\n",
      "      \"loss\": 0.0307,\n",
      "      \"step\": 16620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0485207473455371,\n",
      "      \"grad_norm\": 0.003364288480952382,\n",
      "      \"learning_rate\": 4.476055450535602e-05,\n",
      "      \"loss\": 0.0269,\n",
      "      \"step\": 16640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0497810264973693,\n",
      "      \"grad_norm\": 2.3518331050872803,\n",
      "      \"learning_rate\": 4.475425330812855e-05,\n",
      "      \"loss\": 0.0407,\n",
      "      \"step\": 16660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0510413056492014,\n",
      "      \"grad_norm\": 0.8854921460151672,\n",
      "      \"learning_rate\": 4.4747952110901076e-05,\n",
      "      \"loss\": 0.0625,\n",
      "      \"step\": 16680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0523015848010335,\n",
      "      \"grad_norm\": 0.009897506795823574,\n",
      "      \"learning_rate\": 4.47416509136736e-05,\n",
      "      \"loss\": 0.0427,\n",
      "      \"step\": 16700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0535618639528657,\n",
      "      \"grad_norm\": 0.40908747911453247,\n",
      "      \"learning_rate\": 4.473534971644613e-05,\n",
      "      \"loss\": 0.0632,\n",
      "      \"step\": 16720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0548221431046978,\n",
      "      \"grad_norm\": 4.545858383178711,\n",
      "      \"learning_rate\": 4.472904851921865e-05,\n",
      "      \"loss\": 0.0896,\n",
      "      \"step\": 16740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0560824222565297,\n",
      "      \"grad_norm\": 0.01296981330960989,\n",
      "      \"learning_rate\": 4.472274732199118e-05,\n",
      "      \"loss\": 0.0643,\n",
      "      \"step\": 16760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0573427014083618,\n",
      "      \"grad_norm\": 0.008661734871566296,\n",
      "      \"learning_rate\": 4.471644612476371e-05,\n",
      "      \"loss\": 0.0228,\n",
      "      \"step\": 16780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.058602980560194,\n",
      "      \"grad_norm\": 0.004124238155782223,\n",
      "      \"learning_rate\": 4.471014492753624e-05,\n",
      "      \"loss\": 0.0342,\n",
      "      \"step\": 16800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.059863259712026,\n",
      "      \"grad_norm\": 0.08674495667219162,\n",
      "      \"learning_rate\": 4.470384373030876e-05,\n",
      "      \"loss\": 0.0132,\n",
      "      \"step\": 16820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0611235388638582,\n",
      "      \"grad_norm\": 0.060250237584114075,\n",
      "      \"learning_rate\": 4.469754253308129e-05,\n",
      "      \"loss\": 0.0384,\n",
      "      \"step\": 16840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0623838180156904,\n",
      "      \"grad_norm\": 0.015147656202316284,\n",
      "      \"learning_rate\": 4.469124133585381e-05,\n",
      "      \"loss\": 0.0794,\n",
      "      \"step\": 16860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0636440971675225,\n",
      "      \"grad_norm\": 23.047487258911133,\n",
      "      \"learning_rate\": 4.468494013862634e-05,\n",
      "      \"loss\": 0.0302,\n",
      "      \"step\": 16880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0649043763193546,\n",
      "      \"grad_norm\": 0.13602688908576965,\n",
      "      \"learning_rate\": 4.467863894139887e-05,\n",
      "      \"loss\": 0.0328,\n",
      "      \"step\": 16900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0661646554711868,\n",
      "      \"grad_norm\": 0.2395007461309433,\n",
      "      \"learning_rate\": 4.467233774417139e-05,\n",
      "      \"loss\": 0.0383,\n",
      "      \"step\": 16920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.067424934623019,\n",
      "      \"grad_norm\": 0.2771163582801819,\n",
      "      \"learning_rate\": 4.466603654694393e-05,\n",
      "      \"loss\": 0.0508,\n",
      "      \"step\": 16940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.068685213774851,\n",
      "      \"grad_norm\": 0.0372806191444397,\n",
      "      \"learning_rate\": 4.465973534971645e-05,\n",
      "      \"loss\": 0.0417,\n",
      "      \"step\": 16960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0699454929266832,\n",
      "      \"grad_norm\": 0.07267556339502335,\n",
      "      \"learning_rate\": 4.465343415248898e-05,\n",
      "      \"loss\": 0.0291,\n",
      "      \"step\": 16980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0712057720785153,\n",
      "      \"grad_norm\": 0.054743073880672455,\n",
      "      \"learning_rate\": 4.46471329552615e-05,\n",
      "      \"loss\": 0.044,\n",
      "      \"step\": 17000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0724660512303474,\n",
      "      \"grad_norm\": 0.016700495034456253,\n",
      "      \"learning_rate\": 4.464083175803403e-05,\n",
      "      \"loss\": 0.0508,\n",
      "      \"step\": 17020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0737263303821796,\n",
      "      \"grad_norm\": 0.10681131482124329,\n",
      "      \"learning_rate\": 4.463453056080655e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 17040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0749866095340117,\n",
      "      \"grad_norm\": 0.02621868997812271,\n",
      "      \"learning_rate\": 4.462854442344045e-05,\n",
      "      \"loss\": 0.0468,\n",
      "      \"step\": 17060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0762468886858438,\n",
      "      \"grad_norm\": 4.803370952606201,\n",
      "      \"learning_rate\": 4.462224322621298e-05,\n",
      "      \"loss\": 0.0487,\n",
      "      \"step\": 17080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.077507167837676,\n",
      "      \"grad_norm\": 0.1213841363787651,\n",
      "      \"learning_rate\": 4.461594202898551e-05,\n",
      "      \"loss\": 0.092,\n",
      "      \"step\": 17100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.078767446989508,\n",
      "      \"grad_norm\": 7.069919109344482,\n",
      "      \"learning_rate\": 4.460964083175804e-05,\n",
      "      \"loss\": 0.0394,\n",
      "      \"step\": 17120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0800277261413402,\n",
      "      \"grad_norm\": 9.211648941040039,\n",
      "      \"learning_rate\": 4.460333963453056e-05,\n",
      "      \"loss\": 0.0232,\n",
      "      \"step\": 17140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0812880052931724,\n",
      "      \"grad_norm\": 0.13068333268165588,\n",
      "      \"learning_rate\": 4.459703843730309e-05,\n",
      "      \"loss\": 0.0726,\n",
      "      \"step\": 17160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0825482844450045,\n",
      "      \"grad_norm\": 43.04506301879883,\n",
      "      \"learning_rate\": 4.459073724007561e-05,\n",
      "      \"loss\": 0.0365,\n",
      "      \"step\": 17180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0838085635968366,\n",
      "      \"grad_norm\": 0.016899246722459793,\n",
      "      \"learning_rate\": 4.458443604284814e-05,\n",
      "      \"loss\": 0.0307,\n",
      "      \"step\": 17200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0850688427486688,\n",
      "      \"grad_norm\": 0.0079581830650568,\n",
      "      \"learning_rate\": 4.457813484562067e-05,\n",
      "      \"loss\": 0.0529,\n",
      "      \"step\": 17220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.086329121900501,\n",
      "      \"grad_norm\": 1.7489570379257202,\n",
      "      \"learning_rate\": 4.4571833648393194e-05,\n",
      "      \"loss\": 0.0387,\n",
      "      \"step\": 17240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.087589401052333,\n",
      "      \"grad_norm\": 0.5202208161354065,\n",
      "      \"learning_rate\": 4.456553245116572e-05,\n",
      "      \"loss\": 0.0991,\n",
      "      \"step\": 17260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0888496802041652,\n",
      "      \"grad_norm\": 1.0175939798355103,\n",
      "      \"learning_rate\": 4.455923125393825e-05,\n",
      "      \"loss\": 0.0263,\n",
      "      \"step\": 17280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0901099593559973,\n",
      "      \"grad_norm\": 0.059012532234191895,\n",
      "      \"learning_rate\": 4.455293005671078e-05,\n",
      "      \"loss\": 0.0334,\n",
      "      \"step\": 17300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0913702385078294,\n",
      "      \"grad_norm\": 0.006521153729408979,\n",
      "      \"learning_rate\": 4.45466288594833e-05,\n",
      "      \"loss\": 0.0405,\n",
      "      \"step\": 17320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0926305176596616,\n",
      "      \"grad_norm\": 9.417393684387207,\n",
      "      \"learning_rate\": 4.454032766225583e-05,\n",
      "      \"loss\": 0.0679,\n",
      "      \"step\": 17340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0938907968114937,\n",
      "      \"grad_norm\": 0.32393336296081543,\n",
      "      \"learning_rate\": 4.4534026465028354e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 17360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0951510759633258,\n",
      "      \"grad_norm\": 0.7054459452629089,\n",
      "      \"learning_rate\": 4.452772526780088e-05,\n",
      "      \"loss\": 0.0511,\n",
      "      \"step\": 17380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.096411355115158,\n",
      "      \"grad_norm\": 0.019786344841122627,\n",
      "      \"learning_rate\": 4.4521424070573405e-05,\n",
      "      \"loss\": 0.0664,\n",
      "      \"step\": 17400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.09767163426699,\n",
      "      \"grad_norm\": 0.5549489855766296,\n",
      "      \"learning_rate\": 4.451512287334594e-05,\n",
      "      \"loss\": 0.0676,\n",
      "      \"step\": 17420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0989319134188222,\n",
      "      \"grad_norm\": 6.280988693237305,\n",
      "      \"learning_rate\": 4.450882167611847e-05,\n",
      "      \"loss\": 0.0411,\n",
      "      \"step\": 17440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1001921925706544,\n",
      "      \"grad_norm\": 0.02839660830795765,\n",
      "      \"learning_rate\": 4.450252047889099e-05,\n",
      "      \"loss\": 0.0423,\n",
      "      \"step\": 17460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1014524717224865,\n",
      "      \"grad_norm\": 0.08248800784349442,\n",
      "      \"learning_rate\": 4.449621928166352e-05,\n",
      "      \"loss\": 0.0197,\n",
      "      \"step\": 17480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1027127508743186,\n",
      "      \"grad_norm\": 0.37434491515159607,\n",
      "      \"learning_rate\": 4.4489918084436044e-05,\n",
      "      \"loss\": 0.017,\n",
      "      \"step\": 17500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1039730300261508,\n",
      "      \"grad_norm\": 0.01039927639067173,\n",
      "      \"learning_rate\": 4.448361688720857e-05,\n",
      "      \"loss\": 0.0334,\n",
      "      \"step\": 17520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.105233309177983,\n",
      "      \"grad_norm\": 0.003924251534044743,\n",
      "      \"learning_rate\": 4.4477315689981095e-05,\n",
      "      \"loss\": 0.0279,\n",
      "      \"step\": 17540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.106493588329815,\n",
      "      \"grad_norm\": 0.050005268305540085,\n",
      "      \"learning_rate\": 4.4471014492753624e-05,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 17560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1077538674816472,\n",
      "      \"grad_norm\": 0.0036116265691816807,\n",
      "      \"learning_rate\": 4.446471329552615e-05,\n",
      "      \"loss\": 0.053,\n",
      "      \"step\": 17580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1090141466334793,\n",
      "      \"grad_norm\": 0.09303200989961624,\n",
      "      \"learning_rate\": 4.445841209829868e-05,\n",
      "      \"loss\": 0.0157,\n",
      "      \"step\": 17600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1102744257853114,\n",
      "      \"grad_norm\": 24.713544845581055,\n",
      "      \"learning_rate\": 4.4452110901071205e-05,\n",
      "      \"loss\": 0.0797,\n",
      "      \"step\": 17620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1115347049371436,\n",
      "      \"grad_norm\": 5.864228248596191,\n",
      "      \"learning_rate\": 4.4445809703843734e-05,\n",
      "      \"loss\": 0.058,\n",
      "      \"step\": 17640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1127949840889757,\n",
      "      \"grad_norm\": 0.09754496067762375,\n",
      "      \"learning_rate\": 4.443950850661626e-05,\n",
      "      \"loss\": 0.03,\n",
      "      \"step\": 17660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1140552632408078,\n",
      "      \"grad_norm\": 0.01902170106768608,\n",
      "      \"learning_rate\": 4.4433207309388785e-05,\n",
      "      \"loss\": 0.0308,\n",
      "      \"step\": 17680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.11531554239264,\n",
      "      \"grad_norm\": 0.0015595892909914255,\n",
      "      \"learning_rate\": 4.4426906112161314e-05,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 17700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.116575821544472,\n",
      "      \"grad_norm\": 0.0032819469925016165,\n",
      "      \"learning_rate\": 4.4420604914933836e-05,\n",
      "      \"loss\": 0.0762,\n",
      "      \"step\": 17720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1178361006963042,\n",
      "      \"grad_norm\": 9.17078971862793,\n",
      "      \"learning_rate\": 4.4414303717706365e-05,\n",
      "      \"loss\": 0.0585,\n",
      "      \"step\": 17740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1190963798481364,\n",
      "      \"grad_norm\": 0.21128059923648834,\n",
      "      \"learning_rate\": 4.4408002520478894e-05,\n",
      "      \"loss\": 0.0175,\n",
      "      \"step\": 17760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1203566589999685,\n",
      "      \"grad_norm\": 0.0019898877944797277,\n",
      "      \"learning_rate\": 4.440170132325142e-05,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 17780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1216169381518006,\n",
      "      \"grad_norm\": 0.004443004727363586,\n",
      "      \"learning_rate\": 4.4395400126023946e-05,\n",
      "      \"loss\": 0.0653,\n",
      "      \"step\": 17800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1228772173036328,\n",
      "      \"grad_norm\": 0.13890056312084198,\n",
      "      \"learning_rate\": 4.4389098928796475e-05,\n",
      "      \"loss\": 0.05,\n",
      "      \"step\": 17820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.124137496455465,\n",
      "      \"grad_norm\": 0.059541843831539154,\n",
      "      \"learning_rate\": 4.4382797731569e-05,\n",
      "      \"loss\": 0.0702,\n",
      "      \"step\": 17840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.125397775607297,\n",
      "      \"grad_norm\": 0.03618399426341057,\n",
      "      \"learning_rate\": 4.4376496534341526e-05,\n",
      "      \"loss\": 0.0261,\n",
      "      \"step\": 17860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1266580547591292,\n",
      "      \"grad_norm\": 0.3519737422466278,\n",
      "      \"learning_rate\": 4.437019533711405e-05,\n",
      "      \"loss\": 0.0568,\n",
      "      \"step\": 17880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1279183339109613,\n",
      "      \"grad_norm\": 0.01565849408507347,\n",
      "      \"learning_rate\": 4.436389413988658e-05,\n",
      "      \"loss\": 0.0295,\n",
      "      \"step\": 17900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1291786130627934,\n",
      "      \"grad_norm\": 0.1499747335910797,\n",
      "      \"learning_rate\": 4.4357592942659106e-05,\n",
      "      \"loss\": 0.0981,\n",
      "      \"step\": 17920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1304388922146256,\n",
      "      \"grad_norm\": 3.2063848972320557,\n",
      "      \"learning_rate\": 4.4351291745431635e-05,\n",
      "      \"loss\": 0.0527,\n",
      "      \"step\": 17940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1316991713664577,\n",
      "      \"grad_norm\": 0.15666277706623077,\n",
      "      \"learning_rate\": 4.4344990548204164e-05,\n",
      "      \"loss\": 0.048,\n",
      "      \"step\": 17960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1329594505182898,\n",
      "      \"grad_norm\": 8.360370635986328,\n",
      "      \"learning_rate\": 4.433868935097669e-05,\n",
      "      \"loss\": 0.0425,\n",
      "      \"step\": 17980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.134219729670122,\n",
      "      \"grad_norm\": 0.106681689620018,\n",
      "      \"learning_rate\": 4.4332388153749216e-05,\n",
      "      \"loss\": 0.0689,\n",
      "      \"step\": 18000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.135480008821954,\n",
      "      \"grad_norm\": 0.10609599202871323,\n",
      "      \"learning_rate\": 4.432608695652174e-05,\n",
      "      \"loss\": 0.035,\n",
      "      \"step\": 18020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1367402879737862,\n",
      "      \"grad_norm\": 0.14231957495212555,\n",
      "      \"learning_rate\": 4.431978575929427e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 18040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1380005671256184,\n",
      "      \"grad_norm\": 1.067090630531311,\n",
      "      \"learning_rate\": 4.4313484562066796e-05,\n",
      "      \"loss\": 0.033,\n",
      "      \"step\": 18060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1392608462774505,\n",
      "      \"grad_norm\": 3.9574272632598877,\n",
      "      \"learning_rate\": 4.4307183364839325e-05,\n",
      "      \"loss\": 0.0769,\n",
      "      \"step\": 18080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1405211254292826,\n",
      "      \"grad_norm\": 5.784379959106445,\n",
      "      \"learning_rate\": 4.430088216761185e-05,\n",
      "      \"loss\": 0.018,\n",
      "      \"step\": 18100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1417814045811148,\n",
      "      \"grad_norm\": 0.02683202549815178,\n",
      "      \"learning_rate\": 4.4294580970384376e-05,\n",
      "      \"loss\": 0.0477,\n",
      "      \"step\": 18120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.143041683732947,\n",
      "      \"grad_norm\": 0.007632911670953035,\n",
      "      \"learning_rate\": 4.4288279773156905e-05,\n",
      "      \"loss\": 0.0438,\n",
      "      \"step\": 18140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.144301962884779,\n",
      "      \"grad_norm\": 0.15106792747974396,\n",
      "      \"learning_rate\": 4.428197857592943e-05,\n",
      "      \"loss\": 0.0617,\n",
      "      \"step\": 18160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1455622420366112,\n",
      "      \"grad_norm\": 0.025853406637907028,\n",
      "      \"learning_rate\": 4.427567737870196e-05,\n",
      "      \"loss\": 0.0125,\n",
      "      \"step\": 18180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1468225211884433,\n",
      "      \"grad_norm\": 0.07303356379270554,\n",
      "      \"learning_rate\": 4.426937618147448e-05,\n",
      "      \"loss\": 0.0446,\n",
      "      \"step\": 18200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1480828003402754,\n",
      "      \"grad_norm\": 5.731409549713135,\n",
      "      \"learning_rate\": 4.426307498424701e-05,\n",
      "      \"loss\": 0.0491,\n",
      "      \"step\": 18220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1493430794921076,\n",
      "      \"grad_norm\": 0.18875949084758759,\n",
      "      \"learning_rate\": 4.425677378701954e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 18240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1506033586439397,\n",
      "      \"grad_norm\": 16.128374099731445,\n",
      "      \"learning_rate\": 4.4250472589792066e-05,\n",
      "      \"loss\": 0.0256,\n",
      "      \"step\": 18260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1518636377957718,\n",
      "      \"grad_norm\": 0.38885241746902466,\n",
      "      \"learning_rate\": 4.424417139256459e-05,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 18280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.153123916947604,\n",
      "      \"grad_norm\": 0.41514861583709717,\n",
      "      \"learning_rate\": 4.423787019533712e-05,\n",
      "      \"loss\": 0.0431,\n",
      "      \"step\": 18300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.154384196099436,\n",
      "      \"grad_norm\": 0.12380453944206238,\n",
      "      \"learning_rate\": 4.423156899810964e-05,\n",
      "      \"loss\": 0.0446,\n",
      "      \"step\": 18320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1556444752512682,\n",
      "      \"grad_norm\": 0.11828195303678513,\n",
      "      \"learning_rate\": 4.422526780088217e-05,\n",
      "      \"loss\": 0.0378,\n",
      "      \"step\": 18340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1569047544031004,\n",
      "      \"grad_norm\": 0.2402976006269455,\n",
      "      \"learning_rate\": 4.42189666036547e-05,\n",
      "      \"loss\": 0.0631,\n",
      "      \"step\": 18360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1581650335549325,\n",
      "      \"grad_norm\": 0.04377627372741699,\n",
      "      \"learning_rate\": 4.421266540642722e-05,\n",
      "      \"loss\": 0.0626,\n",
      "      \"step\": 18380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1594253127067646,\n",
      "      \"grad_norm\": 0.01170011144131422,\n",
      "      \"learning_rate\": 4.420636420919975e-05,\n",
      "      \"loss\": 0.0448,\n",
      "      \"step\": 18400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1606855918585968,\n",
      "      \"grad_norm\": 16.927465438842773,\n",
      "      \"learning_rate\": 4.420006301197228e-05,\n",
      "      \"loss\": 0.0231,\n",
      "      \"step\": 18420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.161945871010429,\n",
      "      \"grad_norm\": 0.004616160411387682,\n",
      "      \"learning_rate\": 4.419376181474481e-05,\n",
      "      \"loss\": 0.0196,\n",
      "      \"step\": 18440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.163206150162261,\n",
      "      \"grad_norm\": 0.28474950790405273,\n",
      "      \"learning_rate\": 4.418746061751733e-05,\n",
      "      \"loss\": 0.0822,\n",
      "      \"step\": 18460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1644664293140932,\n",
      "      \"grad_norm\": 0.3018057346343994,\n",
      "      \"learning_rate\": 4.418115942028986e-05,\n",
      "      \"loss\": 0.0375,\n",
      "      \"step\": 18480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1657267084659253,\n",
      "      \"grad_norm\": 3.132457971572876,\n",
      "      \"learning_rate\": 4.417485822306238e-05,\n",
      "      \"loss\": 0.1014,\n",
      "      \"step\": 18500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1669869876177574,\n",
      "      \"grad_norm\": 0.08741777390241623,\n",
      "      \"learning_rate\": 4.416855702583491e-05,\n",
      "      \"loss\": 0.033,\n",
      "      \"step\": 18520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1682472667695896,\n",
      "      \"grad_norm\": 0.09019878506660461,\n",
      "      \"learning_rate\": 4.416225582860743e-05,\n",
      "      \"loss\": 0.0285,\n",
      "      \"step\": 18540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1695075459214217,\n",
      "      \"grad_norm\": 0.010260983370244503,\n",
      "      \"learning_rate\": 4.415595463137996e-05,\n",
      "      \"loss\": 0.0686,\n",
      "      \"step\": 18560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1707678250732538,\n",
      "      \"grad_norm\": 0.23585692048072815,\n",
      "      \"learning_rate\": 4.41496534341525e-05,\n",
      "      \"loss\": 0.0665,\n",
      "      \"step\": 18580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.172028104225086,\n",
      "      \"grad_norm\": 0.05452439934015274,\n",
      "      \"learning_rate\": 4.414335223692502e-05,\n",
      "      \"loss\": 0.0597,\n",
      "      \"step\": 18600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.173288383376918,\n",
      "      \"grad_norm\": 0.24962544441223145,\n",
      "      \"learning_rate\": 4.413705103969755e-05,\n",
      "      \"loss\": 0.0456,\n",
      "      \"step\": 18620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1745486625287502,\n",
      "      \"grad_norm\": 0.23788204789161682,\n",
      "      \"learning_rate\": 4.413074984247007e-05,\n",
      "      \"loss\": 0.0746,\n",
      "      \"step\": 18640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1758089416805824,\n",
      "      \"grad_norm\": 0.03424888104200363,\n",
      "      \"learning_rate\": 4.41244486452426e-05,\n",
      "      \"loss\": 0.0526,\n",
      "      \"step\": 18660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1770692208324145,\n",
      "      \"grad_norm\": 0.09903573989868164,\n",
      "      \"learning_rate\": 4.411814744801512e-05,\n",
      "      \"loss\": 0.0657,\n",
      "      \"step\": 18680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1783294999842466,\n",
      "      \"grad_norm\": 4.0643157958984375,\n",
      "      \"learning_rate\": 4.411184625078765e-05,\n",
      "      \"loss\": 0.0624,\n",
      "      \"step\": 18700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1795897791360788,\n",
      "      \"grad_norm\": 0.17502278089523315,\n",
      "      \"learning_rate\": 4.410554505356018e-05,\n",
      "      \"loss\": 0.05,\n",
      "      \"step\": 18720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1808500582879107,\n",
      "      \"grad_norm\": 0.10758624225854874,\n",
      "      \"learning_rate\": 4.409924385633271e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 18740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1821103374397428,\n",
      "      \"grad_norm\": 0.0164969339966774,\n",
      "      \"learning_rate\": 4.409294265910523e-05,\n",
      "      \"loss\": 0.0188,\n",
      "      \"step\": 18760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.183370616591575,\n",
      "      \"grad_norm\": 0.31339341402053833,\n",
      "      \"learning_rate\": 4.408664146187776e-05,\n",
      "      \"loss\": 0.0249,\n",
      "      \"step\": 18780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.184630895743407,\n",
      "      \"grad_norm\": 0.057904843240976334,\n",
      "      \"learning_rate\": 4.408034026465029e-05,\n",
      "      \"loss\": 0.0718,\n",
      "      \"step\": 18800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1858911748952392,\n",
      "      \"grad_norm\": 0.006359202321618795,\n",
      "      \"learning_rate\": 4.407403906742281e-05,\n",
      "      \"loss\": 0.0571,\n",
      "      \"step\": 18820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1871514540470713,\n",
      "      \"grad_norm\": 21.601863861083984,\n",
      "      \"learning_rate\": 4.406773787019534e-05,\n",
      "      \"loss\": 0.0219,\n",
      "      \"step\": 18840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1884117331989035,\n",
      "      \"grad_norm\": 0.017758281901478767,\n",
      "      \"learning_rate\": 4.406143667296786e-05,\n",
      "      \"loss\": 0.0445,\n",
      "      \"step\": 18860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1896720123507356,\n",
      "      \"grad_norm\": 26.396455764770508,\n",
      "      \"learning_rate\": 4.405513547574039e-05,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 18880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1909322915025677,\n",
      "      \"grad_norm\": 0.38742077350616455,\n",
      "      \"learning_rate\": 4.404883427851292e-05,\n",
      "      \"loss\": 0.0058,\n",
      "      \"step\": 18900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1921925706543999,\n",
      "      \"grad_norm\": 0.6084016561508179,\n",
      "      \"learning_rate\": 4.404253308128545e-05,\n",
      "      \"loss\": 0.0623,\n",
      "      \"step\": 18920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.193452849806232,\n",
      "      \"grad_norm\": 0.022122716531157494,\n",
      "      \"learning_rate\": 4.403623188405797e-05,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 18940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1947131289580641,\n",
      "      \"grad_norm\": 0.006429395172744989,\n",
      "      \"learning_rate\": 4.40299306868305e-05,\n",
      "      \"loss\": 0.0667,\n",
      "      \"step\": 18960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1959734081098963,\n",
      "      \"grad_norm\": 1.7296841144561768,\n",
      "      \"learning_rate\": 4.4023629489603023e-05,\n",
      "      \"loss\": 0.0577,\n",
      "      \"step\": 18980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1972336872617284,\n",
      "      \"grad_norm\": 0.04227267950773239,\n",
      "      \"learning_rate\": 4.401732829237555e-05,\n",
      "      \"loss\": 0.0408,\n",
      "      \"step\": 19000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1984939664135605,\n",
      "      \"grad_norm\": 0.5997779369354248,\n",
      "      \"learning_rate\": 4.401102709514808e-05,\n",
      "      \"loss\": 0.0169,\n",
      "      \"step\": 19020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1997542455653927,\n",
      "      \"grad_norm\": 0.002415398368611932,\n",
      "      \"learning_rate\": 4.4004725897920604e-05,\n",
      "      \"loss\": 0.0422,\n",
      "      \"step\": 19040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2010145247172248,\n",
      "      \"grad_norm\": 0.05807948857545853,\n",
      "      \"learning_rate\": 4.399842470069313e-05,\n",
      "      \"loss\": 0.035,\n",
      "      \"step\": 19060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.202274803869057,\n",
      "      \"grad_norm\": 0.004662883002310991,\n",
      "      \"learning_rate\": 4.399212350346566e-05,\n",
      "      \"loss\": 0.0377,\n",
      "      \"step\": 19080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.203535083020889,\n",
      "      \"grad_norm\": 0.03523355349898338,\n",
      "      \"learning_rate\": 4.398582230623819e-05,\n",
      "      \"loss\": 0.0726,\n",
      "      \"step\": 19100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2047953621727212,\n",
      "      \"grad_norm\": 0.0029157372191548347,\n",
      "      \"learning_rate\": 4.397952110901071e-05,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 19120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2060556413245533,\n",
      "      \"grad_norm\": 0.6932699084281921,\n",
      "      \"learning_rate\": 4.397321991178324e-05,\n",
      "      \"loss\": 0.13,\n",
      "      \"step\": 19140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2073159204763855,\n",
      "      \"grad_norm\": 0.2845991849899292,\n",
      "      \"learning_rate\": 4.3966918714555764e-05,\n",
      "      \"loss\": 0.061,\n",
      "      \"step\": 19160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2085761996282176,\n",
      "      \"grad_norm\": 5.071639537811279,\n",
      "      \"learning_rate\": 4.3960617517328293e-05,\n",
      "      \"loss\": 0.0129,\n",
      "      \"step\": 19180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2098364787800497,\n",
      "      \"grad_norm\": 0.06429824978113174,\n",
      "      \"learning_rate\": 4.3954316320100816e-05,\n",
      "      \"loss\": 0.0305,\n",
      "      \"step\": 19200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2110967579318819,\n",
      "      \"grad_norm\": 0.06136491522192955,\n",
      "      \"learning_rate\": 4.394801512287335e-05,\n",
      "      \"loss\": 0.03,\n",
      "      \"step\": 19220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.212357037083714,\n",
      "      \"grad_norm\": 6.82271146774292,\n",
      "      \"learning_rate\": 4.3941713925645874e-05,\n",
      "      \"loss\": 0.0408,\n",
      "      \"step\": 19240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2136173162355461,\n",
      "      \"grad_norm\": 6.6863112449646,\n",
      "      \"learning_rate\": 4.39354127284184e-05,\n",
      "      \"loss\": 0.0922,\n",
      "      \"step\": 19260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2148775953873783,\n",
      "      \"grad_norm\": 0.01919279247522354,\n",
      "      \"learning_rate\": 4.392911153119093e-05,\n",
      "      \"loss\": 0.0506,\n",
      "      \"step\": 19280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2161378745392104,\n",
      "      \"grad_norm\": 0.07870964705944061,\n",
      "      \"learning_rate\": 4.3922810333963454e-05,\n",
      "      \"loss\": 0.0352,\n",
      "      \"step\": 19300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2173981536910425,\n",
      "      \"grad_norm\": 1.2609950304031372,\n",
      "      \"learning_rate\": 4.3916824196597355e-05,\n",
      "      \"loss\": 0.0848,\n",
      "      \"step\": 19320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2186584328428747,\n",
      "      \"grad_norm\": 8.322342872619629,\n",
      "      \"learning_rate\": 4.391052299936988e-05,\n",
      "      \"loss\": 0.0366,\n",
      "      \"step\": 19340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2199187119947068,\n",
      "      \"grad_norm\": 4.049097537994385,\n",
      "      \"learning_rate\": 4.3904221802142406e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 19360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.221178991146539,\n",
      "      \"grad_norm\": 0.3576177954673767,\n",
      "      \"learning_rate\": 4.3897920604914935e-05,\n",
      "      \"loss\": 0.0161,\n",
      "      \"step\": 19380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.222439270298371,\n",
      "      \"grad_norm\": 0.13433316349983215,\n",
      "      \"learning_rate\": 4.3891619407687464e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 19400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2236995494502032,\n",
      "      \"grad_norm\": 0.01877482794225216,\n",
      "      \"learning_rate\": 4.3885318210459993e-05,\n",
      "      \"loss\": 0.0613,\n",
      "      \"step\": 19420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2249598286020353,\n",
      "      \"grad_norm\": 0.021133827045559883,\n",
      "      \"learning_rate\": 4.3879017013232516e-05,\n",
      "      \"loss\": 0.0619,\n",
      "      \"step\": 19440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2262201077538675,\n",
      "      \"grad_norm\": 0.0676744356751442,\n",
      "      \"learning_rate\": 4.3872715816005045e-05,\n",
      "      \"loss\": 0.0328,\n",
      "      \"step\": 19460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2274803869056996,\n",
      "      \"grad_norm\": 0.03986883908510208,\n",
      "      \"learning_rate\": 4.386641461877757e-05,\n",
      "      \"loss\": 0.026,\n",
      "      \"step\": 19480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2287406660575317,\n",
      "      \"grad_norm\": 0.9266495704650879,\n",
      "      \"learning_rate\": 4.3860113421550096e-05,\n",
      "      \"loss\": 0.0727,\n",
      "      \"step\": 19500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2300009452093639,\n",
      "      \"grad_norm\": 3.2997663021087646,\n",
      "      \"learning_rate\": 4.385381222432262e-05,\n",
      "      \"loss\": 0.0231,\n",
      "      \"step\": 19520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.231261224361196,\n",
      "      \"grad_norm\": 0.049155913293361664,\n",
      "      \"learning_rate\": 4.384751102709515e-05,\n",
      "      \"loss\": 0.0694,\n",
      "      \"step\": 19540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2325215035130281,\n",
      "      \"grad_norm\": 9.876266479492188,\n",
      "      \"learning_rate\": 4.3841209829867676e-05,\n",
      "      \"loss\": 0.0228,\n",
      "      \"step\": 19560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2337817826648603,\n",
      "      \"grad_norm\": 0.0003880082513205707,\n",
      "      \"learning_rate\": 4.3834908632640205e-05,\n",
      "      \"loss\": 0.0344,\n",
      "      \"step\": 19580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2350420618166924,\n",
      "      \"grad_norm\": 0.05595291405916214,\n",
      "      \"learning_rate\": 4.3828607435412735e-05,\n",
      "      \"loss\": 0.0402,\n",
      "      \"step\": 19600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2363023409685245,\n",
      "      \"grad_norm\": 1.6536129713058472,\n",
      "      \"learning_rate\": 4.382230623818526e-05,\n",
      "      \"loss\": 0.0265,\n",
      "      \"step\": 19620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2375626201203567,\n",
      "      \"grad_norm\": 0.05733131617307663,\n",
      "      \"learning_rate\": 4.3816005040957786e-05,\n",
      "      \"loss\": 0.05,\n",
      "      \"step\": 19640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2388228992721888,\n",
      "      \"grad_norm\": 0.002011047676205635,\n",
      "      \"learning_rate\": 4.380970384373031e-05,\n",
      "      \"loss\": 0.019,\n",
      "      \"step\": 19660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.240083178424021,\n",
      "      \"grad_norm\": 0.001571707078255713,\n",
      "      \"learning_rate\": 4.380340264650284e-05,\n",
      "      \"loss\": 0.0346,\n",
      "      \"step\": 19680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.241343457575853,\n",
      "      \"grad_norm\": 0.0035556256771087646,\n",
      "      \"learning_rate\": 4.3797101449275366e-05,\n",
      "      \"loss\": 0.0324,\n",
      "      \"step\": 19700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2426037367276852,\n",
      "      \"grad_norm\": 0.011570357717573643,\n",
      "      \"learning_rate\": 4.3790800252047895e-05,\n",
      "      \"loss\": 0.0542,\n",
      "      \"step\": 19720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2438640158795173,\n",
      "      \"grad_norm\": 11.853752136230469,\n",
      "      \"learning_rate\": 4.378449905482042e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 19740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2451242950313495,\n",
      "      \"grad_norm\": 6.796846389770508,\n",
      "      \"learning_rate\": 4.3778197857592947e-05,\n",
      "      \"loss\": 0.0501,\n",
      "      \"step\": 19760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2463845741831816,\n",
      "      \"grad_norm\": 0.01514968741685152,\n",
      "      \"learning_rate\": 4.377189666036547e-05,\n",
      "      \"loss\": 0.0622,\n",
      "      \"step\": 19780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2476448533350137,\n",
      "      \"grad_norm\": 7.964497089385986,\n",
      "      \"learning_rate\": 4.3765595463138e-05,\n",
      "      \"loss\": 0.0917,\n",
      "      \"step\": 19800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2489051324868459,\n",
      "      \"grad_norm\": 0.42572587728500366,\n",
      "      \"learning_rate\": 4.375929426591053e-05,\n",
      "      \"loss\": 0.0483,\n",
      "      \"step\": 19820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.250165411638678,\n",
      "      \"grad_norm\": 4.810235977172852,\n",
      "      \"learning_rate\": 4.375299306868305e-05,\n",
      "      \"loss\": 0.0253,\n",
      "      \"step\": 19840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2514256907905101,\n",
      "      \"grad_norm\": 0.01131090335547924,\n",
      "      \"learning_rate\": 4.374669187145558e-05,\n",
      "      \"loss\": 0.0341,\n",
      "      \"step\": 19860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2526859699423423,\n",
      "      \"grad_norm\": 0.00742731848731637,\n",
      "      \"learning_rate\": 4.374039067422811e-05,\n",
      "      \"loss\": 0.0028,\n",
      "      \"step\": 19880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2539462490941744,\n",
      "      \"grad_norm\": 0.0021564429625868797,\n",
      "      \"learning_rate\": 4.3734089477000636e-05,\n",
      "      \"loss\": 0.05,\n",
      "      \"step\": 19900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2552065282460065,\n",
      "      \"grad_norm\": 0.0006840360583737493,\n",
      "      \"learning_rate\": 4.372778827977316e-05,\n",
      "      \"loss\": 0.0299,\n",
      "      \"step\": 19920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2564668073978387,\n",
      "      \"grad_norm\": 0.005002916324883699,\n",
      "      \"learning_rate\": 4.372148708254569e-05,\n",
      "      \"loss\": 0.0407,\n",
      "      \"step\": 19940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2577270865496708,\n",
      "      \"grad_norm\": 0.06373218446969986,\n",
      "      \"learning_rate\": 4.371518588531821e-05,\n",
      "      \"loss\": 0.0521,\n",
      "      \"step\": 19960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.258987365701503,\n",
      "      \"grad_norm\": 1.4053148031234741,\n",
      "      \"learning_rate\": 4.370888468809074e-05,\n",
      "      \"loss\": 0.0336,\n",
      "      \"step\": 19980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.260247644853335,\n",
      "      \"grad_norm\": 7.219913959503174,\n",
      "      \"learning_rate\": 4.370258349086326e-05,\n",
      "      \"loss\": 0.0566,\n",
      "      \"step\": 20000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2615079240051672,\n",
      "      \"grad_norm\": 0.025568537414073944,\n",
      "      \"learning_rate\": 4.369628229363579e-05,\n",
      "      \"loss\": 0.0751,\n",
      "      \"step\": 20020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2627682031569993,\n",
      "      \"grad_norm\": 0.2434614896774292,\n",
      "      \"learning_rate\": 4.368998109640832e-05,\n",
      "      \"loss\": 0.0159,\n",
      "      \"step\": 20040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2640284823088315,\n",
      "      \"grad_norm\": 0.03543324023485184,\n",
      "      \"learning_rate\": 4.368367989918085e-05,\n",
      "      \"loss\": 0.033,\n",
      "      \"step\": 20060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2652887614606636,\n",
      "      \"grad_norm\": 0.20100893080234528,\n",
      "      \"learning_rate\": 4.367737870195338e-05,\n",
      "      \"loss\": 0.0691,\n",
      "      \"step\": 20080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2665490406124957,\n",
      "      \"grad_norm\": 0.08745775371789932,\n",
      "      \"learning_rate\": 4.36710775047259e-05,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 20100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2678093197643279,\n",
      "      \"grad_norm\": 0.004986774642020464,\n",
      "      \"learning_rate\": 4.366477630749843e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 20120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.26906959891616,\n",
      "      \"grad_norm\": 0.2691393792629242,\n",
      "      \"learning_rate\": 4.365847511027095e-05,\n",
      "      \"loss\": 0.0563,\n",
      "      \"step\": 20140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2703298780679921,\n",
      "      \"grad_norm\": 0.7422245144844055,\n",
      "      \"learning_rate\": 4.365217391304348e-05,\n",
      "      \"loss\": 0.0534,\n",
      "      \"step\": 20160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2715901572198243,\n",
      "      \"grad_norm\": 0.43185627460479736,\n",
      "      \"learning_rate\": 4.3645872715816e-05,\n",
      "      \"loss\": 0.0306,\n",
      "      \"step\": 20180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2728504363716564,\n",
      "      \"grad_norm\": 0.10898005217313766,\n",
      "      \"learning_rate\": 4.363957151858853e-05,\n",
      "      \"loss\": 0.0732,\n",
      "      \"step\": 20200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2741107155234885,\n",
      "      \"grad_norm\": 0.2096797525882721,\n",
      "      \"learning_rate\": 4.363327032136106e-05,\n",
      "      \"loss\": 0.0474,\n",
      "      \"step\": 20220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2753709946753204,\n",
      "      \"grad_norm\": 4.058366298675537,\n",
      "      \"learning_rate\": 4.362696912413359e-05,\n",
      "      \"loss\": 0.0373,\n",
      "      \"step\": 20240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2766312738271526,\n",
      "      \"grad_norm\": 0.16781960427761078,\n",
      "      \"learning_rate\": 4.362066792690612e-05,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 20260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2778915529789847,\n",
      "      \"grad_norm\": 0.04120326787233353,\n",
      "      \"learning_rate\": 4.361436672967864e-05,\n",
      "      \"loss\": 0.0268,\n",
      "      \"step\": 20280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2791518321308168,\n",
      "      \"grad_norm\": 9.813346862792969,\n",
      "      \"learning_rate\": 4.360806553245117e-05,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 20300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.280412111282649,\n",
      "      \"grad_norm\": 7.36313009262085,\n",
      "      \"learning_rate\": 4.360176433522369e-05,\n",
      "      \"loss\": 0.0258,\n",
      "      \"step\": 20320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2816723904344811,\n",
      "      \"grad_norm\": 0.03917276859283447,\n",
      "      \"learning_rate\": 4.359546313799622e-05,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 20340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2829326695863132,\n",
      "      \"grad_norm\": 9.007603645324707,\n",
      "      \"learning_rate\": 4.358916194076875e-05,\n",
      "      \"loss\": 0.0749,\n",
      "      \"step\": 20360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2841929487381454,\n",
      "      \"grad_norm\": 0.03874281048774719,\n",
      "      \"learning_rate\": 4.358286074354128e-05,\n",
      "      \"loss\": 0.0263,\n",
      "      \"step\": 20380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2854532278899775,\n",
      "      \"grad_norm\": 0.10727261006832123,\n",
      "      \"learning_rate\": 4.35765595463138e-05,\n",
      "      \"loss\": 0.0436,\n",
      "      \"step\": 20400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2867135070418096,\n",
      "      \"grad_norm\": 0.27818959951400757,\n",
      "      \"learning_rate\": 4.357025834908633e-05,\n",
      "      \"loss\": 0.071,\n",
      "      \"step\": 20420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2879737861936418,\n",
      "      \"grad_norm\": 7.491820812225342,\n",
      "      \"learning_rate\": 4.356395715185885e-05,\n",
      "      \"loss\": 0.0375,\n",
      "      \"step\": 20440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.289234065345474,\n",
      "      \"grad_norm\": 0.08061572909355164,\n",
      "      \"learning_rate\": 4.355765595463138e-05,\n",
      "      \"loss\": 0.032,\n",
      "      \"step\": 20460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.290494344497306,\n",
      "      \"grad_norm\": 0.11060896515846252,\n",
      "      \"learning_rate\": 4.355135475740391e-05,\n",
      "      \"loss\": 0.0198,\n",
      "      \"step\": 20480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2917546236491382,\n",
      "      \"grad_norm\": 0.0056663863360881805,\n",
      "      \"learning_rate\": 4.354505356017643e-05,\n",
      "      \"loss\": 0.0312,\n",
      "      \"step\": 20500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2930149028009703,\n",
      "      \"grad_norm\": 0.007384070195257664,\n",
      "      \"learning_rate\": 4.353875236294896e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 20520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2942751819528024,\n",
      "      \"grad_norm\": 15.082735061645508,\n",
      "      \"learning_rate\": 4.353245116572149e-05,\n",
      "      \"loss\": 0.0384,\n",
      "      \"step\": 20540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2955354611046346,\n",
      "      \"grad_norm\": 0.5321580767631531,\n",
      "      \"learning_rate\": 4.352614996849402e-05,\n",
      "      \"loss\": 0.0466,\n",
      "      \"step\": 20560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2967957402564667,\n",
      "      \"grad_norm\": 0.09431122243404388,\n",
      "      \"learning_rate\": 4.351984877126654e-05,\n",
      "      \"loss\": 0.0656,\n",
      "      \"step\": 20580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2980560194082988,\n",
      "      \"grad_norm\": 0.016678176820278168,\n",
      "      \"learning_rate\": 4.351354757403907e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 20600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.299316298560131,\n",
      "      \"grad_norm\": 4.303460121154785,\n",
      "      \"learning_rate\": 4.3507246376811594e-05,\n",
      "      \"loss\": 0.097,\n",
      "      \"step\": 20620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3005765777119631,\n",
      "      \"grad_norm\": 0.03669741004705429,\n",
      "      \"learning_rate\": 4.350094517958412e-05,\n",
      "      \"loss\": 0.0311,\n",
      "      \"step\": 20640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3018368568637952,\n",
      "      \"grad_norm\": 0.3785635530948639,\n",
      "      \"learning_rate\": 4.3494643982356645e-05,\n",
      "      \"loss\": 0.0288,\n",
      "      \"step\": 20660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3030971360156274,\n",
      "      \"grad_norm\": 0.07628165185451508,\n",
      "      \"learning_rate\": 4.3488342785129174e-05,\n",
      "      \"loss\": 0.0381,\n",
      "      \"step\": 20680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3043574151674595,\n",
      "      \"grad_norm\": 0.015163017436861992,\n",
      "      \"learning_rate\": 4.34820415879017e-05,\n",
      "      \"loss\": 0.0503,\n",
      "      \"step\": 20700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3056176943192916,\n",
      "      \"grad_norm\": 0.8134609460830688,\n",
      "      \"learning_rate\": 4.347574039067423e-05,\n",
      "      \"loss\": 0.067,\n",
      "      \"step\": 20720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3068779734711238,\n",
      "      \"grad_norm\": 0.014371032826602459,\n",
      "      \"learning_rate\": 4.346943919344676e-05,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 20740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.308138252622956,\n",
      "      \"grad_norm\": 0.16514365375041962,\n",
      "      \"learning_rate\": 4.346313799621928e-05,\n",
      "      \"loss\": 0.0393,\n",
      "      \"step\": 20760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.309398531774788,\n",
      "      \"grad_norm\": 0.05751127749681473,\n",
      "      \"learning_rate\": 4.345683679899181e-05,\n",
      "      \"loss\": 0.029,\n",
      "      \"step\": 20780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3106588109266202,\n",
      "      \"grad_norm\": 0.3975435495376587,\n",
      "      \"learning_rate\": 4.3450535601764335e-05,\n",
      "      \"loss\": 0.067,\n",
      "      \"step\": 20800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3119190900784523,\n",
      "      \"grad_norm\": 0.02878315933048725,\n",
      "      \"learning_rate\": 4.3444234404536864e-05,\n",
      "      \"loss\": 0.0454,\n",
      "      \"step\": 20820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3131793692302844,\n",
      "      \"grad_norm\": 3.4472575187683105,\n",
      "      \"learning_rate\": 4.3437933207309386e-05,\n",
      "      \"loss\": 0.0782,\n",
      "      \"step\": 20840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3144396483821166,\n",
      "      \"grad_norm\": 0.036944009363651276,\n",
      "      \"learning_rate\": 4.343163201008192e-05,\n",
      "      \"loss\": 0.0217,\n",
      "      \"step\": 20860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3156999275339487,\n",
      "      \"grad_norm\": 0.27108708024024963,\n",
      "      \"learning_rate\": 4.3425330812854444e-05,\n",
      "      \"loss\": 0.0609,\n",
      "      \"step\": 20880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3169602066857808,\n",
      "      \"grad_norm\": 0.003458718303591013,\n",
      "      \"learning_rate\": 4.341902961562697e-05,\n",
      "      \"loss\": 0.0156,\n",
      "      \"step\": 20900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.318220485837613,\n",
      "      \"grad_norm\": 0.003295452566817403,\n",
      "      \"learning_rate\": 4.3412728418399495e-05,\n",
      "      \"loss\": 0.0507,\n",
      "      \"step\": 20920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.319480764989445,\n",
      "      \"grad_norm\": 0.10016552358865738,\n",
      "      \"learning_rate\": 4.3406427221172024e-05,\n",
      "      \"loss\": 0.0559,\n",
      "      \"step\": 20940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3207410441412772,\n",
      "      \"grad_norm\": 0.1387462168931961,\n",
      "      \"learning_rate\": 4.340012602394455e-05,\n",
      "      \"loss\": 0.0462,\n",
      "      \"step\": 20960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3220013232931094,\n",
      "      \"grad_norm\": 0.004054745193570852,\n",
      "      \"learning_rate\": 4.3393824826717076e-05,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 20980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3232616024449415,\n",
      "      \"grad_norm\": 0.014357971027493477,\n",
      "      \"learning_rate\": 4.3387523629489605e-05,\n",
      "      \"loss\": 0.0171,\n",
      "      \"step\": 21000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3245218815967736,\n",
      "      \"grad_norm\": 0.00996073056012392,\n",
      "      \"learning_rate\": 4.3381222432262134e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 21020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3257821607486058,\n",
      "      \"grad_norm\": 0.0011129004415124655,\n",
      "      \"learning_rate\": 4.337492123503466e-05,\n",
      "      \"loss\": 0.032,\n",
      "      \"step\": 21040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.327042439900438,\n",
      "      \"grad_norm\": 0.0005678176530636847,\n",
      "      \"learning_rate\": 4.3368620037807185e-05,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 21060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.32830271905227,\n",
      "      \"grad_norm\": 14.350126266479492,\n",
      "      \"learning_rate\": 4.3362318840579714e-05,\n",
      "      \"loss\": 0.0665,\n",
      "      \"step\": 21080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3295629982041022,\n",
      "      \"grad_norm\": 0.04229610413312912,\n",
      "      \"learning_rate\": 4.3356017643352236e-05,\n",
      "      \"loss\": 0.0612,\n",
      "      \"step\": 21100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3308232773559343,\n",
      "      \"grad_norm\": 0.057008083909749985,\n",
      "      \"learning_rate\": 4.3349716446124765e-05,\n",
      "      \"loss\": 0.0297,\n",
      "      \"step\": 21120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3320835565077664,\n",
      "      \"grad_norm\": 27.27493667602539,\n",
      "      \"learning_rate\": 4.334341524889729e-05,\n",
      "      \"loss\": 0.0338,\n",
      "      \"step\": 21140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3333438356595986,\n",
      "      \"grad_norm\": 0.0058808112516999245,\n",
      "      \"learning_rate\": 4.333711405166982e-05,\n",
      "      \"loss\": 0.0597,\n",
      "      \"step\": 21160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3346041148114307,\n",
      "      \"grad_norm\": 0.026149535551667213,\n",
      "      \"learning_rate\": 4.3330812854442346e-05,\n",
      "      \"loss\": 0.0432,\n",
      "      \"step\": 21180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3358643939632628,\n",
      "      \"grad_norm\": 0.055879294872283936,\n",
      "      \"learning_rate\": 4.3324511657214875e-05,\n",
      "      \"loss\": 0.0166,\n",
      "      \"step\": 21200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.337124673115095,\n",
      "      \"grad_norm\": 0.382542222738266,\n",
      "      \"learning_rate\": 4.3318210459987404e-05,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 21220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.338384952266927,\n",
      "      \"grad_norm\": 0.08038888871669769,\n",
      "      \"learning_rate\": 4.3311909262759926e-05,\n",
      "      \"loss\": 0.06,\n",
      "      \"step\": 21240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3396452314187592,\n",
      "      \"grad_norm\": 0.3887518346309662,\n",
      "      \"learning_rate\": 4.3305608065532455e-05,\n",
      "      \"loss\": 0.0288,\n",
      "      \"step\": 21260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3409055105705914,\n",
      "      \"grad_norm\": 3.7625184059143066,\n",
      "      \"learning_rate\": 4.329930686830498e-05,\n",
      "      \"loss\": 0.0555,\n",
      "      \"step\": 21280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3421657897224235,\n",
      "      \"grad_norm\": 0.4645053446292877,\n",
      "      \"learning_rate\": 4.3293005671077506e-05,\n",
      "      \"loss\": 0.0033,\n",
      "      \"step\": 21300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3434260688742556,\n",
      "      \"grad_norm\": 0.07084473967552185,\n",
      "      \"learning_rate\": 4.328670447385003e-05,\n",
      "      \"loss\": 0.0441,\n",
      "      \"step\": 21320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3446863480260878,\n",
      "      \"grad_norm\": 0.5418137907981873,\n",
      "      \"learning_rate\": 4.328040327662256e-05,\n",
      "      \"loss\": 0.0389,\n",
      "      \"step\": 21340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.34594662717792,\n",
      "      \"grad_norm\": 3.2188899517059326,\n",
      "      \"learning_rate\": 4.327410207939509e-05,\n",
      "      \"loss\": 0.0611,\n",
      "      \"step\": 21360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.347206906329752,\n",
      "      \"grad_norm\": 0.1175667867064476,\n",
      "      \"learning_rate\": 4.3267800882167616e-05,\n",
      "      \"loss\": 0.0581,\n",
      "      \"step\": 21380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3484671854815842,\n",
      "      \"grad_norm\": 29.511341094970703,\n",
      "      \"learning_rate\": 4.3261499684940145e-05,\n",
      "      \"loss\": 0.0213,\n",
      "      \"step\": 21400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3497274646334163,\n",
      "      \"grad_norm\": 0.02530491165816784,\n",
      "      \"learning_rate\": 4.325519848771267e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 21420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3509877437852484,\n",
      "      \"grad_norm\": 0.08905300498008728,\n",
      "      \"learning_rate\": 4.3248897290485196e-05,\n",
      "      \"loss\": 0.0498,\n",
      "      \"step\": 21440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3522480229370806,\n",
      "      \"grad_norm\": 0.19273807108402252,\n",
      "      \"learning_rate\": 4.324259609325772e-05,\n",
      "      \"loss\": 0.0614,\n",
      "      \"step\": 21460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3535083020889127,\n",
      "      \"grad_norm\": 4.0444254875183105,\n",
      "      \"learning_rate\": 4.323629489603025e-05,\n",
      "      \"loss\": 0.0632,\n",
      "      \"step\": 21480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3547685812407448,\n",
      "      \"grad_norm\": 3.2709295749664307,\n",
      "      \"learning_rate\": 4.3229993698802776e-05,\n",
      "      \"loss\": 0.0589,\n",
      "      \"step\": 21500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.356028860392577,\n",
      "      \"grad_norm\": 0.23837882280349731,\n",
      "      \"learning_rate\": 4.3223692501575305e-05,\n",
      "      \"loss\": 0.0585,\n",
      "      \"step\": 21520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.357289139544409,\n",
      "      \"grad_norm\": 13.780202865600586,\n",
      "      \"learning_rate\": 4.321739130434783e-05,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 21540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3585494186962412,\n",
      "      \"grad_norm\": 0.2649848461151123,\n",
      "      \"learning_rate\": 4.321109010712036e-05,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 21560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3598096978480734,\n",
      "      \"grad_norm\": 0.0014995535602793097,\n",
      "      \"learning_rate\": 4.320478890989288e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 21580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3610699769999055,\n",
      "      \"grad_norm\": 0.004413412883877754,\n",
      "      \"learning_rate\": 4.319880277252678e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 21600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3623302561517376,\n",
      "      \"grad_norm\": 4.894413948059082,\n",
      "      \"learning_rate\": 4.319250157529931e-05,\n",
      "      \"loss\": 0.0357,\n",
      "      \"step\": 21620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3635905353035698,\n",
      "      \"grad_norm\": 0.15357518196105957,\n",
      "      \"learning_rate\": 4.318620037807183e-05,\n",
      "      \"loss\": 0.0704,\n",
      "      \"step\": 21640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.364850814455402,\n",
      "      \"grad_norm\": 0.027603862807154655,\n",
      "      \"learning_rate\": 4.317989918084436e-05,\n",
      "      \"loss\": 0.0353,\n",
      "      \"step\": 21660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.366111093607234,\n",
      "      \"grad_norm\": 0.013253137469291687,\n",
      "      \"learning_rate\": 4.317359798361689e-05,\n",
      "      \"loss\": 0.0495,\n",
      "      \"step\": 21680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3673713727590662,\n",
      "      \"grad_norm\": 0.006059205159544945,\n",
      "      \"learning_rate\": 4.316729678638942e-05,\n",
      "      \"loss\": 0.0425,\n",
      "      \"step\": 21700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3686316519108983,\n",
      "      \"grad_norm\": 0.12987235188484192,\n",
      "      \"learning_rate\": 4.316099558916195e-05,\n",
      "      \"loss\": 0.0548,\n",
      "      \"step\": 21720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3698919310627304,\n",
      "      \"grad_norm\": 6.2933807373046875,\n",
      "      \"learning_rate\": 4.315469439193447e-05,\n",
      "      \"loss\": 0.041,\n",
      "      \"step\": 21740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3711522102145626,\n",
      "      \"grad_norm\": 4.695031642913818,\n",
      "      \"learning_rate\": 4.3148393194707e-05,\n",
      "      \"loss\": 0.0691,\n",
      "      \"step\": 21760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3724124893663947,\n",
      "      \"grad_norm\": 0.33957067131996155,\n",
      "      \"learning_rate\": 4.314209199747952e-05,\n",
      "      \"loss\": 0.0607,\n",
      "      \"step\": 21780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3736727685182268,\n",
      "      \"grad_norm\": 5.309919357299805,\n",
      "      \"learning_rate\": 4.313579080025205e-05,\n",
      "      \"loss\": 0.0441,\n",
      "      \"step\": 21800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.374933047670059,\n",
      "      \"grad_norm\": 0.03368017077445984,\n",
      "      \"learning_rate\": 4.312948960302457e-05,\n",
      "      \"loss\": 0.0344,\n",
      "      \"step\": 21820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.376193326821891,\n",
      "      \"grad_norm\": 0.10436008870601654,\n",
      "      \"learning_rate\": 4.312318840579711e-05,\n",
      "      \"loss\": 0.0147,\n",
      "      \"step\": 21840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3774536059737232,\n",
      "      \"grad_norm\": 16.479366302490234,\n",
      "      \"learning_rate\": 4.311688720856963e-05,\n",
      "      \"loss\": 0.034,\n",
      "      \"step\": 21860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3787138851255554,\n",
      "      \"grad_norm\": 7.464805603027344,\n",
      "      \"learning_rate\": 4.311058601134216e-05,\n",
      "      \"loss\": 0.0268,\n",
      "      \"step\": 21880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3799741642773875,\n",
      "      \"grad_norm\": 13.536584854125977,\n",
      "      \"learning_rate\": 4.310428481411468e-05,\n",
      "      \"loss\": 0.0423,\n",
      "      \"step\": 21900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3812344434292196,\n",
      "      \"grad_norm\": 0.17896206676959991,\n",
      "      \"learning_rate\": 4.309798361688721e-05,\n",
      "      \"loss\": 0.066,\n",
      "      \"step\": 21920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3824947225810518,\n",
      "      \"grad_norm\": 0.026667846366763115,\n",
      "      \"learning_rate\": 4.309168241965974e-05,\n",
      "      \"loss\": 0.0593,\n",
      "      \"step\": 21940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.383755001732884,\n",
      "      \"grad_norm\": 3.5340771675109863,\n",
      "      \"learning_rate\": 4.308538122243226e-05,\n",
      "      \"loss\": 0.0191,\n",
      "      \"step\": 21960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.385015280884716,\n",
      "      \"grad_norm\": 14.029637336730957,\n",
      "      \"learning_rate\": 4.307908002520479e-05,\n",
      "      \"loss\": 0.0046,\n",
      "      \"step\": 21980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3862755600365482,\n",
      "      \"grad_norm\": 16.62363624572754,\n",
      "      \"learning_rate\": 4.307277882797732e-05,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 22000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3875358391883803,\n",
      "      \"grad_norm\": 0.0021982730831950903,\n",
      "      \"learning_rate\": 4.306647763074985e-05,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 22020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3887961183402124,\n",
      "      \"grad_norm\": 7.961224555969238,\n",
      "      \"learning_rate\": 4.306017643352237e-05,\n",
      "      \"loss\": 0.0951,\n",
      "      \"step\": 22040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3900563974920446,\n",
      "      \"grad_norm\": 0.059224240481853485,\n",
      "      \"learning_rate\": 4.30538752362949e-05,\n",
      "      \"loss\": 0.048,\n",
      "      \"step\": 22060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3913166766438767,\n",
      "      \"grad_norm\": 0.053349073976278305,\n",
      "      \"learning_rate\": 4.304757403906742e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 22080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3925769557957088,\n",
      "      \"grad_norm\": 0.016534103080630302,\n",
      "      \"learning_rate\": 4.304127284183995e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 22100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.393837234947541,\n",
      "      \"grad_norm\": 10.783868789672852,\n",
      "      \"learning_rate\": 4.3034971644612474e-05,\n",
      "      \"loss\": 0.1025,\n",
      "      \"step\": 22120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.395097514099373,\n",
      "      \"grad_norm\": 8.64253044128418,\n",
      "      \"learning_rate\": 4.3028670447385e-05,\n",
      "      \"loss\": 0.0475,\n",
      "      \"step\": 22140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3963577932512052,\n",
      "      \"grad_norm\": 0.2596186101436615,\n",
      "      \"learning_rate\": 4.302236925015753e-05,\n",
      "      \"loss\": 0.0222,\n",
      "      \"step\": 22160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3976180724030374,\n",
      "      \"grad_norm\": 0.013531319797039032,\n",
      "      \"learning_rate\": 4.301606805293006e-05,\n",
      "      \"loss\": 0.0285,\n",
      "      \"step\": 22180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3988783515548695,\n",
      "      \"grad_norm\": 0.07416485249996185,\n",
      "      \"learning_rate\": 4.300976685570259e-05,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 22200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4001386307067016,\n",
      "      \"grad_norm\": 0.09176908433437347,\n",
      "      \"learning_rate\": 4.300346565847511e-05,\n",
      "      \"loss\": 0.0602,\n",
      "      \"step\": 22220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4013989098585338,\n",
      "      \"grad_norm\": 10.821348190307617,\n",
      "      \"learning_rate\": 4.299716446124764e-05,\n",
      "      \"loss\": 0.0541,\n",
      "      \"step\": 22240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.402659189010366,\n",
      "      \"grad_norm\": 0.3947030305862427,\n",
      "      \"learning_rate\": 4.2990863264020164e-05,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 22260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.403919468162198,\n",
      "      \"grad_norm\": 0.0609908364713192,\n",
      "      \"learning_rate\": 4.298456206679269e-05,\n",
      "      \"loss\": 0.0573,\n",
      "      \"step\": 22280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4051797473140302,\n",
      "      \"grad_norm\": 0.09916607290506363,\n",
      "      \"learning_rate\": 4.2978260869565215e-05,\n",
      "      \"loss\": 0.1014,\n",
      "      \"step\": 22300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4064400264658623,\n",
      "      \"grad_norm\": 0.18241530656814575,\n",
      "      \"learning_rate\": 4.2971959672337744e-05,\n",
      "      \"loss\": 0.058,\n",
      "      \"step\": 22320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4077003056176944,\n",
      "      \"grad_norm\": 3.398787260055542,\n",
      "      \"learning_rate\": 4.296565847511027e-05,\n",
      "      \"loss\": 0.0352,\n",
      "      \"step\": 22340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4089605847695266,\n",
      "      \"grad_norm\": 0.03055276907980442,\n",
      "      \"learning_rate\": 4.29593572778828e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 22360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4102208639213587,\n",
      "      \"grad_norm\": 0.07015115767717361,\n",
      "      \"learning_rate\": 4.2953056080655324e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 22380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4114811430731908,\n",
      "      \"grad_norm\": 0.029673177748918533,\n",
      "      \"learning_rate\": 4.2946754883427853e-05,\n",
      "      \"loss\": 0.0403,\n",
      "      \"step\": 22400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.412741422225023,\n",
      "      \"grad_norm\": 0.7331266403198242,\n",
      "      \"learning_rate\": 4.294045368620038e-05,\n",
      "      \"loss\": 0.0522,\n",
      "      \"step\": 22420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.414001701376855,\n",
      "      \"grad_norm\": 0.005207516718655825,\n",
      "      \"learning_rate\": 4.2934152488972905e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 22440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4152619805286872,\n",
      "      \"grad_norm\": 0.30166906118392944,\n",
      "      \"learning_rate\": 4.2927851291745434e-05,\n",
      "      \"loss\": 0.0172,\n",
      "      \"step\": 22460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4165222596805194,\n",
      "      \"grad_norm\": 0.05902920290827751,\n",
      "      \"learning_rate\": 4.2921550094517956e-05,\n",
      "      \"loss\": 0.0481,\n",
      "      \"step\": 22480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4177825388323515,\n",
      "      \"grad_norm\": 0.03850346803665161,\n",
      "      \"learning_rate\": 4.291524889729049e-05,\n",
      "      \"loss\": 0.037,\n",
      "      \"step\": 22500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4190428179841836,\n",
      "      \"grad_norm\": 0.002372692571952939,\n",
      "      \"learning_rate\": 4.2908947700063014e-05,\n",
      "      \"loss\": 0.0302,\n",
      "      \"step\": 22520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4203030971360155,\n",
      "      \"grad_norm\": 0.6590802073478699,\n",
      "      \"learning_rate\": 4.290264650283554e-05,\n",
      "      \"loss\": 0.0219,\n",
      "      \"step\": 22540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4215633762878477,\n",
      "      \"grad_norm\": 2.5646097660064697,\n",
      "      \"learning_rate\": 4.2896345305608065e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 22560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4228236554396798,\n",
      "      \"grad_norm\": 0.0019195930799469352,\n",
      "      \"learning_rate\": 4.2890044108380594e-05,\n",
      "      \"loss\": 0.0193,\n",
      "      \"step\": 22580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.424083934591512,\n",
      "      \"grad_norm\": 0.010599538683891296,\n",
      "      \"learning_rate\": 4.288374291115312e-05,\n",
      "      \"loss\": 0.065,\n",
      "      \"step\": 22600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.425344213743344,\n",
      "      \"grad_norm\": 0.0364827997982502,\n",
      "      \"learning_rate\": 4.2877441713925646e-05,\n",
      "      \"loss\": 0.0237,\n",
      "      \"step\": 22620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4266044928951762,\n",
      "      \"grad_norm\": 0.0032344465143978596,\n",
      "      \"learning_rate\": 4.2871140516698175e-05,\n",
      "      \"loss\": 0.055,\n",
      "      \"step\": 22640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4278647720470083,\n",
      "      \"grad_norm\": 35.64586639404297,\n",
      "      \"learning_rate\": 4.2864839319470704e-05,\n",
      "      \"loss\": 0.0695,\n",
      "      \"step\": 22660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4291250511988405,\n",
      "      \"grad_norm\": 0.05587756261229515,\n",
      "      \"learning_rate\": 4.285853812224323e-05,\n",
      "      \"loss\": 0.0336,\n",
      "      \"step\": 22680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4303853303506726,\n",
      "      \"grad_norm\": 0.2592407464981079,\n",
      "      \"learning_rate\": 4.2852236925015755e-05,\n",
      "      \"loss\": 0.0493,\n",
      "      \"step\": 22700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4316456095025047,\n",
      "      \"grad_norm\": 0.01266488991677761,\n",
      "      \"learning_rate\": 4.2845935727788284e-05,\n",
      "      \"loss\": 0.0275,\n",
      "      \"step\": 22720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4329058886543369,\n",
      "      \"grad_norm\": 7.10114860534668,\n",
      "      \"learning_rate\": 4.2839634530560806e-05,\n",
      "      \"loss\": 0.0518,\n",
      "      \"step\": 22740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.434166167806169,\n",
      "      \"grad_norm\": 4.630430698394775,\n",
      "      \"learning_rate\": 4.2833333333333335e-05,\n",
      "      \"loss\": 0.0385,\n",
      "      \"step\": 22760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4354264469580011,\n",
      "      \"grad_norm\": 0.3347186744213104,\n",
      "      \"learning_rate\": 4.282703213610586e-05,\n",
      "      \"loss\": 0.0176,\n",
      "      \"step\": 22780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4366867261098333,\n",
      "      \"grad_norm\": 0.07351645827293396,\n",
      "      \"learning_rate\": 4.282073093887839e-05,\n",
      "      \"loss\": 0.0368,\n",
      "      \"step\": 22800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4379470052616654,\n",
      "      \"grad_norm\": 0.07523594796657562,\n",
      "      \"learning_rate\": 4.2814429741650916e-05,\n",
      "      \"loss\": 0.0336,\n",
      "      \"step\": 22820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4392072844134975,\n",
      "      \"grad_norm\": 0.0038968517910689116,\n",
      "      \"learning_rate\": 4.2808128544423445e-05,\n",
      "      \"loss\": 0.0587,\n",
      "      \"step\": 22840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4404675635653297,\n",
      "      \"grad_norm\": 0.12672321498394012,\n",
      "      \"learning_rate\": 4.2801827347195974e-05,\n",
      "      \"loss\": 0.0047,\n",
      "      \"step\": 22860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4417278427171618,\n",
      "      \"grad_norm\": 0.001218809629790485,\n",
      "      \"learning_rate\": 4.2795526149968496e-05,\n",
      "      \"loss\": 0.036,\n",
      "      \"step\": 22880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.442988121868994,\n",
      "      \"grad_norm\": 0.020178355276584625,\n",
      "      \"learning_rate\": 4.2789224952741025e-05,\n",
      "      \"loss\": 0.0238,\n",
      "      \"step\": 22900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.444248401020826,\n",
      "      \"grad_norm\": 0.038263801485300064,\n",
      "      \"learning_rate\": 4.278292375551355e-05,\n",
      "      \"loss\": 0.0279,\n",
      "      \"step\": 22920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4455086801726582,\n",
      "      \"grad_norm\": 1.5085442066192627,\n",
      "      \"learning_rate\": 4.2776622558286076e-05,\n",
      "      \"loss\": 0.0997,\n",
      "      \"step\": 22940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4467689593244903,\n",
      "      \"grad_norm\": 0.646101713180542,\n",
      "      \"learning_rate\": 4.27703213610586e-05,\n",
      "      \"loss\": 0.0337,\n",
      "      \"step\": 22960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4480292384763225,\n",
      "      \"grad_norm\": 0.8464711308479309,\n",
      "      \"learning_rate\": 4.276402016383113e-05,\n",
      "      \"loss\": 0.034,\n",
      "      \"step\": 22980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4492895176281546,\n",
      "      \"grad_norm\": 0.2787590026855469,\n",
      "      \"learning_rate\": 4.275771896660366e-05,\n",
      "      \"loss\": 0.0408,\n",
      "      \"step\": 23000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4505497967799867,\n",
      "      \"grad_norm\": 0.0696021020412445,\n",
      "      \"learning_rate\": 4.2751417769376186e-05,\n",
      "      \"loss\": 0.0351,\n",
      "      \"step\": 23020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4518100759318189,\n",
      "      \"grad_norm\": 0.005206484347581863,\n",
      "      \"learning_rate\": 4.274511657214871e-05,\n",
      "      \"loss\": 0.0069,\n",
      "      \"step\": 23040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.453070355083651,\n",
      "      \"grad_norm\": 12.28260612487793,\n",
      "      \"learning_rate\": 4.273881537492124e-05,\n",
      "      \"loss\": 0.0073,\n",
      "      \"step\": 23060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4543306342354831,\n",
      "      \"grad_norm\": 0.027161920443177223,\n",
      "      \"learning_rate\": 4.2732514177693766e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 23080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4555909133873153,\n",
      "      \"grad_norm\": 0.04145295172929764,\n",
      "      \"learning_rate\": 4.272621298046629e-05,\n",
      "      \"loss\": 0.0502,\n",
      "      \"step\": 23100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4568511925391474,\n",
      "      \"grad_norm\": 0.5023464560508728,\n",
      "      \"learning_rate\": 4.271991178323882e-05,\n",
      "      \"loss\": 0.0415,\n",
      "      \"step\": 23120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4581114716909795,\n",
      "      \"grad_norm\": 0.19983558356761932,\n",
      "      \"learning_rate\": 4.2713610586011347e-05,\n",
      "      \"loss\": 0.0328,\n",
      "      \"step\": 23140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4593717508428117,\n",
      "      \"grad_norm\": 0.003190030576661229,\n",
      "      \"learning_rate\": 4.2707309388783876e-05,\n",
      "      \"loss\": 0.0328,\n",
      "      \"step\": 23160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4606320299946438,\n",
      "      \"grad_norm\": 0.6646450757980347,\n",
      "      \"learning_rate\": 4.27010081915564e-05,\n",
      "      \"loss\": 0.0377,\n",
      "      \"step\": 23180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.461892309146476,\n",
      "      \"grad_norm\": 0.0033206313382834196,\n",
      "      \"learning_rate\": 4.269470699432893e-05,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 23200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.463152588298308,\n",
      "      \"grad_norm\": 0.01911044307053089,\n",
      "      \"learning_rate\": 4.268840579710145e-05,\n",
      "      \"loss\": 0.0808,\n",
      "      \"step\": 23220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4644128674501402,\n",
      "      \"grad_norm\": 0.020665902644395828,\n",
      "      \"learning_rate\": 4.268210459987398e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 23240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4656731466019723,\n",
      "      \"grad_norm\": 0.0010377693688496947,\n",
      "      \"learning_rate\": 4.26758034026465e-05,\n",
      "      \"loss\": 0.0439,\n",
      "      \"step\": 23260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4669334257538045,\n",
      "      \"grad_norm\": 0.002636111108586192,\n",
      "      \"learning_rate\": 4.266950220541903e-05,\n",
      "      \"loss\": 0.0534,\n",
      "      \"step\": 23280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4681937049056366,\n",
      "      \"grad_norm\": 0.016665710136294365,\n",
      "      \"learning_rate\": 4.266320100819156e-05,\n",
      "      \"loss\": 0.0455,\n",
      "      \"step\": 23300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4694539840574687,\n",
      "      \"grad_norm\": 0.13657504320144653,\n",
      "      \"learning_rate\": 4.265689981096409e-05,\n",
      "      \"loss\": 0.0621,\n",
      "      \"step\": 23320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4707142632093009,\n",
      "      \"grad_norm\": 12.201032638549805,\n",
      "      \"learning_rate\": 4.2650598613736617e-05,\n",
      "      \"loss\": 0.0995,\n",
      "      \"step\": 23340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.471974542361133,\n",
      "      \"grad_norm\": 0.1440749317407608,\n",
      "      \"learning_rate\": 4.264429741650914e-05,\n",
      "      \"loss\": 0.0491,\n",
      "      \"step\": 23360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4732348215129651,\n",
      "      \"grad_norm\": 0.07406903058290482,\n",
      "      \"learning_rate\": 4.263799621928167e-05,\n",
      "      \"loss\": 0.0449,\n",
      "      \"step\": 23380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4744951006647973,\n",
      "      \"grad_norm\": 0.16737495362758636,\n",
      "      \"learning_rate\": 4.263169502205419e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 23400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4757553798166294,\n",
      "      \"grad_norm\": 2.7252755165100098,\n",
      "      \"learning_rate\": 4.262539382482672e-05,\n",
      "      \"loss\": 0.05,\n",
      "      \"step\": 23420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4770156589684615,\n",
      "      \"grad_norm\": 0.5101537704467773,\n",
      "      \"learning_rate\": 4.261909262759924e-05,\n",
      "      \"loss\": 0.0456,\n",
      "      \"step\": 23440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4782759381202937,\n",
      "      \"grad_norm\": 0.013553053140640259,\n",
      "      \"learning_rate\": 4.261279143037177e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 23460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4795362172721258,\n",
      "      \"grad_norm\": 0.04782120883464813,\n",
      "      \"learning_rate\": 4.26064902331443e-05,\n",
      "      \"loss\": 0.0399,\n",
      "      \"step\": 23480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.480796496423958,\n",
      "      \"grad_norm\": 0.44339224696159363,\n",
      "      \"learning_rate\": 4.260018903591683e-05,\n",
      "      \"loss\": 0.0335,\n",
      "      \"step\": 23500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.48205677557579,\n",
      "      \"grad_norm\": 0.003999562934041023,\n",
      "      \"learning_rate\": 4.259388783868935e-05,\n",
      "      \"loss\": 0.0136,\n",
      "      \"step\": 23520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4833170547276222,\n",
      "      \"grad_norm\": 0.00011269795504631475,\n",
      "      \"learning_rate\": 4.258758664146188e-05,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 23540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4845773338794543,\n",
      "      \"grad_norm\": 0.0011162736918777227,\n",
      "      \"learning_rate\": 4.258128544423441e-05,\n",
      "      \"loss\": 0.0414,\n",
      "      \"step\": 23560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4858376130312865,\n",
      "      \"grad_norm\": 0.008106376975774765,\n",
      "      \"learning_rate\": 4.257498424700693e-05,\n",
      "      \"loss\": 0.0267,\n",
      "      \"step\": 23580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4870978921831186,\n",
      "      \"grad_norm\": 0.7811987996101379,\n",
      "      \"learning_rate\": 4.256899810964083e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 23600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4883581713349507,\n",
      "      \"grad_norm\": 0.513135552406311,\n",
      "      \"learning_rate\": 4.256269691241336e-05,\n",
      "      \"loss\": 0.0212,\n",
      "      \"step\": 23620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4896184504867829,\n",
      "      \"grad_norm\": 0.01598479226231575,\n",
      "      \"learning_rate\": 4.255639571518589e-05,\n",
      "      \"loss\": 0.0565,\n",
      "      \"step\": 23640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.490878729638615,\n",
      "      \"grad_norm\": 0.021449534222483635,\n",
      "      \"learning_rate\": 4.255009451795842e-05,\n",
      "      \"loss\": 0.0027,\n",
      "      \"step\": 23660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4921390087904471,\n",
      "      \"grad_norm\": 3.582902193069458,\n",
      "      \"learning_rate\": 4.254379332073094e-05,\n",
      "      \"loss\": 0.0304,\n",
      "      \"step\": 23680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4933992879422793,\n",
      "      \"grad_norm\": 0.018607735633850098,\n",
      "      \"learning_rate\": 4.253749212350347e-05,\n",
      "      \"loss\": 0.0623,\n",
      "      \"step\": 23700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4946595670941114,\n",
      "      \"grad_norm\": 0.016843881458044052,\n",
      "      \"learning_rate\": 4.253119092627599e-05,\n",
      "      \"loss\": 0.0569,\n",
      "      \"step\": 23720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4959198462459435,\n",
      "      \"grad_norm\": 0.08643890172243118,\n",
      "      \"learning_rate\": 4.252488972904852e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 23740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4971801253977757,\n",
      "      \"grad_norm\": 5.320949077606201,\n",
      "      \"learning_rate\": 4.2518588531821044e-05,\n",
      "      \"loss\": 0.0609,\n",
      "      \"step\": 23760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4984404045496078,\n",
      "      \"grad_norm\": 0.009448842145502567,\n",
      "      \"learning_rate\": 4.251228733459357e-05,\n",
      "      \"loss\": 0.033,\n",
      "      \"step\": 23780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.49970068370144,\n",
      "      \"grad_norm\": 1.5195761919021606,\n",
      "      \"learning_rate\": 4.25059861373661e-05,\n",
      "      \"loss\": 0.0373,\n",
      "      \"step\": 23800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5009609628532719,\n",
      "      \"grad_norm\": 3.0596811771392822,\n",
      "      \"learning_rate\": 4.249968494013863e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 23820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.502221242005104,\n",
      "      \"grad_norm\": 0.023758983239531517,\n",
      "      \"learning_rate\": 4.2493383742911153e-05,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 23840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5034815211569361,\n",
      "      \"grad_norm\": 0.06942848116159439,\n",
      "      \"learning_rate\": 4.248708254568368e-05,\n",
      "      \"loss\": 0.0395,\n",
      "      \"step\": 23860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5047418003087683,\n",
      "      \"grad_norm\": 0.012752380222082138,\n",
      "      \"learning_rate\": 4.248078134845621e-05,\n",
      "      \"loss\": 0.0379,\n",
      "      \"step\": 23880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5060020794606004,\n",
      "      \"grad_norm\": 0.013003882020711899,\n",
      "      \"learning_rate\": 4.2474480151228734e-05,\n",
      "      \"loss\": 0.0143,\n",
      "      \"step\": 23900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5072623586124325,\n",
      "      \"grad_norm\": 0.02259734459221363,\n",
      "      \"learning_rate\": 4.246817895400126e-05,\n",
      "      \"loss\": 0.0206,\n",
      "      \"step\": 23920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5085226377642647,\n",
      "      \"grad_norm\": 0.15284469723701477,\n",
      "      \"learning_rate\": 4.2461877756773785e-05,\n",
      "      \"loss\": 0.0322,\n",
      "      \"step\": 23940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5097829169160968,\n",
      "      \"grad_norm\": 9.644636154174805,\n",
      "      \"learning_rate\": 4.2455576559546314e-05,\n",
      "      \"loss\": 0.0614,\n",
      "      \"step\": 23960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.511043196067929,\n",
      "      \"grad_norm\": 1.296894907951355,\n",
      "      \"learning_rate\": 4.244927536231884e-05,\n",
      "      \"loss\": 0.0482,\n",
      "      \"step\": 23980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.512303475219761,\n",
      "      \"grad_norm\": 0.04647845774888992,\n",
      "      \"learning_rate\": 4.244297416509137e-05,\n",
      "      \"loss\": 0.0479,\n",
      "      \"step\": 24000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5135637543715932,\n",
      "      \"grad_norm\": 3.7379612922668457,\n",
      "      \"learning_rate\": 4.2436672967863894e-05,\n",
      "      \"loss\": 0.0397,\n",
      "      \"step\": 24020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5148240335234253,\n",
      "      \"grad_norm\": 0.06664597988128662,\n",
      "      \"learning_rate\": 4.2430371770636424e-05,\n",
      "      \"loss\": 0.0449,\n",
      "      \"step\": 24040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5160843126752575,\n",
      "      \"grad_norm\": 0.055898863822221756,\n",
      "      \"learning_rate\": 4.2424070573408946e-05,\n",
      "      \"loss\": 0.0036,\n",
      "      \"step\": 24060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5173445918270896,\n",
      "      \"grad_norm\": 0.0016602110117673874,\n",
      "      \"learning_rate\": 4.2417769376181475e-05,\n",
      "      \"loss\": 0.0052,\n",
      "      \"step\": 24080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5186048709789217,\n",
      "      \"grad_norm\": 0.004454238805919886,\n",
      "      \"learning_rate\": 4.2411468178954004e-05,\n",
      "      \"loss\": 0.0594,\n",
      "      \"step\": 24100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5198651501307539,\n",
      "      \"grad_norm\": 0.18364065885543823,\n",
      "      \"learning_rate\": 4.240516698172653e-05,\n",
      "      \"loss\": 0.0376,\n",
      "      \"step\": 24120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.521125429282586,\n",
      "      \"grad_norm\": 9.490260124206543,\n",
      "      \"learning_rate\": 4.239886578449906e-05,\n",
      "      \"loss\": 0.0487,\n",
      "      \"step\": 24140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5223857084344181,\n",
      "      \"grad_norm\": 0.10065486282110214,\n",
      "      \"learning_rate\": 4.2392564587271584e-05,\n",
      "      \"loss\": 0.041,\n",
      "      \"step\": 24160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5236459875862502,\n",
      "      \"grad_norm\": 0.07076084613800049,\n",
      "      \"learning_rate\": 4.238626339004411e-05,\n",
      "      \"loss\": 0.0168,\n",
      "      \"step\": 24180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5249062667380824,\n",
      "      \"grad_norm\": 0.030629556626081467,\n",
      "      \"learning_rate\": 4.2379962192816636e-05,\n",
      "      \"loss\": 0.0288,\n",
      "      \"step\": 24200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5261665458899145,\n",
      "      \"grad_norm\": 0.011325854808092117,\n",
      "      \"learning_rate\": 4.2373660995589165e-05,\n",
      "      \"loss\": 0.0351,\n",
      "      \"step\": 24220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5274268250417466,\n",
      "      \"grad_norm\": 0.00687573803588748,\n",
      "      \"learning_rate\": 4.236735979836169e-05,\n",
      "      \"loss\": 0.0294,\n",
      "      \"step\": 24240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5286871041935788,\n",
      "      \"grad_norm\": 0.12089859694242477,\n",
      "      \"learning_rate\": 4.2361058601134216e-05,\n",
      "      \"loss\": 0.0401,\n",
      "      \"step\": 24260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.529947383345411,\n",
      "      \"grad_norm\": 1.9033582210540771,\n",
      "      \"learning_rate\": 4.2354757403906745e-05,\n",
      "      \"loss\": 0.0714,\n",
      "      \"step\": 24280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.531207662497243,\n",
      "      \"grad_norm\": 3.8817975521087646,\n",
      "      \"learning_rate\": 4.2348456206679274e-05,\n",
      "      \"loss\": 0.0309,\n",
      "      \"step\": 24300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5324679416490752,\n",
      "      \"grad_norm\": 0.39716029167175293,\n",
      "      \"learning_rate\": 4.23421550094518e-05,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 24320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5337282208009073,\n",
      "      \"grad_norm\": 0.003723799018189311,\n",
      "      \"learning_rate\": 4.2335853812224325e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 24340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5349884999527394,\n",
      "      \"grad_norm\": 0.004301774315536022,\n",
      "      \"learning_rate\": 4.2329552614996854e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 24360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5362487791045716,\n",
      "      \"grad_norm\": 0.02258962392807007,\n",
      "      \"learning_rate\": 4.2323251417769377e-05,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 24380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5375090582564037,\n",
      "      \"grad_norm\": 3.7104527950286865,\n",
      "      \"learning_rate\": 4.2316950220541906e-05,\n",
      "      \"loss\": 0.0378,\n",
      "      \"step\": 24400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5387693374082358,\n",
      "      \"grad_norm\": 0.009686756879091263,\n",
      "      \"learning_rate\": 4.231064902331443e-05,\n",
      "      \"loss\": 0.0255,\n",
      "      \"step\": 24420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.540029616560068,\n",
      "      \"grad_norm\": 0.0282315481454134,\n",
      "      \"learning_rate\": 4.230434782608696e-05,\n",
      "      \"loss\": 0.1084,\n",
      "      \"step\": 24440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5412898957119001,\n",
      "      \"grad_norm\": 0.005458223167806864,\n",
      "      \"learning_rate\": 4.2298046628859486e-05,\n",
      "      \"loss\": 0.0257,\n",
      "      \"step\": 24460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5425501748637322,\n",
      "      \"grad_norm\": 0.4312244951725006,\n",
      "      \"learning_rate\": 4.2291745431632015e-05,\n",
      "      \"loss\": 0.0205,\n",
      "      \"step\": 24480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5438104540155644,\n",
      "      \"grad_norm\": 0.008805041201412678,\n",
      "      \"learning_rate\": 4.228544423440454e-05,\n",
      "      \"loss\": 0.0879,\n",
      "      \"step\": 24500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5450707331673965,\n",
      "      \"grad_norm\": 0.058777738362550735,\n",
      "      \"learning_rate\": 4.2279143037177066e-05,\n",
      "      \"loss\": 0.0552,\n",
      "      \"step\": 24520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5463310123192286,\n",
      "      \"grad_norm\": 0.8400774002075195,\n",
      "      \"learning_rate\": 4.2272841839949595e-05,\n",
      "      \"loss\": 0.0456,\n",
      "      \"step\": 24540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5475912914710608,\n",
      "      \"grad_norm\": 3.5728795528411865,\n",
      "      \"learning_rate\": 4.226654064272212e-05,\n",
      "      \"loss\": 0.0535,\n",
      "      \"step\": 24560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.548851570622893,\n",
      "      \"grad_norm\": 0.014849232509732246,\n",
      "      \"learning_rate\": 4.2260239445494647e-05,\n",
      "      \"loss\": 0.0216,\n",
      "      \"step\": 24580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.550111849774725,\n",
      "      \"grad_norm\": 0.005544004961848259,\n",
      "      \"learning_rate\": 4.225393824826717e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 24600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5513721289265572,\n",
      "      \"grad_norm\": 0.23499400913715363,\n",
      "      \"learning_rate\": 4.22476370510397e-05,\n",
      "      \"loss\": 0.0411,\n",
      "      \"step\": 24620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5526324080783893,\n",
      "      \"grad_norm\": 4.176117897033691,\n",
      "      \"learning_rate\": 4.224133585381223e-05,\n",
      "      \"loss\": 0.0314,\n",
      "      \"step\": 24640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5538926872302214,\n",
      "      \"grad_norm\": 0.0026016084011644125,\n",
      "      \"learning_rate\": 4.2235034656584756e-05,\n",
      "      \"loss\": 0.0471,\n",
      "      \"step\": 24660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5551529663820536,\n",
      "      \"grad_norm\": 0.33833739161491394,\n",
      "      \"learning_rate\": 4.222873345935728e-05,\n",
      "      \"loss\": 0.005,\n",
      "      \"step\": 24680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5564132455338857,\n",
      "      \"grad_norm\": 1.9483572244644165,\n",
      "      \"learning_rate\": 4.222243226212981e-05,\n",
      "      \"loss\": 0.0325,\n",
      "      \"step\": 24700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5576735246857178,\n",
      "      \"grad_norm\": 9.489052772521973,\n",
      "      \"learning_rate\": 4.221613106490233e-05,\n",
      "      \"loss\": 0.042,\n",
      "      \"step\": 24720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.55893380383755,\n",
      "      \"grad_norm\": 8.483729362487793,\n",
      "      \"learning_rate\": 4.220982986767486e-05,\n",
      "      \"loss\": 0.0612,\n",
      "      \"step\": 24740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5601940829893821,\n",
      "      \"grad_norm\": 0.3624950349330902,\n",
      "      \"learning_rate\": 4.220352867044739e-05,\n",
      "      \"loss\": 0.0324,\n",
      "      \"step\": 24760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5614543621412142,\n",
      "      \"grad_norm\": 0.008708717301487923,\n",
      "      \"learning_rate\": 4.219722747321992e-05,\n",
      "      \"loss\": 0.0357,\n",
      "      \"step\": 24780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5627146412930464,\n",
      "      \"grad_norm\": 0.08388043195009232,\n",
      "      \"learning_rate\": 4.2190926275992446e-05,\n",
      "      \"loss\": 0.0431,\n",
      "      \"step\": 24800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5639749204448785,\n",
      "      \"grad_norm\": 4.457087516784668,\n",
      "      \"learning_rate\": 4.218462507876497e-05,\n",
      "      \"loss\": 0.0276,\n",
      "      \"step\": 24820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5652351995967106,\n",
      "      \"grad_norm\": 6.656737327575684,\n",
      "      \"learning_rate\": 4.21783238815375e-05,\n",
      "      \"loss\": 0.0147,\n",
      "      \"step\": 24840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5664954787485428,\n",
      "      \"grad_norm\": 0.03879878297448158,\n",
      "      \"learning_rate\": 4.217202268431002e-05,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 24860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.567755757900375,\n",
      "      \"grad_norm\": 0.03589163348078728,\n",
      "      \"learning_rate\": 4.216572148708255e-05,\n",
      "      \"loss\": 0.049,\n",
      "      \"step\": 24880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.569016037052207,\n",
      "      \"grad_norm\": 0.0010955111356452107,\n",
      "      \"learning_rate\": 4.215942028985507e-05,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 24900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5702763162040392,\n",
      "      \"grad_norm\": 0.010753822512924671,\n",
      "      \"learning_rate\": 4.21531190926276e-05,\n",
      "      \"loss\": 0.0075,\n",
      "      \"step\": 24920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5715365953558713,\n",
      "      \"grad_norm\": 0.009469859302043915,\n",
      "      \"learning_rate\": 4.214681789540013e-05,\n",
      "      \"loss\": 0.0696,\n",
      "      \"step\": 24940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5727968745077034,\n",
      "      \"grad_norm\": 0.2654537856578827,\n",
      "      \"learning_rate\": 4.214051669817266e-05,\n",
      "      \"loss\": 0.0205,\n",
      "      \"step\": 24960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5740571536595356,\n",
      "      \"grad_norm\": 0.059182438999414444,\n",
      "      \"learning_rate\": 4.213421550094518e-05,\n",
      "      \"loss\": 0.0244,\n",
      "      \"step\": 24980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5753174328113677,\n",
      "      \"grad_norm\": 0.02421192079782486,\n",
      "      \"learning_rate\": 4.212791430371771e-05,\n",
      "      \"loss\": 0.0532,\n",
      "      \"step\": 25000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5765777119631998,\n",
      "      \"grad_norm\": 7.13401460647583,\n",
      "      \"learning_rate\": 4.212161310649024e-05,\n",
      "      \"loss\": 0.0527,\n",
      "      \"step\": 25020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.577837991115032,\n",
      "      \"grad_norm\": 0.006600949913263321,\n",
      "      \"learning_rate\": 4.211531190926276e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 25040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5790982702668641,\n",
      "      \"grad_norm\": 0.002717000897973776,\n",
      "      \"learning_rate\": 4.210901071203529e-05,\n",
      "      \"loss\": 0.0028,\n",
      "      \"step\": 25060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5803585494186962,\n",
      "      \"grad_norm\": 0.0039096553809940815,\n",
      "      \"learning_rate\": 4.210270951480781e-05,\n",
      "      \"loss\": 0.1035,\n",
      "      \"step\": 25080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5816188285705284,\n",
      "      \"grad_norm\": 0.019751152023673058,\n",
      "      \"learning_rate\": 4.209640831758034e-05,\n",
      "      \"loss\": 0.0242,\n",
      "      \"step\": 25100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5828791077223605,\n",
      "      \"grad_norm\": 0.035438086837530136,\n",
      "      \"learning_rate\": 4.209010712035287e-05,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 25120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5841393868741926,\n",
      "      \"grad_norm\": 0.02041502110660076,\n",
      "      \"learning_rate\": 4.20838059231254e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 25140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5853996660260248,\n",
      "      \"grad_norm\": 0.035316359251737595,\n",
      "      \"learning_rate\": 4.207750472589792e-05,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 25160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.586659945177857,\n",
      "      \"grad_norm\": 15.455246925354004,\n",
      "      \"learning_rate\": 4.207120352867045e-05,\n",
      "      \"loss\": 0.0364,\n",
      "      \"step\": 25180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.587920224329689,\n",
      "      \"grad_norm\": 0.5134974122047424,\n",
      "      \"learning_rate\": 4.206490233144297e-05,\n",
      "      \"loss\": 0.0395,\n",
      "      \"step\": 25200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5891805034815212,\n",
      "      \"grad_norm\": 7.707915782928467,\n",
      "      \"learning_rate\": 4.20586011342155e-05,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 25220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5904407826333533,\n",
      "      \"grad_norm\": 0.06699516624212265,\n",
      "      \"learning_rate\": 4.205229993698803e-05,\n",
      "      \"loss\": 0.0345,\n",
      "      \"step\": 25240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5917010617851854,\n",
      "      \"grad_norm\": 0.015331760048866272,\n",
      "      \"learning_rate\": 4.204599873976055e-05,\n",
      "      \"loss\": 0.0354,\n",
      "      \"step\": 25260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5929613409370176,\n",
      "      \"grad_norm\": 0.043719466775655746,\n",
      "      \"learning_rate\": 4.203969754253309e-05,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 25280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5942216200888497,\n",
      "      \"grad_norm\": 0.004264509305357933,\n",
      "      \"learning_rate\": 4.203339634530561e-05,\n",
      "      \"loss\": 0.0188,\n",
      "      \"step\": 25300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5954818992406818,\n",
      "      \"grad_norm\": 0.02603111043572426,\n",
      "      \"learning_rate\": 4.202709514807814e-05,\n",
      "      \"loss\": 0.0147,\n",
      "      \"step\": 25320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.596742178392514,\n",
      "      \"grad_norm\": 0.02084115706384182,\n",
      "      \"learning_rate\": 4.202079395085066e-05,\n",
      "      \"loss\": 0.035,\n",
      "      \"step\": 25340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.598002457544346,\n",
      "      \"grad_norm\": 0.0016724510351195931,\n",
      "      \"learning_rate\": 4.201449275362319e-05,\n",
      "      \"loss\": 0.0405,\n",
      "      \"step\": 25360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5992627366961782,\n",
      "      \"grad_norm\": 0.004286257084459066,\n",
      "      \"learning_rate\": 4.200819155639571e-05,\n",
      "      \"loss\": 0.0172,\n",
      "      \"step\": 25380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6005230158480104,\n",
      "      \"grad_norm\": 0.003176339203491807,\n",
      "      \"learning_rate\": 4.200189035916824e-05,\n",
      "      \"loss\": 0.0528,\n",
      "      \"step\": 25400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6017832949998425,\n",
      "      \"grad_norm\": 0.016368111595511436,\n",
      "      \"learning_rate\": 4.199558916194077e-05,\n",
      "      \"loss\": 0.0304,\n",
      "      \"step\": 25420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6030435741516746,\n",
      "      \"grad_norm\": 0.0574004203081131,\n",
      "      \"learning_rate\": 4.19892879647133e-05,\n",
      "      \"loss\": 0.0217,\n",
      "      \"step\": 25440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6043038533035068,\n",
      "      \"grad_norm\": 0.1291651576757431,\n",
      "      \"learning_rate\": 4.198298676748583e-05,\n",
      "      \"loss\": 0.0601,\n",
      "      \"step\": 25460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.605564132455339,\n",
      "      \"grad_norm\": 0.12067878246307373,\n",
      "      \"learning_rate\": 4.197668557025835e-05,\n",
      "      \"loss\": 0.0604,\n",
      "      \"step\": 25480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.606824411607171,\n",
      "      \"grad_norm\": 0.1424192637205124,\n",
      "      \"learning_rate\": 4.197038437303088e-05,\n",
      "      \"loss\": 0.0623,\n",
      "      \"step\": 25500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6080846907590032,\n",
      "      \"grad_norm\": 28.890043258666992,\n",
      "      \"learning_rate\": 4.19640831758034e-05,\n",
      "      \"loss\": 0.059,\n",
      "      \"step\": 25520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6093449699108353,\n",
      "      \"grad_norm\": 0.14044466614723206,\n",
      "      \"learning_rate\": 4.195778197857593e-05,\n",
      "      \"loss\": 0.0062,\n",
      "      \"step\": 25540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6106052490626674,\n",
      "      \"grad_norm\": 0.018432486802339554,\n",
      "      \"learning_rate\": 4.1951480781348454e-05,\n",
      "      \"loss\": 0.0174,\n",
      "      \"step\": 25560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6118655282144996,\n",
      "      \"grad_norm\": 0.5275846719741821,\n",
      "      \"learning_rate\": 4.194517958412098e-05,\n",
      "      \"loss\": 0.038,\n",
      "      \"step\": 25580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6131258073663317,\n",
      "      \"grad_norm\": 0.010468807071447372,\n",
      "      \"learning_rate\": 4.193887838689351e-05,\n",
      "      \"loss\": 0.0546,\n",
      "      \"step\": 25600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6143860865181638,\n",
      "      \"grad_norm\": 0.0897686555981636,\n",
      "      \"learning_rate\": 4.193257718966604e-05,\n",
      "      \"loss\": 0.0389,\n",
      "      \"step\": 25620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.615646365669996,\n",
      "      \"grad_norm\": 0.009738096036016941,\n",
      "      \"learning_rate\": 4.1926275992438564e-05,\n",
      "      \"loss\": 0.0542,\n",
      "      \"step\": 25640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.616906644821828,\n",
      "      \"grad_norm\": 0.10784891992807388,\n",
      "      \"learning_rate\": 4.191997479521109e-05,\n",
      "      \"loss\": 0.0535,\n",
      "      \"step\": 25660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6181669239736602,\n",
      "      \"grad_norm\": 4.570141792297363,\n",
      "      \"learning_rate\": 4.191367359798362e-05,\n",
      "      \"loss\": 0.0537,\n",
      "      \"step\": 25680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6194272031254924,\n",
      "      \"grad_norm\": 0.3607409596443176,\n",
      "      \"learning_rate\": 4.1907372400756144e-05,\n",
      "      \"loss\": 0.0305,\n",
      "      \"step\": 25700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6206874822773245,\n",
      "      \"grad_norm\": 0.013025296851992607,\n",
      "      \"learning_rate\": 4.190107120352867e-05,\n",
      "      \"loss\": 0.0422,\n",
      "      \"step\": 25720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6219477614291566,\n",
      "      \"grad_norm\": 4.848323822021484,\n",
      "      \"learning_rate\": 4.1894770006301195e-05,\n",
      "      \"loss\": 0.0376,\n",
      "      \"step\": 25740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6232080405809888,\n",
      "      \"grad_norm\": 0.005705324001610279,\n",
      "      \"learning_rate\": 4.1888468809073724e-05,\n",
      "      \"loss\": 0.0374,\n",
      "      \"step\": 25760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.624468319732821,\n",
      "      \"grad_norm\": 4.548442363739014,\n",
      "      \"learning_rate\": 4.1882167611846253e-05,\n",
      "      \"loss\": 0.041,\n",
      "      \"step\": 25780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.625728598884653,\n",
      "      \"grad_norm\": 0.022708894684910774,\n",
      "      \"learning_rate\": 4.187586641461878e-05,\n",
      "      \"loss\": 0.0233,\n",
      "      \"step\": 25800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6269888780364852,\n",
      "      \"grad_norm\": 0.24633245170116425,\n",
      "      \"learning_rate\": 4.1869565217391305e-05,\n",
      "      \"loss\": 0.0298,\n",
      "      \"step\": 25820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6282491571883173,\n",
      "      \"grad_norm\": 0.09874968975782394,\n",
      "      \"learning_rate\": 4.1863264020163834e-05,\n",
      "      \"loss\": 0.0437,\n",
      "      \"step\": 25840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6295094363401494,\n",
      "      \"grad_norm\": 10.351831436157227,\n",
      "      \"learning_rate\": 4.1856962822936356e-05,\n",
      "      \"loss\": 0.0311,\n",
      "      \"step\": 25860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6307697154919816,\n",
      "      \"grad_norm\": 24.132863998413086,\n",
      "      \"learning_rate\": 4.1850661625708885e-05,\n",
      "      \"loss\": 0.0037,\n",
      "      \"step\": 25880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6320299946438137,\n",
      "      \"grad_norm\": 0.00825303141027689,\n",
      "      \"learning_rate\": 4.1844360428481414e-05,\n",
      "      \"loss\": 0.0373,\n",
      "      \"step\": 25900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6332902737956458,\n",
      "      \"grad_norm\": 0.8066233396530151,\n",
      "      \"learning_rate\": 4.183805923125394e-05,\n",
      "      \"loss\": 0.0262,\n",
      "      \"step\": 25920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.634550552947478,\n",
      "      \"grad_norm\": 1.5358449220657349,\n",
      "      \"learning_rate\": 4.183175803402647e-05,\n",
      "      \"loss\": 0.034,\n",
      "      \"step\": 25940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.63581083209931,\n",
      "      \"grad_norm\": 0.0012500971788540483,\n",
      "      \"learning_rate\": 4.1825456836798994e-05,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 25960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6370711112511422,\n",
      "      \"grad_norm\": 0.0006498941802419722,\n",
      "      \"learning_rate\": 4.1819155639571524e-05,\n",
      "      \"loss\": 0.0349,\n",
      "      \"step\": 25980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6383313904029744,\n",
      "      \"grad_norm\": 3.9388885498046875,\n",
      "      \"learning_rate\": 4.1812854442344046e-05,\n",
      "      \"loss\": 0.0439,\n",
      "      \"step\": 26000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6395916695548065,\n",
      "      \"grad_norm\": 0.0027196891605854034,\n",
      "      \"learning_rate\": 4.1806553245116575e-05,\n",
      "      \"loss\": 0.0325,\n",
      "      \"step\": 26020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6408519487066386,\n",
      "      \"grad_norm\": 6.094076633453369,\n",
      "      \"learning_rate\": 4.18002520478891e-05,\n",
      "      \"loss\": 0.0373,\n",
      "      \"step\": 26040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6421122278584708,\n",
      "      \"grad_norm\": 0.017052579671144485,\n",
      "      \"learning_rate\": 4.1793950850661626e-05,\n",
      "      \"loss\": 0.0429,\n",
      "      \"step\": 26060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.643372507010303,\n",
      "      \"grad_norm\": 0.0010568754514679313,\n",
      "      \"learning_rate\": 4.1787649653434155e-05,\n",
      "      \"loss\": 0.0124,\n",
      "      \"step\": 26080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.644632786162135,\n",
      "      \"grad_norm\": 0.18226562440395355,\n",
      "      \"learning_rate\": 4.1781348456206684e-05,\n",
      "      \"loss\": 0.0136,\n",
      "      \"step\": 26100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6458930653139672,\n",
      "      \"grad_norm\": 0.09762951731681824,\n",
      "      \"learning_rate\": 4.177504725897921e-05,\n",
      "      \"loss\": 0.0349,\n",
      "      \"step\": 26120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6471533444657993,\n",
      "      \"grad_norm\": 0.0003159023472107947,\n",
      "      \"learning_rate\": 4.1768746061751735e-05,\n",
      "      \"loss\": 0.0787,\n",
      "      \"step\": 26140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6484136236176314,\n",
      "      \"grad_norm\": 5.049532413482666,\n",
      "      \"learning_rate\": 4.1762444864524265e-05,\n",
      "      \"loss\": 0.0339,\n",
      "      \"step\": 26160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6496739027694636,\n",
      "      \"grad_norm\": 0.005397699773311615,\n",
      "      \"learning_rate\": 4.175614366729679e-05,\n",
      "      \"loss\": 0.0267,\n",
      "      \"step\": 26180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6509341819212957,\n",
      "      \"grad_norm\": 0.051941726356744766,\n",
      "      \"learning_rate\": 4.1749842470069316e-05,\n",
      "      \"loss\": 0.05,\n",
      "      \"step\": 26200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6521944610731278,\n",
      "      \"grad_norm\": 0.03837692737579346,\n",
      "      \"learning_rate\": 4.174354127284184e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 26220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.65345474022496,\n",
      "      \"grad_norm\": 0.012043509632349014,\n",
      "      \"learning_rate\": 4.173724007561437e-05,\n",
      "      \"loss\": 0.0342,\n",
      "      \"step\": 26240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.654715019376792,\n",
      "      \"grad_norm\": 0.008248850703239441,\n",
      "      \"learning_rate\": 4.1730938878386896e-05,\n",
      "      \"loss\": 0.0201,\n",
      "      \"step\": 26260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6559752985286242,\n",
      "      \"grad_norm\": 0.02730593830347061,\n",
      "      \"learning_rate\": 4.1724637681159425e-05,\n",
      "      \"loss\": 0.0222,\n",
      "      \"step\": 26280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6572355776804564,\n",
      "      \"grad_norm\": 0.008971674367785454,\n",
      "      \"learning_rate\": 4.171833648393195e-05,\n",
      "      \"loss\": 0.0263,\n",
      "      \"step\": 26300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6584958568322885,\n",
      "      \"grad_norm\": 0.020656628534197807,\n",
      "      \"learning_rate\": 4.1712035286704477e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 26320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6597561359841206,\n",
      "      \"grad_norm\": 0.003314437111839652,\n",
      "      \"learning_rate\": 4.1705734089477e-05,\n",
      "      \"loss\": 0.0281,\n",
      "      \"step\": 26340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6610164151359528,\n",
      "      \"grad_norm\": 0.004774265922605991,\n",
      "      \"learning_rate\": 4.169943289224953e-05,\n",
      "      \"loss\": 0.0422,\n",
      "      \"step\": 26360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.662276694287785,\n",
      "      \"grad_norm\": 0.0010029254481196404,\n",
      "      \"learning_rate\": 4.169313169502206e-05,\n",
      "      \"loss\": 0.0332,\n",
      "      \"step\": 26380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.663536973439617,\n",
      "      \"grad_norm\": 0.0030868262983858585,\n",
      "      \"learning_rate\": 4.168683049779458e-05,\n",
      "      \"loss\": 0.0218,\n",
      "      \"step\": 26400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.664797252591449,\n",
      "      \"grad_norm\": 4.0843915939331055,\n",
      "      \"learning_rate\": 4.1680529300567115e-05,\n",
      "      \"loss\": 0.0751,\n",
      "      \"step\": 26420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.666057531743281,\n",
      "      \"grad_norm\": 0.23440465331077576,\n",
      "      \"learning_rate\": 4.167422810333964e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 26440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6673178108951132,\n",
      "      \"grad_norm\": 0.007038609124720097,\n",
      "      \"learning_rate\": 4.1667926906112166e-05,\n",
      "      \"loss\": 0.0368,\n",
      "      \"step\": 26460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6685780900469454,\n",
      "      \"grad_norm\": 2.7241263389587402,\n",
      "      \"learning_rate\": 4.166162570888469e-05,\n",
      "      \"loss\": 0.0255,\n",
      "      \"step\": 26480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6698383691987775,\n",
      "      \"grad_norm\": 0.010302608832716942,\n",
      "      \"learning_rate\": 4.165532451165722e-05,\n",
      "      \"loss\": 0.041,\n",
      "      \"step\": 26500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6710986483506096,\n",
      "      \"grad_norm\": 0.1693652719259262,\n",
      "      \"learning_rate\": 4.164902331442974e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 26520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6723589275024417,\n",
      "      \"grad_norm\": 0.0008734301081858575,\n",
      "      \"learning_rate\": 4.164272211720227e-05,\n",
      "      \"loss\": 0.0474,\n",
      "      \"step\": 26540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6736192066542739,\n",
      "      \"grad_norm\": 0.105494424700737,\n",
      "      \"learning_rate\": 4.16364209199748e-05,\n",
      "      \"loss\": 0.0811,\n",
      "      \"step\": 26560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.674879485806106,\n",
      "      \"grad_norm\": 34.68035125732422,\n",
      "      \"learning_rate\": 4.163011972274733e-05,\n",
      "      \"loss\": 0.0457,\n",
      "      \"step\": 26580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6761397649579381,\n",
      "      \"grad_norm\": 0.12814590334892273,\n",
      "      \"learning_rate\": 4.1623818525519856e-05,\n",
      "      \"loss\": 0.0299,\n",
      "      \"step\": 26600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6774000441097703,\n",
      "      \"grad_norm\": 0.004091688897460699,\n",
      "      \"learning_rate\": 4.161751732829238e-05,\n",
      "      \"loss\": 0.0163,\n",
      "      \"step\": 26620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6786603232616024,\n",
      "      \"grad_norm\": 0.16686391830444336,\n",
      "      \"learning_rate\": 4.161121613106491e-05,\n",
      "      \"loss\": 0.0528,\n",
      "      \"step\": 26640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6799206024134345,\n",
      "      \"grad_norm\": 0.008884682320058346,\n",
      "      \"learning_rate\": 4.160491493383743e-05,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 26660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6811808815652667,\n",
      "      \"grad_norm\": 0.12251589447259903,\n",
      "      \"learning_rate\": 4.159861373660996e-05,\n",
      "      \"loss\": 0.0701,\n",
      "      \"step\": 26680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6824411607170988,\n",
      "      \"grad_norm\": 0.013260462321341038,\n",
      "      \"learning_rate\": 4.159231253938248e-05,\n",
      "      \"loss\": 0.0614,\n",
      "      \"step\": 26700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.683701439868931,\n",
      "      \"grad_norm\": 0.6394619345664978,\n",
      "      \"learning_rate\": 4.158601134215501e-05,\n",
      "      \"loss\": 0.0678,\n",
      "      \"step\": 26720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.684961719020763,\n",
      "      \"grad_norm\": 0.008845373056828976,\n",
      "      \"learning_rate\": 4.157971014492754e-05,\n",
      "      \"loss\": 0.033,\n",
      "      \"step\": 26740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6862219981725952,\n",
      "      \"grad_norm\": 0.010451245121657848,\n",
      "      \"learning_rate\": 4.157340894770007e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 26760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6874822773244273,\n",
      "      \"grad_norm\": 0.007242836523801088,\n",
      "      \"learning_rate\": 4.156710775047259e-05,\n",
      "      \"loss\": 0.0262,\n",
      "      \"step\": 26780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6887425564762595,\n",
      "      \"grad_norm\": 0.013880939222872257,\n",
      "      \"learning_rate\": 4.156080655324512e-05,\n",
      "      \"loss\": 0.0246,\n",
      "      \"step\": 26800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6900028356280916,\n",
      "      \"grad_norm\": 0.536506175994873,\n",
      "      \"learning_rate\": 4.155450535601765e-05,\n",
      "      \"loss\": 0.0497,\n",
      "      \"step\": 26820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6912631147799237,\n",
      "      \"grad_norm\": 4.505918502807617,\n",
      "      \"learning_rate\": 4.154820415879017e-05,\n",
      "      \"loss\": 0.0507,\n",
      "      \"step\": 26840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6925233939317559,\n",
      "      \"grad_norm\": 0.026007534936070442,\n",
      "      \"learning_rate\": 4.15419029615627e-05,\n",
      "      \"loss\": 0.0403,\n",
      "      \"step\": 26860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.693783673083588,\n",
      "      \"grad_norm\": 0.004388552624732256,\n",
      "      \"learning_rate\": 4.153560176433522e-05,\n",
      "      \"loss\": 0.019,\n",
      "      \"step\": 26880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6950439522354201,\n",
      "      \"grad_norm\": 0.016930950805544853,\n",
      "      \"learning_rate\": 4.152930056710775e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 26900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6963042313872523,\n",
      "      \"grad_norm\": 0.04760251194238663,\n",
      "      \"learning_rate\": 4.152299936988028e-05,\n",
      "      \"loss\": 0.04,\n",
      "      \"step\": 26920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6975645105390844,\n",
      "      \"grad_norm\": 0.006877664942294359,\n",
      "      \"learning_rate\": 4.151669817265281e-05,\n",
      "      \"loss\": 0.0487,\n",
      "      \"step\": 26940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6988247896909165,\n",
      "      \"grad_norm\": 0.2156272977590561,\n",
      "      \"learning_rate\": 4.151039697542533e-05,\n",
      "      \"loss\": 0.0073,\n",
      "      \"step\": 26960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7000850688427487,\n",
      "      \"grad_norm\": 0.011834677308797836,\n",
      "      \"learning_rate\": 4.150409577819786e-05,\n",
      "      \"loss\": 0.0395,\n",
      "      \"step\": 26980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7013453479945808,\n",
      "      \"grad_norm\": 0.29515567421913147,\n",
      "      \"learning_rate\": 4.149779458097038e-05,\n",
      "      \"loss\": 0.0585,\n",
      "      \"step\": 27000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.702605627146413,\n",
      "      \"grad_norm\": 8.24032211303711,\n",
      "      \"learning_rate\": 4.149149338374291e-05,\n",
      "      \"loss\": 0.0459,\n",
      "      \"step\": 27020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.703865906298245,\n",
      "      \"grad_norm\": 0.11097241938114166,\n",
      "      \"learning_rate\": 4.148519218651544e-05,\n",
      "      \"loss\": 0.0241,\n",
      "      \"step\": 27040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7051261854500772,\n",
      "      \"grad_norm\": 9.369555473327637,\n",
      "      \"learning_rate\": 4.147889098928796e-05,\n",
      "      \"loss\": 0.0344,\n",
      "      \"step\": 27060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7063864646019093,\n",
      "      \"grad_norm\": 0.0033312703017145395,\n",
      "      \"learning_rate\": 4.14725897920605e-05,\n",
      "      \"loss\": 0.0135,\n",
      "      \"step\": 27080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7076467437537415,\n",
      "      \"grad_norm\": 0.006791968364268541,\n",
      "      \"learning_rate\": 4.146628859483302e-05,\n",
      "      \"loss\": 0.036,\n",
      "      \"step\": 27100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7089070229055736,\n",
      "      \"grad_norm\": 0.07647954672574997,\n",
      "      \"learning_rate\": 4.145998739760555e-05,\n",
      "      \"loss\": 0.0285,\n",
      "      \"step\": 27120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7101673020574057,\n",
      "      \"grad_norm\": 0.29110655188560486,\n",
      "      \"learning_rate\": 4.145368620037807e-05,\n",
      "      \"loss\": 0.0243,\n",
      "      \"step\": 27140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7114275812092379,\n",
      "      \"grad_norm\": 0.0350763164460659,\n",
      "      \"learning_rate\": 4.14473850031506e-05,\n",
      "      \"loss\": 0.0209,\n",
      "      \"step\": 27160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.71268786036107,\n",
      "      \"grad_norm\": 0.006080380640923977,\n",
      "      \"learning_rate\": 4.1441083805923124e-05,\n",
      "      \"loss\": 0.0204,\n",
      "      \"step\": 27180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7139481395129021,\n",
      "      \"grad_norm\": 3.1280758380889893,\n",
      "      \"learning_rate\": 4.143478260869565e-05,\n",
      "      \"loss\": 0.0993,\n",
      "      \"step\": 27200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7152084186647343,\n",
      "      \"grad_norm\": 0.3452787697315216,\n",
      "      \"learning_rate\": 4.142848141146818e-05,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 27220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7164686978165664,\n",
      "      \"grad_norm\": 0.03802134096622467,\n",
      "      \"learning_rate\": 4.142218021424071e-05,\n",
      "      \"loss\": 0.0505,\n",
      "      \"step\": 27240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7177289769683985,\n",
      "      \"grad_norm\": 0.19137196242809296,\n",
      "      \"learning_rate\": 4.141587901701324e-05,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 27260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7189892561202307,\n",
      "      \"grad_norm\": 0.3154961168766022,\n",
      "      \"learning_rate\": 4.140957781978576e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 27280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7202495352720628,\n",
      "      \"grad_norm\": 0.0108602624386549,\n",
      "      \"learning_rate\": 4.140327662255829e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 27300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.721509814423895,\n",
      "      \"grad_norm\": 0.01558133214712143,\n",
      "      \"learning_rate\": 4.139697542533081e-05,\n",
      "      \"loss\": 0.0317,\n",
      "      \"step\": 27320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7227700935757269,\n",
      "      \"grad_norm\": 0.048481494188308716,\n",
      "      \"learning_rate\": 4.139067422810334e-05,\n",
      "      \"loss\": 0.0466,\n",
      "      \"step\": 27340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.724030372727559,\n",
      "      \"grad_norm\": 0.5321018099784851,\n",
      "      \"learning_rate\": 4.1384373030875865e-05,\n",
      "      \"loss\": 0.0365,\n",
      "      \"step\": 27360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7252906518793911,\n",
      "      \"grad_norm\": 0.009571162052452564,\n",
      "      \"learning_rate\": 4.1378071833648394e-05,\n",
      "      \"loss\": 0.0426,\n",
      "      \"step\": 27380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7265509310312233,\n",
      "      \"grad_norm\": 9.125941276550293,\n",
      "      \"learning_rate\": 4.137177063642092e-05,\n",
      "      \"loss\": 0.1097,\n",
      "      \"step\": 27400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7278112101830554,\n",
      "      \"grad_norm\": 0.5714760422706604,\n",
      "      \"learning_rate\": 4.136546943919345e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 27420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7290714893348875,\n",
      "      \"grad_norm\": 0.4254215955734253,\n",
      "      \"learning_rate\": 4.1359168241965974e-05,\n",
      "      \"loss\": 0.0513,\n",
      "      \"step\": 27440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7303317684867197,\n",
      "      \"grad_norm\": 0.20071253180503845,\n",
      "      \"learning_rate\": 4.13528670447385e-05,\n",
      "      \"loss\": 0.0346,\n",
      "      \"step\": 27460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7315920476385518,\n",
      "      \"grad_norm\": 0.003439955413341522,\n",
      "      \"learning_rate\": 4.134656584751103e-05,\n",
      "      \"loss\": 0.0516,\n",
      "      \"step\": 27480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.732852326790384,\n",
      "      \"grad_norm\": 0.047047220170497894,\n",
      "      \"learning_rate\": 4.1340264650283554e-05,\n",
      "      \"loss\": 0.0354,\n",
      "      \"step\": 27500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.734112605942216,\n",
      "      \"grad_norm\": 0.2764487862586975,\n",
      "      \"learning_rate\": 4.133396345305608e-05,\n",
      "      \"loss\": 0.0306,\n",
      "      \"step\": 27520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7353728850940482,\n",
      "      \"grad_norm\": 0.4521043598651886,\n",
      "      \"learning_rate\": 4.1327662255828606e-05,\n",
      "      \"loss\": 0.0297,\n",
      "      \"step\": 27540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7366331642458803,\n",
      "      \"grad_norm\": 0.13080701231956482,\n",
      "      \"learning_rate\": 4.1321361058601135e-05,\n",
      "      \"loss\": 0.0303,\n",
      "      \"step\": 27560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7378934433977125,\n",
      "      \"grad_norm\": 4.993782043457031,\n",
      "      \"learning_rate\": 4.1315059861373664e-05,\n",
      "      \"loss\": 0.0518,\n",
      "      \"step\": 27580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7391537225495446,\n",
      "      \"grad_norm\": 0.08404839783906937,\n",
      "      \"learning_rate\": 4.130875866414619e-05,\n",
      "      \"loss\": 0.0399,\n",
      "      \"step\": 27600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7404140017013767,\n",
      "      \"grad_norm\": 0.024184415116906166,\n",
      "      \"learning_rate\": 4.1302457466918715e-05,\n",
      "      \"loss\": 0.0553,\n",
      "      \"step\": 27620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7416742808532089,\n",
      "      \"grad_norm\": 0.11716935783624649,\n",
      "      \"learning_rate\": 4.1296156269691244e-05,\n",
      "      \"loss\": 0.0191,\n",
      "      \"step\": 27640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.742934560005041,\n",
      "      \"grad_norm\": 3.497126340866089,\n",
      "      \"learning_rate\": 4.1289855072463766e-05,\n",
      "      \"loss\": 0.0408,\n",
      "      \"step\": 27660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7441948391568731,\n",
      "      \"grad_norm\": 0.0018190242117270827,\n",
      "      \"learning_rate\": 4.1283553875236295e-05,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 27680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7454551183087053,\n",
      "      \"grad_norm\": 5.758531093597412,\n",
      "      \"learning_rate\": 4.127725267800882e-05,\n",
      "      \"loss\": 0.1197,\n",
      "      \"step\": 27700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7467153974605374,\n",
      "      \"grad_norm\": 0.5539929866790771,\n",
      "      \"learning_rate\": 4.1270951480781353e-05,\n",
      "      \"loss\": 0.063,\n",
      "      \"step\": 27720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7479756766123695,\n",
      "      \"grad_norm\": 0.05430680885910988,\n",
      "      \"learning_rate\": 4.126465028355388e-05,\n",
      "      \"loss\": 0.0315,\n",
      "      \"step\": 27740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7492359557642017,\n",
      "      \"grad_norm\": 0.521154522895813,\n",
      "      \"learning_rate\": 4.1258349086326405e-05,\n",
      "      \"loss\": 0.0411,\n",
      "      \"step\": 27760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7504962349160338,\n",
      "      \"grad_norm\": 7.999485969543457,\n",
      "      \"learning_rate\": 4.1252047889098934e-05,\n",
      "      \"loss\": 0.0265,\n",
      "      \"step\": 27780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.751756514067866,\n",
      "      \"grad_norm\": 0.04703344777226448,\n",
      "      \"learning_rate\": 4.1245746691871456e-05,\n",
      "      \"loss\": 0.0491,\n",
      "      \"step\": 27800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.753016793219698,\n",
      "      \"grad_norm\": 4.374411106109619,\n",
      "      \"learning_rate\": 4.1239445494643985e-05,\n",
      "      \"loss\": 0.0537,\n",
      "      \"step\": 27820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7542770723715302,\n",
      "      \"grad_norm\": 0.0417620949447155,\n",
      "      \"learning_rate\": 4.123314429741651e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 27840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7555373515233623,\n",
      "      \"grad_norm\": 0.03496541455388069,\n",
      "      \"learning_rate\": 4.1226843100189036e-05,\n",
      "      \"loss\": 0.0511,\n",
      "      \"step\": 27860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7567976306751945,\n",
      "      \"grad_norm\": 18.53247833251953,\n",
      "      \"learning_rate\": 4.1220541902961565e-05,\n",
      "      \"loss\": 0.0315,\n",
      "      \"step\": 27880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7580579098270266,\n",
      "      \"grad_norm\": 0.592815637588501,\n",
      "      \"learning_rate\": 4.1214240705734094e-05,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 27900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7593181889788587,\n",
      "      \"grad_norm\": 0.011954373680055141,\n",
      "      \"learning_rate\": 4.120793950850662e-05,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 27920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7605784681306909,\n",
      "      \"grad_norm\": 0.03767158091068268,\n",
      "      \"learning_rate\": 4.1201638311279146e-05,\n",
      "      \"loss\": 0.0339,\n",
      "      \"step\": 27940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.761838747282523,\n",
      "      \"grad_norm\": 3.1857588291168213,\n",
      "      \"learning_rate\": 4.1195337114051675e-05,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 27960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7630990264343551,\n",
      "      \"grad_norm\": 0.024963419884443283,\n",
      "      \"learning_rate\": 4.11890359168242e-05,\n",
      "      \"loss\": 0.0176,\n",
      "      \"step\": 27980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7643593055861873,\n",
      "      \"grad_norm\": 0.02090955525636673,\n",
      "      \"learning_rate\": 4.1182734719596726e-05,\n",
      "      \"loss\": 0.0335,\n",
      "      \"step\": 28000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7656195847380194,\n",
      "      \"grad_norm\": 0.07873004674911499,\n",
      "      \"learning_rate\": 4.117643352236925e-05,\n",
      "      \"loss\": 0.0437,\n",
      "      \"step\": 28020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7668798638898515,\n",
      "      \"grad_norm\": 0.008460338227450848,\n",
      "      \"learning_rate\": 4.117013232514178e-05,\n",
      "      \"loss\": 0.0377,\n",
      "      \"step\": 28040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7681401430416837,\n",
      "      \"grad_norm\": 0.0027693852316588163,\n",
      "      \"learning_rate\": 4.1163831127914306e-05,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 28060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7694004221935158,\n",
      "      \"grad_norm\": 0.0018479903228580952,\n",
      "      \"learning_rate\": 4.1157529930686835e-05,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 28080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.770660701345348,\n",
      "      \"grad_norm\": 0.10712843388319016,\n",
      "      \"learning_rate\": 4.115122873345936e-05,\n",
      "      \"loss\": 0.0438,\n",
      "      \"step\": 28100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.77192098049718,\n",
      "      \"grad_norm\": 0.01567145250737667,\n",
      "      \"learning_rate\": 4.114492753623189e-05,\n",
      "      \"loss\": 0.0165,\n",
      "      \"step\": 28120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7731812596490122,\n",
      "      \"grad_norm\": 0.07548859715461731,\n",
      "      \"learning_rate\": 4.113862633900441e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 28140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7744415388008443,\n",
      "      \"grad_norm\": 0.0008544385782442987,\n",
      "      \"learning_rate\": 4.113232514177694e-05,\n",
      "      \"loss\": 0.0405,\n",
      "      \"step\": 28160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7757018179526765,\n",
      "      \"grad_norm\": 4.63501501083374,\n",
      "      \"learning_rate\": 4.112633900441084e-05,\n",
      "      \"loss\": 0.0273,\n",
      "      \"step\": 28180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7769620971045086,\n",
      "      \"grad_norm\": 0.0607600063085556,\n",
      "      \"learning_rate\": 4.112003780718337e-05,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 28200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7782223762563407,\n",
      "      \"grad_norm\": 0.039636436849832535,\n",
      "      \"learning_rate\": 4.11137366099559e-05,\n",
      "      \"loss\": 0.0056,\n",
      "      \"step\": 28220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7794826554081729,\n",
      "      \"grad_norm\": 7.544936656951904,\n",
      "      \"learning_rate\": 4.110743541272842e-05,\n",
      "      \"loss\": 0.0801,\n",
      "      \"step\": 28240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.780742934560005,\n",
      "      \"grad_norm\": 0.4300505816936493,\n",
      "      \"learning_rate\": 4.110113421550095e-05,\n",
      "      \"loss\": 0.0301,\n",
      "      \"step\": 28260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7820032137118371,\n",
      "      \"grad_norm\": 0.0010266667231917381,\n",
      "      \"learning_rate\": 4.109483301827348e-05,\n",
      "      \"loss\": 0.044,\n",
      "      \"step\": 28280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7832634928636693,\n",
      "      \"grad_norm\": 0.3911454975605011,\n",
      "      \"learning_rate\": 4.1088531821046e-05,\n",
      "      \"loss\": 0.0516,\n",
      "      \"step\": 28300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7845237720155014,\n",
      "      \"grad_norm\": 0.06240764632821083,\n",
      "      \"learning_rate\": 4.108223062381853e-05,\n",
      "      \"loss\": 0.0248,\n",
      "      \"step\": 28320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7857840511673335,\n",
      "      \"grad_norm\": 0.010891571640968323,\n",
      "      \"learning_rate\": 4.107592942659105e-05,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 28340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7870443303191657,\n",
      "      \"grad_norm\": 44.297725677490234,\n",
      "      \"learning_rate\": 4.106962822936358e-05,\n",
      "      \"loss\": 0.0418,\n",
      "      \"step\": 28360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7883046094709978,\n",
      "      \"grad_norm\": 0.2012038379907608,\n",
      "      \"learning_rate\": 4.106332703213611e-05,\n",
      "      \"loss\": 0.0352,\n",
      "      \"step\": 28380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.78956488862283,\n",
      "      \"grad_norm\": 0.42744767665863037,\n",
      "      \"learning_rate\": 4.105702583490864e-05,\n",
      "      \"loss\": 0.0484,\n",
      "      \"step\": 28400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.790825167774662,\n",
      "      \"grad_norm\": 0.04290149360895157,\n",
      "      \"learning_rate\": 4.105072463768116e-05,\n",
      "      \"loss\": 0.0836,\n",
      "      \"step\": 28420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7920854469264942,\n",
      "      \"grad_norm\": 0.14378538727760315,\n",
      "      \"learning_rate\": 4.104442344045369e-05,\n",
      "      \"loss\": 0.0569,\n",
      "      \"step\": 28440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7933457260783263,\n",
      "      \"grad_norm\": 0.06388873606920242,\n",
      "      \"learning_rate\": 4.103812224322621e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 28460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7946060052301585,\n",
      "      \"grad_norm\": 2.7922489643096924,\n",
      "      \"learning_rate\": 4.103182104599874e-05,\n",
      "      \"loss\": 0.0415,\n",
      "      \"step\": 28480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7958662843819906,\n",
      "      \"grad_norm\": 0.26086923480033875,\n",
      "      \"learning_rate\": 4.102551984877127e-05,\n",
      "      \"loss\": 0.0208,\n",
      "      \"step\": 28500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7971265635338227,\n",
      "      \"grad_norm\": 0.00019146379781886935,\n",
      "      \"learning_rate\": 4.101921865154379e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 28520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7983868426856549,\n",
      "      \"grad_norm\": 0.2793452739715576,\n",
      "      \"learning_rate\": 4.101291745431632e-05,\n",
      "      \"loss\": 0.0382,\n",
      "      \"step\": 28540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.799647121837487,\n",
      "      \"grad_norm\": 0.531239926815033,\n",
      "      \"learning_rate\": 4.100661625708885e-05,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 28560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8009074009893191,\n",
      "      \"grad_norm\": 0.557936429977417,\n",
      "      \"learning_rate\": 4.100031505986138e-05,\n",
      "      \"loss\": 0.0586,\n",
      "      \"step\": 28580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8021676801411513,\n",
      "      \"grad_norm\": 0.01915748417377472,\n",
      "      \"learning_rate\": 4.09940138626339e-05,\n",
      "      \"loss\": 0.0607,\n",
      "      \"step\": 28600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8034279592929834,\n",
      "      \"grad_norm\": 0.012544477358460426,\n",
      "      \"learning_rate\": 4.098771266540643e-05,\n",
      "      \"loss\": 0.0171,\n",
      "      \"step\": 28620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8046882384448155,\n",
      "      \"grad_norm\": 0.038039952516555786,\n",
      "      \"learning_rate\": 4.098141146817895e-05,\n",
      "      \"loss\": 0.0262,\n",
      "      \"step\": 28640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8059485175966477,\n",
      "      \"grad_norm\": 0.009403522126376629,\n",
      "      \"learning_rate\": 4.097511027095148e-05,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 28660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8072087967484798,\n",
      "      \"grad_norm\": 3.0241026878356934,\n",
      "      \"learning_rate\": 4.0968809073724004e-05,\n",
      "      \"loss\": 0.0734,\n",
      "      \"step\": 28680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.808469075900312,\n",
      "      \"grad_norm\": 0.011567090637981892,\n",
      "      \"learning_rate\": 4.096250787649654e-05,\n",
      "      \"loss\": 0.0069,\n",
      "      \"step\": 28700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.809729355052144,\n",
      "      \"grad_norm\": 0.5578116178512573,\n",
      "      \"learning_rate\": 4.095620667926907e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 28720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8109896342039762,\n",
      "      \"grad_norm\": 0.020713096484541893,\n",
      "      \"learning_rate\": 4.094990548204159e-05,\n",
      "      \"loss\": 0.0596,\n",
      "      \"step\": 28740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8122499133558083,\n",
      "      \"grad_norm\": 0.252782940864563,\n",
      "      \"learning_rate\": 4.094360428481412e-05,\n",
      "      \"loss\": 0.041,\n",
      "      \"step\": 28760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8135101925076405,\n",
      "      \"grad_norm\": 0.4837597906589508,\n",
      "      \"learning_rate\": 4.093730308758664e-05,\n",
      "      \"loss\": 0.0322,\n",
      "      \"step\": 28780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8147704716594726,\n",
      "      \"grad_norm\": 0.0017236553831025958,\n",
      "      \"learning_rate\": 4.093100189035917e-05,\n",
      "      \"loss\": 0.006,\n",
      "      \"step\": 28800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8160307508113047,\n",
      "      \"grad_norm\": 0.16491439938545227,\n",
      "      \"learning_rate\": 4.0924700693131694e-05,\n",
      "      \"loss\": 0.0545,\n",
      "      \"step\": 28820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8172910299631369,\n",
      "      \"grad_norm\": 0.006012536119669676,\n",
      "      \"learning_rate\": 4.091839949590422e-05,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 28840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.818551309114969,\n",
      "      \"grad_norm\": 8.484601020812988,\n",
      "      \"learning_rate\": 4.091209829867675e-05,\n",
      "      \"loss\": 0.0742,\n",
      "      \"step\": 28860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8198115882668011,\n",
      "      \"grad_norm\": 0.007613230496644974,\n",
      "      \"learning_rate\": 4.090579710144928e-05,\n",
      "      \"loss\": 0.0507,\n",
      "      \"step\": 28880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8210718674186332,\n",
      "      \"grad_norm\": 7.059075832366943,\n",
      "      \"learning_rate\": 4.08994959042218e-05,\n",
      "      \"loss\": 0.019,\n",
      "      \"step\": 28900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8223321465704654,\n",
      "      \"grad_norm\": 5.262857913970947,\n",
      "      \"learning_rate\": 4.089319470699433e-05,\n",
      "      \"loss\": 0.0329,\n",
      "      \"step\": 28920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8235924257222975,\n",
      "      \"grad_norm\": 0.4035423994064331,\n",
      "      \"learning_rate\": 4.088689350976686e-05,\n",
      "      \"loss\": 0.0242,\n",
      "      \"step\": 28940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8248527048741296,\n",
      "      \"grad_norm\": 0.0052215033210814,\n",
      "      \"learning_rate\": 4.0880592312539383e-05,\n",
      "      \"loss\": 0.0429,\n",
      "      \"step\": 28960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8261129840259618,\n",
      "      \"grad_norm\": 4.07187032699585,\n",
      "      \"learning_rate\": 4.087429111531191e-05,\n",
      "      \"loss\": 0.0191,\n",
      "      \"step\": 28980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.827373263177794,\n",
      "      \"grad_norm\": 0.003787330351769924,\n",
      "      \"learning_rate\": 4.0867989918084435e-05,\n",
      "      \"loss\": 0.0393,\n",
      "      \"step\": 29000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.828633542329626,\n",
      "      \"grad_norm\": 0.09644276648759842,\n",
      "      \"learning_rate\": 4.0861688720856964e-05,\n",
      "      \"loss\": 0.1007,\n",
      "      \"step\": 29020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8298938214814582,\n",
      "      \"grad_norm\": 2.1264431476593018,\n",
      "      \"learning_rate\": 4.085538752362949e-05,\n",
      "      \"loss\": 0.0496,\n",
      "      \"step\": 29040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8311541006332903,\n",
      "      \"grad_norm\": 0.12640351057052612,\n",
      "      \"learning_rate\": 4.084908632640202e-05,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 29060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8324143797851224,\n",
      "      \"grad_norm\": 9.566868782043457,\n",
      "      \"learning_rate\": 4.0842785129174544e-05,\n",
      "      \"loss\": 0.0393,\n",
      "      \"step\": 29080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8336746589369546,\n",
      "      \"grad_norm\": 0.06825325638055801,\n",
      "      \"learning_rate\": 4.083648393194707e-05,\n",
      "      \"loss\": 0.026,\n",
      "      \"step\": 29100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8349349380887867,\n",
      "      \"grad_norm\": 0.07031095772981644,\n",
      "      \"learning_rate\": 4.0830182734719595e-05,\n",
      "      \"loss\": 0.0281,\n",
      "      \"step\": 29120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8361952172406188,\n",
      "      \"grad_norm\": 0.003431221004575491,\n",
      "      \"learning_rate\": 4.0823881537492124e-05,\n",
      "      \"loss\": 0.0334,\n",
      "      \"step\": 29140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.837455496392451,\n",
      "      \"grad_norm\": 0.0027109805960208178,\n",
      "      \"learning_rate\": 4.081758034026465e-05,\n",
      "      \"loss\": 0.0194,\n",
      "      \"step\": 29160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8387157755442831,\n",
      "      \"grad_norm\": 0.0004677876131609082,\n",
      "      \"learning_rate\": 4.0811279143037176e-05,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 29180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8399760546961152,\n",
      "      \"grad_norm\": 0.06611873209476471,\n",
      "      \"learning_rate\": 4.0804977945809705e-05,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 29200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8412363338479474,\n",
      "      \"grad_norm\": 0.007149500772356987,\n",
      "      \"learning_rate\": 4.0798676748582234e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 29220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8424966129997795,\n",
      "      \"grad_norm\": 11.228106498718262,\n",
      "      \"learning_rate\": 4.079237555135476e-05,\n",
      "      \"loss\": 0.0344,\n",
      "      \"step\": 29240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8437568921516116,\n",
      "      \"grad_norm\": 0.0015520305605605245,\n",
      "      \"learning_rate\": 4.0786074354127285e-05,\n",
      "      \"loss\": 0.047,\n",
      "      \"step\": 29260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8450171713034438,\n",
      "      \"grad_norm\": 0.029924428090453148,\n",
      "      \"learning_rate\": 4.0779773156899814e-05,\n",
      "      \"loss\": 0.0282,\n",
      "      \"step\": 29280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.846277450455276,\n",
      "      \"grad_norm\": 0.011442343704402447,\n",
      "      \"learning_rate\": 4.0773471959672336e-05,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 29300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.847537729607108,\n",
      "      \"grad_norm\": 0.0031324848532676697,\n",
      "      \"learning_rate\": 4.0767170762444865e-05,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 29320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8487980087589402,\n",
      "      \"grad_norm\": 4.1769304275512695,\n",
      "      \"learning_rate\": 4.0761184625078766e-05,\n",
      "      \"loss\": 0.0496,\n",
      "      \"step\": 29340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8500582879107723,\n",
      "      \"grad_norm\": 0.014107259921729565,\n",
      "      \"learning_rate\": 4.0754883427851295e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 29360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8513185670626044,\n",
      "      \"grad_norm\": 0.013123001903295517,\n",
      "      \"learning_rate\": 4.0748582230623824e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 29380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8525788462144366,\n",
      "      \"grad_norm\": 0.1093888133764267,\n",
      "      \"learning_rate\": 4.074228103339635e-05,\n",
      "      \"loss\": 0.0353,\n",
      "      \"step\": 29400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8538391253662687,\n",
      "      \"grad_norm\": 0.10860809683799744,\n",
      "      \"learning_rate\": 4.0735979836168876e-05,\n",
      "      \"loss\": 0.0623,\n",
      "      \"step\": 29420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8550994045181008,\n",
      "      \"grad_norm\": 0.013650093227624893,\n",
      "      \"learning_rate\": 4.07296786389414e-05,\n",
      "      \"loss\": 0.066,\n",
      "      \"step\": 29440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.856359683669933,\n",
      "      \"grad_norm\": 0.1700296700000763,\n",
      "      \"learning_rate\": 4.072337744171393e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 29460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8576199628217651,\n",
      "      \"grad_norm\": 0.01928645744919777,\n",
      "      \"learning_rate\": 4.071707624448645e-05,\n",
      "      \"loss\": 0.0411,\n",
      "      \"step\": 29480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8588802419735972,\n",
      "      \"grad_norm\": 0.06639521569013596,\n",
      "      \"learning_rate\": 4.071077504725898e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 29500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8601405211254294,\n",
      "      \"grad_norm\": 0.0033374750055372715,\n",
      "      \"learning_rate\": 4.070447385003151e-05,\n",
      "      \"loss\": 0.0559,\n",
      "      \"step\": 29520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8614008002772615,\n",
      "      \"grad_norm\": 0.0053720008581876755,\n",
      "      \"learning_rate\": 4.0698172652804036e-05,\n",
      "      \"loss\": 0.0309,\n",
      "      \"step\": 29540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8626610794290936,\n",
      "      \"grad_norm\": 0.07933226972818375,\n",
      "      \"learning_rate\": 4.0691871455576565e-05,\n",
      "      \"loss\": 0.0446,\n",
      "      \"step\": 29560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8639213585809258,\n",
      "      \"grad_norm\": 0.004626046400517225,\n",
      "      \"learning_rate\": 4.068557025834909e-05,\n",
      "      \"loss\": 0.0278,\n",
      "      \"step\": 29580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.865181637732758,\n",
      "      \"grad_norm\": 0.018307581543922424,\n",
      "      \"learning_rate\": 4.067926906112162e-05,\n",
      "      \"loss\": 0.0261,\n",
      "      \"step\": 29600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.86644191688459,\n",
      "      \"grad_norm\": 0.010256716050207615,\n",
      "      \"learning_rate\": 4.067296786389414e-05,\n",
      "      \"loss\": 0.0429,\n",
      "      \"step\": 29620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8677021960364222,\n",
      "      \"grad_norm\": 0.11827708780765533,\n",
      "      \"learning_rate\": 4.066666666666667e-05,\n",
      "      \"loss\": 0.0346,\n",
      "      \"step\": 29640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8689624751882543,\n",
      "      \"grad_norm\": 0.0799805298447609,\n",
      "      \"learning_rate\": 4.066036546943919e-05,\n",
      "      \"loss\": 0.0451,\n",
      "      \"step\": 29660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8702227543400864,\n",
      "      \"grad_norm\": 0.06832703948020935,\n",
      "      \"learning_rate\": 4.065406427221172e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 29680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8714830334919186,\n",
      "      \"grad_norm\": 0.009321955032646656,\n",
      "      \"learning_rate\": 4.064776307498425e-05,\n",
      "      \"loss\": 0.0302,\n",
      "      \"step\": 29700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8727433126437507,\n",
      "      \"grad_norm\": 0.08032972365617752,\n",
      "      \"learning_rate\": 4.064146187775678e-05,\n",
      "      \"loss\": 0.0754,\n",
      "      \"step\": 29720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8740035917955828,\n",
      "      \"grad_norm\": 0.054980091750621796,\n",
      "      \"learning_rate\": 4.0635160680529306e-05,\n",
      "      \"loss\": 0.0639,\n",
      "      \"step\": 29740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.875263870947415,\n",
      "      \"grad_norm\": 11.737914085388184,\n",
      "      \"learning_rate\": 4.062885948330183e-05,\n",
      "      \"loss\": 0.0505,\n",
      "      \"step\": 29760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8765241500992471,\n",
      "      \"grad_norm\": 0.3207036852836609,\n",
      "      \"learning_rate\": 4.062255828607436e-05,\n",
      "      \"loss\": 0.028,\n",
      "      \"step\": 29780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8777844292510792,\n",
      "      \"grad_norm\": 0.08461952954530716,\n",
      "      \"learning_rate\": 4.061625708884688e-05,\n",
      "      \"loss\": 0.0376,\n",
      "      \"step\": 29800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8790447084029114,\n",
      "      \"grad_norm\": 0.0631355345249176,\n",
      "      \"learning_rate\": 4.060995589161941e-05,\n",
      "      \"loss\": 0.0497,\n",
      "      \"step\": 29820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8803049875547435,\n",
      "      \"grad_norm\": 8.444725036621094,\n",
      "      \"learning_rate\": 4.060365469439194e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 29840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8815652667065756,\n",
      "      \"grad_norm\": 0.4329465627670288,\n",
      "      \"learning_rate\": 4.059735349716447e-05,\n",
      "      \"loss\": 0.0161,\n",
      "      \"step\": 29860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8828255458584078,\n",
      "      \"grad_norm\": 0.9734792113304138,\n",
      "      \"learning_rate\": 4.059105229993699e-05,\n",
      "      \"loss\": 0.0625,\n",
      "      \"step\": 29880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.88408582501024,\n",
      "      \"grad_norm\": 2.202857494354248,\n",
      "      \"learning_rate\": 4.058475110270952e-05,\n",
      "      \"loss\": 0.0319,\n",
      "      \"step\": 29900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.885346104162072,\n",
      "      \"grad_norm\": 0.6508840322494507,\n",
      "      \"learning_rate\": 4.057844990548204e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 29920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8866063833139042,\n",
      "      \"grad_norm\": 2.1361026763916016,\n",
      "      \"learning_rate\": 4.057214870825457e-05,\n",
      "      \"loss\": 0.0145,\n",
      "      \"step\": 29940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8878666624657363,\n",
      "      \"grad_norm\": 7.485363483428955,\n",
      "      \"learning_rate\": 4.05658475110271e-05,\n",
      "      \"loss\": 0.0228,\n",
      "      \"step\": 29960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8891269416175684,\n",
      "      \"grad_norm\": 0.0005065388977527618,\n",
      "      \"learning_rate\": 4.055954631379962e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 29980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8903872207694006,\n",
      "      \"grad_norm\": 0.006009466014802456,\n",
      "      \"learning_rate\": 4.055324511657215e-05,\n",
      "      \"loss\": 0.0297,\n",
      "      \"step\": 30000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8916474999212327,\n",
      "      \"grad_norm\": 0.012219826690852642,\n",
      "      \"learning_rate\": 4.054694391934468e-05,\n",
      "      \"loss\": 0.0583,\n",
      "      \"step\": 30020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8929077790730648,\n",
      "      \"grad_norm\": 0.0007286056643351912,\n",
      "      \"learning_rate\": 4.054064272211721e-05,\n",
      "      \"loss\": 0.0215,\n",
      "      \"step\": 30040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.894168058224897,\n",
      "      \"grad_norm\": 0.036479052156209946,\n",
      "      \"learning_rate\": 4.053434152488973e-05,\n",
      "      \"loss\": 0.0432,\n",
      "      \"step\": 30060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.895428337376729,\n",
      "      \"grad_norm\": 0.0068790544755756855,\n",
      "      \"learning_rate\": 4.052804032766226e-05,\n",
      "      \"loss\": 0.0352,\n",
      "      \"step\": 30080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.896688616528561,\n",
      "      \"grad_norm\": 0.006900071166455746,\n",
      "      \"learning_rate\": 4.052173913043478e-05,\n",
      "      \"loss\": 0.0289,\n",
      "      \"step\": 30100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8979488956803932,\n",
      "      \"grad_norm\": 4.978405475616455,\n",
      "      \"learning_rate\": 4.051543793320731e-05,\n",
      "      \"loss\": 0.0342,\n",
      "      \"step\": 30120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8992091748322253,\n",
      "      \"grad_norm\": 0.1390538215637207,\n",
      "      \"learning_rate\": 4.050913673597983e-05,\n",
      "      \"loss\": 0.037,\n",
      "      \"step\": 30140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9004694539840574,\n",
      "      \"grad_norm\": 7.8936285972595215,\n",
      "      \"learning_rate\": 4.050283553875236e-05,\n",
      "      \"loss\": 0.0438,\n",
      "      \"step\": 30160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9017297331358896,\n",
      "      \"grad_norm\": 0.022173535078763962,\n",
      "      \"learning_rate\": 4.049653434152489e-05,\n",
      "      \"loss\": 0.0325,\n",
      "      \"step\": 30180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9029900122877217,\n",
      "      \"grad_norm\": 3.9464943408966064,\n",
      "      \"learning_rate\": 4.049023314429742e-05,\n",
      "      \"loss\": 0.0584,\n",
      "      \"step\": 30200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9042502914395538,\n",
      "      \"grad_norm\": 0.008177787996828556,\n",
      "      \"learning_rate\": 4.048393194706995e-05,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 30220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.905510570591386,\n",
      "      \"grad_norm\": 11.26997184753418,\n",
      "      \"learning_rate\": 4.047763074984247e-05,\n",
      "      \"loss\": 0.0255,\n",
      "      \"step\": 30240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.906770849743218,\n",
      "      \"grad_norm\": 0.08804873377084732,\n",
      "      \"learning_rate\": 4.0471329552615e-05,\n",
      "      \"loss\": 0.0475,\n",
      "      \"step\": 30260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9080311288950502,\n",
      "      \"grad_norm\": 0.011397885158658028,\n",
      "      \"learning_rate\": 4.046502835538752e-05,\n",
      "      \"loss\": 0.0039,\n",
      "      \"step\": 30280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9092914080468824,\n",
      "      \"grad_norm\": 0.15167251229286194,\n",
      "      \"learning_rate\": 4.045872715816005e-05,\n",
      "      \"loss\": 0.0278,\n",
      "      \"step\": 30300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9105516871987145,\n",
      "      \"grad_norm\": 0.03084094263613224,\n",
      "      \"learning_rate\": 4.0452425960932574e-05,\n",
      "      \"loss\": 0.0252,\n",
      "      \"step\": 30320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9118119663505466,\n",
      "      \"grad_norm\": 0.1645214557647705,\n",
      "      \"learning_rate\": 4.044612476370511e-05,\n",
      "      \"loss\": 0.0387,\n",
      "      \"step\": 30340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9130722455023788,\n",
      "      \"grad_norm\": 0.03718998283147812,\n",
      "      \"learning_rate\": 4.043982356647763e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 30360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9143325246542109,\n",
      "      \"grad_norm\": 4.755482196807861,\n",
      "      \"learning_rate\": 4.043352236925016e-05,\n",
      "      \"loss\": 0.032,\n",
      "      \"step\": 30380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.915592803806043,\n",
      "      \"grad_norm\": 0.011580966413021088,\n",
      "      \"learning_rate\": 4.042722117202269e-05,\n",
      "      \"loss\": 0.0401,\n",
      "      \"step\": 30400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9168530829578752,\n",
      "      \"grad_norm\": 0.05291130393743515,\n",
      "      \"learning_rate\": 4.042091997479521e-05,\n",
      "      \"loss\": 0.0317,\n",
      "      \"step\": 30420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9181133621097073,\n",
      "      \"grad_norm\": 0.008177048526704311,\n",
      "      \"learning_rate\": 4.041461877756774e-05,\n",
      "      \"loss\": 0.0334,\n",
      "      \"step\": 30440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9193736412615394,\n",
      "      \"grad_norm\": 0.07384705543518066,\n",
      "      \"learning_rate\": 4.0408317580340264e-05,\n",
      "      \"loss\": 0.0388,\n",
      "      \"step\": 30460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9206339204133716,\n",
      "      \"grad_norm\": 0.023102648556232452,\n",
      "      \"learning_rate\": 4.040201638311279e-05,\n",
      "      \"loss\": 0.0266,\n",
      "      \"step\": 30480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9218941995652037,\n",
      "      \"grad_norm\": 0.7264997363090515,\n",
      "      \"learning_rate\": 4.039571518588532e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 30500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9231544787170358,\n",
      "      \"grad_norm\": 0.0053491112776100636,\n",
      "      \"learning_rate\": 4.038941398865785e-05,\n",
      "      \"loss\": 0.0372,\n",
      "      \"step\": 30520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.924414757868868,\n",
      "      \"grad_norm\": 0.12677648663520813,\n",
      "      \"learning_rate\": 4.038311279143037e-05,\n",
      "      \"loss\": 0.0696,\n",
      "      \"step\": 30540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9256750370207,\n",
      "      \"grad_norm\": 1.765691876411438,\n",
      "      \"learning_rate\": 4.03768115942029e-05,\n",
      "      \"loss\": 0.023,\n",
      "      \"step\": 30560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9269353161725322,\n",
      "      \"grad_norm\": 0.04448646679520607,\n",
      "      \"learning_rate\": 4.0370510396975425e-05,\n",
      "      \"loss\": 0.0264,\n",
      "      \"step\": 30580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9281955953243644,\n",
      "      \"grad_norm\": 0.011950606480240822,\n",
      "      \"learning_rate\": 4.0364209199747954e-05,\n",
      "      \"loss\": 0.0359,\n",
      "      \"step\": 30600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9294558744761965,\n",
      "      \"grad_norm\": 0.19084173440933228,\n",
      "      \"learning_rate\": 4.0357908002520476e-05,\n",
      "      \"loss\": 0.0272,\n",
      "      \"step\": 30620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9307161536280286,\n",
      "      \"grad_norm\": 0.028789864853024483,\n",
      "      \"learning_rate\": 4.0351606805293005e-05,\n",
      "      \"loss\": 0.0336,\n",
      "      \"step\": 30640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9319764327798608,\n",
      "      \"grad_norm\": 0.03846509009599686,\n",
      "      \"learning_rate\": 4.0345305608065534e-05,\n",
      "      \"loss\": 0.0432,\n",
      "      \"step\": 30660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9332367119316929,\n",
      "      \"grad_norm\": 3.639738082885742,\n",
      "      \"learning_rate\": 4.033900441083806e-05,\n",
      "      \"loss\": 0.0377,\n",
      "      \"step\": 30680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.934496991083525,\n",
      "      \"grad_norm\": 0.013892168179154396,\n",
      "      \"learning_rate\": 4.033270321361059e-05,\n",
      "      \"loss\": 0.0464,\n",
      "      \"step\": 30700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9357572702353572,\n",
      "      \"grad_norm\": 0.1557592898607254,\n",
      "      \"learning_rate\": 4.0326402016383114e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 30720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9370175493871893,\n",
      "      \"grad_norm\": 0.01599564403295517,\n",
      "      \"learning_rate\": 4.032010081915564e-05,\n",
      "      \"loss\": 0.0469,\n",
      "      \"step\": 30740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9382778285390214,\n",
      "      \"grad_norm\": 0.016732530668377876,\n",
      "      \"learning_rate\": 4.0313799621928166e-05,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 30760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9395381076908536,\n",
      "      \"grad_norm\": 0.019630152732133865,\n",
      "      \"learning_rate\": 4.0307498424700695e-05,\n",
      "      \"loss\": 0.0231,\n",
      "      \"step\": 30780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9407983868426857,\n",
      "      \"grad_norm\": 0.02566969022154808,\n",
      "      \"learning_rate\": 4.030119722747322e-05,\n",
      "      \"loss\": 0.0191,\n",
      "      \"step\": 30800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9420586659945178,\n",
      "      \"grad_norm\": 9.799111366271973,\n",
      "      \"learning_rate\": 4.0294896030245746e-05,\n",
      "      \"loss\": 0.0349,\n",
      "      \"step\": 30820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.94331894514635,\n",
      "      \"grad_norm\": 6.14193058013916,\n",
      "      \"learning_rate\": 4.0288594833018275e-05,\n",
      "      \"loss\": 0.0264,\n",
      "      \"step\": 30840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.944579224298182,\n",
      "      \"grad_norm\": 13.929350852966309,\n",
      "      \"learning_rate\": 4.0282293635790804e-05,\n",
      "      \"loss\": 0.0857,\n",
      "      \"step\": 30860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9458395034500142,\n",
      "      \"grad_norm\": 0.7691524028778076,\n",
      "      \"learning_rate\": 4.027599243856333e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 30880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9470997826018464,\n",
      "      \"grad_norm\": 0.014656724408268929,\n",
      "      \"learning_rate\": 4.0269691241335855e-05,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 30900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9483600617536785,\n",
      "      \"grad_norm\": 6.321503639221191,\n",
      "      \"learning_rate\": 4.0263390044108384e-05,\n",
      "      \"loss\": 0.0316,\n",
      "      \"step\": 30920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9496203409055106,\n",
      "      \"grad_norm\": 4.800273895263672,\n",
      "      \"learning_rate\": 4.0257088846880907e-05,\n",
      "      \"loss\": 0.0222,\n",
      "      \"step\": 30940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9508806200573428,\n",
      "      \"grad_norm\": 0.008478125557303429,\n",
      "      \"learning_rate\": 4.0250787649653436e-05,\n",
      "      \"loss\": 0.0315,\n",
      "      \"step\": 30960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9521408992091749,\n",
      "      \"grad_norm\": 0.03111526183784008,\n",
      "      \"learning_rate\": 4.0244486452425965e-05,\n",
      "      \"loss\": 0.0327,\n",
      "      \"step\": 30980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.953401178361007,\n",
      "      \"grad_norm\": 20.079530715942383,\n",
      "      \"learning_rate\": 4.0238185255198494e-05,\n",
      "      \"loss\": 0.0574,\n",
      "      \"step\": 31000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.954661457512839,\n",
      "      \"grad_norm\": 0.02365957759320736,\n",
      "      \"learning_rate\": 4.0231884057971016e-05,\n",
      "      \"loss\": 0.0019,\n",
      "      \"step\": 31020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.955921736664671,\n",
      "      \"grad_norm\": 0.03440722078084946,\n",
      "      \"learning_rate\": 4.0225582860743545e-05,\n",
      "      \"loss\": 0.0248,\n",
      "      \"step\": 31040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9571820158165032,\n",
      "      \"grad_norm\": 0.9218676090240479,\n",
      "      \"learning_rate\": 4.021928166351607e-05,\n",
      "      \"loss\": 0.0019,\n",
      "      \"step\": 31060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9584422949683353,\n",
      "      \"grad_norm\": 0.1416459083557129,\n",
      "      \"learning_rate\": 4.0212980466288596e-05,\n",
      "      \"loss\": 0.0285,\n",
      "      \"step\": 31080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9597025741201675,\n",
      "      \"grad_norm\": 0.0061954231932759285,\n",
      "      \"learning_rate\": 4.0206679269061125e-05,\n",
      "      \"loss\": 0.082,\n",
      "      \"step\": 31100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9609628532719996,\n",
      "      \"grad_norm\": 0.18120859563350677,\n",
      "      \"learning_rate\": 4.020037807183365e-05,\n",
      "      \"loss\": 0.0021,\n",
      "      \"step\": 31120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9622231324238317,\n",
      "      \"grad_norm\": 0.010651302523911,\n",
      "      \"learning_rate\": 4.019407687460618e-05,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 31140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9634834115756639,\n",
      "      \"grad_norm\": 0.032326169312000275,\n",
      "      \"learning_rate\": 4.0187775677378706e-05,\n",
      "      \"loss\": 0.0297,\n",
      "      \"step\": 31160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.964743690727496,\n",
      "      \"grad_norm\": 0.005936654284596443,\n",
      "      \"learning_rate\": 4.0181474480151235e-05,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 31180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9660039698793281,\n",
      "      \"grad_norm\": 0.033962592482566833,\n",
      "      \"learning_rate\": 4.017517328292376e-05,\n",
      "      \"loss\": 0.0146,\n",
      "      \"step\": 31200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9672642490311603,\n",
      "      \"grad_norm\": 0.012286234647035599,\n",
      "      \"learning_rate\": 4.0168872085696286e-05,\n",
      "      \"loss\": 0.0306,\n",
      "      \"step\": 31220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9685245281829924,\n",
      "      \"grad_norm\": 0.0026072298642247915,\n",
      "      \"learning_rate\": 4.016257088846881e-05,\n",
      "      \"loss\": 0.0744,\n",
      "      \"step\": 31240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9697848073348245,\n",
      "      \"grad_norm\": 0.009904772974550724,\n",
      "      \"learning_rate\": 4.015626969124134e-05,\n",
      "      \"loss\": 0.0219,\n",
      "      \"step\": 31260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9710450864866567,\n",
      "      \"grad_norm\": 0.07810986787080765,\n",
      "      \"learning_rate\": 4.014996849401386e-05,\n",
      "      \"loss\": 0.0402,\n",
      "      \"step\": 31280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9723053656384888,\n",
      "      \"grad_norm\": 0.056363120675086975,\n",
      "      \"learning_rate\": 4.014366729678639e-05,\n",
      "      \"loss\": 0.0302,\n",
      "      \"step\": 31300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.973565644790321,\n",
      "      \"grad_norm\": 35.521366119384766,\n",
      "      \"learning_rate\": 4.013736609955892e-05,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 31320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.974825923942153,\n",
      "      \"grad_norm\": 0.07958953082561493,\n",
      "      \"learning_rate\": 4.013106490233145e-05,\n",
      "      \"loss\": 0.0403,\n",
      "      \"step\": 31340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9760862030939852,\n",
      "      \"grad_norm\": 0.029601234942674637,\n",
      "      \"learning_rate\": 4.0124763705103976e-05,\n",
      "      \"loss\": 0.0567,\n",
      "      \"step\": 31360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9773464822458173,\n",
      "      \"grad_norm\": 0.020034635439515114,\n",
      "      \"learning_rate\": 4.01184625078765e-05,\n",
      "      \"loss\": 0.0024,\n",
      "      \"step\": 31380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9786067613976495,\n",
      "      \"grad_norm\": 0.008480981923639774,\n",
      "      \"learning_rate\": 4.011216131064903e-05,\n",
      "      \"loss\": 0.0716,\n",
      "      \"step\": 31400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9798670405494816,\n",
      "      \"grad_norm\": 0.04009925574064255,\n",
      "      \"learning_rate\": 4.010586011342155e-05,\n",
      "      \"loss\": 0.0105,\n",
      "      \"step\": 31420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9811273197013137,\n",
      "      \"grad_norm\": 0.0333896167576313,\n",
      "      \"learning_rate\": 4.009955891619408e-05,\n",
      "      \"loss\": 0.0387,\n",
      "      \"step\": 31440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9823875988531459,\n",
      "      \"grad_norm\": 0.02453148365020752,\n",
      "      \"learning_rate\": 4.00932577189666e-05,\n",
      "      \"loss\": 0.0375,\n",
      "      \"step\": 31460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.983647878004978,\n",
      "      \"grad_norm\": 0.014922508038580418,\n",
      "      \"learning_rate\": 4.008695652173913e-05,\n",
      "      \"loss\": 0.0077,\n",
      "      \"step\": 31480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9849081571568101,\n",
      "      \"grad_norm\": 3.696202039718628,\n",
      "      \"learning_rate\": 4.008065532451166e-05,\n",
      "      \"loss\": 0.0552,\n",
      "      \"step\": 31500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9861684363086423,\n",
      "      \"grad_norm\": 0.03729525953531265,\n",
      "      \"learning_rate\": 4.007435412728419e-05,\n",
      "      \"loss\": 0.0217,\n",
      "      \"step\": 31520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9874287154604744,\n",
      "      \"grad_norm\": 0.024127988144755363,\n",
      "      \"learning_rate\": 4.006805293005672e-05,\n",
      "      \"loss\": 0.0276,\n",
      "      \"step\": 31540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9886889946123065,\n",
      "      \"grad_norm\": 6.252056121826172,\n",
      "      \"learning_rate\": 4.006175173282924e-05,\n",
      "      \"loss\": 0.0887,\n",
      "      \"step\": 31560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9899492737641387,\n",
      "      \"grad_norm\": 0.11348844319581985,\n",
      "      \"learning_rate\": 4.005545053560177e-05,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 31580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9912095529159708,\n",
      "      \"grad_norm\": 0.004556872416287661,\n",
      "      \"learning_rate\": 4.004914933837429e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 31600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.992469832067803,\n",
      "      \"grad_norm\": 0.007935957051813602,\n",
      "      \"learning_rate\": 4.004284814114682e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 31620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.993730111219635,\n",
      "      \"grad_norm\": 0.012451867572963238,\n",
      "      \"learning_rate\": 4.003654694391935e-05,\n",
      "      \"loss\": 0.0357,\n",
      "      \"step\": 31640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9949903903714672,\n",
      "      \"grad_norm\": 0.010885301977396011,\n",
      "      \"learning_rate\": 4.003024574669188e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 31660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9962506695232993,\n",
      "      \"grad_norm\": 0.005967572797089815,\n",
      "      \"learning_rate\": 4.00239445494644e-05,\n",
      "      \"loss\": 0.0413,\n",
      "      \"step\": 31680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9975109486751315,\n",
      "      \"grad_norm\": 0.12113110721111298,\n",
      "      \"learning_rate\": 4.001764335223693e-05,\n",
      "      \"loss\": 0.0351,\n",
      "      \"step\": 31700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9987712278269636,\n",
      "      \"grad_norm\": 5.467923164367676,\n",
      "      \"learning_rate\": 4.001134215500945e-05,\n",
      "      \"loss\": 0.0384,\n",
      "      \"step\": 31720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0,\n",
      "      \"grad_norm\": 0.0004398809396661818,\n",
      "      \"learning_rate\": 4.000504095778198e-05,\n",
      "      \"loss\": 0.041,\n",
      "      \"step\": 31740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0,\n",
      "      \"eval_accuracy\": 0.9988343519626992,\n",
      "      \"eval_f1\": 0.9988344621200189,\n",
      "      \"eval_loss\": 0.004508224315941334,\n",
      "      \"eval_precision\": 0.9987400781151569,\n",
      "      \"eval_recall\": 0.9989288639657237,\n",
      "      \"eval_runtime\": 535.5623,\n",
      "      \"eval_samples_per_second\": 59.269,\n",
      "      \"eval_steps_per_second\": 7.409,\n",
      "      \"step\": 31740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.001260279151832,\n",
      "      \"grad_norm\": 0.008096410892903805,\n",
      "      \"learning_rate\": 3.999873976055451e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 31760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0025205583036643,\n",
      "      \"grad_norm\": 7.735353469848633,\n",
      "      \"learning_rate\": 3.999243856332703e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 31780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0037808374554964,\n",
      "      \"grad_norm\": 0.26505666971206665,\n",
      "      \"learning_rate\": 3.998613736609956e-05,\n",
      "      \"loss\": 0.0376,\n",
      "      \"step\": 31800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0050411166073285,\n",
      "      \"grad_norm\": 0.0027724981773644686,\n",
      "      \"learning_rate\": 3.997983616887209e-05,\n",
      "      \"loss\": 0.0262,\n",
      "      \"step\": 31820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0063013957591607,\n",
      "      \"grad_norm\": 0.006261945236474276,\n",
      "      \"learning_rate\": 3.997353497164462e-05,\n",
      "      \"loss\": 0.0321,\n",
      "      \"step\": 31840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.007561674910993,\n",
      "      \"grad_norm\": 0.0009334099595434964,\n",
      "      \"learning_rate\": 3.996723377441714e-05,\n",
      "      \"loss\": 0.0564,\n",
      "      \"step\": 31860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.008821954062825,\n",
      "      \"grad_norm\": 0.013665297068655491,\n",
      "      \"learning_rate\": 3.996093257718967e-05,\n",
      "      \"loss\": 0.0564,\n",
      "      \"step\": 31880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.010082233214657,\n",
      "      \"grad_norm\": 0.13860857486724854,\n",
      "      \"learning_rate\": 3.995463137996219e-05,\n",
      "      \"loss\": 0.0404,\n",
      "      \"step\": 31900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.011342512366489,\n",
      "      \"grad_norm\": 0.11206196248531342,\n",
      "      \"learning_rate\": 3.994833018273472e-05,\n",
      "      \"loss\": 0.0247,\n",
      "      \"step\": 31920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0126027915183213,\n",
      "      \"grad_norm\": 0.2731085419654846,\n",
      "      \"learning_rate\": 3.994202898550724e-05,\n",
      "      \"loss\": 0.0387,\n",
      "      \"step\": 31940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0138630706701535,\n",
      "      \"grad_norm\": 0.09243763238191605,\n",
      "      \"learning_rate\": 3.993572778827977e-05,\n",
      "      \"loss\": 0.0301,\n",
      "      \"step\": 31960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0151233498219856,\n",
      "      \"grad_norm\": 0.07161548733711243,\n",
      "      \"learning_rate\": 3.99294265910523e-05,\n",
      "      \"loss\": 0.0298,\n",
      "      \"step\": 31980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0163836289738177,\n",
      "      \"grad_norm\": 0.021101076155900955,\n",
      "      \"learning_rate\": 3.992312539382483e-05,\n",
      "      \"loss\": 0.0318,\n",
      "      \"step\": 32000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.01764390812565,\n",
      "      \"grad_norm\": 0.006077277474105358,\n",
      "      \"learning_rate\": 3.991682419659736e-05,\n",
      "      \"loss\": 0.0439,\n",
      "      \"step\": 32020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.018904187277482,\n",
      "      \"grad_norm\": 0.009370645508170128,\n",
      "      \"learning_rate\": 3.991052299936988e-05,\n",
      "      \"loss\": 0.0593,\n",
      "      \"step\": 32040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.020164466429314,\n",
      "      \"grad_norm\": 4.1947126388549805,\n",
      "      \"learning_rate\": 3.990422180214241e-05,\n",
      "      \"loss\": 0.0285,\n",
      "      \"step\": 32060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0214247455811463,\n",
      "      \"grad_norm\": 0.04218808934092522,\n",
      "      \"learning_rate\": 3.989792060491493e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 32080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0226850247329784,\n",
      "      \"grad_norm\": 0.006127246655523777,\n",
      "      \"learning_rate\": 3.989161940768746e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 32100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0239453038848105,\n",
      "      \"grad_norm\": 0.002745202276855707,\n",
      "      \"learning_rate\": 3.9885318210459984e-05,\n",
      "      \"loss\": 0.0129,\n",
      "      \"step\": 32120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0252055830366427,\n",
      "      \"grad_norm\": 4.416895389556885,\n",
      "      \"learning_rate\": 3.987901701323252e-05,\n",
      "      \"loss\": 0.035,\n",
      "      \"step\": 32140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.026465862188475,\n",
      "      \"grad_norm\": 23.445514678955078,\n",
      "      \"learning_rate\": 3.987271581600504e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 32160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.027726141340307,\n",
      "      \"grad_norm\": 0.0036000083200633526,\n",
      "      \"learning_rate\": 3.986641461877757e-05,\n",
      "      \"loss\": 0.0204,\n",
      "      \"step\": 32180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.028986420492139,\n",
      "      \"grad_norm\": 0.014755875803530216,\n",
      "      \"learning_rate\": 3.9860113421550094e-05,\n",
      "      \"loss\": 0.1018,\n",
      "      \"step\": 32200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.030246699643971,\n",
      "      \"grad_norm\": 0.0869600921869278,\n",
      "      \"learning_rate\": 3.985381222432262e-05,\n",
      "      \"loss\": 0.0353,\n",
      "      \"step\": 32220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0315069787958033,\n",
      "      \"grad_norm\": 0.057661134749650955,\n",
      "      \"learning_rate\": 3.984751102709515e-05,\n",
      "      \"loss\": 0.0778,\n",
      "      \"step\": 32240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0327672579476355,\n",
      "      \"grad_norm\": 0.5805184245109558,\n",
      "      \"learning_rate\": 3.9841209829867674e-05,\n",
      "      \"loss\": 0.0253,\n",
      "      \"step\": 32260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0340275370994676,\n",
      "      \"grad_norm\": 0.05863849073648453,\n",
      "      \"learning_rate\": 3.98349086326402e-05,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 32280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0352878162512997,\n",
      "      \"grad_norm\": 5.104504108428955,\n",
      "      \"learning_rate\": 3.982860743541273e-05,\n",
      "      \"loss\": 0.0276,\n",
      "      \"step\": 32300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.036548095403132,\n",
      "      \"grad_norm\": 0.05843436345458031,\n",
      "      \"learning_rate\": 3.982230623818526e-05,\n",
      "      \"loss\": 0.0408,\n",
      "      \"step\": 32320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.037808374554964,\n",
      "      \"grad_norm\": 0.007619617041200399,\n",
      "      \"learning_rate\": 3.9816005040957783e-05,\n",
      "      \"loss\": 0.0023,\n",
      "      \"step\": 32340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.039068653706796,\n",
      "      \"grad_norm\": 0.02159597910940647,\n",
      "      \"learning_rate\": 3.980970384373031e-05,\n",
      "      \"loss\": 0.0161,\n",
      "      \"step\": 32360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0403289328586283,\n",
      "      \"grad_norm\": 0.12966787815093994,\n",
      "      \"learning_rate\": 3.9803402646502835e-05,\n",
      "      \"loss\": 0.0641,\n",
      "      \"step\": 32380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0415892120104604,\n",
      "      \"grad_norm\": 0.603870689868927,\n",
      "      \"learning_rate\": 3.9797101449275364e-05,\n",
      "      \"loss\": 0.0333,\n",
      "      \"step\": 32400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0428494911622925,\n",
      "      \"grad_norm\": 0.0696420893073082,\n",
      "      \"learning_rate\": 3.9790800252047886e-05,\n",
      "      \"loss\": 0.0303,\n",
      "      \"step\": 32420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0441097703141247,\n",
      "      \"grad_norm\": 0.25012099742889404,\n",
      "      \"learning_rate\": 3.9784499054820415e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 32440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.045370049465957,\n",
      "      \"grad_norm\": 0.010307750664651394,\n",
      "      \"learning_rate\": 3.9778197857592944e-05,\n",
      "      \"loss\": 0.0426,\n",
      "      \"step\": 32460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.046630328617789,\n",
      "      \"grad_norm\": 0.010805834084749222,\n",
      "      \"learning_rate\": 3.977189666036547e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 32480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.047890607769621,\n",
      "      \"grad_norm\": 0.0182709451764822,\n",
      "      \"learning_rate\": 3.9765595463138e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 32500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.049150886921453,\n",
      "      \"grad_norm\": 0.029097246006131172,\n",
      "      \"learning_rate\": 3.9759294265910524e-05,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 32520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0504111660732853,\n",
      "      \"grad_norm\": 0.006372610107064247,\n",
      "      \"learning_rate\": 3.9752993068683054e-05,\n",
      "      \"loss\": 0.0501,\n",
      "      \"step\": 32540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0516714452251175,\n",
      "      \"grad_norm\": 7.633491516113281,\n",
      "      \"learning_rate\": 3.9746691871455576e-05,\n",
      "      \"loss\": 0.0333,\n",
      "      \"step\": 32560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0529317243769496,\n",
      "      \"grad_norm\": 0.16936735808849335,\n",
      "      \"learning_rate\": 3.9740390674228105e-05,\n",
      "      \"loss\": 0.0609,\n",
      "      \"step\": 32580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0541920035287817,\n",
      "      \"grad_norm\": 0.3169606924057007,\n",
      "      \"learning_rate\": 3.973408947700063e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 32600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.055452282680614,\n",
      "      \"grad_norm\": 0.03533468022942543,\n",
      "      \"learning_rate\": 3.9727788279773156e-05,\n",
      "      \"loss\": 0.0551,\n",
      "      \"step\": 32620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.056712561832446,\n",
      "      \"grad_norm\": 0.1070268377661705,\n",
      "      \"learning_rate\": 3.9721487082545685e-05,\n",
      "      \"loss\": 0.0196,\n",
      "      \"step\": 32640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.057972840984278,\n",
      "      \"grad_norm\": 0.00845655333250761,\n",
      "      \"learning_rate\": 3.9715185885318214e-05,\n",
      "      \"loss\": 0.0241,\n",
      "      \"step\": 32660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0592331201361103,\n",
      "      \"grad_norm\": 0.02710413932800293,\n",
      "      \"learning_rate\": 3.970888468809074e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 32680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0604933992879424,\n",
      "      \"grad_norm\": 0.004859929904341698,\n",
      "      \"learning_rate\": 3.9702583490863266e-05,\n",
      "      \"loss\": 0.0064,\n",
      "      \"step\": 32700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0617536784397745,\n",
      "      \"grad_norm\": 5.264411926269531,\n",
      "      \"learning_rate\": 3.9696282293635795e-05,\n",
      "      \"loss\": 0.0937,\n",
      "      \"step\": 32720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0630139575916067,\n",
      "      \"grad_norm\": 15.209662437438965,\n",
      "      \"learning_rate\": 3.968998109640832e-05,\n",
      "      \"loss\": 0.0459,\n",
      "      \"step\": 32740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.064274236743439,\n",
      "      \"grad_norm\": 28.31075668334961,\n",
      "      \"learning_rate\": 3.9683679899180846e-05,\n",
      "      \"loss\": 0.0443,\n",
      "      \"step\": 32760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.065534515895271,\n",
      "      \"grad_norm\": 0.01682300493121147,\n",
      "      \"learning_rate\": 3.9677378701953375e-05,\n",
      "      \"loss\": 0.0196,\n",
      "      \"step\": 32780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.066794795047103,\n",
      "      \"grad_norm\": 0.01082697231322527,\n",
      "      \"learning_rate\": 3.9671077504725904e-05,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 32800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.068055074198935,\n",
      "      \"grad_norm\": 4.10493278503418,\n",
      "      \"learning_rate\": 3.9664776307498426e-05,\n",
      "      \"loss\": 0.062,\n",
      "      \"step\": 32820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0693153533507673,\n",
      "      \"grad_norm\": 0.017440272495150566,\n",
      "      \"learning_rate\": 3.9658475110270955e-05,\n",
      "      \"loss\": 0.0358,\n",
      "      \"step\": 32840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0705756325025995,\n",
      "      \"grad_norm\": 0.03053422085940838,\n",
      "      \"learning_rate\": 3.965217391304348e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 32860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0718359116544316,\n",
      "      \"grad_norm\": 0.027092216536402702,\n",
      "      \"learning_rate\": 3.9645872715816007e-05,\n",
      "      \"loss\": 0.0275,\n",
      "      \"step\": 32880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0730961908062637,\n",
      "      \"grad_norm\": 0.01002572849392891,\n",
      "      \"learning_rate\": 3.9639571518588536e-05,\n",
      "      \"loss\": 0.022,\n",
      "      \"step\": 32900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.074356469958096,\n",
      "      \"grad_norm\": 0.0012022214941680431,\n",
      "      \"learning_rate\": 3.963327032136106e-05,\n",
      "      \"loss\": 0.026,\n",
      "      \"step\": 32920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.075616749109928,\n",
      "      \"grad_norm\": 6.744375705718994,\n",
      "      \"learning_rate\": 3.962696912413359e-05,\n",
      "      \"loss\": 0.0704,\n",
      "      \"step\": 32940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.07687702826176,\n",
      "      \"grad_norm\": 0.1946805715560913,\n",
      "      \"learning_rate\": 3.9620667926906116e-05,\n",
      "      \"loss\": 0.0405,\n",
      "      \"step\": 32960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0781373074135923,\n",
      "      \"grad_norm\": 10.537125587463379,\n",
      "      \"learning_rate\": 3.9614366729678645e-05,\n",
      "      \"loss\": 0.0304,\n",
      "      \"step\": 32980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0793975865654244,\n",
      "      \"grad_norm\": 0.005963405594229698,\n",
      "      \"learning_rate\": 3.960806553245117e-05,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 33000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0806578657172565,\n",
      "      \"grad_norm\": 5.618429660797119,\n",
      "      \"learning_rate\": 3.9601764335223696e-05,\n",
      "      \"loss\": 0.0532,\n",
      "      \"step\": 33020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0819181448690887,\n",
      "      \"grad_norm\": 0.2285783737897873,\n",
      "      \"learning_rate\": 3.959546313799622e-05,\n",
      "      \"loss\": 0.0251,\n",
      "      \"step\": 33040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.083178424020921,\n",
      "      \"grad_norm\": 0.10619624704122543,\n",
      "      \"learning_rate\": 3.958916194076875e-05,\n",
      "      \"loss\": 0.0056,\n",
      "      \"step\": 33060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.084438703172753,\n",
      "      \"grad_norm\": 0.0062048165127635,\n",
      "      \"learning_rate\": 3.958286074354127e-05,\n",
      "      \"loss\": 0.0371,\n",
      "      \"step\": 33080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.085698982324585,\n",
      "      \"grad_norm\": 5.251025676727295,\n",
      "      \"learning_rate\": 3.95765595463138e-05,\n",
      "      \"loss\": 0.0786,\n",
      "      \"step\": 33100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.086959261476417,\n",
      "      \"grad_norm\": 0.041961610317230225,\n",
      "      \"learning_rate\": 3.957025834908633e-05,\n",
      "      \"loss\": 0.0217,\n",
      "      \"step\": 33120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0882195406282493,\n",
      "      \"grad_norm\": 0.9341555833816528,\n",
      "      \"learning_rate\": 3.956395715185886e-05,\n",
      "      \"loss\": 0.0609,\n",
      "      \"step\": 33140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0894798197800815,\n",
      "      \"grad_norm\": 0.31354743242263794,\n",
      "      \"learning_rate\": 3.9557655954631386e-05,\n",
      "      \"loss\": 0.0397,\n",
      "      \"step\": 33160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0907400989319136,\n",
      "      \"grad_norm\": 0.12059802561998367,\n",
      "      \"learning_rate\": 3.955135475740391e-05,\n",
      "      \"loss\": 0.0311,\n",
      "      \"step\": 33180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0920003780837457,\n",
      "      \"grad_norm\": 0.025441715493798256,\n",
      "      \"learning_rate\": 3.954505356017644e-05,\n",
      "      \"loss\": 0.035,\n",
      "      \"step\": 33200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.093260657235578,\n",
      "      \"grad_norm\": 22.107955932617188,\n",
      "      \"learning_rate\": 3.953875236294896e-05,\n",
      "      \"loss\": 0.0293,\n",
      "      \"step\": 33220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.09452093638741,\n",
      "      \"grad_norm\": 0.500706136226654,\n",
      "      \"learning_rate\": 3.953245116572149e-05,\n",
      "      \"loss\": 0.0289,\n",
      "      \"step\": 33240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.095781215539242,\n",
      "      \"grad_norm\": 0.13201917707920074,\n",
      "      \"learning_rate\": 3.952614996849401e-05,\n",
      "      \"loss\": 0.0437,\n",
      "      \"step\": 33260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0970414946910743,\n",
      "      \"grad_norm\": 18.257972717285156,\n",
      "      \"learning_rate\": 3.951984877126655e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 33280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0983017738429064,\n",
      "      \"grad_norm\": 3.6322181224823,\n",
      "      \"learning_rate\": 3.951354757403907e-05,\n",
      "      \"loss\": 0.0715,\n",
      "      \"step\": 33300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0995620529947385,\n",
      "      \"grad_norm\": 0.008960475213825703,\n",
      "      \"learning_rate\": 3.95072463768116e-05,\n",
      "      \"loss\": 0.0382,\n",
      "      \"step\": 33320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1008223321465707,\n",
      "      \"grad_norm\": 0.0013699335977435112,\n",
      "      \"learning_rate\": 3.950094517958412e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 33340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.102082611298403,\n",
      "      \"grad_norm\": 0.20518435537815094,\n",
      "      \"learning_rate\": 3.949464398235665e-05,\n",
      "      \"loss\": 0.0554,\n",
      "      \"step\": 33360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.103342890450235,\n",
      "      \"grad_norm\": 0.049805205315351486,\n",
      "      \"learning_rate\": 3.948834278512918e-05,\n",
      "      \"loss\": 0.0409,\n",
      "      \"step\": 33380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.104603169602067,\n",
      "      \"grad_norm\": 0.1006191074848175,\n",
      "      \"learning_rate\": 3.94820415879017e-05,\n",
      "      \"loss\": 0.0272,\n",
      "      \"step\": 33400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.105863448753899,\n",
      "      \"grad_norm\": 0.17180633544921875,\n",
      "      \"learning_rate\": 3.947574039067423e-05,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 33420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1071237279057313,\n",
      "      \"grad_norm\": 0.031847987323999405,\n",
      "      \"learning_rate\": 3.946943919344676e-05,\n",
      "      \"loss\": 0.0499,\n",
      "      \"step\": 33440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1083840070575635,\n",
      "      \"grad_norm\": 0.09052549302577972,\n",
      "      \"learning_rate\": 3.946313799621929e-05,\n",
      "      \"loss\": 0.0209,\n",
      "      \"step\": 33460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1096442862093956,\n",
      "      \"grad_norm\": 0.19979876279830933,\n",
      "      \"learning_rate\": 3.945683679899181e-05,\n",
      "      \"loss\": 0.0649,\n",
      "      \"step\": 33480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1109045653612277,\n",
      "      \"grad_norm\": 0.0021485292818397284,\n",
      "      \"learning_rate\": 3.945085066162571e-05,\n",
      "      \"loss\": 0.0562,\n",
      "      \"step\": 33500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1121648445130594,\n",
      "      \"grad_norm\": 0.01707600988447666,\n",
      "      \"learning_rate\": 3.944454946439824e-05,\n",
      "      \"loss\": 0.0176,\n",
      "      \"step\": 33520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.113425123664892,\n",
      "      \"grad_norm\": 0.023980695754289627,\n",
      "      \"learning_rate\": 3.943824826717076e-05,\n",
      "      \"loss\": 0.0416,\n",
      "      \"step\": 33540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1146854028167237,\n",
      "      \"grad_norm\": 0.0803903341293335,\n",
      "      \"learning_rate\": 3.943194706994329e-05,\n",
      "      \"loss\": 0.0488,\n",
      "      \"step\": 33560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1159456819685563,\n",
      "      \"grad_norm\": 0.11116617172956467,\n",
      "      \"learning_rate\": 3.9425645872715813e-05,\n",
      "      \"loss\": 0.029,\n",
      "      \"step\": 33580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.117205961120388,\n",
      "      \"grad_norm\": 0.0008236098219640553,\n",
      "      \"learning_rate\": 3.941934467548834e-05,\n",
      "      \"loss\": 0.0496,\n",
      "      \"step\": 33600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1184662402722205,\n",
      "      \"grad_norm\": 0.044144097715616226,\n",
      "      \"learning_rate\": 3.941304347826087e-05,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 33620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.119726519424052,\n",
      "      \"grad_norm\": 0.2281867265701294,\n",
      "      \"learning_rate\": 3.94067422810334e-05,\n",
      "      \"loss\": 0.0305,\n",
      "      \"step\": 33640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1209867985758843,\n",
      "      \"grad_norm\": 0.0014219790464267135,\n",
      "      \"learning_rate\": 3.940044108380592e-05,\n",
      "      \"loss\": 0.0044,\n",
      "      \"step\": 33660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1222470777277165,\n",
      "      \"grad_norm\": 1.0287076234817505,\n",
      "      \"learning_rate\": 3.939413988657845e-05,\n",
      "      \"loss\": 0.0785,\n",
      "      \"step\": 33680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1235073568795486,\n",
      "      \"grad_norm\": 0.023464400321245193,\n",
      "      \"learning_rate\": 3.938783868935098e-05,\n",
      "      \"loss\": 0.0414,\n",
      "      \"step\": 33700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1247676360313807,\n",
      "      \"grad_norm\": 0.009325630962848663,\n",
      "      \"learning_rate\": 3.93815374921235e-05,\n",
      "      \"loss\": 0.0183,\n",
      "      \"step\": 33720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.126027915183213,\n",
      "      \"grad_norm\": 0.08518939465284348,\n",
      "      \"learning_rate\": 3.937523629489603e-05,\n",
      "      \"loss\": 0.0382,\n",
      "      \"step\": 33740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.127288194335045,\n",
      "      \"grad_norm\": 0.02005791664123535,\n",
      "      \"learning_rate\": 3.9368935097668554e-05,\n",
      "      \"loss\": 0.0647,\n",
      "      \"step\": 33760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.128548473486877,\n",
      "      \"grad_norm\": 0.04903065040707588,\n",
      "      \"learning_rate\": 3.936263390044109e-05,\n",
      "      \"loss\": 0.0481,\n",
      "      \"step\": 33780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1298087526387093,\n",
      "      \"grad_norm\": 0.006515697576105595,\n",
      "      \"learning_rate\": 3.935633270321361e-05,\n",
      "      \"loss\": 0.0213,\n",
      "      \"step\": 33800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1310690317905414,\n",
      "      \"grad_norm\": 0.015241077169775963,\n",
      "      \"learning_rate\": 3.935003150598614e-05,\n",
      "      \"loss\": 0.0125,\n",
      "      \"step\": 33820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1323293109423735,\n",
      "      \"grad_norm\": 0.020268026739358902,\n",
      "      \"learning_rate\": 3.9343730308758664e-05,\n",
      "      \"loss\": 0.0033,\n",
      "      \"step\": 33840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1335895900942057,\n",
      "      \"grad_norm\": 0.002214018488302827,\n",
      "      \"learning_rate\": 3.933742911153119e-05,\n",
      "      \"loss\": 0.0408,\n",
      "      \"step\": 33860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.134849869246038,\n",
      "      \"grad_norm\": 0.005909857340157032,\n",
      "      \"learning_rate\": 3.9331127914303715e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 33880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.13611014839787,\n",
      "      \"grad_norm\": 0.046805012971162796,\n",
      "      \"learning_rate\": 3.9324826717076244e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 33900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.137370427549702,\n",
      "      \"grad_norm\": 0.07821561396121979,\n",
      "      \"learning_rate\": 3.931852551984877e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 33920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.138630706701534,\n",
      "      \"grad_norm\": 0.0824919193983078,\n",
      "      \"learning_rate\": 3.93122243226213e-05,\n",
      "      \"loss\": 0.0196,\n",
      "      \"step\": 33940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1398909858533663,\n",
      "      \"grad_norm\": 0.0014460785314440727,\n",
      "      \"learning_rate\": 3.930592312539383e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 33960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1411512650051985,\n",
      "      \"grad_norm\": 5.851388454437256,\n",
      "      \"learning_rate\": 3.9299621928166354e-05,\n",
      "      \"loss\": 0.0276,\n",
      "      \"step\": 33980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1424115441570306,\n",
      "      \"grad_norm\": 0.144552081823349,\n",
      "      \"learning_rate\": 3.929332073093888e-05,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 34000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1436718233088627,\n",
      "      \"grad_norm\": 0.0023797263856977224,\n",
      "      \"learning_rate\": 3.9287019533711405e-05,\n",
      "      \"loss\": 0.0386,\n",
      "      \"step\": 34020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.144932102460695,\n",
      "      \"grad_norm\": 0.003393943654373288,\n",
      "      \"learning_rate\": 3.9280718336483934e-05,\n",
      "      \"loss\": 0.0438,\n",
      "      \"step\": 34040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.146192381612527,\n",
      "      \"grad_norm\": 0.0010449521942064166,\n",
      "      \"learning_rate\": 3.9274417139256456e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 34060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.147452660764359,\n",
      "      \"grad_norm\": 0.04130016639828682,\n",
      "      \"learning_rate\": 3.9268115942028985e-05,\n",
      "      \"loss\": 0.0634,\n",
      "      \"step\": 34080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1487129399161913,\n",
      "      \"grad_norm\": 0.010124079883098602,\n",
      "      \"learning_rate\": 3.9261814744801514e-05,\n",
      "      \"loss\": 0.0689,\n",
      "      \"step\": 34100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1499732190680234,\n",
      "      \"grad_norm\": 3.709185838699341,\n",
      "      \"learning_rate\": 3.925551354757404e-05,\n",
      "      \"loss\": 0.0351,\n",
      "      \"step\": 34120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1512334982198555,\n",
      "      \"grad_norm\": 0.014456360600888729,\n",
      "      \"learning_rate\": 3.924921235034657e-05,\n",
      "      \"loss\": 0.0268,\n",
      "      \"step\": 34140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1524937773716877,\n",
      "      \"grad_norm\": 0.018479924649000168,\n",
      "      \"learning_rate\": 3.9242911153119095e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 34160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.15375405652352,\n",
      "      \"grad_norm\": 0.0049339784309268,\n",
      "      \"learning_rate\": 3.9236609955891624e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 34180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.155014335675352,\n",
      "      \"grad_norm\": 0.00677050044760108,\n",
      "      \"learning_rate\": 3.9230308758664146e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 34200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.156274614827184,\n",
      "      \"grad_norm\": 8.164508819580078,\n",
      "      \"learning_rate\": 3.9224007561436675e-05,\n",
      "      \"loss\": 0.0561,\n",
      "      \"step\": 34220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.157534893979016,\n",
      "      \"grad_norm\": 0.02039516344666481,\n",
      "      \"learning_rate\": 3.92177063642092e-05,\n",
      "      \"loss\": 0.1098,\n",
      "      \"step\": 34240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1587951731308483,\n",
      "      \"grad_norm\": 0.27507686614990234,\n",
      "      \"learning_rate\": 3.9211405166981726e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 34260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1600554522826805,\n",
      "      \"grad_norm\": 0.014097634702920914,\n",
      "      \"learning_rate\": 3.9205103969754255e-05,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 34280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1613157314345126,\n",
      "      \"grad_norm\": 0.018225299194455147,\n",
      "      \"learning_rate\": 3.9198802772526784e-05,\n",
      "      \"loss\": 0.0202,\n",
      "      \"step\": 34300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1625760105863447,\n",
      "      \"grad_norm\": 0.007455579005181789,\n",
      "      \"learning_rate\": 3.9192501575299307e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 34320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.163836289738177,\n",
      "      \"grad_norm\": 4.763580799102783,\n",
      "      \"learning_rate\": 3.9186200378071836e-05,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 34340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.165096568890009,\n",
      "      \"grad_norm\": 0.013225793838500977,\n",
      "      \"learning_rate\": 3.9179899180844365e-05,\n",
      "      \"loss\": 0.0367,\n",
      "      \"step\": 34360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.166356848041841,\n",
      "      \"grad_norm\": 7.936633586883545,\n",
      "      \"learning_rate\": 3.917359798361689e-05,\n",
      "      \"loss\": 0.0383,\n",
      "      \"step\": 34380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1676171271936733,\n",
      "      \"grad_norm\": 0.06274639815092087,\n",
      "      \"learning_rate\": 3.9167296786389416e-05,\n",
      "      \"loss\": 0.0517,\n",
      "      \"step\": 34400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1688774063455054,\n",
      "      \"grad_norm\": 0.04207644611597061,\n",
      "      \"learning_rate\": 3.9160995589161945e-05,\n",
      "      \"loss\": 0.0147,\n",
      "      \"step\": 34420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1701376854973375,\n",
      "      \"grad_norm\": 0.03799327090382576,\n",
      "      \"learning_rate\": 3.9154694391934474e-05,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 34440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1713979646491697,\n",
      "      \"grad_norm\": 0.1924269050359726,\n",
      "      \"learning_rate\": 3.9148393194706996e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 34460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.172658243801002,\n",
      "      \"grad_norm\": 6.439049243927002,\n",
      "      \"learning_rate\": 3.9142091997479525e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 34480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.173918522952834,\n",
      "      \"grad_norm\": 0.0410161130130291,\n",
      "      \"learning_rate\": 3.913579080025205e-05,\n",
      "      \"loss\": 0.0197,\n",
      "      \"step\": 34500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.175178802104666,\n",
      "      \"grad_norm\": 0.005483880639076233,\n",
      "      \"learning_rate\": 3.912948960302458e-05,\n",
      "      \"loss\": 0.0193,\n",
      "      \"step\": 34520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.176439081256498,\n",
      "      \"grad_norm\": 4.086828708648682,\n",
      "      \"learning_rate\": 3.91231884057971e-05,\n",
      "      \"loss\": 0.0909,\n",
      "      \"step\": 34540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1776993604083303,\n",
      "      \"grad_norm\": 0.014890571124851704,\n",
      "      \"learning_rate\": 3.911688720856963e-05,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 34560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1789596395601625,\n",
      "      \"grad_norm\": 0.045863859355449677,\n",
      "      \"learning_rate\": 3.911058601134216e-05,\n",
      "      \"loss\": 0.0471,\n",
      "      \"step\": 34580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1802199187119946,\n",
      "      \"grad_norm\": 0.0741739571094513,\n",
      "      \"learning_rate\": 3.9104284814114686e-05,\n",
      "      \"loss\": 0.0251,\n",
      "      \"step\": 34600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1814801978638267,\n",
      "      \"grad_norm\": 0.011983809992671013,\n",
      "      \"learning_rate\": 3.9097983616887215e-05,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 34620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.182740477015659,\n",
      "      \"grad_norm\": 0.006788569036871195,\n",
      "      \"learning_rate\": 3.909168241965974e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 34640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.184000756167491,\n",
      "      \"grad_norm\": 0.004693908151239157,\n",
      "      \"learning_rate\": 3.9085381222432266e-05,\n",
      "      \"loss\": 0.0677,\n",
      "      \"step\": 34660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.185261035319323,\n",
      "      \"grad_norm\": 14.403846740722656,\n",
      "      \"learning_rate\": 3.907908002520479e-05,\n",
      "      \"loss\": 0.0274,\n",
      "      \"step\": 34680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1865213144711553,\n",
      "      \"grad_norm\": 0.01701369136571884,\n",
      "      \"learning_rate\": 3.907277882797732e-05,\n",
      "      \"loss\": 0.0241,\n",
      "      \"step\": 34700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1877815936229874,\n",
      "      \"grad_norm\": 0.5712404251098633,\n",
      "      \"learning_rate\": 3.906647763074984e-05,\n",
      "      \"loss\": 0.0375,\n",
      "      \"step\": 34720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1890418727748195,\n",
      "      \"grad_norm\": 1.2497693300247192,\n",
      "      \"learning_rate\": 3.906017643352237e-05,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 34740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1903021519266517,\n",
      "      \"grad_norm\": 0.05639127641916275,\n",
      "      \"learning_rate\": 3.90538752362949e-05,\n",
      "      \"loss\": 0.0165,\n",
      "      \"step\": 34760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.191562431078484,\n",
      "      \"grad_norm\": 17.815555572509766,\n",
      "      \"learning_rate\": 3.904757403906743e-05,\n",
      "      \"loss\": 0.0513,\n",
      "      \"step\": 34780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.192822710230316,\n",
      "      \"grad_norm\": 0.02177150547504425,\n",
      "      \"learning_rate\": 3.904127284183995e-05,\n",
      "      \"loss\": 0.0387,\n",
      "      \"step\": 34800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.194082989382148,\n",
      "      \"grad_norm\": 3.0868563652038574,\n",
      "      \"learning_rate\": 3.903497164461248e-05,\n",
      "      \"loss\": 0.005,\n",
      "      \"step\": 34820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.19534326853398,\n",
      "      \"grad_norm\": 0.003356940345838666,\n",
      "      \"learning_rate\": 3.902867044738501e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 34840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1966035476858123,\n",
      "      \"grad_norm\": 0.008129375986754894,\n",
      "      \"learning_rate\": 3.902236925015753e-05,\n",
      "      \"loss\": 0.0331,\n",
      "      \"step\": 34860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1978638268376445,\n",
      "      \"grad_norm\": 0.002185795921832323,\n",
      "      \"learning_rate\": 3.901606805293006e-05,\n",
      "      \"loss\": 0.0716,\n",
      "      \"step\": 34880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1991241059894766,\n",
      "      \"grad_norm\": 0.5667441487312317,\n",
      "      \"learning_rate\": 3.900976685570258e-05,\n",
      "      \"loss\": 0.0394,\n",
      "      \"step\": 34900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2003843851413087,\n",
      "      \"grad_norm\": 0.04226568341255188,\n",
      "      \"learning_rate\": 3.900346565847512e-05,\n",
      "      \"loss\": 0.0488,\n",
      "      \"step\": 34920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.201644664293141,\n",
      "      \"grad_norm\": 0.38367319107055664,\n",
      "      \"learning_rate\": 3.899716446124764e-05,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 34940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.202904943444973,\n",
      "      \"grad_norm\": 1.6317728757858276,\n",
      "      \"learning_rate\": 3.899117832388154e-05,\n",
      "      \"loss\": 0.0221,\n",
      "      \"step\": 34960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.204165222596805,\n",
      "      \"grad_norm\": 0.008046348579227924,\n",
      "      \"learning_rate\": 3.898487712665407e-05,\n",
      "      \"loss\": 0.0358,\n",
      "      \"step\": 34980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2054255017486373,\n",
      "      \"grad_norm\": 0.10045316070318222,\n",
      "      \"learning_rate\": 3.897857592942659e-05,\n",
      "      \"loss\": 0.0257,\n",
      "      \"step\": 35000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2066857809004694,\n",
      "      \"grad_norm\": 0.017664413899183273,\n",
      "      \"learning_rate\": 3.897227473219912e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 35020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2079460600523015,\n",
      "      \"grad_norm\": 0.0107799107208848,\n",
      "      \"learning_rate\": 3.896597353497164e-05,\n",
      "      \"loss\": 0.0506,\n",
      "      \"step\": 35040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2092063392041337,\n",
      "      \"grad_norm\": 0.002649964764714241,\n",
      "      \"learning_rate\": 3.895967233774417e-05,\n",
      "      \"loss\": 0.0121,\n",
      "      \"step\": 35060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.210466618355966,\n",
      "      \"grad_norm\": 0.20526091754436493,\n",
      "      \"learning_rate\": 3.89533711405167e-05,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 35080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.211726897507798,\n",
      "      \"grad_norm\": 0.013234679587185383,\n",
      "      \"learning_rate\": 3.894706994328923e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 35100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.21298717665963,\n",
      "      \"grad_norm\": 0.003620945382863283,\n",
      "      \"learning_rate\": 3.894076874606175e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 35120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.214247455811462,\n",
      "      \"grad_norm\": 0.0020842733792960644,\n",
      "      \"learning_rate\": 3.893446754883428e-05,\n",
      "      \"loss\": 0.0208,\n",
      "      \"step\": 35140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2155077349632943,\n",
      "      \"grad_norm\": 0.07501622289419174,\n",
      "      \"learning_rate\": 3.892816635160681e-05,\n",
      "      \"loss\": 0.0235,\n",
      "      \"step\": 35160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2167680141151265,\n",
      "      \"grad_norm\": 0.007386238779872656,\n",
      "      \"learning_rate\": 3.892186515437933e-05,\n",
      "      \"loss\": 0.025,\n",
      "      \"step\": 35180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2180282932669586,\n",
      "      \"grad_norm\": 0.022804703563451767,\n",
      "      \"learning_rate\": 3.891556395715186e-05,\n",
      "      \"loss\": 0.0399,\n",
      "      \"step\": 35200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2192885724187907,\n",
      "      \"grad_norm\": 0.009596508927643299,\n",
      "      \"learning_rate\": 3.8909262759924384e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 35220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.220548851570623,\n",
      "      \"grad_norm\": 8.473226547241211,\n",
      "      \"learning_rate\": 3.890296156269691e-05,\n",
      "      \"loss\": 0.0418,\n",
      "      \"step\": 35240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.221809130722455,\n",
      "      \"grad_norm\": 0.00930023193359375,\n",
      "      \"learning_rate\": 3.889666036546944e-05,\n",
      "      \"loss\": 0.0021,\n",
      "      \"step\": 35260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.223069409874287,\n",
      "      \"grad_norm\": 0.0542401522397995,\n",
      "      \"learning_rate\": 3.889035916824197e-05,\n",
      "      \"loss\": 0.0375,\n",
      "      \"step\": 35280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2243296890261193,\n",
      "      \"grad_norm\": 0.002144771395251155,\n",
      "      \"learning_rate\": 3.888405797101449e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 35300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2255899681779514,\n",
      "      \"grad_norm\": 15.673821449279785,\n",
      "      \"learning_rate\": 3.887775677378702e-05,\n",
      "      \"loss\": 0.0312,\n",
      "      \"step\": 35320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2268502473297835,\n",
      "      \"grad_norm\": 0.2506030201911926,\n",
      "      \"learning_rate\": 3.8871455576559544e-05,\n",
      "      \"loss\": 0.0276,\n",
      "      \"step\": 35340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2281105264816157,\n",
      "      \"grad_norm\": 0.0444740429520607,\n",
      "      \"learning_rate\": 3.886515437933207e-05,\n",
      "      \"loss\": 0.0076,\n",
      "      \"step\": 35360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.229370805633448,\n",
      "      \"grad_norm\": 0.03760000690817833,\n",
      "      \"learning_rate\": 3.88588531821046e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 35380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.23063108478528,\n",
      "      \"grad_norm\": 0.018995746970176697,\n",
      "      \"learning_rate\": 3.885255198487713e-05,\n",
      "      \"loss\": 0.0971,\n",
      "      \"step\": 35400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.231891363937112,\n",
      "      \"grad_norm\": 0.16225337982177734,\n",
      "      \"learning_rate\": 3.884625078764966e-05,\n",
      "      \"loss\": 0.0163,\n",
      "      \"step\": 35420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.233151643088944,\n",
      "      \"grad_norm\": 4.774168968200684,\n",
      "      \"learning_rate\": 3.883994959042218e-05,\n",
      "      \"loss\": 0.0461,\n",
      "      \"step\": 35440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2344119222407763,\n",
      "      \"grad_norm\": 0.18554292619228363,\n",
      "      \"learning_rate\": 3.883364839319471e-05,\n",
      "      \"loss\": 0.0387,\n",
      "      \"step\": 35460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2356722013926085,\n",
      "      \"grad_norm\": 0.015347878448665142,\n",
      "      \"learning_rate\": 3.8827347195967234e-05,\n",
      "      \"loss\": 0.0247,\n",
      "      \"step\": 35480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2369324805444406,\n",
      "      \"grad_norm\": 0.8324204087257385,\n",
      "      \"learning_rate\": 3.882104599873976e-05,\n",
      "      \"loss\": 0.0473,\n",
      "      \"step\": 35500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2381927596962727,\n",
      "      \"grad_norm\": 0.05534575134515762,\n",
      "      \"learning_rate\": 3.8814744801512285e-05,\n",
      "      \"loss\": 0.0707,\n",
      "      \"step\": 35520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.239453038848105,\n",
      "      \"grad_norm\": 0.08205008506774902,\n",
      "      \"learning_rate\": 3.8808443604284814e-05,\n",
      "      \"loss\": 0.0496,\n",
      "      \"step\": 35540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.240713317999937,\n",
      "      \"grad_norm\": 0.07501820474863052,\n",
      "      \"learning_rate\": 3.880214240705734e-05,\n",
      "      \"loss\": 0.0305,\n",
      "      \"step\": 35560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.241973597151769,\n",
      "      \"grad_norm\": 0.009118088521063328,\n",
      "      \"learning_rate\": 3.879584120982987e-05,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 35580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2432338763036013,\n",
      "      \"grad_norm\": 0.06598669290542603,\n",
      "      \"learning_rate\": 3.87895400126024e-05,\n",
      "      \"loss\": 0.0362,\n",
      "      \"step\": 35600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2444941554554334,\n",
      "      \"grad_norm\": 0.0660262480378151,\n",
      "      \"learning_rate\": 3.8783238815374924e-05,\n",
      "      \"loss\": 0.0438,\n",
      "      \"step\": 35620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2457544346072655,\n",
      "      \"grad_norm\": 0.05102632939815521,\n",
      "      \"learning_rate\": 3.877693761814745e-05,\n",
      "      \"loss\": 0.0231,\n",
      "      \"step\": 35640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2470147137590977,\n",
      "      \"grad_norm\": 0.07939766347408295,\n",
      "      \"learning_rate\": 3.8770636420919975e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 35660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.24827499291093,\n",
      "      \"grad_norm\": 0.007063310593366623,\n",
      "      \"learning_rate\": 3.8764335223692504e-05,\n",
      "      \"loss\": 0.0223,\n",
      "      \"step\": 35680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.249535272062762,\n",
      "      \"grad_norm\": 0.0074538919143378735,\n",
      "      \"learning_rate\": 3.8758034026465026e-05,\n",
      "      \"loss\": 0.0291,\n",
      "      \"step\": 35700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.250795551214594,\n",
      "      \"grad_norm\": 0.2835122346878052,\n",
      "      \"learning_rate\": 3.8751732829237555e-05,\n",
      "      \"loss\": 0.0233,\n",
      "      \"step\": 35720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.252055830366426,\n",
      "      \"grad_norm\": 4.90279483795166,\n",
      "      \"learning_rate\": 3.8745431632010084e-05,\n",
      "      \"loss\": 0.0484,\n",
      "      \"step\": 35740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2533161095182583,\n",
      "      \"grad_norm\": 5.020031452178955,\n",
      "      \"learning_rate\": 3.8739130434782613e-05,\n",
      "      \"loss\": 0.0331,\n",
      "      \"step\": 35760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2545763886700905,\n",
      "      \"grad_norm\": 0.014727977104485035,\n",
      "      \"learning_rate\": 3.8732829237555136e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 35780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2558366678219226,\n",
      "      \"grad_norm\": 0.0563390851020813,\n",
      "      \"learning_rate\": 3.8726528040327665e-05,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 35800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2570969469737547,\n",
      "      \"grad_norm\": 8.365666389465332,\n",
      "      \"learning_rate\": 3.8720226843100194e-05,\n",
      "      \"loss\": 0.1116,\n",
      "      \"step\": 35820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.258357226125587,\n",
      "      \"grad_norm\": 0.0687992200255394,\n",
      "      \"learning_rate\": 3.8713925645872716e-05,\n",
      "      \"loss\": 0.002,\n",
      "      \"step\": 35840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.259617505277419,\n",
      "      \"grad_norm\": 0.014441742561757565,\n",
      "      \"learning_rate\": 3.8707624448645245e-05,\n",
      "      \"loss\": 0.0474,\n",
      "      \"step\": 35860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.260877784429251,\n",
      "      \"grad_norm\": 0.20606467127799988,\n",
      "      \"learning_rate\": 3.870132325141777e-05,\n",
      "      \"loss\": 0.0116,\n",
      "      \"step\": 35880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2621380635810833,\n",
      "      \"grad_norm\": 0.013041127473115921,\n",
      "      \"learning_rate\": 3.8695022054190296e-05,\n",
      "      \"loss\": 0.023,\n",
      "      \"step\": 35900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2633983427329154,\n",
      "      \"grad_norm\": 0.0156394112855196,\n",
      "      \"learning_rate\": 3.8688720856962825e-05,\n",
      "      \"loss\": 0.0455,\n",
      "      \"step\": 35920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2646586218847475,\n",
      "      \"grad_norm\": 0.21375279128551483,\n",
      "      \"learning_rate\": 3.8682419659735354e-05,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 35940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2659189010365797,\n",
      "      \"grad_norm\": 0.024869410321116447,\n",
      "      \"learning_rate\": 3.867611846250788e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 35960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.267179180188412,\n",
      "      \"grad_norm\": 0.002084028208628297,\n",
      "      \"learning_rate\": 3.8669817265280406e-05,\n",
      "      \"loss\": 0.0343,\n",
      "      \"step\": 35980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.268439459340244,\n",
      "      \"grad_norm\": 0.01116291806101799,\n",
      "      \"learning_rate\": 3.866351606805293e-05,\n",
      "      \"loss\": 0.0465,\n",
      "      \"step\": 36000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.269699738492076,\n",
      "      \"grad_norm\": 0.027780303731560707,\n",
      "      \"learning_rate\": 3.865721487082546e-05,\n",
      "      \"loss\": 0.104,\n",
      "      \"step\": 36020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.270960017643908,\n",
      "      \"grad_norm\": 0.10390398651361465,\n",
      "      \"learning_rate\": 3.8650913673597986e-05,\n",
      "      \"loss\": 0.037,\n",
      "      \"step\": 36040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2722202967957403,\n",
      "      \"grad_norm\": 0.013199049979448318,\n",
      "      \"learning_rate\": 3.8644612476370515e-05,\n",
      "      \"loss\": 0.0283,\n",
      "      \"step\": 36060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2734805759475725,\n",
      "      \"grad_norm\": 0.02581375651061535,\n",
      "      \"learning_rate\": 3.8638311279143044e-05,\n",
      "      \"loss\": 0.0061,\n",
      "      \"step\": 36080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2747408550994046,\n",
      "      \"grad_norm\": 0.02556166797876358,\n",
      "      \"learning_rate\": 3.8632010081915566e-05,\n",
      "      \"loss\": 0.0303,\n",
      "      \"step\": 36100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2760011342512367,\n",
      "      \"grad_norm\": 0.13524070382118225,\n",
      "      \"learning_rate\": 3.8625708884688095e-05,\n",
      "      \"loss\": 0.0519,\n",
      "      \"step\": 36120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.277261413403069,\n",
      "      \"grad_norm\": 6.283788681030273,\n",
      "      \"learning_rate\": 3.861940768746062e-05,\n",
      "      \"loss\": 0.0358,\n",
      "      \"step\": 36140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.278521692554901,\n",
      "      \"grad_norm\": 2.487431049346924,\n",
      "      \"learning_rate\": 3.861310649023315e-05,\n",
      "      \"loss\": 0.0018,\n",
      "      \"step\": 36160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.279781971706733,\n",
      "      \"grad_norm\": 0.041708242148160934,\n",
      "      \"learning_rate\": 3.860680529300567e-05,\n",
      "      \"loss\": 0.0412,\n",
      "      \"step\": 36180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2810422508585653,\n",
      "      \"grad_norm\": 0.011743990704417229,\n",
      "      \"learning_rate\": 3.86005040957782e-05,\n",
      "      \"loss\": 0.0421,\n",
      "      \"step\": 36200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2823025300103974,\n",
      "      \"grad_norm\": 0.32070037722587585,\n",
      "      \"learning_rate\": 3.859420289855073e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 36220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2835628091622295,\n",
      "      \"grad_norm\": 0.15542683005332947,\n",
      "      \"learning_rate\": 3.8587901701323256e-05,\n",
      "      \"loss\": 0.0317,\n",
      "      \"step\": 36240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2848230883140617,\n",
      "      \"grad_norm\": 0.023779677227139473,\n",
      "      \"learning_rate\": 3.858160050409578e-05,\n",
      "      \"loss\": 0.0245,\n",
      "      \"step\": 36260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.286083367465894,\n",
      "      \"grad_norm\": 0.1286456286907196,\n",
      "      \"learning_rate\": 3.857529930686831e-05,\n",
      "      \"loss\": 0.0367,\n",
      "      \"step\": 36280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.287343646617726,\n",
      "      \"grad_norm\": 0.050265833735466,\n",
      "      \"learning_rate\": 3.8568998109640837e-05,\n",
      "      \"loss\": 0.0454,\n",
      "      \"step\": 36300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.288603925769558,\n",
      "      \"grad_norm\": 0.05688910558819771,\n",
      "      \"learning_rate\": 3.856269691241336e-05,\n",
      "      \"loss\": 0.0322,\n",
      "      \"step\": 36320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.28986420492139,\n",
      "      \"grad_norm\": 0.0024515283294022083,\n",
      "      \"learning_rate\": 3.855639571518589e-05,\n",
      "      \"loss\": 0.0269,\n",
      "      \"step\": 36340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2911244840732223,\n",
      "      \"grad_norm\": 0.006128398235887289,\n",
      "      \"learning_rate\": 3.855009451795841e-05,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 36360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2923847632250545,\n",
      "      \"grad_norm\": 0.07110512256622314,\n",
      "      \"learning_rate\": 3.854379332073094e-05,\n",
      "      \"loss\": 0.0124,\n",
      "      \"step\": 36380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2936450423768866,\n",
      "      \"grad_norm\": 0.005311820190399885,\n",
      "      \"learning_rate\": 3.853749212350347e-05,\n",
      "      \"loss\": 0.0337,\n",
      "      \"step\": 36400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2949053215287187,\n",
      "      \"grad_norm\": 6.537595748901367,\n",
      "      \"learning_rate\": 3.8531190926276e-05,\n",
      "      \"loss\": 0.0178,\n",
      "      \"step\": 36420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.296165600680551,\n",
      "      \"grad_norm\": 0.07760020345449448,\n",
      "      \"learning_rate\": 3.852488972904852e-05,\n",
      "      \"loss\": 0.0043,\n",
      "      \"step\": 36440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.297425879832383,\n",
      "      \"grad_norm\": 0.0006097493460401893,\n",
      "      \"learning_rate\": 3.851858853182105e-05,\n",
      "      \"loss\": 0.0337,\n",
      "      \"step\": 36460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.298686158984215,\n",
      "      \"grad_norm\": 0.015586815774440765,\n",
      "      \"learning_rate\": 3.851228733459357e-05,\n",
      "      \"loss\": 0.0463,\n",
      "      \"step\": 36480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.2999464381360473,\n",
      "      \"grad_norm\": 0.035266291350126266,\n",
      "      \"learning_rate\": 3.85059861373661e-05,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 36500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3012067172878794,\n",
      "      \"grad_norm\": 0.08899227529764175,\n",
      "      \"learning_rate\": 3.849968494013863e-05,\n",
      "      \"loss\": 0.0316,\n",
      "      \"step\": 36520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3024669964397115,\n",
      "      \"grad_norm\": 0.011300232261419296,\n",
      "      \"learning_rate\": 3.849338374291115e-05,\n",
      "      \"loss\": 0.031,\n",
      "      \"step\": 36540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3037272755915437,\n",
      "      \"grad_norm\": 0.0009815864032134414,\n",
      "      \"learning_rate\": 3.848708254568369e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 36560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.304987554743376,\n",
      "      \"grad_norm\": 0.006857090163975954,\n",
      "      \"learning_rate\": 3.848078134845621e-05,\n",
      "      \"loss\": 0.0489,\n",
      "      \"step\": 36580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.306247833895208,\n",
      "      \"grad_norm\": 0.013036534190177917,\n",
      "      \"learning_rate\": 3.847448015122874e-05,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 36600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.30750811304704,\n",
      "      \"grad_norm\": 17.37676239013672,\n",
      "      \"learning_rate\": 3.846817895400126e-05,\n",
      "      \"loss\": 0.0461,\n",
      "      \"step\": 36620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.308768392198872,\n",
      "      \"grad_norm\": 0.009428419172763824,\n",
      "      \"learning_rate\": 3.846187775677379e-05,\n",
      "      \"loss\": 0.0378,\n",
      "      \"step\": 36640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3100286713507043,\n",
      "      \"grad_norm\": 0.016089491546154022,\n",
      "      \"learning_rate\": 3.845557655954631e-05,\n",
      "      \"loss\": 0.0046,\n",
      "      \"step\": 36660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3112889505025365,\n",
      "      \"grad_norm\": 0.021221058443188667,\n",
      "      \"learning_rate\": 3.844927536231884e-05,\n",
      "      \"loss\": 0.0305,\n",
      "      \"step\": 36680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3125492296543686,\n",
      "      \"grad_norm\": 2.9899730682373047,\n",
      "      \"learning_rate\": 3.844297416509137e-05,\n",
      "      \"loss\": 0.0504,\n",
      "      \"step\": 36700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3138095088062007,\n",
      "      \"grad_norm\": 0.04119446128606796,\n",
      "      \"learning_rate\": 3.84366729678639e-05,\n",
      "      \"loss\": 0.0211,\n",
      "      \"step\": 36720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.315069787958033,\n",
      "      \"grad_norm\": 5.48861837387085,\n",
      "      \"learning_rate\": 3.843037177063643e-05,\n",
      "      \"loss\": 0.0311,\n",
      "      \"step\": 36740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.316330067109865,\n",
      "      \"grad_norm\": 0.0012042135931551456,\n",
      "      \"learning_rate\": 3.842407057340895e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 36760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.317590346261697,\n",
      "      \"grad_norm\": 0.0036651352420449257,\n",
      "      \"learning_rate\": 3.841776937618148e-05,\n",
      "      \"loss\": 0.0378,\n",
      "      \"step\": 36780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3188506254135293,\n",
      "      \"grad_norm\": 0.004182062577456236,\n",
      "      \"learning_rate\": 3.8411468178954e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 36800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3201109045653614,\n",
      "      \"grad_norm\": 1.3981585502624512,\n",
      "      \"learning_rate\": 3.840516698172653e-05,\n",
      "      \"loss\": 0.0665,\n",
      "      \"step\": 36820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3213711837171935,\n",
      "      \"grad_norm\": 0.012613662518560886,\n",
      "      \"learning_rate\": 3.839886578449905e-05,\n",
      "      \"loss\": 0.0059,\n",
      "      \"step\": 36840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3226314628690257,\n",
      "      \"grad_norm\": 0.0021415010560303926,\n",
      "      \"learning_rate\": 3.839256458727158e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 36860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.323891742020858,\n",
      "      \"grad_norm\": 0.021921591833233833,\n",
      "      \"learning_rate\": 3.838626339004411e-05,\n",
      "      \"loss\": 0.0264,\n",
      "      \"step\": 36880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.32515202117269,\n",
      "      \"grad_norm\": 0.0034196560736745596,\n",
      "      \"learning_rate\": 3.837996219281664e-05,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 36900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.326412300324522,\n",
      "      \"grad_norm\": 1.4340962171554565,\n",
      "      \"learning_rate\": 3.837366099558916e-05,\n",
      "      \"loss\": 0.0935,\n",
      "      \"step\": 36920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.327672579476354,\n",
      "      \"grad_norm\": 0.02580364979803562,\n",
      "      \"learning_rate\": 3.836735979836169e-05,\n",
      "      \"loss\": 0.0222,\n",
      "      \"step\": 36940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3289328586281863,\n",
      "      \"grad_norm\": 9.682116508483887,\n",
      "      \"learning_rate\": 3.836105860113422e-05,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 36960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3301931377800185,\n",
      "      \"grad_norm\": 0.009867271408438683,\n",
      "      \"learning_rate\": 3.835475740390674e-05,\n",
      "      \"loss\": 0.0452,\n",
      "      \"step\": 36980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3314534169318506,\n",
      "      \"grad_norm\": 0.02305462956428528,\n",
      "      \"learning_rate\": 3.834845620667927e-05,\n",
      "      \"loss\": 0.0223,\n",
      "      \"step\": 37000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3327136960836823,\n",
      "      \"grad_norm\": 0.03258337453007698,\n",
      "      \"learning_rate\": 3.8342155009451794e-05,\n",
      "      \"loss\": 0.0373,\n",
      "      \"step\": 37020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.333973975235515,\n",
      "      \"grad_norm\": 0.11573263257741928,\n",
      "      \"learning_rate\": 3.833585381222432e-05,\n",
      "      \"loss\": 0.0366,\n",
      "      \"step\": 37040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3352342543873466,\n",
      "      \"grad_norm\": 0.029799122363328934,\n",
      "      \"learning_rate\": 3.832955261499685e-05,\n",
      "      \"loss\": 0.0532,\n",
      "      \"step\": 37060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.336494533539179,\n",
      "      \"grad_norm\": 0.015960972756147385,\n",
      "      \"learning_rate\": 3.832325141776938e-05,\n",
      "      \"loss\": 0.0205,\n",
      "      \"step\": 37080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.337754812691011,\n",
      "      \"grad_norm\": 0.00966063141822815,\n",
      "      \"learning_rate\": 3.83169502205419e-05,\n",
      "      \"loss\": 0.0243,\n",
      "      \"step\": 37100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3390150918428434,\n",
      "      \"grad_norm\": 0.034521445631980896,\n",
      "      \"learning_rate\": 3.831064902331443e-05,\n",
      "      \"loss\": 0.0322,\n",
      "      \"step\": 37120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.340275370994675,\n",
      "      \"grad_norm\": 11.282449722290039,\n",
      "      \"learning_rate\": 3.8304347826086955e-05,\n",
      "      \"loss\": 0.0238,\n",
      "      \"step\": 37140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3415356501465077,\n",
      "      \"grad_norm\": 0.26753950119018555,\n",
      "      \"learning_rate\": 3.8298046628859484e-05,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 37160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3427959292983394,\n",
      "      \"grad_norm\": 4.9940876960754395,\n",
      "      \"learning_rate\": 3.829174543163201e-05,\n",
      "      \"loss\": 0.02,\n",
      "      \"step\": 37180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.344056208450172,\n",
      "      \"grad_norm\": 0.0619150847196579,\n",
      "      \"learning_rate\": 3.828544423440454e-05,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 37200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3453164876020036,\n",
      "      \"grad_norm\": 0.004831541329622269,\n",
      "      \"learning_rate\": 3.827914303717707e-05,\n",
      "      \"loss\": 0.0454,\n",
      "      \"step\": 37220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.346576766753836,\n",
      "      \"grad_norm\": 12.931707382202148,\n",
      "      \"learning_rate\": 3.827284183994959e-05,\n",
      "      \"loss\": 0.0492,\n",
      "      \"step\": 37240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.347837045905668,\n",
      "      \"grad_norm\": 4.641165256500244,\n",
      "      \"learning_rate\": 3.826654064272212e-05,\n",
      "      \"loss\": 0.037,\n",
      "      \"step\": 37260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3490973250575005,\n",
      "      \"grad_norm\": 0.009503315202891827,\n",
      "      \"learning_rate\": 3.8260239445494644e-05,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 37280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.350357604209332,\n",
      "      \"grad_norm\": 0.001273035304620862,\n",
      "      \"learning_rate\": 3.825393824826717e-05,\n",
      "      \"loss\": 0.0124,\n",
      "      \"step\": 37300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3516178833611647,\n",
      "      \"grad_norm\": 0.19734157621860504,\n",
      "      \"learning_rate\": 3.8247637051039696e-05,\n",
      "      \"loss\": 0.0545,\n",
      "      \"step\": 37320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3528781625129964,\n",
      "      \"grad_norm\": 0.02011774852871895,\n",
      "      \"learning_rate\": 3.8241335853812225e-05,\n",
      "      \"loss\": 0.0021,\n",
      "      \"step\": 37340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.354138441664829,\n",
      "      \"grad_norm\": 0.007449548225849867,\n",
      "      \"learning_rate\": 3.8235034656584754e-05,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 37360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3553987208166607,\n",
      "      \"grad_norm\": 0.014422566629946232,\n",
      "      \"learning_rate\": 3.822873345935728e-05,\n",
      "      \"loss\": 0.0477,\n",
      "      \"step\": 37380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3566589999684933,\n",
      "      \"grad_norm\": 0.03694730997085571,\n",
      "      \"learning_rate\": 3.822243226212981e-05,\n",
      "      \"loss\": 0.0313,\n",
      "      \"step\": 37400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.357919279120325,\n",
      "      \"grad_norm\": 0.06846363097429276,\n",
      "      \"learning_rate\": 3.8216131064902334e-05,\n",
      "      \"loss\": 0.0465,\n",
      "      \"step\": 37420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3591795582721575,\n",
      "      \"grad_norm\": 0.21667112410068512,\n",
      "      \"learning_rate\": 3.820982986767486e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 37440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.360439837423989,\n",
      "      \"grad_norm\": 0.027597807347774506,\n",
      "      \"learning_rate\": 3.8203528670447385e-05,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 37460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3617001165758214,\n",
      "      \"grad_norm\": 0.0027750141452997923,\n",
      "      \"learning_rate\": 3.8197227473219914e-05,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 37480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3629603957276535,\n",
      "      \"grad_norm\": 2.051090717315674,\n",
      "      \"learning_rate\": 3.8190926275992437e-05,\n",
      "      \"loss\": 0.0502,\n",
      "      \"step\": 37500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3642206748794856,\n",
      "      \"grad_norm\": 0.01547189336270094,\n",
      "      \"learning_rate\": 3.8184625078764966e-05,\n",
      "      \"loss\": 0.0575,\n",
      "      \"step\": 37520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3654809540313178,\n",
      "      \"grad_norm\": 0.02213175781071186,\n",
      "      \"learning_rate\": 3.8178323881537495e-05,\n",
      "      \"loss\": 0.019,\n",
      "      \"step\": 37540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.36674123318315,\n",
      "      \"grad_norm\": 0.027659989893436432,\n",
      "      \"learning_rate\": 3.8172022684310024e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 37560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.368001512334982,\n",
      "      \"grad_norm\": 0.014940012246370316,\n",
      "      \"learning_rate\": 3.8165721487082546e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 37580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.369261791486814,\n",
      "      \"grad_norm\": 0.025609444826841354,\n",
      "      \"learning_rate\": 3.8159420289855075e-05,\n",
      "      \"loss\": 0.0261,\n",
      "      \"step\": 37600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3705220706386463,\n",
      "      \"grad_norm\": 0.03774374723434448,\n",
      "      \"learning_rate\": 3.81531190926276e-05,\n",
      "      \"loss\": 0.0293,\n",
      "      \"step\": 37620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3717823497904784,\n",
      "      \"grad_norm\": 0.014940536580979824,\n",
      "      \"learning_rate\": 3.8146817895400126e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 37640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3730426289423106,\n",
      "      \"grad_norm\": 0.005916012451052666,\n",
      "      \"learning_rate\": 3.8140516698172655e-05,\n",
      "      \"loss\": 0.0021,\n",
      "      \"step\": 37660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3743029080941427,\n",
      "      \"grad_norm\": 0.05371337756514549,\n",
      "      \"learning_rate\": 3.813421550094518e-05,\n",
      "      \"loss\": 0.0408,\n",
      "      \"step\": 37680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.375563187245975,\n",
      "      \"grad_norm\": 0.035701725631952286,\n",
      "      \"learning_rate\": 3.812791430371771e-05,\n",
      "      \"loss\": 0.0314,\n",
      "      \"step\": 37700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.376823466397807,\n",
      "      \"grad_norm\": 0.05074141547083855,\n",
      "      \"learning_rate\": 3.8121613106490236e-05,\n",
      "      \"loss\": 0.013,\n",
      "      \"step\": 37720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.378083745549639,\n",
      "      \"grad_norm\": 4.494729042053223,\n",
      "      \"learning_rate\": 3.8115311909262765e-05,\n",
      "      \"loss\": 0.0594,\n",
      "      \"step\": 37740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.379344024701471,\n",
      "      \"grad_norm\": 0.3165304660797119,\n",
      "      \"learning_rate\": 3.810901071203529e-05,\n",
      "      \"loss\": 0.0174,\n",
      "      \"step\": 37760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3806043038533033,\n",
      "      \"grad_norm\": 0.0037636018823832273,\n",
      "      \"learning_rate\": 3.8102709514807816e-05,\n",
      "      \"loss\": 0.003,\n",
      "      \"step\": 37780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3818645830051355,\n",
      "      \"grad_norm\": 0.10503719002008438,\n",
      "      \"learning_rate\": 3.809640831758034e-05,\n",
      "      \"loss\": 0.0237,\n",
      "      \"step\": 37800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3831248621569676,\n",
      "      \"grad_norm\": 0.2008758783340454,\n",
      "      \"learning_rate\": 3.809010712035287e-05,\n",
      "      \"loss\": 0.0289,\n",
      "      \"step\": 37820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3843851413087997,\n",
      "      \"grad_norm\": 4.654323101043701,\n",
      "      \"learning_rate\": 3.808380592312539e-05,\n",
      "      \"loss\": 0.0556,\n",
      "      \"step\": 37840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.385645420460632,\n",
      "      \"grad_norm\": 0.008523290045559406,\n",
      "      \"learning_rate\": 3.8077504725897925e-05,\n",
      "      \"loss\": 0.0288,\n",
      "      \"step\": 37860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.386905699612464,\n",
      "      \"grad_norm\": 0.01607896015048027,\n",
      "      \"learning_rate\": 3.8071203528670454e-05,\n",
      "      \"loss\": 0.0425,\n",
      "      \"step\": 37880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.388165978764296,\n",
      "      \"grad_norm\": 0.004856565967202187,\n",
      "      \"learning_rate\": 3.806490233144298e-05,\n",
      "      \"loss\": 0.0041,\n",
      "      \"step\": 37900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3894262579161283,\n",
      "      \"grad_norm\": 0.10053007304668427,\n",
      "      \"learning_rate\": 3.8058601134215506e-05,\n",
      "      \"loss\": 0.0252,\n",
      "      \"step\": 37920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3906865370679604,\n",
      "      \"grad_norm\": 0.00717555359005928,\n",
      "      \"learning_rate\": 3.805229993698803e-05,\n",
      "      \"loss\": 0.0145,\n",
      "      \"step\": 37940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3919468162197925,\n",
      "      \"grad_norm\": 0.1081068366765976,\n",
      "      \"learning_rate\": 3.804599873976056e-05,\n",
      "      \"loss\": 0.037,\n",
      "      \"step\": 37960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3932070953716247,\n",
      "      \"grad_norm\": 0.09030026942491531,\n",
      "      \"learning_rate\": 3.803969754253308e-05,\n",
      "      \"loss\": 0.0266,\n",
      "      \"step\": 37980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.394467374523457,\n",
      "      \"grad_norm\": 0.013170061632990837,\n",
      "      \"learning_rate\": 3.803371140516698e-05,\n",
      "      \"loss\": 0.0445,\n",
      "      \"step\": 38000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.395727653675289,\n",
      "      \"grad_norm\": 0.0021761273965239525,\n",
      "      \"learning_rate\": 3.802741020793951e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 38020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.396987932827121,\n",
      "      \"grad_norm\": 11.93361759185791,\n",
      "      \"learning_rate\": 3.802110901071204e-05,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 38040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.398248211978953,\n",
      "      \"grad_norm\": 5.271316051483154,\n",
      "      \"learning_rate\": 3.801480781348457e-05,\n",
      "      \"loss\": 0.0823,\n",
      "      \"step\": 38060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3995084911307853,\n",
      "      \"grad_norm\": 0.02567063458263874,\n",
      "      \"learning_rate\": 3.800850661625709e-05,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 38080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4007687702826175,\n",
      "      \"grad_norm\": 0.23965202271938324,\n",
      "      \"learning_rate\": 3.800220541902962e-05,\n",
      "      \"loss\": 0.0205,\n",
      "      \"step\": 38100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4020290494344496,\n",
      "      \"grad_norm\": 0.028229061514139175,\n",
      "      \"learning_rate\": 3.799590422180214e-05,\n",
      "      \"loss\": 0.0506,\n",
      "      \"step\": 38120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4032893285862817,\n",
      "      \"grad_norm\": 6.485469341278076,\n",
      "      \"learning_rate\": 3.798960302457467e-05,\n",
      "      \"loss\": 0.0564,\n",
      "      \"step\": 38140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.404549607738114,\n",
      "      \"grad_norm\": 1.0601099729537964,\n",
      "      \"learning_rate\": 3.798330182734719e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 38160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.405809886889946,\n",
      "      \"grad_norm\": 0.012059659697115421,\n",
      "      \"learning_rate\": 3.797700063011972e-05,\n",
      "      \"loss\": 0.0383,\n",
      "      \"step\": 38180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.407070166041778,\n",
      "      \"grad_norm\": 0.0022999246139079332,\n",
      "      \"learning_rate\": 3.797069943289226e-05,\n",
      "      \"loss\": 0.0501,\n",
      "      \"step\": 38200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4083304451936103,\n",
      "      \"grad_norm\": 5.138985633850098,\n",
      "      \"learning_rate\": 3.796439823566478e-05,\n",
      "      \"loss\": 0.0706,\n",
      "      \"step\": 38220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4095907243454424,\n",
      "      \"grad_norm\": 0.025630030781030655,\n",
      "      \"learning_rate\": 3.795809703843731e-05,\n",
      "      \"loss\": 0.0429,\n",
      "      \"step\": 38240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4108510034972745,\n",
      "      \"grad_norm\": 0.05337449535727501,\n",
      "      \"learning_rate\": 3.795179584120983e-05,\n",
      "      \"loss\": 0.0308,\n",
      "      \"step\": 38260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4121112826491067,\n",
      "      \"grad_norm\": 0.10093989968299866,\n",
      "      \"learning_rate\": 3.794549464398236e-05,\n",
      "      \"loss\": 0.0105,\n",
      "      \"step\": 38280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.413371561800939,\n",
      "      \"grad_norm\": 0.009875266812741756,\n",
      "      \"learning_rate\": 3.793919344675488e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 38300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.414631840952771,\n",
      "      \"grad_norm\": 0.009539654478430748,\n",
      "      \"learning_rate\": 3.793289224952741e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 38320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.415892120104603,\n",
      "      \"grad_norm\": 0.6066067218780518,\n",
      "      \"learning_rate\": 3.792659105229994e-05,\n",
      "      \"loss\": 0.0206,\n",
      "      \"step\": 38340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.417152399256435,\n",
      "      \"grad_norm\": 0.08782538771629333,\n",
      "      \"learning_rate\": 3.792028985507247e-05,\n",
      "      \"loss\": 0.0129,\n",
      "      \"step\": 38360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4184126784082673,\n",
      "      \"grad_norm\": 0.009883259423077106,\n",
      "      \"learning_rate\": 3.791398865784499e-05,\n",
      "      \"loss\": 0.0529,\n",
      "      \"step\": 38380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4196729575600995,\n",
      "      \"grad_norm\": 0.007034139707684517,\n",
      "      \"learning_rate\": 3.790768746061752e-05,\n",
      "      \"loss\": 0.032,\n",
      "      \"step\": 38400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4209332367119316,\n",
      "      \"grad_norm\": 0.016852036118507385,\n",
      "      \"learning_rate\": 3.790138626339005e-05,\n",
      "      \"loss\": 0.0603,\n",
      "      \"step\": 38420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4221935158637637,\n",
      "      \"grad_norm\": 0.5615798830986023,\n",
      "      \"learning_rate\": 3.789508506616257e-05,\n",
      "      \"loss\": 0.0285,\n",
      "      \"step\": 38440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.423453795015596,\n",
      "      \"grad_norm\": 0.03491221368312836,\n",
      "      \"learning_rate\": 3.78887838689351e-05,\n",
      "      \"loss\": 0.0277,\n",
      "      \"step\": 38460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.424714074167428,\n",
      "      \"grad_norm\": 6.904287338256836,\n",
      "      \"learning_rate\": 3.788248267170762e-05,\n",
      "      \"loss\": 0.0331,\n",
      "      \"step\": 38480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.42597435331926,\n",
      "      \"grad_norm\": 0.6747028827667236,\n",
      "      \"learning_rate\": 3.787618147448015e-05,\n",
      "      \"loss\": 0.0264,\n",
      "      \"step\": 38500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4272346324710923,\n",
      "      \"grad_norm\": 0.03512069955468178,\n",
      "      \"learning_rate\": 3.786988027725268e-05,\n",
      "      \"loss\": 0.0056,\n",
      "      \"step\": 38520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4284949116229244,\n",
      "      \"grad_norm\": 0.013015644624829292,\n",
      "      \"learning_rate\": 3.786357908002521e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 38540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4297551907747565,\n",
      "      \"grad_norm\": 0.021527040749788284,\n",
      "      \"learning_rate\": 3.785727788279773e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 38560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4310154699265887,\n",
      "      \"grad_norm\": 0.0010035991435870528,\n",
      "      \"learning_rate\": 3.785097668557026e-05,\n",
      "      \"loss\": 0.0248,\n",
      "      \"step\": 38580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.432275749078421,\n",
      "      \"grad_norm\": 0.01054706983268261,\n",
      "      \"learning_rate\": 3.7844675488342784e-05,\n",
      "      \"loss\": 0.0322,\n",
      "      \"step\": 38600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.433536028230253,\n",
      "      \"grad_norm\": 0.002114906208589673,\n",
      "      \"learning_rate\": 3.783837429111531e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 38620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.434796307382085,\n",
      "      \"grad_norm\": 0.41397824883461,\n",
      "      \"learning_rate\": 3.783207309388784e-05,\n",
      "      \"loss\": 0.0211,\n",
      "      \"step\": 38640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.436056586533917,\n",
      "      \"grad_norm\": 0.03865022957324982,\n",
      "      \"learning_rate\": 3.7825771896660364e-05,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 38660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4373168656857493,\n",
      "      \"grad_norm\": 0.001261247438378632,\n",
      "      \"learning_rate\": 3.781947069943289e-05,\n",
      "      \"loss\": 0.0188,\n",
      "      \"step\": 38680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4385771448375815,\n",
      "      \"grad_norm\": 0.002073464449495077,\n",
      "      \"learning_rate\": 3.781316950220542e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 38700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4398374239894136,\n",
      "      \"grad_norm\": 8.542803764343262,\n",
      "      \"learning_rate\": 3.780686830497795e-05,\n",
      "      \"loss\": 0.0332,\n",
      "      \"step\": 38720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4410977031412457,\n",
      "      \"grad_norm\": 0.0005214374396018684,\n",
      "      \"learning_rate\": 3.780056710775047e-05,\n",
      "      \"loss\": 0.0468,\n",
      "      \"step\": 38740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.442357982293078,\n",
      "      \"grad_norm\": 0.01857198216021061,\n",
      "      \"learning_rate\": 3.7794265910523e-05,\n",
      "      \"loss\": 0.0044,\n",
      "      \"step\": 38760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.44361826144491,\n",
      "      \"grad_norm\": 12.199918746948242,\n",
      "      \"learning_rate\": 3.7787964713295525e-05,\n",
      "      \"loss\": 0.1024,\n",
      "      \"step\": 38780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.444878540596742,\n",
      "      \"grad_norm\": 0.004641812294721603,\n",
      "      \"learning_rate\": 3.7781663516068054e-05,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 38800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4461388197485743,\n",
      "      \"grad_norm\": 1.11799156665802,\n",
      "      \"learning_rate\": 3.7775362318840576e-05,\n",
      "      \"loss\": 0.0215,\n",
      "      \"step\": 38820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4473990989004064,\n",
      "      \"grad_norm\": 0.09124330431222916,\n",
      "      \"learning_rate\": 3.776906112161311e-05,\n",
      "      \"loss\": 0.0267,\n",
      "      \"step\": 38840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4486593780522385,\n",
      "      \"grad_norm\": 0.0033982249442487955,\n",
      "      \"learning_rate\": 3.776275992438564e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 38860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4499196572040707,\n",
      "      \"grad_norm\": 0.01798505336046219,\n",
      "      \"learning_rate\": 3.775645872715816e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 38880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.451179936355903,\n",
      "      \"grad_norm\": 0.011119969189167023,\n",
      "      \"learning_rate\": 3.775015752993069e-05,\n",
      "      \"loss\": 0.0465,\n",
      "      \"step\": 38900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.452440215507735,\n",
      "      \"grad_norm\": 0.2051292359828949,\n",
      "      \"learning_rate\": 3.7743856332703214e-05,\n",
      "      \"loss\": 0.0408,\n",
      "      \"step\": 38920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.453700494659567,\n",
      "      \"grad_norm\": 0.07631293684244156,\n",
      "      \"learning_rate\": 3.7737555135475743e-05,\n",
      "      \"loss\": 0.0458,\n",
      "      \"step\": 38940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.454960773811399,\n",
      "      \"grad_norm\": 0.03499255329370499,\n",
      "      \"learning_rate\": 3.7731253938248266e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 38960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4562210529632313,\n",
      "      \"grad_norm\": 0.05165452882647514,\n",
      "      \"learning_rate\": 3.7724952741020795e-05,\n",
      "      \"loss\": 0.0067,\n",
      "      \"step\": 38980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4574813321150635,\n",
      "      \"grad_norm\": 0.00175671826582402,\n",
      "      \"learning_rate\": 3.7718651543793324e-05,\n",
      "      \"loss\": 0.0433,\n",
      "      \"step\": 39000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4587416112668956,\n",
      "      \"grad_norm\": 0.0035578603856265545,\n",
      "      \"learning_rate\": 3.771235034656585e-05,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 39020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4600018904187277,\n",
      "      \"grad_norm\": 0.03461030498147011,\n",
      "      \"learning_rate\": 3.7706049149338375e-05,\n",
      "      \"loss\": 0.0264,\n",
      "      \"step\": 39040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.46126216957056,\n",
      "      \"grad_norm\": 0.0683397501707077,\n",
      "      \"learning_rate\": 3.7699747952110904e-05,\n",
      "      \"loss\": 0.0474,\n",
      "      \"step\": 39060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.462522448722392,\n",
      "      \"grad_norm\": 0.0059998114593327045,\n",
      "      \"learning_rate\": 3.7693446754883426e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 39080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.463782727874224,\n",
      "      \"grad_norm\": 0.0466841459274292,\n",
      "      \"learning_rate\": 3.7687145557655955e-05,\n",
      "      \"loss\": 0.0441,\n",
      "      \"step\": 39100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4650430070260563,\n",
      "      \"grad_norm\": 5.447483539581299,\n",
      "      \"learning_rate\": 3.7680844360428484e-05,\n",
      "      \"loss\": 0.074,\n",
      "      \"step\": 39120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4663032861778884,\n",
      "      \"grad_norm\": 0.07614393532276154,\n",
      "      \"learning_rate\": 3.767454316320101e-05,\n",
      "      \"loss\": 0.039,\n",
      "      \"step\": 39140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4675635653297205,\n",
      "      \"grad_norm\": 0.004745021462440491,\n",
      "      \"learning_rate\": 3.7668241965973536e-05,\n",
      "      \"loss\": 0.0345,\n",
      "      \"step\": 39160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4688238444815527,\n",
      "      \"grad_norm\": 0.00422444473952055,\n",
      "      \"learning_rate\": 3.7661940768746065e-05,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 39180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.470084123633385,\n",
      "      \"grad_norm\": 0.0292112585157156,\n",
      "      \"learning_rate\": 3.7655639571518594e-05,\n",
      "      \"loss\": 0.0165,\n",
      "      \"step\": 39200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.471344402785217,\n",
      "      \"grad_norm\": 0.13537625968456268,\n",
      "      \"learning_rate\": 3.7649338374291116e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 39220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.472604681937049,\n",
      "      \"grad_norm\": 6.890130519866943,\n",
      "      \"learning_rate\": 3.7643037177063645e-05,\n",
      "      \"loss\": 0.0374,\n",
      "      \"step\": 39240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.473864961088881,\n",
      "      \"grad_norm\": 0.06380186229944229,\n",
      "      \"learning_rate\": 3.763673597983617e-05,\n",
      "      \"loss\": 0.026,\n",
      "      \"step\": 39260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4751252402407133,\n",
      "      \"grad_norm\": 1.7948890924453735,\n",
      "      \"learning_rate\": 3.7630434782608696e-05,\n",
      "      \"loss\": 0.0357,\n",
      "      \"step\": 39280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4763855193925455,\n",
      "      \"grad_norm\": 0.03905072808265686,\n",
      "      \"learning_rate\": 3.762413358538122e-05,\n",
      "      \"loss\": 0.0387,\n",
      "      \"step\": 39300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4776457985443776,\n",
      "      \"grad_norm\": 0.07271962612867355,\n",
      "      \"learning_rate\": 3.761783238815375e-05,\n",
      "      \"loss\": 0.0222,\n",
      "      \"step\": 39320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4789060776962097,\n",
      "      \"grad_norm\": 0.023115020245313644,\n",
      "      \"learning_rate\": 3.7611531190926284e-05,\n",
      "      \"loss\": 0.0517,\n",
      "      \"step\": 39340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.480166356848042,\n",
      "      \"grad_norm\": 0.11028986424207687,\n",
      "      \"learning_rate\": 3.7605229993698806e-05,\n",
      "      \"loss\": 0.0359,\n",
      "      \"step\": 39360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.481426635999874,\n",
      "      \"grad_norm\": 0.032816559076309204,\n",
      "      \"learning_rate\": 3.7598928796471335e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 39380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.482686915151706,\n",
      "      \"grad_norm\": 0.020208431407809258,\n",
      "      \"learning_rate\": 3.759262759924386e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 39400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4839471943035383,\n",
      "      \"grad_norm\": 0.006190390791743994,\n",
      "      \"learning_rate\": 3.7586326402016386e-05,\n",
      "      \"loss\": 0.026,\n",
      "      \"step\": 39420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4852074734553704,\n",
      "      \"grad_norm\": 0.0012938050786033273,\n",
      "      \"learning_rate\": 3.758002520478891e-05,\n",
      "      \"loss\": 0.0159,\n",
      "      \"step\": 39440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4864677526072025,\n",
      "      \"grad_norm\": 0.004433105234056711,\n",
      "      \"learning_rate\": 3.757372400756144e-05,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 39460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4877280317590347,\n",
      "      \"grad_norm\": 0.035553377121686935,\n",
      "      \"learning_rate\": 3.7567422810333966e-05,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 39480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.488988310910867,\n",
      "      \"grad_norm\": 0.1879391223192215,\n",
      "      \"learning_rate\": 3.7561121613106496e-05,\n",
      "      \"loss\": 0.0585,\n",
      "      \"step\": 39500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.490248590062699,\n",
      "      \"grad_norm\": 4.0274858474731445,\n",
      "      \"learning_rate\": 3.755482041587902e-05,\n",
      "      \"loss\": 0.0493,\n",
      "      \"step\": 39520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.491508869214531,\n",
      "      \"grad_norm\": 0.007676196750253439,\n",
      "      \"learning_rate\": 3.754851921865155e-05,\n",
      "      \"loss\": 0.004,\n",
      "      \"step\": 39540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.492769148366363,\n",
      "      \"grad_norm\": 0.011202430352568626,\n",
      "      \"learning_rate\": 3.7542218021424076e-05,\n",
      "      \"loss\": 0.0393,\n",
      "      \"step\": 39560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4940294275181953,\n",
      "      \"grad_norm\": 0.2234639674425125,\n",
      "      \"learning_rate\": 3.75359168241966e-05,\n",
      "      \"loss\": 0.0288,\n",
      "      \"step\": 39580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4952897066700275,\n",
      "      \"grad_norm\": 11.548147201538086,\n",
      "      \"learning_rate\": 3.752961562696913e-05,\n",
      "      \"loss\": 0.038,\n",
      "      \"step\": 39600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4965499858218596,\n",
      "      \"grad_norm\": 0.19936959445476532,\n",
      "      \"learning_rate\": 3.752331442974165e-05,\n",
      "      \"loss\": 0.0371,\n",
      "      \"step\": 39620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4978102649736917,\n",
      "      \"grad_norm\": 7.0568037033081055,\n",
      "      \"learning_rate\": 3.751701323251418e-05,\n",
      "      \"loss\": 0.0261,\n",
      "      \"step\": 39640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.499070544125524,\n",
      "      \"grad_norm\": 0.01545361801981926,\n",
      "      \"learning_rate\": 3.751071203528671e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 39660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.500330823277356,\n",
      "      \"grad_norm\": 0.003955660853534937,\n",
      "      \"learning_rate\": 3.7504410838059237e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 39680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.501591102429188,\n",
      "      \"grad_norm\": 0.36241665482521057,\n",
      "      \"learning_rate\": 3.749810964083176e-05,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 39700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5028513815810203,\n",
      "      \"grad_norm\": 12.202237129211426,\n",
      "      \"learning_rate\": 3.749180844360429e-05,\n",
      "      \"loss\": 0.0392,\n",
      "      \"step\": 39720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5041116607328524,\n",
      "      \"grad_norm\": 0.045785754919052124,\n",
      "      \"learning_rate\": 3.748550724637681e-05,\n",
      "      \"loss\": 0.0159,\n",
      "      \"step\": 39740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5053719398846845,\n",
      "      \"grad_norm\": 0.009932938031852245,\n",
      "      \"learning_rate\": 3.747920604914934e-05,\n",
      "      \"loss\": 0.0575,\n",
      "      \"step\": 39760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5066322190365167,\n",
      "      \"grad_norm\": 0.05010039359331131,\n",
      "      \"learning_rate\": 3.747290485192187e-05,\n",
      "      \"loss\": 0.0769,\n",
      "      \"step\": 39780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.507892498188349,\n",
      "      \"grad_norm\": 0.02413087710738182,\n",
      "      \"learning_rate\": 3.746660365469439e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 39800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.509152777340181,\n",
      "      \"grad_norm\": 0.012851000763475895,\n",
      "      \"learning_rate\": 3.746030245746692e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 39820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.510413056492013,\n",
      "      \"grad_norm\": 0.08081550151109695,\n",
      "      \"learning_rate\": 3.745400126023945e-05,\n",
      "      \"loss\": 0.0229,\n",
      "      \"step\": 39840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.511673335643845,\n",
      "      \"grad_norm\": 7.1014323234558105,\n",
      "      \"learning_rate\": 3.744770006301198e-05,\n",
      "      \"loss\": 0.103,\n",
      "      \"step\": 39860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5129336147956773,\n",
      "      \"grad_norm\": 5.321142196655273,\n",
      "      \"learning_rate\": 3.74413988657845e-05,\n",
      "      \"loss\": 0.0745,\n",
      "      \"step\": 39880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5141938939475095,\n",
      "      \"grad_norm\": 0.029828066006302834,\n",
      "      \"learning_rate\": 3.743509766855703e-05,\n",
      "      \"loss\": 0.021,\n",
      "      \"step\": 39900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5154541730993416,\n",
      "      \"grad_norm\": 0.01311588566750288,\n",
      "      \"learning_rate\": 3.742879647132955e-05,\n",
      "      \"loss\": 0.0635,\n",
      "      \"step\": 39920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5167144522511737,\n",
      "      \"grad_norm\": 0.02349197492003441,\n",
      "      \"learning_rate\": 3.742249527410208e-05,\n",
      "      \"loss\": 0.0567,\n",
      "      \"step\": 39940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.517974731403006,\n",
      "      \"grad_norm\": 0.01833203248679638,\n",
      "      \"learning_rate\": 3.74161940768746e-05,\n",
      "      \"loss\": 0.0376,\n",
      "      \"step\": 39960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.519235010554838,\n",
      "      \"grad_norm\": 0.016479874029755592,\n",
      "      \"learning_rate\": 3.740989287964713e-05,\n",
      "      \"loss\": 0.0421,\n",
      "      \"step\": 39980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.52049528970667,\n",
      "      \"grad_norm\": 0.005730504635721445,\n",
      "      \"learning_rate\": 3.740359168241967e-05,\n",
      "      \"loss\": 0.0205,\n",
      "      \"step\": 40000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5217555688585023,\n",
      "      \"grad_norm\": 0.010835899971425533,\n",
      "      \"learning_rate\": 3.739729048519219e-05,\n",
      "      \"loss\": 0.0033,\n",
      "      \"step\": 40020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5230158480103344,\n",
      "      \"grad_norm\": 0.03487284854054451,\n",
      "      \"learning_rate\": 3.739098928796472e-05,\n",
      "      \"loss\": 0.0539,\n",
      "      \"step\": 40040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5242761271621665,\n",
      "      \"grad_norm\": 0.027709996327757835,\n",
      "      \"learning_rate\": 3.738468809073724e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 40060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5255364063139987,\n",
      "      \"grad_norm\": 0.08321843296289444,\n",
      "      \"learning_rate\": 3.737838689350977e-05,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 40080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.526796685465831,\n",
      "      \"grad_norm\": 0.0019975982140749693,\n",
      "      \"learning_rate\": 3.737208569628229e-05,\n",
      "      \"loss\": 0.0276,\n",
      "      \"step\": 40100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.528056964617663,\n",
      "      \"grad_norm\": 0.001789177069440484,\n",
      "      \"learning_rate\": 3.736578449905482e-05,\n",
      "      \"loss\": 0.0131,\n",
      "      \"step\": 40120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.529317243769495,\n",
      "      \"grad_norm\": 0.38755837082862854,\n",
      "      \"learning_rate\": 3.735948330182735e-05,\n",
      "      \"loss\": 0.033,\n",
      "      \"step\": 40140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.530577522921327,\n",
      "      \"grad_norm\": 0.0029429346323013306,\n",
      "      \"learning_rate\": 3.735318210459988e-05,\n",
      "      \"loss\": 0.0272,\n",
      "      \"step\": 40160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5318378020731593,\n",
      "      \"grad_norm\": 0.3506234884262085,\n",
      "      \"learning_rate\": 3.73468809073724e-05,\n",
      "      \"loss\": 0.0273,\n",
      "      \"step\": 40180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5330980812249915,\n",
      "      \"grad_norm\": 0.03159360960125923,\n",
      "      \"learning_rate\": 3.734057971014493e-05,\n",
      "      \"loss\": 0.0258,\n",
      "      \"step\": 40200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5343583603768236,\n",
      "      \"grad_norm\": 6.915365219116211,\n",
      "      \"learning_rate\": 3.733427851291746e-05,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 40220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5356186395286557,\n",
      "      \"grad_norm\": 0.24561326205730438,\n",
      "      \"learning_rate\": 3.732797731568998e-05,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 40240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.536878918680488,\n",
      "      \"grad_norm\": 5.2353739738464355,\n",
      "      \"learning_rate\": 3.732167611846251e-05,\n",
      "      \"loss\": 0.0402,\n",
      "      \"step\": 40260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.53813919783232,\n",
      "      \"grad_norm\": 5.160123348236084,\n",
      "      \"learning_rate\": 3.731537492123503e-05,\n",
      "      \"loss\": 0.0514,\n",
      "      \"step\": 40280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.539399476984152,\n",
      "      \"grad_norm\": 0.034546975046396255,\n",
      "      \"learning_rate\": 3.730907372400756e-05,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 40300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5406597561359843,\n",
      "      \"grad_norm\": 0.06455317884683609,\n",
      "      \"learning_rate\": 3.730277252678009e-05,\n",
      "      \"loss\": 0.0288,\n",
      "      \"step\": 40320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5419200352878164,\n",
      "      \"grad_norm\": 3.009267568588257,\n",
      "      \"learning_rate\": 3.729647132955262e-05,\n",
      "      \"loss\": 0.069,\n",
      "      \"step\": 40340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5431803144396485,\n",
      "      \"grad_norm\": 13.205991744995117,\n",
      "      \"learning_rate\": 3.729017013232514e-05,\n",
      "      \"loss\": 0.037,\n",
      "      \"step\": 40360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5444405935914807,\n",
      "      \"grad_norm\": 0.4368673861026764,\n",
      "      \"learning_rate\": 3.728386893509767e-05,\n",
      "      \"loss\": 0.0145,\n",
      "      \"step\": 40380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.545700872743313,\n",
      "      \"grad_norm\": 5.218903541564941,\n",
      "      \"learning_rate\": 3.7277567737870194e-05,\n",
      "      \"loss\": 0.0678,\n",
      "      \"step\": 40400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.546961151895145,\n",
      "      \"grad_norm\": 0.002871250733733177,\n",
      "      \"learning_rate\": 3.727126654064272e-05,\n",
      "      \"loss\": 0.0383,\n",
      "      \"step\": 40420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.548221431046977,\n",
      "      \"grad_norm\": 0.0031345656607300043,\n",
      "      \"learning_rate\": 3.7264965343415245e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 40440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.549481710198809,\n",
      "      \"grad_norm\": 0.017141900956630707,\n",
      "      \"learning_rate\": 3.7258664146187774e-05,\n",
      "      \"loss\": 0.0699,\n",
      "      \"step\": 40460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.550741989350641,\n",
      "      \"grad_norm\": 0.03274000808596611,\n",
      "      \"learning_rate\": 3.72523629489603e-05,\n",
      "      \"loss\": 0.0366,\n",
      "      \"step\": 40480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5520022685024735,\n",
      "      \"grad_norm\": 0.08030380308628082,\n",
      "      \"learning_rate\": 3.724606175173283e-05,\n",
      "      \"loss\": 0.0256,\n",
      "      \"step\": 40500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.553262547654305,\n",
      "      \"grad_norm\": 0.04013645276427269,\n",
      "      \"learning_rate\": 3.723976055450536e-05,\n",
      "      \"loss\": 0.0437,\n",
      "      \"step\": 40520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5545228268061377,\n",
      "      \"grad_norm\": 7.151578903198242,\n",
      "      \"learning_rate\": 3.7233459357277884e-05,\n",
      "      \"loss\": 0.0352,\n",
      "      \"step\": 40540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5557831059579694,\n",
      "      \"grad_norm\": 0.0019606666173785925,\n",
      "      \"learning_rate\": 3.722715816005041e-05,\n",
      "      \"loss\": 0.0299,\n",
      "      \"step\": 40560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.557043385109802,\n",
      "      \"grad_norm\": 0.1008012443780899,\n",
      "      \"learning_rate\": 3.7220856962822935e-05,\n",
      "      \"loss\": 0.0297,\n",
      "      \"step\": 40580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5583036642616337,\n",
      "      \"grad_norm\": 0.07298315316438675,\n",
      "      \"learning_rate\": 3.7214555765595464e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 40600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5595639434134663,\n",
      "      \"grad_norm\": 0.004189600236713886,\n",
      "      \"learning_rate\": 3.7208254568367986e-05,\n",
      "      \"loss\": 0.0174,\n",
      "      \"step\": 40620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.560824222565298,\n",
      "      \"grad_norm\": 0.028364140540361404,\n",
      "      \"learning_rate\": 3.720195337114052e-05,\n",
      "      \"loss\": 0.046,\n",
      "      \"step\": 40640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5620845017171305,\n",
      "      \"grad_norm\": 0.0009668288403190672,\n",
      "      \"learning_rate\": 3.7195652173913044e-05,\n",
      "      \"loss\": 0.0115,\n",
      "      \"step\": 40660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5633447808689622,\n",
      "      \"grad_norm\": 0.0023558526299893856,\n",
      "      \"learning_rate\": 3.718935097668557e-05,\n",
      "      \"loss\": 0.0194,\n",
      "      \"step\": 40680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.564605060020795,\n",
      "      \"grad_norm\": 0.28341659903526306,\n",
      "      \"learning_rate\": 3.71830497794581e-05,\n",
      "      \"loss\": 0.0403,\n",
      "      \"step\": 40700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5658653391726265,\n",
      "      \"grad_norm\": 0.05800189822912216,\n",
      "      \"learning_rate\": 3.7176748582230625e-05,\n",
      "      \"loss\": 0.0044,\n",
      "      \"step\": 40720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.567125618324459,\n",
      "      \"grad_norm\": 0.006966562941670418,\n",
      "      \"learning_rate\": 3.7170447385003154e-05,\n",
      "      \"loss\": 0.0073,\n",
      "      \"step\": 40740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5683858974762908,\n",
      "      \"grad_norm\": 0.0035806901287287474,\n",
      "      \"learning_rate\": 3.7164146187775676e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 40760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5696461766281233,\n",
      "      \"grad_norm\": 0.0009392175707034767,\n",
      "      \"learning_rate\": 3.7157844990548205e-05,\n",
      "      \"loss\": 0.0052,\n",
      "      \"step\": 40780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.570906455779955,\n",
      "      \"grad_norm\": 0.007771157659590244,\n",
      "      \"learning_rate\": 3.7151543793320734e-05,\n",
      "      \"loss\": 0.0339,\n",
      "      \"step\": 40800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5721667349317876,\n",
      "      \"grad_norm\": 6.033803939819336,\n",
      "      \"learning_rate\": 3.714524259609326e-05,\n",
      "      \"loss\": 0.0797,\n",
      "      \"step\": 40820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5734270140836193,\n",
      "      \"grad_norm\": 0.013025673106312752,\n",
      "      \"learning_rate\": 3.7138941398865785e-05,\n",
      "      \"loss\": 0.0311,\n",
      "      \"step\": 40840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.574687293235452,\n",
      "      \"grad_norm\": 5.316647529602051,\n",
      "      \"learning_rate\": 3.7132640201638314e-05,\n",
      "      \"loss\": 0.0412,\n",
      "      \"step\": 40860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5759475723872836,\n",
      "      \"grad_norm\": 0.013778472319245338,\n",
      "      \"learning_rate\": 3.712633900441084e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 40880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.577207851539116,\n",
      "      \"grad_norm\": 0.07475238293409348,\n",
      "      \"learning_rate\": 3.7120037807183366e-05,\n",
      "      \"loss\": 0.0233,\n",
      "      \"step\": 40900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.578468130690948,\n",
      "      \"grad_norm\": 1.7179099321365356,\n",
      "      \"learning_rate\": 3.7113736609955895e-05,\n",
      "      \"loss\": 0.0136,\n",
      "      \"step\": 40920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5797284098427804,\n",
      "      \"grad_norm\": 0.7381354570388794,\n",
      "      \"learning_rate\": 3.710743541272842e-05,\n",
      "      \"loss\": 0.0055,\n",
      "      \"step\": 40940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.580988688994612,\n",
      "      \"grad_norm\": 0.15556682646274567,\n",
      "      \"learning_rate\": 3.7101134215500946e-05,\n",
      "      \"loss\": 0.1021,\n",
      "      \"step\": 40960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5822489681464447,\n",
      "      \"grad_norm\": 0.10905405133962631,\n",
      "      \"learning_rate\": 3.7094833018273475e-05,\n",
      "      \"loss\": 0.0272,\n",
      "      \"step\": 40980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5835092472982764,\n",
      "      \"grad_norm\": 0.02228802815079689,\n",
      "      \"learning_rate\": 3.7088531821046004e-05,\n",
      "      \"loss\": 0.0213,\n",
      "      \"step\": 41000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.584769526450109,\n",
      "      \"grad_norm\": 0.03960822522640228,\n",
      "      \"learning_rate\": 3.7082230623818526e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 41020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5860298056019406,\n",
      "      \"grad_norm\": 0.005124134477227926,\n",
      "      \"learning_rate\": 3.7075929426591055e-05,\n",
      "      \"loss\": 0.0365,\n",
      "      \"step\": 41040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.587290084753773,\n",
      "      \"grad_norm\": 0.003991920035332441,\n",
      "      \"learning_rate\": 3.706962822936358e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 41060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.588550363905605,\n",
      "      \"grad_norm\": 0.0033377755898982286,\n",
      "      \"learning_rate\": 3.706332703213611e-05,\n",
      "      \"loss\": 0.0385,\n",
      "      \"step\": 41080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5898106430574375,\n",
      "      \"grad_norm\": 0.529429018497467,\n",
      "      \"learning_rate\": 3.705702583490863e-05,\n",
      "      \"loss\": 0.0397,\n",
      "      \"step\": 41100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.591070922209269,\n",
      "      \"grad_norm\": 0.025681912899017334,\n",
      "      \"learning_rate\": 3.705072463768116e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 41120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5923312013611017,\n",
      "      \"grad_norm\": 0.0719548761844635,\n",
      "      \"learning_rate\": 3.7044423440453694e-05,\n",
      "      \"loss\": 0.0613,\n",
      "      \"step\": 41140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5935914805129334,\n",
      "      \"grad_norm\": 0.32792556285858154,\n",
      "      \"learning_rate\": 3.7038122243226216e-05,\n",
      "      \"loss\": 0.0416,\n",
      "      \"step\": 41160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.594851759664766,\n",
      "      \"grad_norm\": 0.18780216574668884,\n",
      "      \"learning_rate\": 3.7031821045998745e-05,\n",
      "      \"loss\": 0.0255,\n",
      "      \"step\": 41180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5961120388165977,\n",
      "      \"grad_norm\": 0.016432836651802063,\n",
      "      \"learning_rate\": 3.702551984877127e-05,\n",
      "      \"loss\": 0.0578,\n",
      "      \"step\": 41200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5973723179684303,\n",
      "      \"grad_norm\": 4.703671932220459,\n",
      "      \"learning_rate\": 3.7019218651543796e-05,\n",
      "      \"loss\": 0.0414,\n",
      "      \"step\": 41220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.598632597120262,\n",
      "      \"grad_norm\": 0.611712634563446,\n",
      "      \"learning_rate\": 3.701291745431632e-05,\n",
      "      \"loss\": 0.0369,\n",
      "      \"step\": 41240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5998928762720945,\n",
      "      \"grad_norm\": 9.288949966430664,\n",
      "      \"learning_rate\": 3.700661625708885e-05,\n",
      "      \"loss\": 0.0143,\n",
      "      \"step\": 41260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6011531554239262,\n",
      "      \"grad_norm\": 0.012001809664070606,\n",
      "      \"learning_rate\": 3.700031505986138e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 41280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.602413434575759,\n",
      "      \"grad_norm\": 0.015339554287493229,\n",
      "      \"learning_rate\": 3.6994013862633906e-05,\n",
      "      \"loss\": 0.0303,\n",
      "      \"step\": 41300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6036737137275905,\n",
      "      \"grad_norm\": 0.00369655922986567,\n",
      "      \"learning_rate\": 3.698771266540643e-05,\n",
      "      \"loss\": 0.0366,\n",
      "      \"step\": 41320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.604933992879423,\n",
      "      \"grad_norm\": 12.448417663574219,\n",
      "      \"learning_rate\": 3.698141146817896e-05,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 41340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6061942720312548,\n",
      "      \"grad_norm\": 0.0030520192813128233,\n",
      "      \"learning_rate\": 3.6975110270951486e-05,\n",
      "      \"loss\": 0.0311,\n",
      "      \"step\": 41360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.607454551183087,\n",
      "      \"grad_norm\": 0.0028809322975575924,\n",
      "      \"learning_rate\": 3.696880907372401e-05,\n",
      "      \"loss\": 0.0258,\n",
      "      \"step\": 41380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.608714830334919,\n",
      "      \"grad_norm\": 4.53482723236084,\n",
      "      \"learning_rate\": 3.696250787649654e-05,\n",
      "      \"loss\": 0.0532,\n",
      "      \"step\": 41400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.609975109486751,\n",
      "      \"grad_norm\": 0.057490650564432144,\n",
      "      \"learning_rate\": 3.695620667926906e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 41420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6112353886385833,\n",
      "      \"grad_norm\": 0.05119725689291954,\n",
      "      \"learning_rate\": 3.694990548204159e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 41440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6124956677904154,\n",
      "      \"grad_norm\": 0.013422940857708454,\n",
      "      \"learning_rate\": 3.694360428481412e-05,\n",
      "      \"loss\": 0.0377,\n",
      "      \"step\": 41460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6137559469422476,\n",
      "      \"grad_norm\": 2.8624541759490967,\n",
      "      \"learning_rate\": 3.693730308758665e-05,\n",
      "      \"loss\": 0.0398,\n",
      "      \"step\": 41480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6150162260940797,\n",
      "      \"grad_norm\": 0.14499737322330475,\n",
      "      \"learning_rate\": 3.693100189035917e-05,\n",
      "      \"loss\": 0.0545,\n",
      "      \"step\": 41500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.616276505245912,\n",
      "      \"grad_norm\": 0.039315931499004364,\n",
      "      \"learning_rate\": 3.69247006931317e-05,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 41520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.617536784397744,\n",
      "      \"grad_norm\": 0.07131870836019516,\n",
      "      \"learning_rate\": 3.691839949590422e-05,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 41540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.618797063549576,\n",
      "      \"grad_norm\": 0.013050700537860394,\n",
      "      \"learning_rate\": 3.691209829867675e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 41560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.620057342701408,\n",
      "      \"grad_norm\": 0.018472781404852867,\n",
      "      \"learning_rate\": 3.690579710144928e-05,\n",
      "      \"loss\": 0.0021,\n",
      "      \"step\": 41580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6213176218532404,\n",
      "      \"grad_norm\": 6.087482452392578,\n",
      "      \"learning_rate\": 3.68994959042218e-05,\n",
      "      \"loss\": 0.0409,\n",
      "      \"step\": 41600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6225779010050725,\n",
      "      \"grad_norm\": 0.02272074483335018,\n",
      "      \"learning_rate\": 3.689319470699433e-05,\n",
      "      \"loss\": 0.0327,\n",
      "      \"step\": 41620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6238381801569046,\n",
      "      \"grad_norm\": 4.879347324371338,\n",
      "      \"learning_rate\": 3.688689350976686e-05,\n",
      "      \"loss\": 0.0202,\n",
      "      \"step\": 41640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6250984593087368,\n",
      "      \"grad_norm\": 0.0057450407184660435,\n",
      "      \"learning_rate\": 3.688059231253939e-05,\n",
      "      \"loss\": 0.017,\n",
      "      \"step\": 41660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.626358738460569,\n",
      "      \"grad_norm\": 0.012458973564207554,\n",
      "      \"learning_rate\": 3.687429111531191e-05,\n",
      "      \"loss\": 0.0479,\n",
      "      \"step\": 41680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.627619017612401,\n",
      "      \"grad_norm\": 0.001065827556885779,\n",
      "      \"learning_rate\": 3.686798991808444e-05,\n",
      "      \"loss\": 0.0121,\n",
      "      \"step\": 41700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.628879296764233,\n",
      "      \"grad_norm\": 0.004635501187294722,\n",
      "      \"learning_rate\": 3.686168872085696e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 41720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6301395759160653,\n",
      "      \"grad_norm\": 0.011705480515956879,\n",
      "      \"learning_rate\": 3.685538752362949e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 41740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6313998550678974,\n",
      "      \"grad_norm\": 0.0012441744329407811,\n",
      "      \"learning_rate\": 3.684908632640201e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 41760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6326601342197296,\n",
      "      \"grad_norm\": 0.0277284923940897,\n",
      "      \"learning_rate\": 3.684278512917455e-05,\n",
      "      \"loss\": 0.0704,\n",
      "      \"step\": 41780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6339204133715617,\n",
      "      \"grad_norm\": 15.563232421875,\n",
      "      \"learning_rate\": 3.683648393194708e-05,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 41800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.635180692523394,\n",
      "      \"grad_norm\": 0.016033655032515526,\n",
      "      \"learning_rate\": 3.683049779458097e-05,\n",
      "      \"loss\": 0.0404,\n",
      "      \"step\": 41820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.636440971675226,\n",
      "      \"grad_norm\": 0.032981038093566895,\n",
      "      \"learning_rate\": 3.68241965973535e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 41840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.637701250827058,\n",
      "      \"grad_norm\": 0.012380826286971569,\n",
      "      \"learning_rate\": 3.681789540012602e-05,\n",
      "      \"loss\": 0.0396,\n",
      "      \"step\": 41860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.63896152997889,\n",
      "      \"grad_norm\": 0.09761817008256912,\n",
      "      \"learning_rate\": 3.681159420289855e-05,\n",
      "      \"loss\": 0.0333,\n",
      "      \"step\": 41880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6402218091307224,\n",
      "      \"grad_norm\": 10.68388557434082,\n",
      "      \"learning_rate\": 3.6805293005671074e-05,\n",
      "      \"loss\": 0.0349,\n",
      "      \"step\": 41900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6414820882825545,\n",
      "      \"grad_norm\": 0.002848485019057989,\n",
      "      \"learning_rate\": 3.67989918084436e-05,\n",
      "      \"loss\": 0.0218,\n",
      "      \"step\": 41920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6427423674343866,\n",
      "      \"grad_norm\": 5.13460636138916,\n",
      "      \"learning_rate\": 3.679269061121613e-05,\n",
      "      \"loss\": 0.0308,\n",
      "      \"step\": 41940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6440026465862188,\n",
      "      \"grad_norm\": 2.7178821563720703,\n",
      "      \"learning_rate\": 3.678638941398866e-05,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 41960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.645262925738051,\n",
      "      \"grad_norm\": 0.005686583463102579,\n",
      "      \"learning_rate\": 3.678008821676119e-05,\n",
      "      \"loss\": 0.0168,\n",
      "      \"step\": 41980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.646523204889883,\n",
      "      \"grad_norm\": 0.02957957424223423,\n",
      "      \"learning_rate\": 3.677378701953371e-05,\n",
      "      \"loss\": 0.0277,\n",
      "      \"step\": 42000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.647783484041715,\n",
      "      \"grad_norm\": 0.011094206012785435,\n",
      "      \"learning_rate\": 3.676748582230624e-05,\n",
      "      \"loss\": 0.006,\n",
      "      \"step\": 42020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6490437631935473,\n",
      "      \"grad_norm\": 0.006212594918906689,\n",
      "      \"learning_rate\": 3.6761184625078764e-05,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 42040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6503040423453794,\n",
      "      \"grad_norm\": 0.12541940808296204,\n",
      "      \"learning_rate\": 3.675488342785129e-05,\n",
      "      \"loss\": 0.0184,\n",
      "      \"step\": 42060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6515643214972116,\n",
      "      \"grad_norm\": 0.002144589088857174,\n",
      "      \"learning_rate\": 3.6748582230623815e-05,\n",
      "      \"loss\": 0.0277,\n",
      "      \"step\": 42080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6528246006490437,\n",
      "      \"grad_norm\": 0.0012896833941340446,\n",
      "      \"learning_rate\": 3.6742281033396344e-05,\n",
      "      \"loss\": 0.0162,\n",
      "      \"step\": 42100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.654084879800876,\n",
      "      \"grad_norm\": 0.7504519820213318,\n",
      "      \"learning_rate\": 3.673597983616887e-05,\n",
      "      \"loss\": 0.0209,\n",
      "      \"step\": 42120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.655345158952708,\n",
      "      \"grad_norm\": 0.002222680952399969,\n",
      "      \"learning_rate\": 3.67296786389414e-05,\n",
      "      \"loss\": 0.0534,\n",
      "      \"step\": 42140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.65660543810454,\n",
      "      \"grad_norm\": 0.048323702067136765,\n",
      "      \"learning_rate\": 3.672337744171393e-05,\n",
      "      \"loss\": 0.0777,\n",
      "      \"step\": 42160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.657865717256372,\n",
      "      \"grad_norm\": 0.05071277916431427,\n",
      "      \"learning_rate\": 3.6717076244486454e-05,\n",
      "      \"loss\": 0.0341,\n",
      "      \"step\": 42180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6591259964082044,\n",
      "      \"grad_norm\": 7.505710601806641,\n",
      "      \"learning_rate\": 3.671077504725898e-05,\n",
      "      \"loss\": 0.0231,\n",
      "      \"step\": 42200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6603862755600365,\n",
      "      \"grad_norm\": 0.026750577613711357,\n",
      "      \"learning_rate\": 3.6704473850031505e-05,\n",
      "      \"loss\": 0.0716,\n",
      "      \"step\": 42220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6616465547118686,\n",
      "      \"grad_norm\": 0.03629186376929283,\n",
      "      \"learning_rate\": 3.6698172652804034e-05,\n",
      "      \"loss\": 0.0105,\n",
      "      \"step\": 42240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6629068338637008,\n",
      "      \"grad_norm\": 0.09457698464393616,\n",
      "      \"learning_rate\": 3.6691871455576556e-05,\n",
      "      \"loss\": 0.056,\n",
      "      \"step\": 42260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.664167113015533,\n",
      "      \"grad_norm\": 0.007686259225010872,\n",
      "      \"learning_rate\": 3.668557025834909e-05,\n",
      "      \"loss\": 0.0204,\n",
      "      \"step\": 42280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.665427392167365,\n",
      "      \"grad_norm\": 0.03191900625824928,\n",
      "      \"learning_rate\": 3.6679269061121614e-05,\n",
      "      \"loss\": 0.0198,\n",
      "      \"step\": 42300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.666687671319197,\n",
      "      \"grad_norm\": 0.015241683460772038,\n",
      "      \"learning_rate\": 3.6672967863894143e-05,\n",
      "      \"loss\": 0.0491,\n",
      "      \"step\": 42320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6679479504710293,\n",
      "      \"grad_norm\": 0.19161543250083923,\n",
      "      \"learning_rate\": 3.6666666666666666e-05,\n",
      "      \"loss\": 0.0625,\n",
      "      \"step\": 42340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6692082296228614,\n",
      "      \"grad_norm\": 10.905184745788574,\n",
      "      \"learning_rate\": 3.6660365469439195e-05,\n",
      "      \"loss\": 0.0189,\n",
      "      \"step\": 42360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6704685087746936,\n",
      "      \"grad_norm\": 0.1325317621231079,\n",
      "      \"learning_rate\": 3.6654064272211724e-05,\n",
      "      \"loss\": 0.0218,\n",
      "      \"step\": 42380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6717287879265257,\n",
      "      \"grad_norm\": 0.0034277120139449835,\n",
      "      \"learning_rate\": 3.6647763074984246e-05,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 42400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.672989067078358,\n",
      "      \"grad_norm\": 0.009459536522626877,\n",
      "      \"learning_rate\": 3.6641461877756775e-05,\n",
      "      \"loss\": 0.0083,\n",
      "      \"step\": 42420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.67424934623019,\n",
      "      \"grad_norm\": 0.3253466784954071,\n",
      "      \"learning_rate\": 3.6635160680529304e-05,\n",
      "      \"loss\": 0.0358,\n",
      "      \"step\": 42440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.675509625382022,\n",
      "      \"grad_norm\": 0.005498915445059538,\n",
      "      \"learning_rate\": 3.662885948330183e-05,\n",
      "      \"loss\": 0.0257,\n",
      "      \"step\": 42460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.676769904533854,\n",
      "      \"grad_norm\": 3.943875789642334,\n",
      "      \"learning_rate\": 3.6622558286074355e-05,\n",
      "      \"loss\": 0.0593,\n",
      "      \"step\": 42480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6780301836856863,\n",
      "      \"grad_norm\": 0.004264147020876408,\n",
      "      \"learning_rate\": 3.6616257088846884e-05,\n",
      "      \"loss\": 0.0434,\n",
      "      \"step\": 42500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6792904628375185,\n",
      "      \"grad_norm\": 0.01706830970942974,\n",
      "      \"learning_rate\": 3.660995589161941e-05,\n",
      "      \"loss\": 0.0169,\n",
      "      \"step\": 42520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6805507419893506,\n",
      "      \"grad_norm\": 0.0021408533211797476,\n",
      "      \"learning_rate\": 3.6603654694391936e-05,\n",
      "      \"loss\": 0.0355,\n",
      "      \"step\": 42540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6818110211411827,\n",
      "      \"grad_norm\": 0.002794287633150816,\n",
      "      \"learning_rate\": 3.659735349716446e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 42560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.683071300293015,\n",
      "      \"grad_norm\": 0.004084675572812557,\n",
      "      \"learning_rate\": 3.659105229993699e-05,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 42580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.684331579444847,\n",
      "      \"grad_norm\": 0.001368359080515802,\n",
      "      \"learning_rate\": 3.6584751102709516e-05,\n",
      "      \"loss\": 0.0176,\n",
      "      \"step\": 42600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.685591858596679,\n",
      "      \"grad_norm\": 0.0005742118810303509,\n",
      "      \"learning_rate\": 3.6578449905482045e-05,\n",
      "      \"loss\": 0.028,\n",
      "      \"step\": 42620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6868521377485113,\n",
      "      \"grad_norm\": 0.15768085420131683,\n",
      "      \"learning_rate\": 3.6572148708254574e-05,\n",
      "      \"loss\": 0.0265,\n",
      "      \"step\": 42640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6881124169003434,\n",
      "      \"grad_norm\": 7.149043083190918,\n",
      "      \"learning_rate\": 3.6565847511027096e-05,\n",
      "      \"loss\": 0.0237,\n",
      "      \"step\": 42660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6893726960521755,\n",
      "      \"grad_norm\": 0.030894266441464424,\n",
      "      \"learning_rate\": 3.6559546313799626e-05,\n",
      "      \"loss\": 0.0206,\n",
      "      \"step\": 42680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6906329752040077,\n",
      "      \"grad_norm\": 0.011830976232886314,\n",
      "      \"learning_rate\": 3.655324511657215e-05,\n",
      "      \"loss\": 0.0273,\n",
      "      \"step\": 42700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.69189325435584,\n",
      "      \"grad_norm\": 7.808355808258057,\n",
      "      \"learning_rate\": 3.654694391934468e-05,\n",
      "      \"loss\": 0.0309,\n",
      "      \"step\": 42720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.693153533507672,\n",
      "      \"grad_norm\": 1.682596206665039,\n",
      "      \"learning_rate\": 3.65406427221172e-05,\n",
      "      \"loss\": 0.0371,\n",
      "      \"step\": 42740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.694413812659504,\n",
      "      \"grad_norm\": 0.1579936146736145,\n",
      "      \"learning_rate\": 3.653434152488973e-05,\n",
      "      \"loss\": 0.0232,\n",
      "      \"step\": 42760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.695674091811336,\n",
      "      \"grad_norm\": 0.006205504760146141,\n",
      "      \"learning_rate\": 3.652804032766226e-05,\n",
      "      \"loss\": 0.0256,\n",
      "      \"step\": 42780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6969343709631683,\n",
      "      \"grad_norm\": 0.018929725512862206,\n",
      "      \"learning_rate\": 3.6521739130434786e-05,\n",
      "      \"loss\": 0.0483,\n",
      "      \"step\": 42800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6981946501150005,\n",
      "      \"grad_norm\": 0.12180592864751816,\n",
      "      \"learning_rate\": 3.6515437933207315e-05,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 42820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6994549292668326,\n",
      "      \"grad_norm\": 0.3906990885734558,\n",
      "      \"learning_rate\": 3.650913673597984e-05,\n",
      "      \"loss\": 0.05,\n",
      "      \"step\": 42840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7007152084186647,\n",
      "      \"grad_norm\": 0.06636734306812286,\n",
      "      \"learning_rate\": 3.6502835538752367e-05,\n",
      "      \"loss\": 0.0312,\n",
      "      \"step\": 42860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.701975487570497,\n",
      "      \"grad_norm\": 0.005369987804442644,\n",
      "      \"learning_rate\": 3.649653434152489e-05,\n",
      "      \"loss\": 0.0297,\n",
      "      \"step\": 42880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.703235766722329,\n",
      "      \"grad_norm\": 0.009749509394168854,\n",
      "      \"learning_rate\": 3.649023314429742e-05,\n",
      "      \"loss\": 0.0417,\n",
      "      \"step\": 42900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.704496045874161,\n",
      "      \"grad_norm\": 10.343611717224121,\n",
      "      \"learning_rate\": 3.648393194706995e-05,\n",
      "      \"loss\": 0.0244,\n",
      "      \"step\": 42920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7057563250259933,\n",
      "      \"grad_norm\": 14.168313026428223,\n",
      "      \"learning_rate\": 3.6477630749842476e-05,\n",
      "      \"loss\": 0.0447,\n",
      "      \"step\": 42940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7070166041778254,\n",
      "      \"grad_norm\": 0.0013706710888072848,\n",
      "      \"learning_rate\": 3.6471329552615e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 42960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7082768833296575,\n",
      "      \"grad_norm\": 0.009827544912695885,\n",
      "      \"learning_rate\": 3.646502835538753e-05,\n",
      "      \"loss\": 0.0329,\n",
      "      \"step\": 42980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7095371624814897,\n",
      "      \"grad_norm\": 2.51157808303833,\n",
      "      \"learning_rate\": 3.645872715816005e-05,\n",
      "      \"loss\": 0.0157,\n",
      "      \"step\": 43000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.710797441633322,\n",
      "      \"grad_norm\": 0.0660676509141922,\n",
      "      \"learning_rate\": 3.645242596093258e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 43020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.712057720785154,\n",
      "      \"grad_norm\": 0.013558246195316315,\n",
      "      \"learning_rate\": 3.644612476370511e-05,\n",
      "      \"loss\": 0.0341,\n",
      "      \"step\": 43040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.713317999936986,\n",
      "      \"grad_norm\": 0.0005604230100288987,\n",
      "      \"learning_rate\": 3.643982356647763e-05,\n",
      "      \"loss\": 0.0345,\n",
      "      \"step\": 43060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.714578279088818,\n",
      "      \"grad_norm\": 5.744854927062988,\n",
      "      \"learning_rate\": 3.643352236925016e-05,\n",
      "      \"loss\": 0.0204,\n",
      "      \"step\": 43080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7158385582406503,\n",
      "      \"grad_norm\": 0.09656001627445221,\n",
      "      \"learning_rate\": 3.642722117202269e-05,\n",
      "      \"loss\": 0.0297,\n",
      "      \"step\": 43100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7170988373924825,\n",
      "      \"grad_norm\": 0.03392110764980316,\n",
      "      \"learning_rate\": 3.642091997479522e-05,\n",
      "      \"loss\": 0.026,\n",
      "      \"step\": 43120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7183591165443146,\n",
      "      \"grad_norm\": 0.08646447956562042,\n",
      "      \"learning_rate\": 3.641461877756774e-05,\n",
      "      \"loss\": 0.049,\n",
      "      \"step\": 43140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7196193956961467,\n",
      "      \"grad_norm\": 2.433159112930298,\n",
      "      \"learning_rate\": 3.640831758034027e-05,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 43160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.720879674847979,\n",
      "      \"grad_norm\": 0.013282006606459618,\n",
      "      \"learning_rate\": 3.640201638311279e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 43180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.722139953999811,\n",
      "      \"grad_norm\": 0.008684912696480751,\n",
      "      \"learning_rate\": 3.639571518588532e-05,\n",
      "      \"loss\": 0.0382,\n",
      "      \"step\": 43200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.723400233151643,\n",
      "      \"grad_norm\": 0.0017587802140042186,\n",
      "      \"learning_rate\": 3.638941398865784e-05,\n",
      "      \"loss\": 0.0258,\n",
      "      \"step\": 43220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7246605123034753,\n",
      "      \"grad_norm\": 5.197752475738525,\n",
      "      \"learning_rate\": 3.638311279143037e-05,\n",
      "      \"loss\": 0.0452,\n",
      "      \"step\": 43240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7259207914553074,\n",
      "      \"grad_norm\": 0.06546669453382492,\n",
      "      \"learning_rate\": 3.63768115942029e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 43260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7271810706071395,\n",
      "      \"grad_norm\": 0.03466494753956795,\n",
      "      \"learning_rate\": 3.637051039697543e-05,\n",
      "      \"loss\": 0.0339,\n",
      "      \"step\": 43280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7284413497589717,\n",
      "      \"grad_norm\": 0.0028302527498453856,\n",
      "      \"learning_rate\": 3.636420919974796e-05,\n",
      "      \"loss\": 0.0115,\n",
      "      \"step\": 43300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.729701628910804,\n",
      "      \"grad_norm\": 0.21188005805015564,\n",
      "      \"learning_rate\": 3.635790800252048e-05,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 43320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.730961908062636,\n",
      "      \"grad_norm\": 0.01097042579203844,\n",
      "      \"learning_rate\": 3.635160680529301e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 43340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.732222187214468,\n",
      "      \"grad_norm\": 0.10706957429647446,\n",
      "      \"learning_rate\": 3.634530560806553e-05,\n",
      "      \"loss\": 0.0297,\n",
      "      \"step\": 43360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7334824663663,\n",
      "      \"grad_norm\": 0.06640172004699707,\n",
      "      \"learning_rate\": 3.633900441083806e-05,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 43380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7347427455181323,\n",
      "      \"grad_norm\": 0.867927074432373,\n",
      "      \"learning_rate\": 3.633270321361058e-05,\n",
      "      \"loss\": 0.0449,\n",
      "      \"step\": 43400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7360030246699645,\n",
      "      \"grad_norm\": 0.000626461929641664,\n",
      "      \"learning_rate\": 3.632640201638312e-05,\n",
      "      \"loss\": 0.031,\n",
      "      \"step\": 43420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7372633038217966,\n",
      "      \"grad_norm\": 0.0025542774237692356,\n",
      "      \"learning_rate\": 3.632010081915564e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 43440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7385235829736287,\n",
      "      \"grad_norm\": 0.007518624886870384,\n",
      "      \"learning_rate\": 3.631379962192817e-05,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 43460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.739783862125461,\n",
      "      \"grad_norm\": 0.021386492997407913,\n",
      "      \"learning_rate\": 3.630749842470069e-05,\n",
      "      \"loss\": 0.0561,\n",
      "      \"step\": 43480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.741044141277293,\n",
      "      \"grad_norm\": 0.04290781542658806,\n",
      "      \"learning_rate\": 3.630119722747322e-05,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 43500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.742304420429125,\n",
      "      \"grad_norm\": 0.026767538860440254,\n",
      "      \"learning_rate\": 3.629489603024575e-05,\n",
      "      \"loss\": 0.0516,\n",
      "      \"step\": 43520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7435646995809573,\n",
      "      \"grad_norm\": 0.001657738583162427,\n",
      "      \"learning_rate\": 3.628859483301827e-05,\n",
      "      \"loss\": 0.0067,\n",
      "      \"step\": 43540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7448249787327894,\n",
      "      \"grad_norm\": 1.4688776731491089,\n",
      "      \"learning_rate\": 3.62822936357908e-05,\n",
      "      \"loss\": 0.0339,\n",
      "      \"step\": 43560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7460852578846215,\n",
      "      \"grad_norm\": 1.0486875772476196,\n",
      "      \"learning_rate\": 3.627599243856333e-05,\n",
      "      \"loss\": 0.0377,\n",
      "      \"step\": 43580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7473455370364537,\n",
      "      \"grad_norm\": 0.032204438000917435,\n",
      "      \"learning_rate\": 3.626969124133586e-05,\n",
      "      \"loss\": 0.0457,\n",
      "      \"step\": 43600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.748605816188286,\n",
      "      \"grad_norm\": 0.15136174857616425,\n",
      "      \"learning_rate\": 3.626339004410838e-05,\n",
      "      \"loss\": 0.0352,\n",
      "      \"step\": 43620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.749866095340118,\n",
      "      \"grad_norm\": 0.11410947144031525,\n",
      "      \"learning_rate\": 3.625708884688091e-05,\n",
      "      \"loss\": 0.0319,\n",
      "      \"step\": 43640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.75112637449195,\n",
      "      \"grad_norm\": 0.00576880294829607,\n",
      "      \"learning_rate\": 3.625078764965343e-05,\n",
      "      \"loss\": 0.0323,\n",
      "      \"step\": 43660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.752386653643782,\n",
      "      \"grad_norm\": 0.0012913577957078815,\n",
      "      \"learning_rate\": 3.624448645242596e-05,\n",
      "      \"loss\": 0.0316,\n",
      "      \"step\": 43680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7536469327956143,\n",
      "      \"grad_norm\": 0.0915713682770729,\n",
      "      \"learning_rate\": 3.6238185255198485e-05,\n",
      "      \"loss\": 0.0623,\n",
      "      \"step\": 43700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7549072119474465,\n",
      "      \"grad_norm\": 0.21253567934036255,\n",
      "      \"learning_rate\": 3.6231884057971014e-05,\n",
      "      \"loss\": 0.0135,\n",
      "      \"step\": 43720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7561674910992786,\n",
      "      \"grad_norm\": 0.015299989841878414,\n",
      "      \"learning_rate\": 3.622558286074354e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 43740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7574277702511107,\n",
      "      \"grad_norm\": 0.0020629405044019222,\n",
      "      \"learning_rate\": 3.621928166351607e-05,\n",
      "      \"loss\": 0.0259,\n",
      "      \"step\": 43760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.758688049402943,\n",
      "      \"grad_norm\": 0.03183325007557869,\n",
      "      \"learning_rate\": 3.62129804662886e-05,\n",
      "      \"loss\": 0.0392,\n",
      "      \"step\": 43780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.759948328554775,\n",
      "      \"grad_norm\": 0.09251338988542557,\n",
      "      \"learning_rate\": 3.620667926906112e-05,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 43800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.761208607706607,\n",
      "      \"grad_norm\": 9.853279113769531,\n",
      "      \"learning_rate\": 3.620037807183365e-05,\n",
      "      \"loss\": 0.0206,\n",
      "      \"step\": 43820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7624688868584393,\n",
      "      \"grad_norm\": 0.011330845765769482,\n",
      "      \"learning_rate\": 3.6194076874606174e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 43840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7637291660102714,\n",
      "      \"grad_norm\": 0.007722944486886263,\n",
      "      \"learning_rate\": 3.61877756773787e-05,\n",
      "      \"loss\": 0.0328,\n",
      "      \"step\": 43860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7649894451621035,\n",
      "      \"grad_norm\": 0.009611285291612148,\n",
      "      \"learning_rate\": 3.6181474480151226e-05,\n",
      "      \"loss\": 0.031,\n",
      "      \"step\": 43880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7662497243139357,\n",
      "      \"grad_norm\": 0.018427656963467598,\n",
      "      \"learning_rate\": 3.6175173282923755e-05,\n",
      "      \"loss\": 0.0279,\n",
      "      \"step\": 43900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.767510003465768,\n",
      "      \"grad_norm\": 0.9386768937110901,\n",
      "      \"learning_rate\": 3.6168872085696284e-05,\n",
      "      \"loss\": 0.0402,\n",
      "      \"step\": 43920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7687702826176,\n",
      "      \"grad_norm\": 1.3855197429656982,\n",
      "      \"learning_rate\": 3.616257088846881e-05,\n",
      "      \"loss\": 0.0069,\n",
      "      \"step\": 43940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.770030561769432,\n",
      "      \"grad_norm\": 0.0028125469107180834,\n",
      "      \"learning_rate\": 3.615626969124134e-05,\n",
      "      \"loss\": 0.025,\n",
      "      \"step\": 43960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.771290840921264,\n",
      "      \"grad_norm\": 0.02388998121023178,\n",
      "      \"learning_rate\": 3.6149968494013864e-05,\n",
      "      \"loss\": 0.035,\n",
      "      \"step\": 43980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7725511200730963,\n",
      "      \"grad_norm\": 0.0037541675847023726,\n",
      "      \"learning_rate\": 3.614366729678639e-05,\n",
      "      \"loss\": 0.0396,\n",
      "      \"step\": 44000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7738113992249285,\n",
      "      \"grad_norm\": 0.11360450088977814,\n",
      "      \"learning_rate\": 3.6137366099558915e-05,\n",
      "      \"loss\": 0.0218,\n",
      "      \"step\": 44020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7750716783767606,\n",
      "      \"grad_norm\": 0.37358906865119934,\n",
      "      \"learning_rate\": 3.6131064902331444e-05,\n",
      "      \"loss\": 0.0163,\n",
      "      \"step\": 44040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7763319575285927,\n",
      "      \"grad_norm\": 0.07232485711574554,\n",
      "      \"learning_rate\": 3.612476370510397e-05,\n",
      "      \"loss\": 0.0342,\n",
      "      \"step\": 44060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.777592236680425,\n",
      "      \"grad_norm\": 0.28065696358680725,\n",
      "      \"learning_rate\": 3.61184625078765e-05,\n",
      "      \"loss\": 0.033,\n",
      "      \"step\": 44080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.778852515832257,\n",
      "      \"grad_norm\": 0.3679734766483307,\n",
      "      \"learning_rate\": 3.6112161310649025e-05,\n",
      "      \"loss\": 0.0398,\n",
      "      \"step\": 44100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.780112794984089,\n",
      "      \"grad_norm\": 0.007425586227327585,\n",
      "      \"learning_rate\": 3.6105860113421554e-05,\n",
      "      \"loss\": 0.0555,\n",
      "      \"step\": 44120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.781373074135921,\n",
      "      \"grad_norm\": 0.026167819276452065,\n",
      "      \"learning_rate\": 3.6099558916194076e-05,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 44140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7826333532877534,\n",
      "      \"grad_norm\": 0.5303566455841064,\n",
      "      \"learning_rate\": 3.6093257718966605e-05,\n",
      "      \"loss\": 0.0194,\n",
      "      \"step\": 44160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.783893632439585,\n",
      "      \"grad_norm\": 0.002127640647813678,\n",
      "      \"learning_rate\": 3.6086956521739134e-05,\n",
      "      \"loss\": 0.0237,\n",
      "      \"step\": 44180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7851539115914177,\n",
      "      \"grad_norm\": 0.04715292155742645,\n",
      "      \"learning_rate\": 3.6080655324511656e-05,\n",
      "      \"loss\": 0.0306,\n",
      "      \"step\": 44200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7864141907432494,\n",
      "      \"grad_norm\": 0.2086862176656723,\n",
      "      \"learning_rate\": 3.6074354127284185e-05,\n",
      "      \"loss\": 0.0247,\n",
      "      \"step\": 44220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.787674469895082,\n",
      "      \"grad_norm\": 0.008774376474320889,\n",
      "      \"learning_rate\": 3.6068052930056714e-05,\n",
      "      \"loss\": 0.0165,\n",
      "      \"step\": 44240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7889347490469136,\n",
      "      \"grad_norm\": 6.394805908203125,\n",
      "      \"learning_rate\": 3.6061751732829243e-05,\n",
      "      \"loss\": 0.0302,\n",
      "      \"step\": 44260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.790195028198746,\n",
      "      \"grad_norm\": 0.3901071548461914,\n",
      "      \"learning_rate\": 3.6055450535601766e-05,\n",
      "      \"loss\": 0.0333,\n",
      "      \"step\": 44280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.791455307350578,\n",
      "      \"grad_norm\": 0.6518846750259399,\n",
      "      \"learning_rate\": 3.6049149338374295e-05,\n",
      "      \"loss\": 0.0061,\n",
      "      \"step\": 44300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7927155865024105,\n",
      "      \"grad_norm\": 0.06806481629610062,\n",
      "      \"learning_rate\": 3.604284814114682e-05,\n",
      "      \"loss\": 0.0313,\n",
      "      \"step\": 44320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.793975865654242,\n",
      "      \"grad_norm\": 0.0023826882243156433,\n",
      "      \"learning_rate\": 3.6036546943919346e-05,\n",
      "      \"loss\": 0.0252,\n",
      "      \"step\": 44340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7952361448060747,\n",
      "      \"grad_norm\": 3.9577674865722656,\n",
      "      \"learning_rate\": 3.603024574669187e-05,\n",
      "      \"loss\": 0.0371,\n",
      "      \"step\": 44360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7964964239579064,\n",
      "      \"grad_norm\": 0.3717704713344574,\n",
      "      \"learning_rate\": 3.60239445494644e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 44380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.797756703109739,\n",
      "      \"grad_norm\": 0.0029102135449647903,\n",
      "      \"learning_rate\": 3.6017643352236926e-05,\n",
      "      \"loss\": 0.0037,\n",
      "      \"step\": 44400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7990169822615707,\n",
      "      \"grad_norm\": 0.6657491326332092,\n",
      "      \"learning_rate\": 3.6011342155009455e-05,\n",
      "      \"loss\": 0.0495,\n",
      "      \"step\": 44420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8002772614134033,\n",
      "      \"grad_norm\": 0.012979051098227501,\n",
      "      \"learning_rate\": 3.6005040957781984e-05,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 44440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.801537540565235,\n",
      "      \"grad_norm\": 0.002040490973740816,\n",
      "      \"learning_rate\": 3.599873976055451e-05,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 44460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8027978197170675,\n",
      "      \"grad_norm\": 0.6612775325775146,\n",
      "      \"learning_rate\": 3.5992438563327036e-05,\n",
      "      \"loss\": 0.0405,\n",
      "      \"step\": 44480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8040580988688992,\n",
      "      \"grad_norm\": 0.017492540180683136,\n",
      "      \"learning_rate\": 3.598613736609956e-05,\n",
      "      \"loss\": 0.0249,\n",
      "      \"step\": 44500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.805318378020732,\n",
      "      \"grad_norm\": 0.10906039923429489,\n",
      "      \"learning_rate\": 3.597983616887209e-05,\n",
      "      \"loss\": 0.002,\n",
      "      \"step\": 44520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8065786571725635,\n",
      "      \"grad_norm\": 0.007312738336622715,\n",
      "      \"learning_rate\": 3.597353497164461e-05,\n",
      "      \"loss\": 0.0326,\n",
      "      \"step\": 44540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.807838936324396,\n",
      "      \"grad_norm\": 0.09161169826984406,\n",
      "      \"learning_rate\": 3.596723377441714e-05,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 44560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8090992154762278,\n",
      "      \"grad_norm\": 0.253048837184906,\n",
      "      \"learning_rate\": 3.596093257718967e-05,\n",
      "      \"loss\": 0.0457,\n",
      "      \"step\": 44580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8103594946280603,\n",
      "      \"grad_norm\": 0.019085589796304703,\n",
      "      \"learning_rate\": 3.5954631379962196e-05,\n",
      "      \"loss\": 0.0288,\n",
      "      \"step\": 44600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.811619773779892,\n",
      "      \"grad_norm\": 0.06517850607633591,\n",
      "      \"learning_rate\": 3.5948330182734725e-05,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 44620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8128800529317246,\n",
      "      \"grad_norm\": 0.06982561200857162,\n",
      "      \"learning_rate\": 3.594202898550725e-05,\n",
      "      \"loss\": 0.0553,\n",
      "      \"step\": 44640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8141403320835563,\n",
      "      \"grad_norm\": 0.0251371581107378,\n",
      "      \"learning_rate\": 3.593572778827978e-05,\n",
      "      \"loss\": 0.0125,\n",
      "      \"step\": 44660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.815400611235389,\n",
      "      \"grad_norm\": 0.020169910043478012,\n",
      "      \"learning_rate\": 3.59294265910523e-05,\n",
      "      \"loss\": 0.0216,\n",
      "      \"step\": 44680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8166608903872206,\n",
      "      \"grad_norm\": 0.010902493260800838,\n",
      "      \"learning_rate\": 3.592312539382483e-05,\n",
      "      \"loss\": 0.0323,\n",
      "      \"step\": 44700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.817921169539053,\n",
      "      \"grad_norm\": 0.4778248965740204,\n",
      "      \"learning_rate\": 3.591682419659736e-05,\n",
      "      \"loss\": 0.0371,\n",
      "      \"step\": 44720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.819181448690885,\n",
      "      \"grad_norm\": 0.00799656379967928,\n",
      "      \"learning_rate\": 3.5910522999369886e-05,\n",
      "      \"loss\": 0.0193,\n",
      "      \"step\": 44740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8204417278427174,\n",
      "      \"grad_norm\": 0.02651931159198284,\n",
      "      \"learning_rate\": 3.590422180214241e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 44760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.821702006994549,\n",
      "      \"grad_norm\": 0.013829600065946579,\n",
      "      \"learning_rate\": 3.589792060491494e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 44780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8229622861463817,\n",
      "      \"grad_norm\": 0.009334069676697254,\n",
      "      \"learning_rate\": 3.589161940768746e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 44800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8242225652982134,\n",
      "      \"grad_norm\": 0.0011606293264776468,\n",
      "      \"learning_rate\": 3.588531821045999e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 44820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.825482844450046,\n",
      "      \"grad_norm\": 0.004495114088058472,\n",
      "      \"learning_rate\": 3.587901701323251e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 44840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8267431236018776,\n",
      "      \"grad_norm\": 0.0005494867800734937,\n",
      "      \"learning_rate\": 3.587271581600504e-05,\n",
      "      \"loss\": 0.0191,\n",
      "      \"step\": 44860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.82800340275371,\n",
      "      \"grad_norm\": 0.04427934065461159,\n",
      "      \"learning_rate\": 3.586641461877757e-05,\n",
      "      \"loss\": 0.055,\n",
      "      \"step\": 44880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.829263681905542,\n",
      "      \"grad_norm\": 0.0014703402994200587,\n",
      "      \"learning_rate\": 3.58601134215501e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 44900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8305239610573745,\n",
      "      \"grad_norm\": 0.04438765347003937,\n",
      "      \"learning_rate\": 3.585381222432263e-05,\n",
      "      \"loss\": 0.0373,\n",
      "      \"step\": 44920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.831784240209206,\n",
      "      \"grad_norm\": 0.021043594926595688,\n",
      "      \"learning_rate\": 3.584751102709515e-05,\n",
      "      \"loss\": 0.0417,\n",
      "      \"step\": 44940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8330445193610387,\n",
      "      \"grad_norm\": 0.015627937391400337,\n",
      "      \"learning_rate\": 3.584120982986768e-05,\n",
      "      \"loss\": 0.0216,\n",
      "      \"step\": 44960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8343047985128704,\n",
      "      \"grad_norm\": 0.8014622330665588,\n",
      "      \"learning_rate\": 3.58349086326402e-05,\n",
      "      \"loss\": 0.0377,\n",
      "      \"step\": 44980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.835565077664703,\n",
      "      \"grad_norm\": 0.02311437390744686,\n",
      "      \"learning_rate\": 3.582860743541273e-05,\n",
      "      \"loss\": 0.0404,\n",
      "      \"step\": 45000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8368253568165347,\n",
      "      \"grad_norm\": 0.0057393466122448444,\n",
      "      \"learning_rate\": 3.582230623818525e-05,\n",
      "      \"loss\": 0.0183,\n",
      "      \"step\": 45020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8380856359683673,\n",
      "      \"grad_norm\": 0.01621226780116558,\n",
      "      \"learning_rate\": 3.581600504095778e-05,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 45040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.839345915120199,\n",
      "      \"grad_norm\": 1.0167211294174194,\n",
      "      \"learning_rate\": 3.580970384373031e-05,\n",
      "      \"loss\": 0.0591,\n",
      "      \"step\": 45060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.840606194272031,\n",
      "      \"grad_norm\": 0.03927284851670265,\n",
      "      \"learning_rate\": 3.580340264650284e-05,\n",
      "      \"loss\": 0.0376,\n",
      "      \"step\": 45080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8418664734238632,\n",
      "      \"grad_norm\": 0.010262842290103436,\n",
      "      \"learning_rate\": 3.579710144927537e-05,\n",
      "      \"loss\": 0.0043,\n",
      "      \"step\": 45100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8431267525756954,\n",
      "      \"grad_norm\": 0.07520966976881027,\n",
      "      \"learning_rate\": 3.579080025204789e-05,\n",
      "      \"loss\": 0.0471,\n",
      "      \"step\": 45120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8443870317275275,\n",
      "      \"grad_norm\": 0.026404444128274918,\n",
      "      \"learning_rate\": 3.578449905482042e-05,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 45140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8456473108793596,\n",
      "      \"grad_norm\": 22.150859832763672,\n",
      "      \"learning_rate\": 3.577819785759294e-05,\n",
      "      \"loss\": 0.0503,\n",
      "      \"step\": 45160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8469075900311918,\n",
      "      \"grad_norm\": 0.07456839829683304,\n",
      "      \"learning_rate\": 3.577189666036547e-05,\n",
      "      \"loss\": 0.0866,\n",
      "      \"step\": 45180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.848167869183024,\n",
      "      \"grad_norm\": 0.7557910084724426,\n",
      "      \"learning_rate\": 3.576559546313799e-05,\n",
      "      \"loss\": 0.0418,\n",
      "      \"step\": 45200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.849428148334856,\n",
      "      \"grad_norm\": 0.02294868230819702,\n",
      "      \"learning_rate\": 3.575929426591053e-05,\n",
      "      \"loss\": 0.0073,\n",
      "      \"step\": 45220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.850688427486688,\n",
      "      \"grad_norm\": 0.04030139744281769,\n",
      "      \"learning_rate\": 3.575299306868305e-05,\n",
      "      \"loss\": 0.0613,\n",
      "      \"step\": 45240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8519487066385203,\n",
      "      \"grad_norm\": 0.003791635623201728,\n",
      "      \"learning_rate\": 3.574669187145558e-05,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 45260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8532089857903524,\n",
      "      \"grad_norm\": 0.004240511450916529,\n",
      "      \"learning_rate\": 3.57403906742281e-05,\n",
      "      \"loss\": 0.0038,\n",
      "      \"step\": 45280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8544692649421846,\n",
      "      \"grad_norm\": 0.013950305990874767,\n",
      "      \"learning_rate\": 3.573408947700063e-05,\n",
      "      \"loss\": 0.0441,\n",
      "      \"step\": 45300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8557295440940167,\n",
      "      \"grad_norm\": 0.259488582611084,\n",
      "      \"learning_rate\": 3.572778827977316e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 45320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.856989823245849,\n",
      "      \"grad_norm\": 0.003241002094000578,\n",
      "      \"learning_rate\": 3.572148708254568e-05,\n",
      "      \"loss\": 0.0189,\n",
      "      \"step\": 45340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.858250102397681,\n",
      "      \"grad_norm\": 0.00650510098785162,\n",
      "      \"learning_rate\": 3.571518588531821e-05,\n",
      "      \"loss\": 0.0135,\n",
      "      \"step\": 45360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.859510381549513,\n",
      "      \"grad_norm\": 0.0028876808937639,\n",
      "      \"learning_rate\": 3.570888468809074e-05,\n",
      "      \"loss\": 0.0467,\n",
      "      \"step\": 45380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8607706607013452,\n",
      "      \"grad_norm\": 0.018016181886196136,\n",
      "      \"learning_rate\": 3.570258349086327e-05,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 45400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8620309398531774,\n",
      "      \"grad_norm\": 2.275397777557373,\n",
      "      \"learning_rate\": 3.569628229363579e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 45420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8632912190050095,\n",
      "      \"grad_norm\": 0.086887888610363,\n",
      "      \"learning_rate\": 3.568998109640832e-05,\n",
      "      \"loss\": 0.0342,\n",
      "      \"step\": 45440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8645514981568416,\n",
      "      \"grad_norm\": 0.005593094043433666,\n",
      "      \"learning_rate\": 3.5683679899180843e-05,\n",
      "      \"loss\": 0.0023,\n",
      "      \"step\": 45460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8658117773086738,\n",
      "      \"grad_norm\": 0.02677580900490284,\n",
      "      \"learning_rate\": 3.567737870195337e-05,\n",
      "      \"loss\": 0.022,\n",
      "      \"step\": 45480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.867072056460506,\n",
      "      \"grad_norm\": 0.013917379081249237,\n",
      "      \"learning_rate\": 3.5671077504725895e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 45500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.868332335612338,\n",
      "      \"grad_norm\": 0.19785015285015106,\n",
      "      \"learning_rate\": 3.5664776307498424e-05,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 45520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.86959261476417,\n",
      "      \"grad_norm\": 0.0005471145268529654,\n",
      "      \"learning_rate\": 3.565847511027095e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 45540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8708528939160023,\n",
      "      \"grad_norm\": 0.0015910061774775386,\n",
      "      \"learning_rate\": 3.5652488972904854e-05,\n",
      "      \"loss\": 0.034,\n",
      "      \"step\": 45560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8721131730678344,\n",
      "      \"grad_norm\": 2.9503846168518066,\n",
      "      \"learning_rate\": 3.564618777567738e-05,\n",
      "      \"loss\": 0.0165,\n",
      "      \"step\": 45580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8733734522196666,\n",
      "      \"grad_norm\": 0.0007103359093889594,\n",
      "      \"learning_rate\": 3.5639886578449905e-05,\n",
      "      \"loss\": 0.05,\n",
      "      \"step\": 45600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8746337313714987,\n",
      "      \"grad_norm\": 2.323545217514038,\n",
      "      \"learning_rate\": 3.5633585381222434e-05,\n",
      "      \"loss\": 0.0535,\n",
      "      \"step\": 45620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.875894010523331,\n",
      "      \"grad_norm\": 0.0031707528978586197,\n",
      "      \"learning_rate\": 3.562728418399496e-05,\n",
      "      \"loss\": 0.0077,\n",
      "      \"step\": 45640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.877154289675163,\n",
      "      \"grad_norm\": 0.006499710958451033,\n",
      "      \"learning_rate\": 3.5620982986767485e-05,\n",
      "      \"loss\": 0.0058,\n",
      "      \"step\": 45660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.878414568826995,\n",
      "      \"grad_norm\": 0.012457016855478287,\n",
      "      \"learning_rate\": 3.5614681789540014e-05,\n",
      "      \"loss\": 0.0635,\n",
      "      \"step\": 45680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8796748479788272,\n",
      "      \"grad_norm\": 0.2280256301164627,\n",
      "      \"learning_rate\": 3.5608380592312543e-05,\n",
      "      \"loss\": 0.0392,\n",
      "      \"step\": 45700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8809351271306594,\n",
      "      \"grad_norm\": 0.004554204177111387,\n",
      "      \"learning_rate\": 3.560207939508507e-05,\n",
      "      \"loss\": 0.0253,\n",
      "      \"step\": 45720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8821954062824915,\n",
      "      \"grad_norm\": 0.016643429175019264,\n",
      "      \"learning_rate\": 3.5595778197857595e-05,\n",
      "      \"loss\": 0.0209,\n",
      "      \"step\": 45740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8834556854343236,\n",
      "      \"grad_norm\": 0.003989764954894781,\n",
      "      \"learning_rate\": 3.5589477000630124e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 45760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8847159645861558,\n",
      "      \"grad_norm\": 0.0020814044401049614,\n",
      "      \"learning_rate\": 3.5583175803402646e-05,\n",
      "      \"loss\": 0.0499,\n",
      "      \"step\": 45780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.885976243737988,\n",
      "      \"grad_norm\": 0.1387823224067688,\n",
      "      \"learning_rate\": 3.5576874606175175e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 45800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.88723652288982,\n",
      "      \"grad_norm\": 0.0010703080333769321,\n",
      "      \"learning_rate\": 3.55705734089477e-05,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 45820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.888496802041652,\n",
      "      \"grad_norm\": 0.01683555543422699,\n",
      "      \"learning_rate\": 3.5564272211720226e-05,\n",
      "      \"loss\": 0.0387,\n",
      "      \"step\": 45840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8897570811934843,\n",
      "      \"grad_norm\": 9.157674789428711,\n",
      "      \"learning_rate\": 3.5557971014492755e-05,\n",
      "      \"loss\": 0.0464,\n",
      "      \"step\": 45860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8910173603453164,\n",
      "      \"grad_norm\": 0.017030350863933563,\n",
      "      \"learning_rate\": 3.5551669817265285e-05,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 45880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8922776394971486,\n",
      "      \"grad_norm\": 0.27712106704711914,\n",
      "      \"learning_rate\": 3.5545368620037814e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 45900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8935379186489807,\n",
      "      \"grad_norm\": 0.11551825702190399,\n",
      "      \"learning_rate\": 3.5539067422810336e-05,\n",
      "      \"loss\": 0.0166,\n",
      "      \"step\": 45920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.894798197800813,\n",
      "      \"grad_norm\": 0.004854084458202124,\n",
      "      \"learning_rate\": 3.5532766225582865e-05,\n",
      "      \"loss\": 0.0329,\n",
      "      \"step\": 45940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.896058476952645,\n",
      "      \"grad_norm\": 0.0011735897278413177,\n",
      "      \"learning_rate\": 3.552646502835539e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 45960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.897318756104477,\n",
      "      \"grad_norm\": 0.011614409275352955,\n",
      "      \"learning_rate\": 3.5520163831127916e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 45980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.898579035256309,\n",
      "      \"grad_norm\": 0.007548046763986349,\n",
      "      \"learning_rate\": 3.551386263390044e-05,\n",
      "      \"loss\": 0.0269,\n",
      "      \"step\": 46000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8998393144081414,\n",
      "      \"grad_norm\": 0.11755532026290894,\n",
      "      \"learning_rate\": 3.550756143667297e-05,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 46020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9010995935599735,\n",
      "      \"grad_norm\": 0.008575315587222576,\n",
      "      \"learning_rate\": 3.5501260239445497e-05,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 46040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9023598727118056,\n",
      "      \"grad_norm\": 0.035672303289175034,\n",
      "      \"learning_rate\": 3.5494959042218026e-05,\n",
      "      \"loss\": 0.0732,\n",
      "      \"step\": 46060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9036201518636378,\n",
      "      \"grad_norm\": 0.012630663812160492,\n",
      "      \"learning_rate\": 3.548865784499055e-05,\n",
      "      \"loss\": 0.0321,\n",
      "      \"step\": 46080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.90488043101547,\n",
      "      \"grad_norm\": 0.0855456292629242,\n",
      "      \"learning_rate\": 3.548235664776308e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 46100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.906140710167302,\n",
      "      \"grad_norm\": 6.9342546463012695,\n",
      "      \"learning_rate\": 3.5476055450535606e-05,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 46120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.907400989319134,\n",
      "      \"grad_norm\": 62.10862350463867,\n",
      "      \"learning_rate\": 3.546975425330813e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 46140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9086612684709663,\n",
      "      \"grad_norm\": 0.0034083868376910686,\n",
      "      \"learning_rate\": 3.546345305608066e-05,\n",
      "      \"loss\": 0.0456,\n",
      "      \"step\": 46160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9099215476227984,\n",
      "      \"grad_norm\": 0.022387124598026276,\n",
      "      \"learning_rate\": 3.545715185885318e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 46180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9111818267746306,\n",
      "      \"grad_norm\": 6.59954309463501,\n",
      "      \"learning_rate\": 3.5450850661625715e-05,\n",
      "      \"loss\": 0.0358,\n",
      "      \"step\": 46200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9124421059264627,\n",
      "      \"grad_norm\": 0.04292643442749977,\n",
      "      \"learning_rate\": 3.544454946439824e-05,\n",
      "      \"loss\": 0.02,\n",
      "      \"step\": 46220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.913702385078295,\n",
      "      \"grad_norm\": 0.13698090612888336,\n",
      "      \"learning_rate\": 3.5438248267170767e-05,\n",
      "      \"loss\": 0.0439,\n",
      "      \"step\": 46240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.914962664230127,\n",
      "      \"grad_norm\": 0.198935404419899,\n",
      "      \"learning_rate\": 3.543194706994329e-05,\n",
      "      \"loss\": 0.0595,\n",
      "      \"step\": 46260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.916222943381959,\n",
      "      \"grad_norm\": 0.11218107491731644,\n",
      "      \"learning_rate\": 3.542564587271582e-05,\n",
      "      \"loss\": 0.0347,\n",
      "      \"step\": 46280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.917483222533791,\n",
      "      \"grad_norm\": 0.08876625448465347,\n",
      "      \"learning_rate\": 3.541934467548834e-05,\n",
      "      \"loss\": 0.0805,\n",
      "      \"step\": 46300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9187435016856234,\n",
      "      \"grad_norm\": 5.431225776672363,\n",
      "      \"learning_rate\": 3.541304347826087e-05,\n",
      "      \"loss\": 0.0442,\n",
      "      \"step\": 46320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9200037808374555,\n",
      "      \"grad_norm\": 0.2524961233139038,\n",
      "      \"learning_rate\": 3.54067422810334e-05,\n",
      "      \"loss\": 0.0401,\n",
      "      \"step\": 46340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9212640599892876,\n",
      "      \"grad_norm\": 0.03894632309675217,\n",
      "      \"learning_rate\": 3.540044108380593e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 46360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9225243391411198,\n",
      "      \"grad_norm\": 0.006400363519787788,\n",
      "      \"learning_rate\": 3.5394139886578456e-05,\n",
      "      \"loss\": 0.0403,\n",
      "      \"step\": 46380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.923784618292952,\n",
      "      \"grad_norm\": 4.532511234283447,\n",
      "      \"learning_rate\": 3.538783868935098e-05,\n",
      "      \"loss\": 0.0191,\n",
      "      \"step\": 46400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.925044897444784,\n",
      "      \"grad_norm\": 0.04782802239060402,\n",
      "      \"learning_rate\": 3.538153749212351e-05,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 46420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.926305176596616,\n",
      "      \"grad_norm\": 0.1646578013896942,\n",
      "      \"learning_rate\": 3.537523629489603e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 46440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9275654557484483,\n",
      "      \"grad_norm\": 0.025722388178110123,\n",
      "      \"learning_rate\": 3.536893509766856e-05,\n",
      "      \"loss\": 0.0235,\n",
      "      \"step\": 46460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9288257349002804,\n",
      "      \"grad_norm\": 0.4041644334793091,\n",
      "      \"learning_rate\": 3.536263390044108e-05,\n",
      "      \"loss\": 0.032,\n",
      "      \"step\": 46480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9300860140521126,\n",
      "      \"grad_norm\": 0.014303207397460938,\n",
      "      \"learning_rate\": 3.535633270321361e-05,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 46500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9313462932039447,\n",
      "      \"grad_norm\": 0.017480021342635155,\n",
      "      \"learning_rate\": 3.535003150598614e-05,\n",
      "      \"loss\": 0.0161,\n",
      "      \"step\": 46520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.932606572355777,\n",
      "      \"grad_norm\": 0.07126062363386154,\n",
      "      \"learning_rate\": 3.534373030875867e-05,\n",
      "      \"loss\": 0.0249,\n",
      "      \"step\": 46540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.933866851507609,\n",
      "      \"grad_norm\": 0.0037520851474255323,\n",
      "      \"learning_rate\": 3.53374291115312e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 46560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.935127130659441,\n",
      "      \"grad_norm\": 0.009466981515288353,\n",
      "      \"learning_rate\": 3.533112791430372e-05,\n",
      "      \"loss\": 0.0166,\n",
      "      \"step\": 46580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.936387409811273,\n",
      "      \"grad_norm\": 0.0023208288475871086,\n",
      "      \"learning_rate\": 3.532482671707625e-05,\n",
      "      \"loss\": 0.0235,\n",
      "      \"step\": 46600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9376476889631054,\n",
      "      \"grad_norm\": 0.0013236789964139462,\n",
      "      \"learning_rate\": 3.531852551984877e-05,\n",
      "      \"loss\": 0.0083,\n",
      "      \"step\": 46620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9389079681149375,\n",
      "      \"grad_norm\": 0.23194819688796997,\n",
      "      \"learning_rate\": 3.53122243226213e-05,\n",
      "      \"loss\": 0.0592,\n",
      "      \"step\": 46640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9401682472667696,\n",
      "      \"grad_norm\": 0.010477311909198761,\n",
      "      \"learning_rate\": 3.530592312539382e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 46660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9414285264186018,\n",
      "      \"grad_norm\": 0.0537622906267643,\n",
      "      \"learning_rate\": 3.529962192816635e-05,\n",
      "      \"loss\": 0.0291,\n",
      "      \"step\": 46680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.942688805570434,\n",
      "      \"grad_norm\": 0.002102671656757593,\n",
      "      \"learning_rate\": 3.529332073093888e-05,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 46700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.943949084722266,\n",
      "      \"grad_norm\": 0.013155029155313969,\n",
      "      \"learning_rate\": 3.528701953371141e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 46720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.945209363874098,\n",
      "      \"grad_norm\": 0.006086347624659538,\n",
      "      \"learning_rate\": 3.528071833648393e-05,\n",
      "      \"loss\": 0.0723,\n",
      "      \"step\": 46740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9464696430259303,\n",
      "      \"grad_norm\": 3.97857928276062,\n",
      "      \"learning_rate\": 3.527441713925646e-05,\n",
      "      \"loss\": 0.0713,\n",
      "      \"step\": 46760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9477299221777624,\n",
      "      \"grad_norm\": 0.010396630503237247,\n",
      "      \"learning_rate\": 3.526811594202899e-05,\n",
      "      \"loss\": 0.016,\n",
      "      \"step\": 46780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9489902013295946,\n",
      "      \"grad_norm\": 7.372612476348877,\n",
      "      \"learning_rate\": 3.526181474480151e-05,\n",
      "      \"loss\": 0.0271,\n",
      "      \"step\": 46800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9502504804814267,\n",
      "      \"grad_norm\": 0.08527341485023499,\n",
      "      \"learning_rate\": 3.525551354757404e-05,\n",
      "      \"loss\": 0.0581,\n",
      "      \"step\": 46820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.951510759633259,\n",
      "      \"grad_norm\": 5.995270729064941,\n",
      "      \"learning_rate\": 3.524921235034656e-05,\n",
      "      \"loss\": 0.0198,\n",
      "      \"step\": 46840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.952771038785091,\n",
      "      \"grad_norm\": 0.036482326686382294,\n",
      "      \"learning_rate\": 3.52429111531191e-05,\n",
      "      \"loss\": 0.0298,\n",
      "      \"step\": 46860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.954031317936923,\n",
      "      \"grad_norm\": 0.0005311388522386551,\n",
      "      \"learning_rate\": 3.523660995589162e-05,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 46880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.955291597088755,\n",
      "      \"grad_norm\": 0.48356518149375916,\n",
      "      \"learning_rate\": 3.523030875866415e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 46900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9565518762405874,\n",
      "      \"grad_norm\": 0.0952572152018547,\n",
      "      \"learning_rate\": 3.522400756143667e-05,\n",
      "      \"loss\": 0.0049,\n",
      "      \"step\": 46920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9578121553924195,\n",
      "      \"grad_norm\": 0.0005700653418898582,\n",
      "      \"learning_rate\": 3.52177063642092e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 46940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9590724345442516,\n",
      "      \"grad_norm\": 0.026148641481995583,\n",
      "      \"learning_rate\": 3.5211405166981724e-05,\n",
      "      \"loss\": 0.0262,\n",
      "      \"step\": 46960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9603327136960838,\n",
      "      \"grad_norm\": 0.06926444917917252,\n",
      "      \"learning_rate\": 3.520510396975425e-05,\n",
      "      \"loss\": 0.0198,\n",
      "      \"step\": 46980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.961592992847916,\n",
      "      \"grad_norm\": 0.005730061326175928,\n",
      "      \"learning_rate\": 3.519880277252678e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 47000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.962853271999748,\n",
      "      \"grad_norm\": 0.0009743878035806119,\n",
      "      \"learning_rate\": 3.519250157529931e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 47020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.96411355115158,\n",
      "      \"grad_norm\": 0.08077763020992279,\n",
      "      \"learning_rate\": 3.518620037807184e-05,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 47040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9653738303034123,\n",
      "      \"grad_norm\": 0.4655361473560333,\n",
      "      \"learning_rate\": 3.517989918084436e-05,\n",
      "      \"loss\": 0.0495,\n",
      "      \"step\": 47060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9666341094552444,\n",
      "      \"grad_norm\": 0.009090066887438297,\n",
      "      \"learning_rate\": 3.517359798361689e-05,\n",
      "      \"loss\": 0.0115,\n",
      "      \"step\": 47080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9678943886070766,\n",
      "      \"grad_norm\": 0.004823402967303991,\n",
      "      \"learning_rate\": 3.5167296786389414e-05,\n",
      "      \"loss\": 0.0244,\n",
      "      \"step\": 47100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9691546677589087,\n",
      "      \"grad_norm\": 0.02988414093852043,\n",
      "      \"learning_rate\": 3.516099558916194e-05,\n",
      "      \"loss\": 0.0523,\n",
      "      \"step\": 47120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.970414946910741,\n",
      "      \"grad_norm\": 1.6742382049560547,\n",
      "      \"learning_rate\": 3.5154694391934465e-05,\n",
      "      \"loss\": 0.019,\n",
      "      \"step\": 47140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.971675226062573,\n",
      "      \"grad_norm\": 0.34945863485336304,\n",
      "      \"learning_rate\": 3.5148393194706994e-05,\n",
      "      \"loss\": 0.0504,\n",
      "      \"step\": 47160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.972935505214405,\n",
      "      \"grad_norm\": 5.500312805175781,\n",
      "      \"learning_rate\": 3.514209199747952e-05,\n",
      "      \"loss\": 0.025,\n",
      "      \"step\": 47180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.974195784366237,\n",
      "      \"grad_norm\": 0.004336615093052387,\n",
      "      \"learning_rate\": 3.513579080025205e-05,\n",
      "      \"loss\": 0.0241,\n",
      "      \"step\": 47200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9754560635180693,\n",
      "      \"grad_norm\": 0.6626218557357788,\n",
      "      \"learning_rate\": 3.512948960302458e-05,\n",
      "      \"loss\": 0.0338,\n",
      "      \"step\": 47220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9767163426699015,\n",
      "      \"grad_norm\": 5.245707988739014,\n",
      "      \"learning_rate\": 3.51231884057971e-05,\n",
      "      \"loss\": 0.0361,\n",
      "      \"step\": 47240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9779766218217336,\n",
      "      \"grad_norm\": 0.005353104323148727,\n",
      "      \"learning_rate\": 3.511688720856963e-05,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 47260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9792369009735657,\n",
      "      \"grad_norm\": 7.848406791687012,\n",
      "      \"learning_rate\": 3.5110586011342155e-05,\n",
      "      \"loss\": 0.0548,\n",
      "      \"step\": 47280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.980497180125398,\n",
      "      \"grad_norm\": 0.004592181649059057,\n",
      "      \"learning_rate\": 3.5104284814114684e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 47300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.98175745927723,\n",
      "      \"grad_norm\": 0.37908923625946045,\n",
      "      \"learning_rate\": 3.5097983616887206e-05,\n",
      "      \"loss\": 0.0563,\n",
      "      \"step\": 47320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.983017738429062,\n",
      "      \"grad_norm\": 0.03271453455090523,\n",
      "      \"learning_rate\": 3.5091682419659735e-05,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 47340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9842780175808943,\n",
      "      \"grad_norm\": 0.00037963365321047604,\n",
      "      \"learning_rate\": 3.5085381222432264e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 47360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9855382967327264,\n",
      "      \"grad_norm\": 0.24723504483699799,\n",
      "      \"learning_rate\": 3.507908002520479e-05,\n",
      "      \"loss\": 0.0569,\n",
      "      \"step\": 47380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9867985758845585,\n",
      "      \"grad_norm\": 0.019835714250802994,\n",
      "      \"learning_rate\": 3.5072778827977315e-05,\n",
      "      \"loss\": 0.0023,\n",
      "      \"step\": 47400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9880588550363907,\n",
      "      \"grad_norm\": 0.019475063309073448,\n",
      "      \"learning_rate\": 3.5066477630749844e-05,\n",
      "      \"loss\": 0.026,\n",
      "      \"step\": 47420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.989319134188223,\n",
      "      \"grad_norm\": 0.023180151358246803,\n",
      "      \"learning_rate\": 3.5060176433522373e-05,\n",
      "      \"loss\": 0.045,\n",
      "      \"step\": 47440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.990579413340055,\n",
      "      \"grad_norm\": 0.006590165663510561,\n",
      "      \"learning_rate\": 3.5053875236294896e-05,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 47460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.991839692491887,\n",
      "      \"grad_norm\": 0.14307112991809845,\n",
      "      \"learning_rate\": 3.5047574039067425e-05,\n",
      "      \"loss\": 0.0067,\n",
      "      \"step\": 47480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.993099971643719,\n",
      "      \"grad_norm\": 0.04798722267150879,\n",
      "      \"learning_rate\": 3.5041272841839954e-05,\n",
      "      \"loss\": 0.0314,\n",
      "      \"step\": 47500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9943602507955513,\n",
      "      \"grad_norm\": 0.008921800181269646,\n",
      "      \"learning_rate\": 3.503497164461248e-05,\n",
      "      \"loss\": 0.0353,\n",
      "      \"step\": 47520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9956205299473835,\n",
      "      \"grad_norm\": 0.29096749424934387,\n",
      "      \"learning_rate\": 3.5028670447385005e-05,\n",
      "      \"loss\": 0.0402,\n",
      "      \"step\": 47540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9968808090992156,\n",
      "      \"grad_norm\": 0.11283610761165619,\n",
      "      \"learning_rate\": 3.5022369250157534e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 47560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9981410882510477,\n",
      "      \"grad_norm\": 0.014420277439057827,\n",
      "      \"learning_rate\": 3.5016068052930056e-05,\n",
      "      \"loss\": 0.0401,\n",
      "      \"step\": 47580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.99940136740288,\n",
      "      \"grad_norm\": 0.014960888773202896,\n",
      "      \"learning_rate\": 3.5009766855702585e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 47600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0,\n",
      "      \"eval_accuracy\": 0.9981097599395123,\n",
      "      \"eval_f1\": 0.9981122577397432,\n",
      "      \"eval_loss\": 0.007480223197489977,\n",
      "      \"eval_precision\": 0.9967950732105826,\n",
      "      \"eval_recall\": 0.9994329279818537,\n",
      "      \"eval_runtime\": 549.7987,\n",
      "      \"eval_samples_per_second\": 57.734,\n",
      "      \"eval_steps_per_second\": 7.217,\n",
      "      \"step\": 47610\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.000630139575916,\n",
      "      \"grad_norm\": 0.010173485614359379,\n",
      "      \"learning_rate\": 3.500346565847511e-05,\n",
      "      \"loss\": 0.0174,\n",
      "      \"step\": 47620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.001890418727748,\n",
      "      \"grad_norm\": 0.007360718213021755,\n",
      "      \"learning_rate\": 3.499716446124764e-05,\n",
      "      \"loss\": 0.0431,\n",
      "      \"step\": 47640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.00315069787958,\n",
      "      \"grad_norm\": 0.3656368553638458,\n",
      "      \"learning_rate\": 3.4990863264020166e-05,\n",
      "      \"loss\": 0.0284,\n",
      "      \"step\": 47660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0044109770314122,\n",
      "      \"grad_norm\": 0.01838533580303192,\n",
      "      \"learning_rate\": 3.4984562066792695e-05,\n",
      "      \"loss\": 0.0524,\n",
      "      \"step\": 47680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0056712561832444,\n",
      "      \"grad_norm\": 6.619208335876465,\n",
      "      \"learning_rate\": 3.4978260869565224e-05,\n",
      "      \"loss\": 0.0388,\n",
      "      \"step\": 47700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0069315353350765,\n",
      "      \"grad_norm\": 0.010527941398322582,\n",
      "      \"learning_rate\": 3.4971959672337746e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 47720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0081918144869086,\n",
      "      \"grad_norm\": 0.027960332110524178,\n",
      "      \"learning_rate\": 3.4965658475110275e-05,\n",
      "      \"loss\": 0.0184,\n",
      "      \"step\": 47740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0094520936387408,\n",
      "      \"grad_norm\": 0.06072966754436493,\n",
      "      \"learning_rate\": 3.49593572778828e-05,\n",
      "      \"loss\": 0.0228,\n",
      "      \"step\": 47760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.010712372790573,\n",
      "      \"grad_norm\": 4.4400482177734375,\n",
      "      \"learning_rate\": 3.4953056080655326e-05,\n",
      "      \"loss\": 0.0412,\n",
      "      \"step\": 47780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.011972651942405,\n",
      "      \"grad_norm\": 0.1713486760854721,\n",
      "      \"learning_rate\": 3.494675488342785e-05,\n",
      "      \"loss\": 0.0659,\n",
      "      \"step\": 47800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.013232931094237,\n",
      "      \"grad_norm\": 0.03210834413766861,\n",
      "      \"learning_rate\": 3.494045368620038e-05,\n",
      "      \"loss\": 0.0244,\n",
      "      \"step\": 47820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0144932102460693,\n",
      "      \"grad_norm\": 0.008721519261598587,\n",
      "      \"learning_rate\": 3.493415248897291e-05,\n",
      "      \"loss\": 0.0428,\n",
      "      \"step\": 47840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0157534893979014,\n",
      "      \"grad_norm\": 0.04918608441948891,\n",
      "      \"learning_rate\": 3.4927851291745436e-05,\n",
      "      \"loss\": 0.0644,\n",
      "      \"step\": 47860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0170137685497336,\n",
      "      \"grad_norm\": 0.0486781969666481,\n",
      "      \"learning_rate\": 3.492155009451796e-05,\n",
      "      \"loss\": 0.0115,\n",
      "      \"step\": 47880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0182740477015657,\n",
      "      \"grad_norm\": 0.06465313583612442,\n",
      "      \"learning_rate\": 3.491524889729049e-05,\n",
      "      \"loss\": 0.0256,\n",
      "      \"step\": 47900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.019534326853398,\n",
      "      \"grad_norm\": 0.13272325694561005,\n",
      "      \"learning_rate\": 3.4908947700063016e-05,\n",
      "      \"loss\": 0.0312,\n",
      "      \"step\": 47920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.02079460600523,\n",
      "      \"grad_norm\": 4.785900592803955,\n",
      "      \"learning_rate\": 3.490264650283554e-05,\n",
      "      \"loss\": 0.0513,\n",
      "      \"step\": 47940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.022054885157062,\n",
      "      \"grad_norm\": 0.027620747685432434,\n",
      "      \"learning_rate\": 3.489634530560807e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 47960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0233151643088942,\n",
      "      \"grad_norm\": 0.010848915204405785,\n",
      "      \"learning_rate\": 3.489004410838059e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 47980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0245754434607264,\n",
      "      \"grad_norm\": 0.00912554282695055,\n",
      "      \"learning_rate\": 3.4883742911153126e-05,\n",
      "      \"loss\": 0.0277,\n",
      "      \"step\": 48000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0258357226125585,\n",
      "      \"grad_norm\": 4.051196575164795,\n",
      "      \"learning_rate\": 3.487744171392565e-05,\n",
      "      \"loss\": 0.034,\n",
      "      \"step\": 48020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0270960017643906,\n",
      "      \"grad_norm\": 0.03702365979552269,\n",
      "      \"learning_rate\": 3.487114051669818e-05,\n",
      "      \"loss\": 0.0578,\n",
      "      \"step\": 48040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0283562809162228,\n",
      "      \"grad_norm\": 0.028181100264191628,\n",
      "      \"learning_rate\": 3.48648393194707e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 48060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.029616560068055,\n",
      "      \"grad_norm\": 0.005970553494989872,\n",
      "      \"learning_rate\": 3.485853812224323e-05,\n",
      "      \"loss\": 0.0378,\n",
      "      \"step\": 48080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.030876839219887,\n",
      "      \"grad_norm\": 0.01028367131948471,\n",
      "      \"learning_rate\": 3.485223692501575e-05,\n",
      "      \"loss\": 0.0531,\n",
      "      \"step\": 48100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.032137118371719,\n",
      "      \"grad_norm\": 0.3151876628398895,\n",
      "      \"learning_rate\": 3.484593572778828e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 48120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0333973975235513,\n",
      "      \"grad_norm\": 8.871977806091309,\n",
      "      \"learning_rate\": 3.483963453056081e-05,\n",
      "      \"loss\": 0.0676,\n",
      "      \"step\": 48140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0346576766753834,\n",
      "      \"grad_norm\": 0.054914895445108414,\n",
      "      \"learning_rate\": 3.483333333333334e-05,\n",
      "      \"loss\": 0.0485,\n",
      "      \"step\": 48160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0359179558272156,\n",
      "      \"grad_norm\": 0.08428942412137985,\n",
      "      \"learning_rate\": 3.4827032136105867e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 48180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0371782349790477,\n",
      "      \"grad_norm\": 3.2818527221679688,\n",
      "      \"learning_rate\": 3.482073093887839e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 48200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.03843851413088,\n",
      "      \"grad_norm\": 4.78464937210083,\n",
      "      \"learning_rate\": 3.481442974165092e-05,\n",
      "      \"loss\": 0.0218,\n",
      "      \"step\": 48220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.039698793282712,\n",
      "      \"grad_norm\": 0.03336138650774956,\n",
      "      \"learning_rate\": 3.480812854442344e-05,\n",
      "      \"loss\": 0.0197,\n",
      "      \"step\": 48240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.040959072434544,\n",
      "      \"grad_norm\": 0.0006742220721207559,\n",
      "      \"learning_rate\": 3.480182734719597e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 48260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0422193515863762,\n",
      "      \"grad_norm\": 0.010469312779605389,\n",
      "      \"learning_rate\": 3.479552614996849e-05,\n",
      "      \"loss\": 0.0241,\n",
      "      \"step\": 48280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0434796307382084,\n",
      "      \"grad_norm\": 0.0009539536549709737,\n",
      "      \"learning_rate\": 3.478922495274102e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 48300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0447399098900405,\n",
      "      \"grad_norm\": 0.7535634636878967,\n",
      "      \"learning_rate\": 3.478292375551355e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 48320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0460001890418726,\n",
      "      \"grad_norm\": 0.0006580838817171752,\n",
      "      \"learning_rate\": 3.477662255828608e-05,\n",
      "      \"loss\": 0.0283,\n",
      "      \"step\": 48340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0472604681937048,\n",
      "      \"grad_norm\": 0.0027574682608246803,\n",
      "      \"learning_rate\": 3.477032136105861e-05,\n",
      "      \"loss\": 0.0278,\n",
      "      \"step\": 48360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.048520747345537,\n",
      "      \"grad_norm\": 0.011836035177111626,\n",
      "      \"learning_rate\": 3.476402016383113e-05,\n",
      "      \"loss\": 0.0202,\n",
      "      \"step\": 48380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.049781026497369,\n",
      "      \"grad_norm\": 11.055289268493652,\n",
      "      \"learning_rate\": 3.475771896660366e-05,\n",
      "      \"loss\": 0.0283,\n",
      "      \"step\": 48400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.051041305649201,\n",
      "      \"grad_norm\": 0.00413689948618412,\n",
      "      \"learning_rate\": 3.475141776937618e-05,\n",
      "      \"loss\": 0.0198,\n",
      "      \"step\": 48420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0523015848010333,\n",
      "      \"grad_norm\": 0.3146761655807495,\n",
      "      \"learning_rate\": 3.474511657214871e-05,\n",
      "      \"loss\": 0.0165,\n",
      "      \"step\": 48440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0535618639528654,\n",
      "      \"grad_norm\": Infinity,\n",
      "      \"learning_rate\": 3.473881537492123e-05,\n",
      "      \"loss\": 0.0241,\n",
      "      \"step\": 48460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0548221431046976,\n",
      "      \"grad_norm\": 0.21915952861309052,\n",
      "      \"learning_rate\": 3.473282923755514e-05,\n",
      "      \"loss\": 0.0343,\n",
      "      \"step\": 48480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0560824222565297,\n",
      "      \"grad_norm\": 0.0012646274408325553,\n",
      "      \"learning_rate\": 3.472652804032767e-05,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 48500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.057342701408362,\n",
      "      \"grad_norm\": 0.017324645072221756,\n",
      "      \"learning_rate\": 3.472022684310019e-05,\n",
      "      \"loss\": 0.0285,\n",
      "      \"step\": 48520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.058602980560194,\n",
      "      \"grad_norm\": 0.005477322265505791,\n",
      "      \"learning_rate\": 3.471392564587272e-05,\n",
      "      \"loss\": 0.0325,\n",
      "      \"step\": 48540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.059863259712026,\n",
      "      \"grad_norm\": 0.0010054581798613071,\n",
      "      \"learning_rate\": 3.470762444864524e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 48560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0611235388638582,\n",
      "      \"grad_norm\": 4.5033416748046875,\n",
      "      \"learning_rate\": 3.470132325141777e-05,\n",
      "      \"loss\": 0.0115,\n",
      "      \"step\": 48580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0623838180156904,\n",
      "      \"grad_norm\": 0.03017948754131794,\n",
      "      \"learning_rate\": 3.4695022054190294e-05,\n",
      "      \"loss\": 0.0262,\n",
      "      \"step\": 48600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0636440971675225,\n",
      "      \"grad_norm\": 0.008603118360042572,\n",
      "      \"learning_rate\": 3.468872085696282e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 48620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0649043763193546,\n",
      "      \"grad_norm\": 0.13700689375400543,\n",
      "      \"learning_rate\": 3.468241965973535e-05,\n",
      "      \"loss\": 0.0685,\n",
      "      \"step\": 48640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0661646554711868,\n",
      "      \"grad_norm\": 0.022455597296357155,\n",
      "      \"learning_rate\": 3.467611846250788e-05,\n",
      "      \"loss\": 0.0273,\n",
      "      \"step\": 48660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.067424934623019,\n",
      "      \"grad_norm\": 0.0013275275705382228,\n",
      "      \"learning_rate\": 3.466981726528041e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 48680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.068685213774851,\n",
      "      \"grad_norm\": 0.05169536545872688,\n",
      "      \"learning_rate\": 3.466351606805293e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 48700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.069945492926683,\n",
      "      \"grad_norm\": 0.06507054716348648,\n",
      "      \"learning_rate\": 3.465721487082546e-05,\n",
      "      \"loss\": 0.0306,\n",
      "      \"step\": 48720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0712057720785153,\n",
      "      \"grad_norm\": 0.0024951728992164135,\n",
      "      \"learning_rate\": 3.4650913673597984e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 48740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0724660512303474,\n",
      "      \"grad_norm\": 0.018149202689528465,\n",
      "      \"learning_rate\": 3.464461247637051e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 48760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0737263303821796,\n",
      "      \"grad_norm\": 0.0007864288636483252,\n",
      "      \"learning_rate\": 3.4638311279143035e-05,\n",
      "      \"loss\": 0.0115,\n",
      "      \"step\": 48780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0749866095340117,\n",
      "      \"grad_norm\": 0.08450117707252502,\n",
      "      \"learning_rate\": 3.4632010081915564e-05,\n",
      "      \"loss\": 0.0572,\n",
      "      \"step\": 48800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.076246888685844,\n",
      "      \"grad_norm\": 0.11471772193908691,\n",
      "      \"learning_rate\": 3.462570888468809e-05,\n",
      "      \"loss\": 0.0268,\n",
      "      \"step\": 48820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.077507167837676,\n",
      "      \"grad_norm\": 3.3291168212890625,\n",
      "      \"learning_rate\": 3.461940768746062e-05,\n",
      "      \"loss\": 0.0243,\n",
      "      \"step\": 48840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.078767446989508,\n",
      "      \"grad_norm\": 4.035711765289307,\n",
      "      \"learning_rate\": 3.4613106490233144e-05,\n",
      "      \"loss\": 0.0432,\n",
      "      \"step\": 48860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0800277261413402,\n",
      "      \"grad_norm\": 0.007913295179605484,\n",
      "      \"learning_rate\": 3.4606805293005673e-05,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 48880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0812880052931724,\n",
      "      \"grad_norm\": 0.35176944732666016,\n",
      "      \"learning_rate\": 3.46005040957782e-05,\n",
      "      \"loss\": 0.018,\n",
      "      \"step\": 48900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0825482844450045,\n",
      "      \"grad_norm\": 0.036072392016649246,\n",
      "      \"learning_rate\": 3.4594202898550725e-05,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 48920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0838085635968366,\n",
      "      \"grad_norm\": 0.29294174909591675,\n",
      "      \"learning_rate\": 3.4587901701323254e-05,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 48940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0850688427486688,\n",
      "      \"grad_norm\": 0.0003363343421369791,\n",
      "      \"learning_rate\": 3.4581600504095776e-05,\n",
      "      \"loss\": 0.0448,\n",
      "      \"step\": 48960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.086329121900501,\n",
      "      \"grad_norm\": 0.06311441212892532,\n",
      "      \"learning_rate\": 3.4575299306868305e-05,\n",
      "      \"loss\": 0.035,\n",
      "      \"step\": 48980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.087589401052333,\n",
      "      \"grad_norm\": 2.9794371128082275,\n",
      "      \"learning_rate\": 3.4568998109640834e-05,\n",
      "      \"loss\": 0.0476,\n",
      "      \"step\": 49000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.088849680204165,\n",
      "      \"grad_norm\": 0.000497444299980998,\n",
      "      \"learning_rate\": 3.456269691241336e-05,\n",
      "      \"loss\": 0.019,\n",
      "      \"step\": 49020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0901099593559973,\n",
      "      \"grad_norm\": 0.000330258219037205,\n",
      "      \"learning_rate\": 3.4556395715185885e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 49040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0913702385078294,\n",
      "      \"grad_norm\": 0.0018320106901228428,\n",
      "      \"learning_rate\": 3.4550094517958414e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 49060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0926305176596616,\n",
      "      \"grad_norm\": 0.13330292701721191,\n",
      "      \"learning_rate\": 3.454379332073094e-05,\n",
      "      \"loss\": 0.0331,\n",
      "      \"step\": 49080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0938907968114937,\n",
      "      \"grad_norm\": 0.08090794086456299,\n",
      "      \"learning_rate\": 3.4537492123503466e-05,\n",
      "      \"loss\": 0.0544,\n",
      "      \"step\": 49100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.095151075963326,\n",
      "      \"grad_norm\": 0.018622104078531265,\n",
      "      \"learning_rate\": 3.453119092627599e-05,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 49120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.096411355115158,\n",
      "      \"grad_norm\": 0.3451365530490875,\n",
      "      \"learning_rate\": 3.4524889729048524e-05,\n",
      "      \"loss\": 0.0371,\n",
      "      \"step\": 49140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.09767163426699,\n",
      "      \"grad_norm\": 0.0022775388788431883,\n",
      "      \"learning_rate\": 3.451858853182105e-05,\n",
      "      \"loss\": 0.0067,\n",
      "      \"step\": 49160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0989319134188222,\n",
      "      \"grad_norm\": 0.1887959986925125,\n",
      "      \"learning_rate\": 3.4512287334593575e-05,\n",
      "      \"loss\": 0.0182,\n",
      "      \"step\": 49180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1001921925706544,\n",
      "      \"grad_norm\": 0.0004594084166456014,\n",
      "      \"learning_rate\": 3.4505986137366104e-05,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 49200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1014524717224865,\n",
      "      \"grad_norm\": 0.010357468388974667,\n",
      "      \"learning_rate\": 3.4499684940138626e-05,\n",
      "      \"loss\": 0.0258,\n",
      "      \"step\": 49220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1027127508743186,\n",
      "      \"grad_norm\": 0.007953830994665623,\n",
      "      \"learning_rate\": 3.4493383742911156e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 49240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1039730300261508,\n",
      "      \"grad_norm\": 0.0007879625190980732,\n",
      "      \"learning_rate\": 3.448708254568368e-05,\n",
      "      \"loss\": 0.019,\n",
      "      \"step\": 49260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.105233309177983,\n",
      "      \"grad_norm\": 20.955463409423828,\n",
      "      \"learning_rate\": 3.448078134845621e-05,\n",
      "      \"loss\": 0.053,\n",
      "      \"step\": 49280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.106493588329815,\n",
      "      \"grad_norm\": 1.1464953422546387,\n",
      "      \"learning_rate\": 3.4474480151228736e-05,\n",
      "      \"loss\": 0.0165,\n",
      "      \"step\": 49300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.107753867481647,\n",
      "      \"grad_norm\": 7.511206150054932,\n",
      "      \"learning_rate\": 3.4468178954001265e-05,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 49320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1090141466334793,\n",
      "      \"grad_norm\": 0.11989640444517136,\n",
      "      \"learning_rate\": 3.446187775677379e-05,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 49340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1102744257853114,\n",
      "      \"grad_norm\": 0.07698966562747955,\n",
      "      \"learning_rate\": 3.4455576559546316e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 49360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1115347049371436,\n",
      "      \"grad_norm\": 0.23392343521118164,\n",
      "      \"learning_rate\": 3.4449275362318845e-05,\n",
      "      \"loss\": 0.0197,\n",
      "      \"step\": 49380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1127949840889757,\n",
      "      \"grad_norm\": 0.00686990562826395,\n",
      "      \"learning_rate\": 3.444297416509137e-05,\n",
      "      \"loss\": 0.0362,\n",
      "      \"step\": 49400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.114055263240808,\n",
      "      \"grad_norm\": 0.007044906262308359,\n",
      "      \"learning_rate\": 3.4436672967863897e-05,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 49420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.11531554239264,\n",
      "      \"grad_norm\": 0.0028634576592594385,\n",
      "      \"learning_rate\": 3.443037177063642e-05,\n",
      "      \"loss\": 0.0253,\n",
      "      \"step\": 49440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.116575821544472,\n",
      "      \"grad_norm\": 0.01391304936259985,\n",
      "      \"learning_rate\": 3.442407057340895e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 49460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1178361006963042,\n",
      "      \"grad_norm\": 0.013800148852169514,\n",
      "      \"learning_rate\": 3.441776937618148e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 49480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1190963798481364,\n",
      "      \"grad_norm\": 0.0027740676887333393,\n",
      "      \"learning_rate\": 3.4411468178954006e-05,\n",
      "      \"loss\": 0.0058,\n",
      "      \"step\": 49500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1203566589999685,\n",
      "      \"grad_norm\": 0.0007452459540218115,\n",
      "      \"learning_rate\": 3.440516698172653e-05,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 49520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1216169381518006,\n",
      "      \"grad_norm\": 0.005089514423161745,\n",
      "      \"learning_rate\": 3.439886578449906e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 49540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1228772173036328,\n",
      "      \"grad_norm\": 0.004128588363528252,\n",
      "      \"learning_rate\": 3.439256458727158e-05,\n",
      "      \"loss\": 0.0219,\n",
      "      \"step\": 49560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.124137496455465,\n",
      "      \"grad_norm\": 0.012167200446128845,\n",
      "      \"learning_rate\": 3.438626339004411e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 49580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.125397775607297,\n",
      "      \"grad_norm\": 0.0048944479785859585,\n",
      "      \"learning_rate\": 3.437996219281664e-05,\n",
      "      \"loss\": 0.046,\n",
      "      \"step\": 49600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.126658054759129,\n",
      "      \"grad_norm\": 0.019014421850442886,\n",
      "      \"learning_rate\": 3.437366099558916e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 49620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1279183339109613,\n",
      "      \"grad_norm\": 4.477403163909912,\n",
      "      \"learning_rate\": 3.4367359798361696e-05,\n",
      "      \"loss\": 0.0774,\n",
      "      \"step\": 49640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1291786130627934,\n",
      "      \"grad_norm\": 0.47099047899246216,\n",
      "      \"learning_rate\": 3.436105860113422e-05,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 49660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1304388922146256,\n",
      "      \"grad_norm\": 0.023122262209653854,\n",
      "      \"learning_rate\": 3.435475740390675e-05,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 49680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1316991713664577,\n",
      "      \"grad_norm\": 6.4573073387146,\n",
      "      \"learning_rate\": 3.434845620667927e-05,\n",
      "      \"loss\": 0.0533,\n",
      "      \"step\": 49700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.13295945051829,\n",
      "      \"grad_norm\": 0.07038340717554092,\n",
      "      \"learning_rate\": 3.43421550094518e-05,\n",
      "      \"loss\": 0.0136,\n",
      "      \"step\": 49720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.134219729670122,\n",
      "      \"grad_norm\": 0.015388078056275845,\n",
      "      \"learning_rate\": 3.433585381222432e-05,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 49740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.135480008821954,\n",
      "      \"grad_norm\": 7.973801136016846,\n",
      "      \"learning_rate\": 3.432955261499685e-05,\n",
      "      \"loss\": 0.0388,\n",
      "      \"step\": 49760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1367402879737862,\n",
      "      \"grad_norm\": 0.1315828561782837,\n",
      "      \"learning_rate\": 3.432325141776938e-05,\n",
      "      \"loss\": 0.0054,\n",
      "      \"step\": 49780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1380005671256184,\n",
      "      \"grad_norm\": 0.11020299792289734,\n",
      "      \"learning_rate\": 3.431695022054191e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 49800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1392608462774505,\n",
      "      \"grad_norm\": 0.06524728238582611,\n",
      "      \"learning_rate\": 3.431064902331444e-05,\n",
      "      \"loss\": 0.0377,\n",
      "      \"step\": 49820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1405211254292826,\n",
      "      \"grad_norm\": 0.012486628256738186,\n",
      "      \"learning_rate\": 3.430434782608696e-05,\n",
      "      \"loss\": 0.038,\n",
      "      \"step\": 49840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1417814045811148,\n",
      "      \"grad_norm\": 0.0425894558429718,\n",
      "      \"learning_rate\": 3.429804662885949e-05,\n",
      "      \"loss\": 0.0206,\n",
      "      \"step\": 49860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.143041683732947,\n",
      "      \"grad_norm\": 0.026202213019132614,\n",
      "      \"learning_rate\": 3.429174543163201e-05,\n",
      "      \"loss\": 0.0028,\n",
      "      \"step\": 49880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.144301962884779,\n",
      "      \"grad_norm\": 0.035916101187467575,\n",
      "      \"learning_rate\": 3.428544423440454e-05,\n",
      "      \"loss\": 0.0347,\n",
      "      \"step\": 49900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.145562242036611,\n",
      "      \"grad_norm\": 0.018817389383912086,\n",
      "      \"learning_rate\": 3.427914303717706e-05,\n",
      "      \"loss\": 0.0471,\n",
      "      \"step\": 49920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1468225211884433,\n",
      "      \"grad_norm\": 0.0057158088311553,\n",
      "      \"learning_rate\": 3.427284183994959e-05,\n",
      "      \"loss\": 0.031,\n",
      "      \"step\": 49940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1480828003402754,\n",
      "      \"grad_norm\": 0.019032645970582962,\n",
      "      \"learning_rate\": 3.426654064272212e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 49960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1493430794921076,\n",
      "      \"grad_norm\": 0.005926418583840132,\n",
      "      \"learning_rate\": 3.426023944549465e-05,\n",
      "      \"loss\": 0.0263,\n",
      "      \"step\": 49980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1506033586439397,\n",
      "      \"grad_norm\": 0.009436994791030884,\n",
      "      \"learning_rate\": 3.425393824826717e-05,\n",
      "      \"loss\": 0.0453,\n",
      "      \"step\": 50000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.151863637795772,\n",
      "      \"grad_norm\": 1.8463517427444458,\n",
      "      \"learning_rate\": 3.42476370510397e-05,\n",
      "      \"loss\": 0.0345,\n",
      "      \"step\": 50020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.153123916947604,\n",
      "      \"grad_norm\": 0.010005399584770203,\n",
      "      \"learning_rate\": 3.424133585381223e-05,\n",
      "      \"loss\": 0.0235,\n",
      "      \"step\": 50040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.154384196099436,\n",
      "      \"grad_norm\": 0.0035597248934209347,\n",
      "      \"learning_rate\": 3.423503465658475e-05,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 50060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1556444752512682,\n",
      "      \"grad_norm\": 0.10730218887329102,\n",
      "      \"learning_rate\": 3.422873345935728e-05,\n",
      "      \"loss\": 0.0408,\n",
      "      \"step\": 50080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1569047544031004,\n",
      "      \"grad_norm\": 0.002085744170472026,\n",
      "      \"learning_rate\": 3.42224322621298e-05,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 50100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1581650335549325,\n",
      "      \"grad_norm\": 0.0023417228367179632,\n",
      "      \"learning_rate\": 3.421613106490233e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 50120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1594253127067646,\n",
      "      \"grad_norm\": 0.06386377662420273,\n",
      "      \"learning_rate\": 3.420982986767486e-05,\n",
      "      \"loss\": 0.0388,\n",
      "      \"step\": 50140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1606855918585968,\n",
      "      \"grad_norm\": 0.001371703576296568,\n",
      "      \"learning_rate\": 3.420352867044739e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 50160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.161945871010429,\n",
      "      \"grad_norm\": 0.002186172641813755,\n",
      "      \"learning_rate\": 3.419722747321991e-05,\n",
      "      \"loss\": 0.0518,\n",
      "      \"step\": 50180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.163206150162261,\n",
      "      \"grad_norm\": 0.1671692430973053,\n",
      "      \"learning_rate\": 3.419092627599244e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 50200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.164466429314093,\n",
      "      \"grad_norm\": 0.11133918911218643,\n",
      "      \"learning_rate\": 3.418462507876496e-05,\n",
      "      \"loss\": 0.0066,\n",
      "      \"step\": 50220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1657267084659253,\n",
      "      \"grad_norm\": 0.003989712335169315,\n",
      "      \"learning_rate\": 3.417832388153749e-05,\n",
      "      \"loss\": 0.028,\n",
      "      \"step\": 50240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1669869876177574,\n",
      "      \"grad_norm\": 0.002161589218303561,\n",
      "      \"learning_rate\": 3.417202268431002e-05,\n",
      "      \"loss\": 0.0699,\n",
      "      \"step\": 50260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1682472667695896,\n",
      "      \"grad_norm\": 0.0032547179143875837,\n",
      "      \"learning_rate\": 3.416572148708255e-05,\n",
      "      \"loss\": 0.0308,\n",
      "      \"step\": 50280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1695075459214217,\n",
      "      \"grad_norm\": 5.085883140563965,\n",
      "      \"learning_rate\": 3.415942028985508e-05,\n",
      "      \"loss\": 0.036,\n",
      "      \"step\": 50300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.170767825073254,\n",
      "      \"grad_norm\": 0.003133045742288232,\n",
      "      \"learning_rate\": 3.41531190926276e-05,\n",
      "      \"loss\": 0.0122,\n",
      "      \"step\": 50320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.172028104225086,\n",
      "      \"grad_norm\": 4.201281547546387,\n",
      "      \"learning_rate\": 3.414681789540013e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 50340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.173288383376918,\n",
      "      \"grad_norm\": 0.0034328452311456203,\n",
      "      \"learning_rate\": 3.414051669817265e-05,\n",
      "      \"loss\": 0.0478,\n",
      "      \"step\": 50360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1745486625287502,\n",
      "      \"grad_norm\": 0.008117846213281155,\n",
      "      \"learning_rate\": 3.413421550094518e-05,\n",
      "      \"loss\": 0.027,\n",
      "      \"step\": 50380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1758089416805824,\n",
      "      \"grad_norm\": 0.005788363981992006,\n",
      "      \"learning_rate\": 3.4127914303717704e-05,\n",
      "      \"loss\": 0.0116,\n",
      "      \"step\": 50400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1770692208324145,\n",
      "      \"grad_norm\": 0.05340051278471947,\n",
      "      \"learning_rate\": 3.412161310649023e-05,\n",
      "      \"loss\": 0.0528,\n",
      "      \"step\": 50420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1783294999842466,\n",
      "      \"grad_norm\": 0.07051093876361847,\n",
      "      \"learning_rate\": 3.411531190926276e-05,\n",
      "      \"loss\": 0.0383,\n",
      "      \"step\": 50440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1795897791360788,\n",
      "      \"grad_norm\": 0.007051668595522642,\n",
      "      \"learning_rate\": 3.410901071203529e-05,\n",
      "      \"loss\": 0.0213,\n",
      "      \"step\": 50460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.180850058287911,\n",
      "      \"grad_norm\": 0.03210582584142685,\n",
      "      \"learning_rate\": 3.4102709514807814e-05,\n",
      "      \"loss\": 0.0441,\n",
      "      \"step\": 50480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.182110337439743,\n",
      "      \"grad_norm\": 0.08795661479234695,\n",
      "      \"learning_rate\": 3.409640831758034e-05,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 50500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.183370616591575,\n",
      "      \"grad_norm\": 0.007955203764140606,\n",
      "      \"learning_rate\": 3.409010712035287e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 50520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1846308957434073,\n",
      "      \"grad_norm\": 0.6200412511825562,\n",
      "      \"learning_rate\": 3.4083805923125394e-05,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 50540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1858911748952394,\n",
      "      \"grad_norm\": 3.9950644969940186,\n",
      "      \"learning_rate\": 3.407750472589792e-05,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 50560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1871514540470716,\n",
      "      \"grad_norm\": 0.0005230131791904569,\n",
      "      \"learning_rate\": 3.4071203528670445e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 50580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1884117331989037,\n",
      "      \"grad_norm\": 4.911944389343262,\n",
      "      \"learning_rate\": 3.4064902331442974e-05,\n",
      "      \"loss\": 0.036,\n",
      "      \"step\": 50600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.189672012350736,\n",
      "      \"grad_norm\": 0.0020474183838814497,\n",
      "      \"learning_rate\": 3.40586011342155e-05,\n",
      "      \"loss\": 0.0336,\n",
      "      \"step\": 50620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.190932291502568,\n",
      "      \"grad_norm\": 0.009376950562000275,\n",
      "      \"learning_rate\": 3.405229993698803e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 50640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1921925706544,\n",
      "      \"grad_norm\": 0.2732519209384918,\n",
      "      \"learning_rate\": 3.4045998739760555e-05,\n",
      "      \"loss\": 0.0219,\n",
      "      \"step\": 50660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1934528498062322,\n",
      "      \"grad_norm\": 0.22152341902256012,\n",
      "      \"learning_rate\": 3.4039697542533084e-05,\n",
      "      \"loss\": 0.0595,\n",
      "      \"step\": 50680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1947131289580644,\n",
      "      \"grad_norm\": 0.1951446384191513,\n",
      "      \"learning_rate\": 3.4033396345305606e-05,\n",
      "      \"loss\": 0.0455,\n",
      "      \"step\": 50700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1959734081098965,\n",
      "      \"grad_norm\": 0.008666732348501682,\n",
      "      \"learning_rate\": 3.4027095148078135e-05,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 50720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1972336872617286,\n",
      "      \"grad_norm\": 0.014586874283850193,\n",
      "      \"learning_rate\": 3.4020793950850664e-05,\n",
      "      \"loss\": 0.0211,\n",
      "      \"step\": 50740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1984939664135608,\n",
      "      \"grad_norm\": 0.0006336933001875877,\n",
      "      \"learning_rate\": 3.4014492753623186e-05,\n",
      "      \"loss\": 0.0169,\n",
      "      \"step\": 50760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.199754245565393,\n",
      "      \"grad_norm\": 0.0013803233159705997,\n",
      "      \"learning_rate\": 3.4008191556395715e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 50780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.201014524717225,\n",
      "      \"grad_norm\": 0.0014127951581031084,\n",
      "      \"learning_rate\": 3.4001890359168244e-05,\n",
      "      \"loss\": 0.023,\n",
      "      \"step\": 50800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.202274803869057,\n",
      "      \"grad_norm\": 0.13772857189178467,\n",
      "      \"learning_rate\": 3.3995589161940773e-05,\n",
      "      \"loss\": 0.0618,\n",
      "      \"step\": 50820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2035350830208893,\n",
      "      \"grad_norm\": 0.017272859811782837,\n",
      "      \"learning_rate\": 3.3989287964713296e-05,\n",
      "      \"loss\": 0.0405,\n",
      "      \"step\": 50840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2047953621727214,\n",
      "      \"grad_norm\": 0.008920454420149326,\n",
      "      \"learning_rate\": 3.3982986767485825e-05,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 50860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2060556413245536,\n",
      "      \"grad_norm\": 0.011603549122810364,\n",
      "      \"learning_rate\": 3.397668557025835e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 50880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2073159204763857,\n",
      "      \"grad_norm\": 0.0013080468634143472,\n",
      "      \"learning_rate\": 3.3970384373030876e-05,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 50900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.208576199628218,\n",
      "      \"grad_norm\": 0.002991772023960948,\n",
      "      \"learning_rate\": 3.3964083175803405e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 50920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.20983647878005,\n",
      "      \"grad_norm\": 10.386129379272461,\n",
      "      \"learning_rate\": 3.3957781978575934e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 50940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.211096757931882,\n",
      "      \"grad_norm\": 0.0011684757191687822,\n",
      "      \"learning_rate\": 3.395148078134846e-05,\n",
      "      \"loss\": 0.013,\n",
      "      \"step\": 50960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2123570370837142,\n",
      "      \"grad_norm\": 0.006509737577289343,\n",
      "      \"learning_rate\": 3.3945179584120985e-05,\n",
      "      \"loss\": 0.0029,\n",
      "      \"step\": 50980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2136173162355464,\n",
      "      \"grad_norm\": 0.003802762134000659,\n",
      "      \"learning_rate\": 3.3938878386893514e-05,\n",
      "      \"loss\": 0.049,\n",
      "      \"step\": 51000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2148775953873785,\n",
      "      \"grad_norm\": 11.36557388305664,\n",
      "      \"learning_rate\": 3.393257718966604e-05,\n",
      "      \"loss\": 0.0333,\n",
      "      \"step\": 51020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2161378745392106,\n",
      "      \"grad_norm\": 0.018912093713879585,\n",
      "      \"learning_rate\": 3.3926275992438566e-05,\n",
      "      \"loss\": 0.0188,\n",
      "      \"step\": 51040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2173981536910428,\n",
      "      \"grad_norm\": 0.13986878097057343,\n",
      "      \"learning_rate\": 3.391997479521109e-05,\n",
      "      \"loss\": 0.0237,\n",
      "      \"step\": 51060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.218658432842875,\n",
      "      \"grad_norm\": 7.8195719718933105,\n",
      "      \"learning_rate\": 3.391367359798362e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 51080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.219918711994707,\n",
      "      \"grad_norm\": 0.0068107591941952705,\n",
      "      \"learning_rate\": 3.3907372400756146e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 51100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.221178991146539,\n",
      "      \"grad_norm\": 0.03509553149342537,\n",
      "      \"learning_rate\": 3.3901071203528675e-05,\n",
      "      \"loss\": 0.0239,\n",
      "      \"step\": 51120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2224392702983713,\n",
      "      \"grad_norm\": 0.030542315915226936,\n",
      "      \"learning_rate\": 3.38947700063012e-05,\n",
      "      \"loss\": 0.0344,\n",
      "      \"step\": 51140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2236995494502034,\n",
      "      \"grad_norm\": 0.004131042864173651,\n",
      "      \"learning_rate\": 3.3888468809073726e-05,\n",
      "      \"loss\": 0.0271,\n",
      "      \"step\": 51160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.224959828602035,\n",
      "      \"grad_norm\": 0.12050282955169678,\n",
      "      \"learning_rate\": 3.3882167611846255e-05,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 51180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2262201077538677,\n",
      "      \"grad_norm\": 0.0023284158669412136,\n",
      "      \"learning_rate\": 3.387586641461878e-05,\n",
      "      \"loss\": 0.0232,\n",
      "      \"step\": 51200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2274803869056994,\n",
      "      \"grad_norm\": 0.0012785322032868862,\n",
      "      \"learning_rate\": 3.386956521739131e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 51220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.228740666057532,\n",
      "      \"grad_norm\": 6.193667411804199,\n",
      "      \"learning_rate\": 3.386326402016383e-05,\n",
      "      \"loss\": 0.0378,\n",
      "      \"step\": 51240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2300009452093636,\n",
      "      \"grad_norm\": 0.06474975496530533,\n",
      "      \"learning_rate\": 3.385696282293636e-05,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 51260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2312612243611962,\n",
      "      \"grad_norm\": 0.3169008195400238,\n",
      "      \"learning_rate\": 3.385066162570889e-05,\n",
      "      \"loss\": 0.0209,\n",
      "      \"step\": 51280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.232521503513028,\n",
      "      \"grad_norm\": 0.14697733521461487,\n",
      "      \"learning_rate\": 3.3844360428481416e-05,\n",
      "      \"loss\": 0.0313,\n",
      "      \"step\": 51300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2337817826648605,\n",
      "      \"grad_norm\": 0.7017186880111694,\n",
      "      \"learning_rate\": 3.383805923125394e-05,\n",
      "      \"loss\": 0.027,\n",
      "      \"step\": 51320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.235042061816692,\n",
      "      \"grad_norm\": 18.576574325561523,\n",
      "      \"learning_rate\": 3.383175803402647e-05,\n",
      "      \"loss\": 0.0162,\n",
      "      \"step\": 51340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2363023409685248,\n",
      "      \"grad_norm\": 0.08389870077371597,\n",
      "      \"learning_rate\": 3.382545683679899e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 51360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2375626201203564,\n",
      "      \"grad_norm\": 0.005480058491230011,\n",
      "      \"learning_rate\": 3.381915563957152e-05,\n",
      "      \"loss\": 0.0069,\n",
      "      \"step\": 51380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2388228992721886,\n",
      "      \"grad_norm\": 0.18669037520885468,\n",
      "      \"learning_rate\": 3.381285444234405e-05,\n",
      "      \"loss\": 0.018,\n",
      "      \"step\": 51400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2400831784240207,\n",
      "      \"grad_norm\": 0.04728931933641434,\n",
      "      \"learning_rate\": 3.380655324511657e-05,\n",
      "      \"loss\": 0.007,\n",
      "      \"step\": 51420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.241343457575853,\n",
      "      \"grad_norm\": 0.2408033311367035,\n",
      "      \"learning_rate\": 3.3800252047889106e-05,\n",
      "      \"loss\": 0.0591,\n",
      "      \"step\": 51440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.242603736727685,\n",
      "      \"grad_norm\": 0.0044690752401947975,\n",
      "      \"learning_rate\": 3.379395085066163e-05,\n",
      "      \"loss\": 0.0351,\n",
      "      \"step\": 51460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.243864015879517,\n",
      "      \"grad_norm\": 3.7193524837493896,\n",
      "      \"learning_rate\": 3.378764965343416e-05,\n",
      "      \"loss\": 0.0143,\n",
      "      \"step\": 51480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2451242950313492,\n",
      "      \"grad_norm\": 0.004123397171497345,\n",
      "      \"learning_rate\": 3.378134845620668e-05,\n",
      "      \"loss\": 0.0206,\n",
      "      \"step\": 51500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2463845741831814,\n",
      "      \"grad_norm\": 0.009434963576495647,\n",
      "      \"learning_rate\": 3.377504725897921e-05,\n",
      "      \"loss\": 0.0653,\n",
      "      \"step\": 51520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2476448533350135,\n",
      "      \"grad_norm\": 0.006233597174286842,\n",
      "      \"learning_rate\": 3.376874606175173e-05,\n",
      "      \"loss\": 0.0336,\n",
      "      \"step\": 51540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2489051324868456,\n",
      "      \"grad_norm\": 0.14023065567016602,\n",
      "      \"learning_rate\": 3.376244486452426e-05,\n",
      "      \"loss\": 0.0208,\n",
      "      \"step\": 51560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.250165411638678,\n",
      "      \"grad_norm\": 0.025115851312875748,\n",
      "      \"learning_rate\": 3.375614366729679e-05,\n",
      "      \"loss\": 0.0457,\n",
      "      \"step\": 51580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.25142569079051,\n",
      "      \"grad_norm\": 0.001508065965026617,\n",
      "      \"learning_rate\": 3.374984247006932e-05,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 51600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.252685969942342,\n",
      "      \"grad_norm\": 0.06960854679346085,\n",
      "      \"learning_rate\": 3.374354127284185e-05,\n",
      "      \"loss\": 0.0215,\n",
      "      \"step\": 51620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.253946249094174,\n",
      "      \"grad_norm\": 0.007181597873568535,\n",
      "      \"learning_rate\": 3.373724007561437e-05,\n",
      "      \"loss\": 0.0328,\n",
      "      \"step\": 51640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2552065282460063,\n",
      "      \"grad_norm\": 0.05579288676381111,\n",
      "      \"learning_rate\": 3.37309388783869e-05,\n",
      "      \"loss\": 0.0121,\n",
      "      \"step\": 51660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2564668073978384,\n",
      "      \"grad_norm\": 0.2974596917629242,\n",
      "      \"learning_rate\": 3.372463768115942e-05,\n",
      "      \"loss\": 0.0447,\n",
      "      \"step\": 51680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2577270865496706,\n",
      "      \"grad_norm\": 0.010239587165415287,\n",
      "      \"learning_rate\": 3.371833648393195e-05,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 51700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2589873657015027,\n",
      "      \"grad_norm\": 0.004934591241180897,\n",
      "      \"learning_rate\": 3.371203528670447e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 51720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.260247644853335,\n",
      "      \"grad_norm\": 0.007106953300535679,\n",
      "      \"learning_rate\": 3.3705734089477e-05,\n",
      "      \"loss\": 0.0553,\n",
      "      \"step\": 51740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.261507924005167,\n",
      "      \"grad_norm\": 0.013903803192079067,\n",
      "      \"learning_rate\": 3.369943289224953e-05,\n",
      "      \"loss\": 0.0231,\n",
      "      \"step\": 51760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.262768203156999,\n",
      "      \"grad_norm\": 0.8253960609436035,\n",
      "      \"learning_rate\": 3.369313169502206e-05,\n",
      "      \"loss\": 0.0029,\n",
      "      \"step\": 51780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2640284823088312,\n",
      "      \"grad_norm\": 15.551237106323242,\n",
      "      \"learning_rate\": 3.368683049779458e-05,\n",
      "      \"loss\": 0.016,\n",
      "      \"step\": 51800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2652887614606634,\n",
      "      \"grad_norm\": 2.073158025741577,\n",
      "      \"learning_rate\": 3.368052930056711e-05,\n",
      "      \"loss\": 0.004,\n",
      "      \"step\": 51820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2665490406124955,\n",
      "      \"grad_norm\": 10.299474716186523,\n",
      "      \"learning_rate\": 3.367422810333963e-05,\n",
      "      \"loss\": 0.0364,\n",
      "      \"step\": 51840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2678093197643276,\n",
      "      \"grad_norm\": 7.547584056854248,\n",
      "      \"learning_rate\": 3.366792690611216e-05,\n",
      "      \"loss\": 0.0266,\n",
      "      \"step\": 51860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.26906959891616,\n",
      "      \"grad_norm\": 0.001588073675520718,\n",
      "      \"learning_rate\": 3.366162570888469e-05,\n",
      "      \"loss\": 0.0216,\n",
      "      \"step\": 51880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.270329878067992,\n",
      "      \"grad_norm\": 0.0003866611223202199,\n",
      "      \"learning_rate\": 3.365532451165721e-05,\n",
      "      \"loss\": 0.0248,\n",
      "      \"step\": 51900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.271590157219824,\n",
      "      \"grad_norm\": 0.003754854202270508,\n",
      "      \"learning_rate\": 3.364902331442974e-05,\n",
      "      \"loss\": 0.0416,\n",
      "      \"step\": 51920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.272850436371656,\n",
      "      \"grad_norm\": 0.5840572714805603,\n",
      "      \"learning_rate\": 3.364272211720227e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 51940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2741107155234883,\n",
      "      \"grad_norm\": 0.060608524829149246,\n",
      "      \"learning_rate\": 3.36364209199748e-05,\n",
      "      \"loss\": 0.0258,\n",
      "      \"step\": 51960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2753709946753204,\n",
      "      \"grad_norm\": 0.1889593005180359,\n",
      "      \"learning_rate\": 3.363011972274732e-05,\n",
      "      \"loss\": 0.02,\n",
      "      \"step\": 51980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2766312738271526,\n",
      "      \"grad_norm\": 0.010230788961052895,\n",
      "      \"learning_rate\": 3.362381852551985e-05,\n",
      "      \"loss\": 0.0237,\n",
      "      \"step\": 52000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2778915529789847,\n",
      "      \"grad_norm\": 7.220754146575928,\n",
      "      \"learning_rate\": 3.3617517328292374e-05,\n",
      "      \"loss\": 0.0293,\n",
      "      \"step\": 52020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.279151832130817,\n",
      "      \"grad_norm\": 0.0002113336813636124,\n",
      "      \"learning_rate\": 3.36112161310649e-05,\n",
      "      \"loss\": 0.0233,\n",
      "      \"step\": 52040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.280412111282649,\n",
      "      \"grad_norm\": 6.986415386199951,\n",
      "      \"learning_rate\": 3.3604914933837425e-05,\n",
      "      \"loss\": 0.0368,\n",
      "      \"step\": 52060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.281672390434481,\n",
      "      \"grad_norm\": 0.593605637550354,\n",
      "      \"learning_rate\": 3.359861373660996e-05,\n",
      "      \"loss\": 0.0142,\n",
      "      \"step\": 52080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2829326695863132,\n",
      "      \"grad_norm\": 0.001656056847423315,\n",
      "      \"learning_rate\": 3.359231253938249e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 52100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2841929487381454,\n",
      "      \"grad_norm\": 5.550096035003662,\n",
      "      \"learning_rate\": 3.358601134215501e-05,\n",
      "      \"loss\": 0.0369,\n",
      "      \"step\": 52120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2854532278899775,\n",
      "      \"grad_norm\": 0.017303189262747765,\n",
      "      \"learning_rate\": 3.357971014492754e-05,\n",
      "      \"loss\": 0.0302,\n",
      "      \"step\": 52140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2867135070418096,\n",
      "      \"grad_norm\": 0.015626171603798866,\n",
      "      \"learning_rate\": 3.357340894770006e-05,\n",
      "      \"loss\": 0.0356,\n",
      "      \"step\": 52160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.287973786193642,\n",
      "      \"grad_norm\": 0.00034382572630420327,\n",
      "      \"learning_rate\": 3.356710775047259e-05,\n",
      "      \"loss\": 0.0054,\n",
      "      \"step\": 52180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.289234065345474,\n",
      "      \"grad_norm\": 0.0018372462363913655,\n",
      "      \"learning_rate\": 3.3560806553245115e-05,\n",
      "      \"loss\": 0.0422,\n",
      "      \"step\": 52200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.290494344497306,\n",
      "      \"grad_norm\": 0.024132102727890015,\n",
      "      \"learning_rate\": 3.3554505356017644e-05,\n",
      "      \"loss\": 0.0666,\n",
      "      \"step\": 52220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.291754623649138,\n",
      "      \"grad_norm\": 0.19694571197032928,\n",
      "      \"learning_rate\": 3.354820415879017e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 52240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2930149028009703,\n",
      "      \"grad_norm\": 0.002001030370593071,\n",
      "      \"learning_rate\": 3.35419029615627e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 52260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2942751819528024,\n",
      "      \"grad_norm\": 1.6633250713348389,\n",
      "      \"learning_rate\": 3.3535601764335224e-05,\n",
      "      \"loss\": 0.0411,\n",
      "      \"step\": 52280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2955354611046346,\n",
      "      \"grad_norm\": 0.3394361138343811,\n",
      "      \"learning_rate\": 3.352930056710775e-05,\n",
      "      \"loss\": 0.063,\n",
      "      \"step\": 52300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2967957402564667,\n",
      "      \"grad_norm\": 0.00458915438503027,\n",
      "      \"learning_rate\": 3.352299936988028e-05,\n",
      "      \"loss\": 0.0333,\n",
      "      \"step\": 52320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.298056019408299,\n",
      "      \"grad_norm\": 0.2465817630290985,\n",
      "      \"learning_rate\": 3.3516698172652804e-05,\n",
      "      \"loss\": 0.0201,\n",
      "      \"step\": 52340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.299316298560131,\n",
      "      \"grad_norm\": 0.02344340644776821,\n",
      "      \"learning_rate\": 3.351039697542533e-05,\n",
      "      \"loss\": 0.0244,\n",
      "      \"step\": 52360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.300576577711963,\n",
      "      \"grad_norm\": 0.23324233293533325,\n",
      "      \"learning_rate\": 3.3504095778197856e-05,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 52380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3018368568637952,\n",
      "      \"grad_norm\": 0.003948043566197157,\n",
      "      \"learning_rate\": 3.3497794580970385e-05,\n",
      "      \"loss\": 0.0208,\n",
      "      \"step\": 52400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3030971360156274,\n",
      "      \"grad_norm\": 0.028726158663630486,\n",
      "      \"learning_rate\": 3.3491493383742914e-05,\n",
      "      \"loss\": 0.0405,\n",
      "      \"step\": 52420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3043574151674595,\n",
      "      \"grad_norm\": 0.009954542852938175,\n",
      "      \"learning_rate\": 3.348519218651544e-05,\n",
      "      \"loss\": 0.0183,\n",
      "      \"step\": 52440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3056176943192916,\n",
      "      \"grad_norm\": 0.0058138491585850716,\n",
      "      \"learning_rate\": 3.3478890989287965e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 52460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3068779734711238,\n",
      "      \"grad_norm\": 0.004864075221121311,\n",
      "      \"learning_rate\": 3.3472589792060494e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 52480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.308138252622956,\n",
      "      \"grad_norm\": 0.0037822371814399958,\n",
      "      \"learning_rate\": 3.3466288594833016e-05,\n",
      "      \"loss\": 0.0237,\n",
      "      \"step\": 52500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.309398531774788,\n",
      "      \"grad_norm\": 0.008555534295737743,\n",
      "      \"learning_rate\": 3.3459987397605545e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 52520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.31065881092662,\n",
      "      \"grad_norm\": 0.0037915788125246763,\n",
      "      \"learning_rate\": 3.3453686200378074e-05,\n",
      "      \"loss\": 0.0223,\n",
      "      \"step\": 52540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3119190900784523,\n",
      "      \"grad_norm\": 0.03024439699947834,\n",
      "      \"learning_rate\": 3.3447700063011975e-05,\n",
      "      \"loss\": 0.0168,\n",
      "      \"step\": 52560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3131793692302844,\n",
      "      \"grad_norm\": 0.0023965940345078707,\n",
      "      \"learning_rate\": 3.3441713925645876e-05,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 52580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3144396483821166,\n",
      "      \"grad_norm\": 0.09122870117425919,\n",
      "      \"learning_rate\": 3.3435412728418405e-05,\n",
      "      \"loss\": 0.0183,\n",
      "      \"step\": 52600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3156999275339487,\n",
      "      \"grad_norm\": 0.052792660892009735,\n",
      "      \"learning_rate\": 3.342911153119093e-05,\n",
      "      \"loss\": 0.0041,\n",
      "      \"step\": 52620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.316960206685781,\n",
      "      \"grad_norm\": 0.07908502221107483,\n",
      "      \"learning_rate\": 3.3422810333963456e-05,\n",
      "      \"loss\": 0.027,\n",
      "      \"step\": 52640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.318220485837613,\n",
      "      \"grad_norm\": 0.26044443249702454,\n",
      "      \"learning_rate\": 3.341650913673598e-05,\n",
      "      \"loss\": 0.0462,\n",
      "      \"step\": 52660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.319480764989445,\n",
      "      \"grad_norm\": 0.012594558298587799,\n",
      "      \"learning_rate\": 3.341020793950851e-05,\n",
      "      \"loss\": 0.0259,\n",
      "      \"step\": 52680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3207410441412772,\n",
      "      \"grad_norm\": 0.02902395837008953,\n",
      "      \"learning_rate\": 3.340390674228103e-05,\n",
      "      \"loss\": 0.0337,\n",
      "      \"step\": 52700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3220013232931094,\n",
      "      \"grad_norm\": 0.3380142152309418,\n",
      "      \"learning_rate\": 3.339760554505356e-05,\n",
      "      \"loss\": 0.0265,\n",
      "      \"step\": 52720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3232616024449415,\n",
      "      \"grad_norm\": 0.04983939230442047,\n",
      "      \"learning_rate\": 3.339130434782609e-05,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 52740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3245218815967736,\n",
      "      \"grad_norm\": 0.008891750127077103,\n",
      "      \"learning_rate\": 3.338500315059862e-05,\n",
      "      \"loss\": 0.0211,\n",
      "      \"step\": 52760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3257821607486058,\n",
      "      \"grad_norm\": 5.584661960601807,\n",
      "      \"learning_rate\": 3.3378701953371146e-05,\n",
      "      \"loss\": 0.0193,\n",
      "      \"step\": 52780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.327042439900438,\n",
      "      \"grad_norm\": 0.011410614475607872,\n",
      "      \"learning_rate\": 3.337240075614367e-05,\n",
      "      \"loss\": 0.0212,\n",
      "      \"step\": 52800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.32830271905227,\n",
      "      \"grad_norm\": 0.1439891755580902,\n",
      "      \"learning_rate\": 3.33660995589162e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 52820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.329562998204102,\n",
      "      \"grad_norm\": 0.003627705154940486,\n",
      "      \"learning_rate\": 3.335979836168872e-05,\n",
      "      \"loss\": 0.0569,\n",
      "      \"step\": 52840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3308232773559343,\n",
      "      \"grad_norm\": 0.00026182731380686164,\n",
      "      \"learning_rate\": 3.335349716446125e-05,\n",
      "      \"loss\": 0.0161,\n",
      "      \"step\": 52860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3320835565077664,\n",
      "      \"grad_norm\": 0.0018734721234068274,\n",
      "      \"learning_rate\": 3.334719596723377e-05,\n",
      "      \"loss\": 0.0193,\n",
      "      \"step\": 52880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3333438356595986,\n",
      "      \"grad_norm\": 0.1287115514278412,\n",
      "      \"learning_rate\": 3.33408947700063e-05,\n",
      "      \"loss\": 0.0372,\n",
      "      \"step\": 52900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3346041148114307,\n",
      "      \"grad_norm\": 0.006232758983969688,\n",
      "      \"learning_rate\": 3.333459357277883e-05,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 52920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.335864393963263,\n",
      "      \"grad_norm\": 0.06034165993332863,\n",
      "      \"learning_rate\": 3.332829237555136e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 52940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.337124673115095,\n",
      "      \"grad_norm\": 0.00520235113799572,\n",
      "      \"learning_rate\": 3.332199117832389e-05,\n",
      "      \"loss\": 0.0267,\n",
      "      \"step\": 52960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.338384952266927,\n",
      "      \"grad_norm\": 0.010437028482556343,\n",
      "      \"learning_rate\": 3.331568998109641e-05,\n",
      "      \"loss\": 0.0396,\n",
      "      \"step\": 52980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3396452314187592,\n",
      "      \"grad_norm\": 0.057214152067899704,\n",
      "      \"learning_rate\": 3.330938878386894e-05,\n",
      "      \"loss\": 0.0285,\n",
      "      \"step\": 53000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3409055105705914,\n",
      "      \"grad_norm\": 0.04908674582839012,\n",
      "      \"learning_rate\": 3.330308758664146e-05,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 53020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3421657897224235,\n",
      "      \"grad_norm\": 0.3846438229084015,\n",
      "      \"learning_rate\": 3.329678638941399e-05,\n",
      "      \"loss\": 0.0468,\n",
      "      \"step\": 53040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3434260688742556,\n",
      "      \"grad_norm\": 0.09522490948438644,\n",
      "      \"learning_rate\": 3.329048519218652e-05,\n",
      "      \"loss\": 0.0219,\n",
      "      \"step\": 53060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3446863480260878,\n",
      "      \"grad_norm\": 0.3171977400779724,\n",
      "      \"learning_rate\": 3.328418399495905e-05,\n",
      "      \"loss\": 0.046,\n",
      "      \"step\": 53080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.34594662717792,\n",
      "      \"grad_norm\": 30.12818145751953,\n",
      "      \"learning_rate\": 3.327788279773157e-05,\n",
      "      \"loss\": 0.0319,\n",
      "      \"step\": 53100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.347206906329752,\n",
      "      \"grad_norm\": 0.10000506788492203,\n",
      "      \"learning_rate\": 3.32715816005041e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 53120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.348467185481584,\n",
      "      \"grad_norm\": 0.0403628796339035,\n",
      "      \"learning_rate\": 3.326528040327662e-05,\n",
      "      \"loss\": 0.041,\n",
      "      \"step\": 53140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3497274646334163,\n",
      "      \"grad_norm\": 0.014652392826974392,\n",
      "      \"learning_rate\": 3.325897920604915e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 53160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3509877437852484,\n",
      "      \"grad_norm\": 0.0021439928095787764,\n",
      "      \"learning_rate\": 3.325267800882167e-05,\n",
      "      \"loss\": 0.0018,\n",
      "      \"step\": 53180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3522480229370806,\n",
      "      \"grad_norm\": 0.7080148458480835,\n",
      "      \"learning_rate\": 3.32463768115942e-05,\n",
      "      \"loss\": 0.0196,\n",
      "      \"step\": 53200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3535083020889127,\n",
      "      \"grad_norm\": 0.030269093811511993,\n",
      "      \"learning_rate\": 3.324007561436673e-05,\n",
      "      \"loss\": 0.0403,\n",
      "      \"step\": 53220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.354768581240745,\n",
      "      \"grad_norm\": 0.0008935809601098299,\n",
      "      \"learning_rate\": 3.323377441713926e-05,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 53240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.356028860392577,\n",
      "      \"grad_norm\": 0.0028192021418362856,\n",
      "      \"learning_rate\": 3.322747321991179e-05,\n",
      "      \"loss\": 0.0033,\n",
      "      \"step\": 53260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.357289139544409,\n",
      "      \"grad_norm\": 0.0002546435280237347,\n",
      "      \"learning_rate\": 3.322117202268431e-05,\n",
      "      \"loss\": 0.0115,\n",
      "      \"step\": 53280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3585494186962412,\n",
      "      \"grad_norm\": 0.0189468115568161,\n",
      "      \"learning_rate\": 3.321487082545684e-05,\n",
      "      \"loss\": 0.0213,\n",
      "      \"step\": 53300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3598096978480734,\n",
      "      \"grad_norm\": 0.009814766235649586,\n",
      "      \"learning_rate\": 3.320856962822936e-05,\n",
      "      \"loss\": 0.019,\n",
      "      \"step\": 53320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3610699769999055,\n",
      "      \"grad_norm\": 6.469706058502197,\n",
      "      \"learning_rate\": 3.320226843100189e-05,\n",
      "      \"loss\": 0.0431,\n",
      "      \"step\": 53340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3623302561517376,\n",
      "      \"grad_norm\": 0.02262682095170021,\n",
      "      \"learning_rate\": 3.3195967233774414e-05,\n",
      "      \"loss\": 0.0328,\n",
      "      \"step\": 53360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3635905353035698,\n",
      "      \"grad_norm\": 0.028353070840239525,\n",
      "      \"learning_rate\": 3.318966603654694e-05,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 53380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.364850814455402,\n",
      "      \"grad_norm\": 0.005946748424321413,\n",
      "      \"learning_rate\": 3.318336483931947e-05,\n",
      "      \"loss\": 0.022,\n",
      "      \"step\": 53400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.366111093607234,\n",
      "      \"grad_norm\": 0.15531107783317566,\n",
      "      \"learning_rate\": 3.3177063642092e-05,\n",
      "      \"loss\": 0.0257,\n",
      "      \"step\": 53420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.367371372759066,\n",
      "      \"grad_norm\": 0.04401061311364174,\n",
      "      \"learning_rate\": 3.317076244486453e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 53440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3686316519108983,\n",
      "      \"grad_norm\": 3.4840710163116455,\n",
      "      \"learning_rate\": 3.316446124763705e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 53460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3698919310627304,\n",
      "      \"grad_norm\": 0.23500289022922516,\n",
      "      \"learning_rate\": 3.315816005040958e-05,\n",
      "      \"loss\": 0.0241,\n",
      "      \"step\": 53480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3711522102145626,\n",
      "      \"grad_norm\": 0.007726541720330715,\n",
      "      \"learning_rate\": 3.3151858853182104e-05,\n",
      "      \"loss\": 0.0362,\n",
      "      \"step\": 53500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3724124893663947,\n",
      "      \"grad_norm\": 0.1188030019402504,\n",
      "      \"learning_rate\": 3.314555765595463e-05,\n",
      "      \"loss\": 0.029,\n",
      "      \"step\": 53520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.373672768518227,\n",
      "      \"grad_norm\": 0.00032813139841891825,\n",
      "      \"learning_rate\": 3.3139256458727155e-05,\n",
      "      \"loss\": 0.0366,\n",
      "      \"step\": 53540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.374933047670059,\n",
      "      \"grad_norm\": 0.01587323285639286,\n",
      "      \"learning_rate\": 3.313295526149969e-05,\n",
      "      \"loss\": 0.0385,\n",
      "      \"step\": 53560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.376193326821891,\n",
      "      \"grad_norm\": 0.019811175763607025,\n",
      "      \"learning_rate\": 3.312665406427221e-05,\n",
      "      \"loss\": 0.0326,\n",
      "      \"step\": 53580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3774536059737232,\n",
      "      \"grad_norm\": 0.043586164712905884,\n",
      "      \"learning_rate\": 3.312035286704474e-05,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 53600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3787138851255554,\n",
      "      \"grad_norm\": 2.8459925651550293,\n",
      "      \"learning_rate\": 3.3114051669817264e-05,\n",
      "      \"loss\": 0.0269,\n",
      "      \"step\": 53620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3799741642773875,\n",
      "      \"grad_norm\": 0.005157107021659613,\n",
      "      \"learning_rate\": 3.310775047258979e-05,\n",
      "      \"loss\": 0.0357,\n",
      "      \"step\": 53640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3812344434292196,\n",
      "      \"grad_norm\": 0.02424737624824047,\n",
      "      \"learning_rate\": 3.310144927536232e-05,\n",
      "      \"loss\": 0.0194,\n",
      "      \"step\": 53660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3824947225810518,\n",
      "      \"grad_norm\": 0.01253113616257906,\n",
      "      \"learning_rate\": 3.3095148078134845e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 53680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.383755001732884,\n",
      "      \"grad_norm\": 0.01637755148112774,\n",
      "      \"learning_rate\": 3.3088846880907374e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 53700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.385015280884716,\n",
      "      \"grad_norm\": 0.007764458656311035,\n",
      "      \"learning_rate\": 3.30825456836799e-05,\n",
      "      \"loss\": 0.0142,\n",
      "      \"step\": 53720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.386275560036548,\n",
      "      \"grad_norm\": 0.16203615069389343,\n",
      "      \"learning_rate\": 3.307624448645243e-05,\n",
      "      \"loss\": 0.0243,\n",
      "      \"step\": 53740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3875358391883803,\n",
      "      \"grad_norm\": 9.712068557739258,\n",
      "      \"learning_rate\": 3.3069943289224954e-05,\n",
      "      \"loss\": 0.0591,\n",
      "      \"step\": 53760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3887961183402124,\n",
      "      \"grad_norm\": 0.00043131859274581075,\n",
      "      \"learning_rate\": 3.306364209199748e-05,\n",
      "      \"loss\": 0.0639,\n",
      "      \"step\": 53780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3900563974920446,\n",
      "      \"grad_norm\": 0.004608056042343378,\n",
      "      \"learning_rate\": 3.3057340894770005e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 53800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3913166766438767,\n",
      "      \"grad_norm\": 0.025481853634119034,\n",
      "      \"learning_rate\": 3.3051039697542534e-05,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 53820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.392576955795709,\n",
      "      \"grad_norm\": 0.37718555331230164,\n",
      "      \"learning_rate\": 3.3044738500315057e-05,\n",
      "      \"loss\": 0.0024,\n",
      "      \"step\": 53840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.393837234947541,\n",
      "      \"grad_norm\": 0.43934473395347595,\n",
      "      \"learning_rate\": 3.3038437303087586e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 53860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.395097514099373,\n",
      "      \"grad_norm\": 0.13759037852287292,\n",
      "      \"learning_rate\": 3.3032136105860115e-05,\n",
      "      \"loss\": 0.0667,\n",
      "      \"step\": 53880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3963577932512052,\n",
      "      \"grad_norm\": 0.004072384908795357,\n",
      "      \"learning_rate\": 3.3025834908632644e-05,\n",
      "      \"loss\": 0.0325,\n",
      "      \"step\": 53900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3976180724030374,\n",
      "      \"grad_norm\": 0.005364683922380209,\n",
      "      \"learning_rate\": 3.301953371140517e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 53920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3988783515548695,\n",
      "      \"grad_norm\": 0.2592785954475403,\n",
      "      \"learning_rate\": 3.3013232514177695e-05,\n",
      "      \"loss\": 0.027,\n",
      "      \"step\": 53940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4001386307067016,\n",
      "      \"grad_norm\": 0.00039847774314694107,\n",
      "      \"learning_rate\": 3.3006931316950224e-05,\n",
      "      \"loss\": 0.0248,\n",
      "      \"step\": 53960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4013989098585338,\n",
      "      \"grad_norm\": 0.04096783697605133,\n",
      "      \"learning_rate\": 3.3000630119722746e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 53980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.402659189010366,\n",
      "      \"grad_norm\": 0.0005518325488083065,\n",
      "      \"learning_rate\": 3.2994328922495275e-05,\n",
      "      \"loss\": 0.0326,\n",
      "      \"step\": 54000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.403919468162198,\n",
      "      \"grad_norm\": 8.966331481933594,\n",
      "      \"learning_rate\": 3.29880277252678e-05,\n",
      "      \"loss\": 0.0422,\n",
      "      \"step\": 54020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.40517974731403,\n",
      "      \"grad_norm\": 0.000704489997588098,\n",
      "      \"learning_rate\": 3.2981726528040327e-05,\n",
      "      \"loss\": 0.0018,\n",
      "      \"step\": 54040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4064400264658623,\n",
      "      \"grad_norm\": 0.004521570634096861,\n",
      "      \"learning_rate\": 3.2975425330812856e-05,\n",
      "      \"loss\": 0.0042,\n",
      "      \"step\": 54060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4077003056176944,\n",
      "      \"grad_norm\": 0.031233496963977814,\n",
      "      \"learning_rate\": 3.2969124133585385e-05,\n",
      "      \"loss\": 0.0311,\n",
      "      \"step\": 54080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4089605847695266,\n",
      "      \"grad_norm\": 0.1585029512643814,\n",
      "      \"learning_rate\": 3.2962822936357914e-05,\n",
      "      \"loss\": 0.0632,\n",
      "      \"step\": 54100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4102208639213587,\n",
      "      \"grad_norm\": 0.04238283634185791,\n",
      "      \"learning_rate\": 3.2956521739130436e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 54120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.411481143073191,\n",
      "      \"grad_norm\": 0.00014849811850581318,\n",
      "      \"learning_rate\": 3.2950220541902965e-05,\n",
      "      \"loss\": 0.036,\n",
      "      \"step\": 54140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.412741422225023,\n",
      "      \"grad_norm\": 0.08473145961761475,\n",
      "      \"learning_rate\": 3.294391934467549e-05,\n",
      "      \"loss\": 0.041,\n",
      "      \"step\": 54160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.414001701376855,\n",
      "      \"grad_norm\": 0.07521440088748932,\n",
      "      \"learning_rate\": 3.2937618147448016e-05,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 54180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4152619805286872,\n",
      "      \"grad_norm\": 0.0038532069884240627,\n",
      "      \"learning_rate\": 3.2931316950220545e-05,\n",
      "      \"loss\": 0.0131,\n",
      "      \"step\": 54200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4165222596805194,\n",
      "      \"grad_norm\": 0.06335330754518509,\n",
      "      \"learning_rate\": 3.2925015752993074e-05,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 54220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4177825388323515,\n",
      "      \"grad_norm\": 0.013421056792140007,\n",
      "      \"learning_rate\": 3.29187145557656e-05,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 54240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4190428179841836,\n",
      "      \"grad_norm\": 0.033722151070833206,\n",
      "      \"learning_rate\": 3.2912413358538126e-05,\n",
      "      \"loss\": 0.0277,\n",
      "      \"step\": 54260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4203030971360158,\n",
      "      \"grad_norm\": 0.0009280691156163812,\n",
      "      \"learning_rate\": 3.290611216131065e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 54280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.421563376287848,\n",
      "      \"grad_norm\": 0.000330281414790079,\n",
      "      \"learning_rate\": 3.289981096408318e-05,\n",
      "      \"loss\": 0.0322,\n",
      "      \"step\": 54300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.42282365543968,\n",
      "      \"grad_norm\": 0.015380602329969406,\n",
      "      \"learning_rate\": 3.2893509766855706e-05,\n",
      "      \"loss\": 0.0211,\n",
      "      \"step\": 54320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.424083934591512,\n",
      "      \"grad_norm\": 0.023566950112581253,\n",
      "      \"learning_rate\": 3.288720856962823e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 54340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4253442137433443,\n",
      "      \"grad_norm\": 0.0007025368395261467,\n",
      "      \"learning_rate\": 3.288090737240076e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 54360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4266044928951764,\n",
      "      \"grad_norm\": 0.007944443263113499,\n",
      "      \"learning_rate\": 3.2874606175173286e-05,\n",
      "      \"loss\": 0.0305,\n",
      "      \"step\": 54380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4278647720470086,\n",
      "      \"grad_norm\": 0.0002938533725682646,\n",
      "      \"learning_rate\": 3.2868304977945815e-05,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 54400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4291250511988407,\n",
      "      \"grad_norm\": 0.03874215856194496,\n",
      "      \"learning_rate\": 3.286200378071834e-05,\n",
      "      \"loss\": 0.0441,\n",
      "      \"step\": 54420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.430385330350673,\n",
      "      \"grad_norm\": 1.0136545896530151,\n",
      "      \"learning_rate\": 3.285570258349087e-05,\n",
      "      \"loss\": 0.0093,\n",
      "      \"step\": 54440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.431645609502505,\n",
      "      \"grad_norm\": 0.13157372176647186,\n",
      "      \"learning_rate\": 3.284940138626339e-05,\n",
      "      \"loss\": 0.0033,\n",
      "      \"step\": 54460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.432905888654337,\n",
      "      \"grad_norm\": 0.0007401692564599216,\n",
      "      \"learning_rate\": 3.284310018903592e-05,\n",
      "      \"loss\": 0.0182,\n",
      "      \"step\": 54480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4341661678061692,\n",
      "      \"grad_norm\": 0.0005744698573835194,\n",
      "      \"learning_rate\": 3.283679899180844e-05,\n",
      "      \"loss\": 0.0483,\n",
      "      \"step\": 54500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4354264469580014,\n",
      "      \"grad_norm\": 0.0005510499468073249,\n",
      "      \"learning_rate\": 3.283049779458097e-05,\n",
      "      \"loss\": 0.0549,\n",
      "      \"step\": 54520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4366867261098335,\n",
      "      \"grad_norm\": 2.3291759490966797,\n",
      "      \"learning_rate\": 3.28241965973535e-05,\n",
      "      \"loss\": 0.0349,\n",
      "      \"step\": 54540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4379470052616656,\n",
      "      \"grad_norm\": 0.033383943140506744,\n",
      "      \"learning_rate\": 3.281789540012603e-05,\n",
      "      \"loss\": 0.052,\n",
      "      \"step\": 54560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4392072844134978,\n",
      "      \"grad_norm\": 0.02139156311750412,\n",
      "      \"learning_rate\": 3.2811594202898556e-05,\n",
      "      \"loss\": 0.0129,\n",
      "      \"step\": 54580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.44046756356533,\n",
      "      \"grad_norm\": 0.026032354682683945,\n",
      "      \"learning_rate\": 3.280529300567108e-05,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 54600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.441727842717162,\n",
      "      \"grad_norm\": 0.4010908305644989,\n",
      "      \"learning_rate\": 3.279899180844361e-05,\n",
      "      \"loss\": 0.0344,\n",
      "      \"step\": 54620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.442988121868994,\n",
      "      \"grad_norm\": 0.004285809583961964,\n",
      "      \"learning_rate\": 3.279269061121613e-05,\n",
      "      \"loss\": 0.0261,\n",
      "      \"step\": 54640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4442484010208263,\n",
      "      \"grad_norm\": 5.235635757446289,\n",
      "      \"learning_rate\": 3.278638941398866e-05,\n",
      "      \"loss\": 0.0262,\n",
      "      \"step\": 54660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4455086801726584,\n",
      "      \"grad_norm\": 0.06896317005157471,\n",
      "      \"learning_rate\": 3.278008821676118e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 54680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4467689593244906,\n",
      "      \"grad_norm\": 0.0017222987953573465,\n",
      "      \"learning_rate\": 3.277378701953372e-05,\n",
      "      \"loss\": 0.0502,\n",
      "      \"step\": 54700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4480292384763223,\n",
      "      \"grad_norm\": 0.06770705431699753,\n",
      "      \"learning_rate\": 3.276748582230624e-05,\n",
      "      \"loss\": 0.0446,\n",
      "      \"step\": 54720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.449289517628155,\n",
      "      \"grad_norm\": 0.04559590294957161,\n",
      "      \"learning_rate\": 3.276118462507877e-05,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 54740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4505497967799865,\n",
      "      \"grad_norm\": 0.009319060482084751,\n",
      "      \"learning_rate\": 3.275488342785129e-05,\n",
      "      \"loss\": 0.0064,\n",
      "      \"step\": 54760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.451810075931819,\n",
      "      \"grad_norm\": 0.009767081588506699,\n",
      "      \"learning_rate\": 3.274858223062382e-05,\n",
      "      \"loss\": 0.0251,\n",
      "      \"step\": 54780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.453070355083651,\n",
      "      \"grad_norm\": 0.10293980687856674,\n",
      "      \"learning_rate\": 3.274228103339635e-05,\n",
      "      \"loss\": 0.0241,\n",
      "      \"step\": 54800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4543306342354834,\n",
      "      \"grad_norm\": 0.012233507819473743,\n",
      "      \"learning_rate\": 3.273597983616887e-05,\n",
      "      \"loss\": 0.027,\n",
      "      \"step\": 54820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.455590913387315,\n",
      "      \"grad_norm\": 0.0005591056542471051,\n",
      "      \"learning_rate\": 3.27296786389414e-05,\n",
      "      \"loss\": 0.0163,\n",
      "      \"step\": 54840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4568511925391476,\n",
      "      \"grad_norm\": 0.0009233055752702057,\n",
      "      \"learning_rate\": 3.272337744171393e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 54860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4581114716909793,\n",
      "      \"grad_norm\": 8.235358238220215,\n",
      "      \"learning_rate\": 3.271707624448646e-05,\n",
      "      \"loss\": 0.0369,\n",
      "      \"step\": 54880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.459371750842812,\n",
      "      \"grad_norm\": 0.024675171822309494,\n",
      "      \"learning_rate\": 3.271077504725898e-05,\n",
      "      \"loss\": 0.0024,\n",
      "      \"step\": 54900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4606320299946436,\n",
      "      \"grad_norm\": 0.08095943927764893,\n",
      "      \"learning_rate\": 3.270447385003151e-05,\n",
      "      \"loss\": 0.0306,\n",
      "      \"step\": 54920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.461892309146476,\n",
      "      \"grad_norm\": 4.931820392608643,\n",
      "      \"learning_rate\": 3.269817265280403e-05,\n",
      "      \"loss\": 0.0406,\n",
      "      \"step\": 54940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.463152588298308,\n",
      "      \"grad_norm\": 0.1820603758096695,\n",
      "      \"learning_rate\": 3.269187145557656e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 54960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4644128674501404,\n",
      "      \"grad_norm\": 9.573208808898926,\n",
      "      \"learning_rate\": 3.268557025834908e-05,\n",
      "      \"loss\": 0.0329,\n",
      "      \"step\": 54980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.465673146601972,\n",
      "      \"grad_norm\": 38.25274658203125,\n",
      "      \"learning_rate\": 3.267926906112161e-05,\n",
      "      \"loss\": 0.0369,\n",
      "      \"step\": 55000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4669334257538047,\n",
      "      \"grad_norm\": 4.808969974517822,\n",
      "      \"learning_rate\": 3.267296786389414e-05,\n",
      "      \"loss\": 0.0484,\n",
      "      \"step\": 55020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4681937049056364,\n",
      "      \"grad_norm\": 0.040120504796504974,\n",
      "      \"learning_rate\": 3.266666666666667e-05,\n",
      "      \"loss\": 0.0447,\n",
      "      \"step\": 55040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.469453984057469,\n",
      "      \"grad_norm\": 8.190974235534668,\n",
      "      \"learning_rate\": 3.26603654694392e-05,\n",
      "      \"loss\": 0.0172,\n",
      "      \"step\": 55060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4707142632093007,\n",
      "      \"grad_norm\": 0.008401810191571712,\n",
      "      \"learning_rate\": 3.265406427221172e-05,\n",
      "      \"loss\": 0.0206,\n",
      "      \"step\": 55080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4719745423611332,\n",
      "      \"grad_norm\": 0.02100919559597969,\n",
      "      \"learning_rate\": 3.264776307498425e-05,\n",
      "      \"loss\": 0.0131,\n",
      "      \"step\": 55100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.473234821512965,\n",
      "      \"grad_norm\": 5.881901264190674,\n",
      "      \"learning_rate\": 3.264146187775677e-05,\n",
      "      \"loss\": 0.0397,\n",
      "      \"step\": 55120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4744951006647975,\n",
      "      \"grad_norm\": 0.027430368587374687,\n",
      "      \"learning_rate\": 3.26351606805293e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 55140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.475755379816629,\n",
      "      \"grad_norm\": 0.007299885153770447,\n",
      "      \"learning_rate\": 3.2628859483301824e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 55160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4770156589684613,\n",
      "      \"grad_norm\": 0.021514063701033592,\n",
      "      \"learning_rate\": 3.2622873345935725e-05,\n",
      "      \"loss\": 0.02,\n",
      "      \"step\": 55180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4782759381202935,\n",
      "      \"grad_norm\": 0.004319521598517895,\n",
      "      \"learning_rate\": 3.261657214870826e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 55200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4795362172721256,\n",
      "      \"grad_norm\": 0.002559188287705183,\n",
      "      \"learning_rate\": 3.261027095148078e-05,\n",
      "      \"loss\": 0.0205,\n",
      "      \"step\": 55220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4807964964239577,\n",
      "      \"grad_norm\": 0.014069026336073875,\n",
      "      \"learning_rate\": 3.260396975425331e-05,\n",
      "      \"loss\": 0.0269,\n",
      "      \"step\": 55240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.48205677557579,\n",
      "      \"grad_norm\": 0.011241056956350803,\n",
      "      \"learning_rate\": 3.2597668557025834e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 55260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.483317054727622,\n",
      "      \"grad_norm\": 0.0004445554513949901,\n",
      "      \"learning_rate\": 3.259136735979836e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 55280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.484577333879454,\n",
      "      \"grad_norm\": 0.012319182977080345,\n",
      "      \"learning_rate\": 3.2585066162570886e-05,\n",
      "      \"loss\": 0.0456,\n",
      "      \"step\": 55300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4858376130312863,\n",
      "      \"grad_norm\": 0.6781240701675415,\n",
      "      \"learning_rate\": 3.2578764965343415e-05,\n",
      "      \"loss\": 0.0516,\n",
      "      \"step\": 55320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4870978921831184,\n",
      "      \"grad_norm\": 0.07685647159814835,\n",
      "      \"learning_rate\": 3.2572463768115944e-05,\n",
      "      \"loss\": 0.0032,\n",
      "      \"step\": 55340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4883581713349505,\n",
      "      \"grad_norm\": 15.225775718688965,\n",
      "      \"learning_rate\": 3.256616257088847e-05,\n",
      "      \"loss\": 0.0414,\n",
      "      \"step\": 55360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4896184504867827,\n",
      "      \"grad_norm\": 0.004120192490518093,\n",
      "      \"learning_rate\": 3.2559861373661e-05,\n",
      "      \"loss\": 0.045,\n",
      "      \"step\": 55380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.490878729638615,\n",
      "      \"grad_norm\": 4.058310508728027,\n",
      "      \"learning_rate\": 3.2553560176433524e-05,\n",
      "      \"loss\": 0.0509,\n",
      "      \"step\": 55400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.492139008790447,\n",
      "      \"grad_norm\": 0.010180958546698093,\n",
      "      \"learning_rate\": 3.254725897920605e-05,\n",
      "      \"loss\": 0.0524,\n",
      "      \"step\": 55420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.493399287942279,\n",
      "      \"grad_norm\": 0.0108338026329875,\n",
      "      \"learning_rate\": 3.2540957781978575e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 55440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.494659567094111,\n",
      "      \"grad_norm\": 0.04442243650555611,\n",
      "      \"learning_rate\": 3.2534656584751104e-05,\n",
      "      \"loss\": 0.0417,\n",
      "      \"step\": 55460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4959198462459433,\n",
      "      \"grad_norm\": 0.015983588993549347,\n",
      "      \"learning_rate\": 3.252835538752363e-05,\n",
      "      \"loss\": 0.0026,\n",
      "      \"step\": 55480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4971801253977755,\n",
      "      \"grad_norm\": 0.23366299271583557,\n",
      "      \"learning_rate\": 3.2522054190296156e-05,\n",
      "      \"loss\": 0.0242,\n",
      "      \"step\": 55500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4984404045496076,\n",
      "      \"grad_norm\": 0.01895386539399624,\n",
      "      \"learning_rate\": 3.2515752993068685e-05,\n",
      "      \"loss\": 0.0352,\n",
      "      \"step\": 55520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.4997006837014397,\n",
      "      \"grad_norm\": 0.05956403538584709,\n",
      "      \"learning_rate\": 3.2509451795841214e-05,\n",
      "      \"loss\": 0.0434,\n",
      "      \"step\": 55540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.500960962853272,\n",
      "      \"grad_norm\": 0.04305478185415268,\n",
      "      \"learning_rate\": 3.250315059861374e-05,\n",
      "      \"loss\": 0.0536,\n",
      "      \"step\": 55560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.502221242005104,\n",
      "      \"grad_norm\": 0.10931618511676788,\n",
      "      \"learning_rate\": 3.2496849401386265e-05,\n",
      "      \"loss\": 0.007,\n",
      "      \"step\": 55580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.503481521156936,\n",
      "      \"grad_norm\": 0.005148328375071287,\n",
      "      \"learning_rate\": 3.2490548204158794e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 55600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5047418003087683,\n",
      "      \"grad_norm\": 0.08701777458190918,\n",
      "      \"learning_rate\": 3.2484247006931316e-05,\n",
      "      \"loss\": 0.0284,\n",
      "      \"step\": 55620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5060020794606004,\n",
      "      \"grad_norm\": 0.016233598813414574,\n",
      "      \"learning_rate\": 3.2477945809703845e-05,\n",
      "      \"loss\": 0.0385,\n",
      "      \"step\": 55640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5072623586124325,\n",
      "      \"grad_norm\": 0.005274884402751923,\n",
      "      \"learning_rate\": 3.247164461247637e-05,\n",
      "      \"loss\": 0.0245,\n",
      "      \"step\": 55660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5085226377642647,\n",
      "      \"grad_norm\": 0.0673070177435875,\n",
      "      \"learning_rate\": 3.24653434152489e-05,\n",
      "      \"loss\": 0.013,\n",
      "      \"step\": 55680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.509782916916097,\n",
      "      \"grad_norm\": 0.034484509378671646,\n",
      "      \"learning_rate\": 3.2459042218021426e-05,\n",
      "      \"loss\": 0.0238,\n",
      "      \"step\": 55700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.511043196067929,\n",
      "      \"grad_norm\": 0.01135457493364811,\n",
      "      \"learning_rate\": 3.2452741020793955e-05,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 55720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.512303475219761,\n",
      "      \"grad_norm\": 0.007159165106713772,\n",
      "      \"learning_rate\": 3.244643982356648e-05,\n",
      "      \"loss\": 0.025,\n",
      "      \"step\": 55740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.513563754371593,\n",
      "      \"grad_norm\": 0.009271898306906223,\n",
      "      \"learning_rate\": 3.2440138626339006e-05,\n",
      "      \"loss\": 0.0364,\n",
      "      \"step\": 55760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5148240335234253,\n",
      "      \"grad_norm\": 0.0033985173795372248,\n",
      "      \"learning_rate\": 3.2433837429111535e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 55780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5160843126752575,\n",
      "      \"grad_norm\": 0.004500094335526228,\n",
      "      \"learning_rate\": 3.242753623188406e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 55800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5173445918270896,\n",
      "      \"grad_norm\": 0.010333701968193054,\n",
      "      \"learning_rate\": 3.2421235034656586e-05,\n",
      "      \"loss\": 0.0442,\n",
      "      \"step\": 55820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5186048709789217,\n",
      "      \"grad_norm\": 0.010495392605662346,\n",
      "      \"learning_rate\": 3.2414933837429115e-05,\n",
      "      \"loss\": 0.0273,\n",
      "      \"step\": 55840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.519865150130754,\n",
      "      \"grad_norm\": 0.02498857118189335,\n",
      "      \"learning_rate\": 3.2408632640201645e-05,\n",
      "      \"loss\": 0.0443,\n",
      "      \"step\": 55860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.521125429282586,\n",
      "      \"grad_norm\": 0.005104546435177326,\n",
      "      \"learning_rate\": 3.240233144297417e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 55880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.522385708434418,\n",
      "      \"grad_norm\": 0.09323915094137192,\n",
      "      \"learning_rate\": 3.2396030245746696e-05,\n",
      "      \"loss\": 0.0371,\n",
      "      \"step\": 55900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5236459875862502,\n",
      "      \"grad_norm\": 0.021298831328749657,\n",
      "      \"learning_rate\": 3.238972904851922e-05,\n",
      "      \"loss\": 0.0284,\n",
      "      \"step\": 55920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5249062667380824,\n",
      "      \"grad_norm\": 0.02449483424425125,\n",
      "      \"learning_rate\": 3.238342785129175e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 55940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5261665458899145,\n",
      "      \"grad_norm\": 0.010228645987808704,\n",
      "      \"learning_rate\": 3.237712665406427e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 55960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5274268250417466,\n",
      "      \"grad_norm\": 0.003962629474699497,\n",
      "      \"learning_rate\": 3.23708254568368e-05,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 55980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.528687104193579,\n",
      "      \"grad_norm\": 0.00718288728967309,\n",
      "      \"learning_rate\": 3.236452425960933e-05,\n",
      "      \"loss\": 0.0169,\n",
      "      \"step\": 56000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.529947383345411,\n",
      "      \"grad_norm\": 0.0014494118513539433,\n",
      "      \"learning_rate\": 3.2358223062381856e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 56020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.531207662497243,\n",
      "      \"grad_norm\": 0.002892665797844529,\n",
      "      \"learning_rate\": 3.2351921865154386e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 56040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.532467941649075,\n",
      "      \"grad_norm\": 0.0030160066671669483,\n",
      "      \"learning_rate\": 3.234562066792691e-05,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 56060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5337282208009073,\n",
      "      \"grad_norm\": 0.018606148660182953,\n",
      "      \"learning_rate\": 3.233931947069944e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 56080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5349884999527394,\n",
      "      \"grad_norm\": 0.0013112295418977737,\n",
      "      \"learning_rate\": 3.233301827347196e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 56100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5362487791045716,\n",
      "      \"grad_norm\": 0.017312616109848022,\n",
      "      \"learning_rate\": 3.232671707624449e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 56120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5375090582564037,\n",
      "      \"grad_norm\": 0.0024099727161228657,\n",
      "      \"learning_rate\": 3.232041587901701e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 56140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.538769337408236,\n",
      "      \"grad_norm\": 0.002832348458468914,\n",
      "      \"learning_rate\": 3.231411468178954e-05,\n",
      "      \"loss\": 0.0491,\n",
      "      \"step\": 56160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.540029616560068,\n",
      "      \"grad_norm\": 0.02503960393369198,\n",
      "      \"learning_rate\": 3.230781348456207e-05,\n",
      "      \"loss\": 0.0142,\n",
      "      \"step\": 56180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5412898957119,\n",
      "      \"grad_norm\": 0.0017740137409418821,\n",
      "      \"learning_rate\": 3.23015122873346e-05,\n",
      "      \"loss\": 0.0216,\n",
      "      \"step\": 56200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5425501748637322,\n",
      "      \"grad_norm\": 0.06420041620731354,\n",
      "      \"learning_rate\": 3.229521109010712e-05,\n",
      "      \"loss\": 0.0572,\n",
      "      \"step\": 56220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5438104540155644,\n",
      "      \"grad_norm\": 0.012647826224565506,\n",
      "      \"learning_rate\": 3.228890989287965e-05,\n",
      "      \"loss\": 0.0327,\n",
      "      \"step\": 56240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5450707331673965,\n",
      "      \"grad_norm\": 9.091089248657227,\n",
      "      \"learning_rate\": 3.228260869565218e-05,\n",
      "      \"loss\": 0.027,\n",
      "      \"step\": 56260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5463310123192286,\n",
      "      \"grad_norm\": 0.03412590175867081,\n",
      "      \"learning_rate\": 3.22763074984247e-05,\n",
      "      \"loss\": 0.064,\n",
      "      \"step\": 56280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.547591291471061,\n",
      "      \"grad_norm\": 0.030574487522244453,\n",
      "      \"learning_rate\": 3.22703213610586e-05,\n",
      "      \"loss\": 0.0272,\n",
      "      \"step\": 56300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.548851570622893,\n",
      "      \"grad_norm\": 2.4335684776306152,\n",
      "      \"learning_rate\": 3.226402016383113e-05,\n",
      "      \"loss\": 0.0213,\n",
      "      \"step\": 56320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.550111849774725,\n",
      "      \"grad_norm\": 0.34135210514068604,\n",
      "      \"learning_rate\": 3.225771896660366e-05,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 56340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.551372128926557,\n",
      "      \"grad_norm\": 0.005838862620294094,\n",
      "      \"learning_rate\": 3.225141776937619e-05,\n",
      "      \"loss\": 0.0183,\n",
      "      \"step\": 56360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5526324080783893,\n",
      "      \"grad_norm\": 0.007959096692502499,\n",
      "      \"learning_rate\": 3.224511657214871e-05,\n",
      "      \"loss\": 0.0407,\n",
      "      \"step\": 56380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5538926872302214,\n",
      "      \"grad_norm\": 0.024303585290908813,\n",
      "      \"learning_rate\": 3.223881537492124e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 56400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5551529663820536,\n",
      "      \"grad_norm\": 27.581140518188477,\n",
      "      \"learning_rate\": 3.223251417769376e-05,\n",
      "      \"loss\": 0.0482,\n",
      "      \"step\": 56420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5564132455338857,\n",
      "      \"grad_norm\": 0.11511431634426117,\n",
      "      \"learning_rate\": 3.222621298046629e-05,\n",
      "      \"loss\": 0.0317,\n",
      "      \"step\": 56440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.557673524685718,\n",
      "      \"grad_norm\": 0.01706746220588684,\n",
      "      \"learning_rate\": 3.221991178323881e-05,\n",
      "      \"loss\": 0.0178,\n",
      "      \"step\": 56460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.55893380383755,\n",
      "      \"grad_norm\": 0.011259019374847412,\n",
      "      \"learning_rate\": 3.221361058601134e-05,\n",
      "      \"loss\": 0.0322,\n",
      "      \"step\": 56480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.560194082989382,\n",
      "      \"grad_norm\": 0.06464182585477829,\n",
      "      \"learning_rate\": 3.220730938878387e-05,\n",
      "      \"loss\": 0.0456,\n",
      "      \"step\": 56500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5614543621412142,\n",
      "      \"grad_norm\": 0.24655884504318237,\n",
      "      \"learning_rate\": 3.22010081915564e-05,\n",
      "      \"loss\": 0.0193,\n",
      "      \"step\": 56520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5627146412930464,\n",
      "      \"grad_norm\": 0.0010580286616459489,\n",
      "      \"learning_rate\": 3.219470699432892e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 56540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5639749204448785,\n",
      "      \"grad_norm\": 0.025399288162589073,\n",
      "      \"learning_rate\": 3.218840579710145e-05,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 56560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5652351995967106,\n",
      "      \"grad_norm\": 3.6933975219726562,\n",
      "      \"learning_rate\": 3.218210459987398e-05,\n",
      "      \"loss\": 0.0209,\n",
      "      \"step\": 56580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.566495478748543,\n",
      "      \"grad_norm\": 0.001241769059561193,\n",
      "      \"learning_rate\": 3.21758034026465e-05,\n",
      "      \"loss\": 0.0594,\n",
      "      \"step\": 56600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.567755757900375,\n",
      "      \"grad_norm\": 0.1691695749759674,\n",
      "      \"learning_rate\": 3.216950220541903e-05,\n",
      "      \"loss\": 0.003,\n",
      "      \"step\": 56620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.569016037052207,\n",
      "      \"grad_norm\": 0.006283280905336142,\n",
      "      \"learning_rate\": 3.2163201008191554e-05,\n",
      "      \"loss\": 0.0056,\n",
      "      \"step\": 56640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.570276316204039,\n",
      "      \"grad_norm\": 0.025938959792256355,\n",
      "      \"learning_rate\": 3.215689981096408e-05,\n",
      "      \"loss\": 0.0664,\n",
      "      \"step\": 56660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5715365953558713,\n",
      "      \"grad_norm\": 0.2843283414840698,\n",
      "      \"learning_rate\": 3.215059861373661e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 56680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5727968745077034,\n",
      "      \"grad_norm\": 0.03870319947600365,\n",
      "      \"learning_rate\": 3.214429741650914e-05,\n",
      "      \"loss\": 0.0597,\n",
      "      \"step\": 56700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5740571536595356,\n",
      "      \"grad_norm\": 4.908112525939941,\n",
      "      \"learning_rate\": 3.2137996219281663e-05,\n",
      "      \"loss\": 0.0253,\n",
      "      \"step\": 56720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5753174328113677,\n",
      "      \"grad_norm\": 0.03650985285639763,\n",
      "      \"learning_rate\": 3.213169502205419e-05,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 56740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5765777119632,\n",
      "      \"grad_norm\": 0.002087558852508664,\n",
      "      \"learning_rate\": 3.2125393824826715e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 56760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.577837991115032,\n",
      "      \"grad_norm\": 0.2868885099887848,\n",
      "      \"learning_rate\": 3.2119092627599244e-05,\n",
      "      \"loss\": 0.0298,\n",
      "      \"step\": 56780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.579098270266864,\n",
      "      \"grad_norm\": 0.18434825539588928,\n",
      "      \"learning_rate\": 3.211279143037177e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 56800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5803585494186962,\n",
      "      \"grad_norm\": 19.14506721496582,\n",
      "      \"learning_rate\": 3.21064902331443e-05,\n",
      "      \"loss\": 0.003,\n",
      "      \"step\": 56820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5816188285705284,\n",
      "      \"grad_norm\": 6.696995735168457,\n",
      "      \"learning_rate\": 3.210018903591683e-05,\n",
      "      \"loss\": 0.0459,\n",
      "      \"step\": 56840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5828791077223605,\n",
      "      \"grad_norm\": 0.019101198762655258,\n",
      "      \"learning_rate\": 3.209388783868935e-05,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 56860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5841393868741926,\n",
      "      \"grad_norm\": 0.14972925186157227,\n",
      "      \"learning_rate\": 3.208758664146188e-05,\n",
      "      \"loss\": 0.0295,\n",
      "      \"step\": 56880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.585399666026025,\n",
      "      \"grad_norm\": 7.292627811431885,\n",
      "      \"learning_rate\": 3.2081285444234404e-05,\n",
      "      \"loss\": 0.0462,\n",
      "      \"step\": 56900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.586659945177857,\n",
      "      \"grad_norm\": 5.465665340423584,\n",
      "      \"learning_rate\": 3.2074984247006933e-05,\n",
      "      \"loss\": 0.0077,\n",
      "      \"step\": 56920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.587920224329689,\n",
      "      \"grad_norm\": 0.013225176371634007,\n",
      "      \"learning_rate\": 3.2068683049779456e-05,\n",
      "      \"loss\": 0.0135,\n",
      "      \"step\": 56940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.589180503481521,\n",
      "      \"grad_norm\": 6.684885501861572,\n",
      "      \"learning_rate\": 3.2062381852551985e-05,\n",
      "      \"loss\": 0.0423,\n",
      "      \"step\": 56960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5904407826333533,\n",
      "      \"grad_norm\": 4.215446949005127,\n",
      "      \"learning_rate\": 3.2056080655324514e-05,\n",
      "      \"loss\": 0.0418,\n",
      "      \"step\": 56980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5917010617851854,\n",
      "      \"grad_norm\": 15.734695434570312,\n",
      "      \"learning_rate\": 3.204977945809704e-05,\n",
      "      \"loss\": 0.0345,\n",
      "      \"step\": 57000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5929613409370176,\n",
      "      \"grad_norm\": 0.0014447277644649148,\n",
      "      \"learning_rate\": 3.204347826086957e-05,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 57020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5942216200888497,\n",
      "      \"grad_norm\": 0.004694733768701553,\n",
      "      \"learning_rate\": 3.2037177063642094e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 57040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.595481899240682,\n",
      "      \"grad_norm\": 0.0007300166762433946,\n",
      "      \"learning_rate\": 3.203087586641462e-05,\n",
      "      \"loss\": 0.0053,\n",
      "      \"step\": 57060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.596742178392514,\n",
      "      \"grad_norm\": 0.011583032086491585,\n",
      "      \"learning_rate\": 3.2024574669187145e-05,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 57080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.598002457544346,\n",
      "      \"grad_norm\": 0.0016817054711282253,\n",
      "      \"learning_rate\": 3.2018273471959675e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 57100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5992627366961782,\n",
      "      \"grad_norm\": 0.0003341195697430521,\n",
      "      \"learning_rate\": 3.20119722747322e-05,\n",
      "      \"loss\": 0.0324,\n",
      "      \"step\": 57120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6005230158480104,\n",
      "      \"grad_norm\": 0.0008417246863245964,\n",
      "      \"learning_rate\": 3.2005671077504726e-05,\n",
      "      \"loss\": 0.0116,\n",
      "      \"step\": 57140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6017832949998425,\n",
      "      \"grad_norm\": 0.012519325129687786,\n",
      "      \"learning_rate\": 3.1999369880277255e-05,\n",
      "      \"loss\": 0.0143,\n",
      "      \"step\": 57160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6030435741516746,\n",
      "      \"grad_norm\": 0.08481200784444809,\n",
      "      \"learning_rate\": 3.1993068683049784e-05,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 57180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6043038533035068,\n",
      "      \"grad_norm\": 1.2323418855667114,\n",
      "      \"learning_rate\": 3.1986767485822306e-05,\n",
      "      \"loss\": 0.0317,\n",
      "      \"step\": 57200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.605564132455339,\n",
      "      \"grad_norm\": 0.0002564944152254611,\n",
      "      \"learning_rate\": 3.1980466288594835e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 57220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.606824411607171,\n",
      "      \"grad_norm\": 0.046701326966285706,\n",
      "      \"learning_rate\": 3.1974165091367364e-05,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 57240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.608084690759003,\n",
      "      \"grad_norm\": 0.23779445886611938,\n",
      "      \"learning_rate\": 3.1967863894139886e-05,\n",
      "      \"loss\": 0.0324,\n",
      "      \"step\": 57260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6093449699108353,\n",
      "      \"grad_norm\": 0.018426381051540375,\n",
      "      \"learning_rate\": 3.1961562696912416e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 57280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6106052490626674,\n",
      "      \"grad_norm\": 0.008780846372246742,\n",
      "      \"learning_rate\": 3.195526149968494e-05,\n",
      "      \"loss\": 0.0328,\n",
      "      \"step\": 57300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6118655282144996,\n",
      "      \"grad_norm\": 0.16462039947509766,\n",
      "      \"learning_rate\": 3.194896030245747e-05,\n",
      "      \"loss\": 0.0223,\n",
      "      \"step\": 57320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6131258073663317,\n",
      "      \"grad_norm\": 0.0007772500975988805,\n",
      "      \"learning_rate\": 3.1942659105229996e-05,\n",
      "      \"loss\": 0.033,\n",
      "      \"step\": 57340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.614386086518164,\n",
      "      \"grad_norm\": 0.000223945127800107,\n",
      "      \"learning_rate\": 3.1936357908002525e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 57360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.615646365669996,\n",
      "      \"grad_norm\": 0.015728844329714775,\n",
      "      \"learning_rate\": 3.193005671077505e-05,\n",
      "      \"loss\": 0.017,\n",
      "      \"step\": 57380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.616906644821828,\n",
      "      \"grad_norm\": 0.0002629516238812357,\n",
      "      \"learning_rate\": 3.1923755513547576e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 57400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6181669239736602,\n",
      "      \"grad_norm\": 0.007459111046046019,\n",
      "      \"learning_rate\": 3.19174543163201e-05,\n",
      "      \"loss\": 0.0468,\n",
      "      \"step\": 57420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6194272031254924,\n",
      "      \"grad_norm\": 0.002502637915313244,\n",
      "      \"learning_rate\": 3.191115311909263e-05,\n",
      "      \"loss\": 0.057,\n",
      "      \"step\": 57440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6206874822773245,\n",
      "      \"grad_norm\": 0.00035487135755829513,\n",
      "      \"learning_rate\": 3.190485192186515e-05,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 57460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6219477614291566,\n",
      "      \"grad_norm\": 5.977418899536133,\n",
      "      \"learning_rate\": 3.1898550724637686e-05,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 57480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6232080405809888,\n",
      "      \"grad_norm\": 0.000814391823951155,\n",
      "      \"learning_rate\": 3.1892249527410215e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 57500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.624468319732821,\n",
      "      \"grad_norm\": 0.007504947949200869,\n",
      "      \"learning_rate\": 3.188594833018274e-05,\n",
      "      \"loss\": 0.0132,\n",
      "      \"step\": 57520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.625728598884653,\n",
      "      \"grad_norm\": 0.0004682919243350625,\n",
      "      \"learning_rate\": 3.1879647132955266e-05,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 57540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.626988878036485,\n",
      "      \"grad_norm\": 0.04785614833235741,\n",
      "      \"learning_rate\": 3.187334593572779e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 57560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6282491571883173,\n",
      "      \"grad_norm\": 0.05328008905053139,\n",
      "      \"learning_rate\": 3.186704473850032e-05,\n",
      "      \"loss\": 0.0345,\n",
      "      \"step\": 57580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6295094363401494,\n",
      "      \"grad_norm\": 0.0003536635485943407,\n",
      "      \"learning_rate\": 3.186074354127284e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 57600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6307697154919816,\n",
      "      \"grad_norm\": 6.04542875289917,\n",
      "      \"learning_rate\": 3.185444234404537e-05,\n",
      "      \"loss\": 0.0131,\n",
      "      \"step\": 57620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6320299946438137,\n",
      "      \"grad_norm\": 0.011364871636033058,\n",
      "      \"learning_rate\": 3.18481411468179e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 57640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.633290273795646,\n",
      "      \"grad_norm\": 0.009463724680244923,\n",
      "      \"learning_rate\": 3.184183994959043e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 57660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.634550552947478,\n",
      "      \"grad_norm\": 0.014595537446439266,\n",
      "      \"learning_rate\": 3.183553875236295e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 57680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.63581083209931,\n",
      "      \"grad_norm\": 0.0002452761400490999,\n",
      "      \"learning_rate\": 3.182923755513548e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 57700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6370711112511422,\n",
      "      \"grad_norm\": 0.0012839788105338812,\n",
      "      \"learning_rate\": 3.182293635790801e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 57720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6383313904029744,\n",
      "      \"grad_norm\": 0.05959424749016762,\n",
      "      \"learning_rate\": 3.181663516068053e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 57740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6395916695548065,\n",
      "      \"grad_norm\": 0.0002274052967550233,\n",
      "      \"learning_rate\": 3.181033396345306e-05,\n",
      "      \"loss\": 0.0553,\n",
      "      \"step\": 57760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6408519487066386,\n",
      "      \"grad_norm\": 0.0003893321845680475,\n",
      "      \"learning_rate\": 3.180403276622558e-05,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 57780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6421122278584708,\n",
      "      \"grad_norm\": 0.0012860542628914118,\n",
      "      \"learning_rate\": 3.179773156899811e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 57800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.643372507010303,\n",
      "      \"grad_norm\": 0.022836370393633842,\n",
      "      \"learning_rate\": 3.179143037177064e-05,\n",
      "      \"loss\": 0.0282,\n",
      "      \"step\": 57820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.644632786162135,\n",
      "      \"grad_norm\": 0.37097570300102234,\n",
      "      \"learning_rate\": 3.178512917454317e-05,\n",
      "      \"loss\": 0.0521,\n",
      "      \"step\": 57840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.645893065313967,\n",
      "      \"grad_norm\": 0.0010446746600791812,\n",
      "      \"learning_rate\": 3.177882797731569e-05,\n",
      "      \"loss\": 0.0026,\n",
      "      \"step\": 57860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6471533444657993,\n",
      "      \"grad_norm\": 0.015240777283906937,\n",
      "      \"learning_rate\": 3.177252678008822e-05,\n",
      "      \"loss\": 0.0324,\n",
      "      \"step\": 57880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6484136236176314,\n",
      "      \"grad_norm\": 0.005849445704370737,\n",
      "      \"learning_rate\": 3.176622558286074e-05,\n",
      "      \"loss\": 0.0392,\n",
      "      \"step\": 57900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6496739027694636,\n",
      "      \"grad_norm\": 0.041753534227609634,\n",
      "      \"learning_rate\": 3.175992438563327e-05,\n",
      "      \"loss\": 0.0388,\n",
      "      \"step\": 57920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6509341819212957,\n",
      "      \"grad_norm\": 0.09847118705511093,\n",
      "      \"learning_rate\": 3.17536231884058e-05,\n",
      "      \"loss\": 0.0356,\n",
      "      \"step\": 57940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.652194461073128,\n",
      "      \"grad_norm\": 0.11938421428203583,\n",
      "      \"learning_rate\": 3.174732199117832e-05,\n",
      "      \"loss\": 0.0099,\n",
      "      \"step\": 57960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.65345474022496,\n",
      "      \"grad_norm\": 0.15333594381809235,\n",
      "      \"learning_rate\": 3.174102079395086e-05,\n",
      "      \"loss\": 0.0394,\n",
      "      \"step\": 57980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.654715019376792,\n",
      "      \"grad_norm\": 0.16493041813373566,\n",
      "      \"learning_rate\": 3.173471959672338e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 58000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6559752985286242,\n",
      "      \"grad_norm\": 0.0021742545068264008,\n",
      "      \"learning_rate\": 3.172841839949591e-05,\n",
      "      \"loss\": 0.0405,\n",
      "      \"step\": 58020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6572355776804564,\n",
      "      \"grad_norm\": 0.020463602617383003,\n",
      "      \"learning_rate\": 3.172211720226843e-05,\n",
      "      \"loss\": 0.0206,\n",
      "      \"step\": 58040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6584958568322885,\n",
      "      \"grad_norm\": 2.402128219604492,\n",
      "      \"learning_rate\": 3.171581600504096e-05,\n",
      "      \"loss\": 0.0257,\n",
      "      \"step\": 58060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6597561359841206,\n",
      "      \"grad_norm\": 0.549361526966095,\n",
      "      \"learning_rate\": 3.170951480781348e-05,\n",
      "      \"loss\": 0.0296,\n",
      "      \"step\": 58080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6610164151359528,\n",
      "      \"grad_norm\": 0.008429701440036297,\n",
      "      \"learning_rate\": 3.170321361058601e-05,\n",
      "      \"loss\": 0.0059,\n",
      "      \"step\": 58100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.662276694287785,\n",
      "      \"grad_norm\": 4.609966278076172,\n",
      "      \"learning_rate\": 3.169691241335854e-05,\n",
      "      \"loss\": 0.0376,\n",
      "      \"step\": 58120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.663536973439617,\n",
      "      \"grad_norm\": 0.0054985517635941505,\n",
      "      \"learning_rate\": 3.169061121613107e-05,\n",
      "      \"loss\": 0.0135,\n",
      "      \"step\": 58140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.664797252591449,\n",
      "      \"grad_norm\": 0.01803303323686123,\n",
      "      \"learning_rate\": 3.16843100189036e-05,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 58160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.666057531743281,\n",
      "      \"grad_norm\": 0.0031310406047850847,\n",
      "      \"learning_rate\": 3.167800882167612e-05,\n",
      "      \"loss\": 0.0307,\n",
      "      \"step\": 58180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6673178108951134,\n",
      "      \"grad_norm\": 0.43051546812057495,\n",
      "      \"learning_rate\": 3.167170762444865e-05,\n",
      "      \"loss\": 0.0553,\n",
      "      \"step\": 58200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.668578090046945,\n",
      "      \"grad_norm\": 0.030175786465406418,\n",
      "      \"learning_rate\": 3.166540642722117e-05,\n",
      "      \"loss\": 0.0567,\n",
      "      \"step\": 58220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6698383691987777,\n",
      "      \"grad_norm\": 4.547175407409668,\n",
      "      \"learning_rate\": 3.16591052299937e-05,\n",
      "      \"loss\": 0.0526,\n",
      "      \"step\": 58240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6710986483506094,\n",
      "      \"grad_norm\": 0.023422053083777428,\n",
      "      \"learning_rate\": 3.165280403276622e-05,\n",
      "      \"loss\": 0.0439,\n",
      "      \"step\": 58260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.672358927502442,\n",
      "      \"grad_norm\": 0.10423547774553299,\n",
      "      \"learning_rate\": 3.164650283553875e-05,\n",
      "      \"loss\": 0.0093,\n",
      "      \"step\": 58280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6736192066542737,\n",
      "      \"grad_norm\": 0.14308978617191315,\n",
      "      \"learning_rate\": 3.164020163831128e-05,\n",
      "      \"loss\": 0.0378,\n",
      "      \"step\": 58300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6748794858061062,\n",
      "      \"grad_norm\": 0.002302557462826371,\n",
      "      \"learning_rate\": 3.163390044108381e-05,\n",
      "      \"loss\": 0.0018,\n",
      "      \"step\": 58320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.676139764957938,\n",
      "      \"grad_norm\": 0.025469619780778885,\n",
      "      \"learning_rate\": 3.162759924385633e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 58340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6774000441097705,\n",
      "      \"grad_norm\": 0.013419490307569504,\n",
      "      \"learning_rate\": 3.162129804662886e-05,\n",
      "      \"loss\": 0.0365,\n",
      "      \"step\": 58360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.678660323261602,\n",
      "      \"grad_norm\": 0.03916573151946068,\n",
      "      \"learning_rate\": 3.161499684940139e-05,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 58380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6799206024134348,\n",
      "      \"grad_norm\": 0.0034615357872098684,\n",
      "      \"learning_rate\": 3.160869565217391e-05,\n",
      "      \"loss\": 0.0293,\n",
      "      \"step\": 58400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6811808815652665,\n",
      "      \"grad_norm\": 0.09147234261035919,\n",
      "      \"learning_rate\": 3.160239445494644e-05,\n",
      "      \"loss\": 0.0583,\n",
      "      \"step\": 58420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.682441160717099,\n",
      "      \"grad_norm\": 0.05989135056734085,\n",
      "      \"learning_rate\": 3.1596093257718964e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 58440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6837014398689307,\n",
      "      \"grad_norm\": 0.45195940136909485,\n",
      "      \"learning_rate\": 3.158979206049149e-05,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 58460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6849617190207633,\n",
      "      \"grad_norm\": 0.04683975875377655,\n",
      "      \"learning_rate\": 3.158349086326402e-05,\n",
      "      \"loss\": 0.0213,\n",
      "      \"step\": 58480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.686221998172595,\n",
      "      \"grad_norm\": 0.0021597619634121656,\n",
      "      \"learning_rate\": 3.157718966603655e-05,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 58500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6874822773244276,\n",
      "      \"grad_norm\": 0.0011431792518123984,\n",
      "      \"learning_rate\": 3.1570888468809074e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 58520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6887425564762593,\n",
      "      \"grad_norm\": 0.1034611165523529,\n",
      "      \"learning_rate\": 3.15645872715816e-05,\n",
      "      \"loss\": 0.027,\n",
      "      \"step\": 58540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.690002835628092,\n",
      "      \"grad_norm\": 0.0024207376409322023,\n",
      "      \"learning_rate\": 3.1558286074354125e-05,\n",
      "      \"loss\": 0.0405,\n",
      "      \"step\": 58560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6912631147799235,\n",
      "      \"grad_norm\": 1.3159430027008057,\n",
      "      \"learning_rate\": 3.1551984877126654e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 58580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.692523393931756,\n",
      "      \"grad_norm\": 0.9050220251083374,\n",
      "      \"learning_rate\": 3.154568367989918e-05,\n",
      "      \"loss\": 0.0488,\n",
      "      \"step\": 58600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.693783673083588,\n",
      "      \"grad_norm\": 9.529241561889648,\n",
      "      \"learning_rate\": 3.153938248267171e-05,\n",
      "      \"loss\": 0.0348,\n",
      "      \"step\": 58620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6950439522354204,\n",
      "      \"grad_norm\": 0.020569942891597748,\n",
      "      \"learning_rate\": 3.153308128544424e-05,\n",
      "      \"loss\": 0.0075,\n",
      "      \"step\": 58640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.696304231387252,\n",
      "      \"grad_norm\": 3.6397063732147217,\n",
      "      \"learning_rate\": 3.1526780088216763e-05,\n",
      "      \"loss\": 0.0295,\n",
      "      \"step\": 58660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6975645105390846,\n",
      "      \"grad_norm\": 0.030238309875130653,\n",
      "      \"learning_rate\": 3.152047889098929e-05,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 58680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6988247896909163,\n",
      "      \"grad_norm\": 0.0049743931740522385,\n",
      "      \"learning_rate\": 3.1514177693761815e-05,\n",
      "      \"loss\": 0.0324,\n",
      "      \"step\": 58700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.700085068842749,\n",
      "      \"grad_norm\": 0.002145943930372596,\n",
      "      \"learning_rate\": 3.1507876496534344e-05,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 58720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7013453479945806,\n",
      "      \"grad_norm\": 0.0037477687001228333,\n",
      "      \"learning_rate\": 3.1501575299306866e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 58740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.702605627146413,\n",
      "      \"grad_norm\": 0.04653381183743477,\n",
      "      \"learning_rate\": 3.1495274102079395e-05,\n",
      "      \"loss\": 0.0464,\n",
      "      \"step\": 58760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.703865906298245,\n",
      "      \"grad_norm\": 0.002174180233851075,\n",
      "      \"learning_rate\": 3.1488972904851924e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 58780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7051261854500774,\n",
      "      \"grad_norm\": 0.04682479053735733,\n",
      "      \"learning_rate\": 3.148267170762445e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 58800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.706386464601909,\n",
      "      \"grad_norm\": 0.028873322531580925,\n",
      "      \"learning_rate\": 3.147637051039698e-05,\n",
      "      \"loss\": 0.0354,\n",
      "      \"step\": 58820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7076467437537417,\n",
      "      \"grad_norm\": 0.015833981335163116,\n",
      "      \"learning_rate\": 3.1470069313169504e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 58840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7089070229055734,\n",
      "      \"grad_norm\": 0.0040348973125219345,\n",
      "      \"learning_rate\": 3.1463768115942033e-05,\n",
      "      \"loss\": 0.0219,\n",
      "      \"step\": 58860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.710167302057406,\n",
      "      \"grad_norm\": 0.027527106925845146,\n",
      "      \"learning_rate\": 3.1457466918714556e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 58880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7114275812092377,\n",
      "      \"grad_norm\": 0.06547371298074722,\n",
      "      \"learning_rate\": 3.1451165721487085e-05,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 58900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7126878603610702,\n",
      "      \"grad_norm\": 0.006480439566075802,\n",
      "      \"learning_rate\": 3.144486452425961e-05,\n",
      "      \"loss\": 0.0489,\n",
      "      \"step\": 58920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.713948139512902,\n",
      "      \"grad_norm\": 0.0481564998626709,\n",
      "      \"learning_rate\": 3.1438563327032136e-05,\n",
      "      \"loss\": 0.0287,\n",
      "      \"step\": 58940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7152084186647345,\n",
      "      \"grad_norm\": 0.00984890479594469,\n",
      "      \"learning_rate\": 3.1432262129804665e-05,\n",
      "      \"loss\": 0.0099,\n",
      "      \"step\": 58960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.716468697816566,\n",
      "      \"grad_norm\": 0.00945415161550045,\n",
      "      \"learning_rate\": 3.1425960932577194e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 58980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7177289769683988,\n",
      "      \"grad_norm\": 0.0009924533078446984,\n",
      "      \"learning_rate\": 3.1419659735349716e-05,\n",
      "      \"loss\": 0.0066,\n",
      "      \"step\": 59000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7189892561202305,\n",
      "      \"grad_norm\": 39.331787109375,\n",
      "      \"learning_rate\": 3.1413358538122245e-05,\n",
      "      \"loss\": 0.0083,\n",
      "      \"step\": 59020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.720249535272063,\n",
      "      \"grad_norm\": 0.0023448907304555178,\n",
      "      \"learning_rate\": 3.140705734089477e-05,\n",
      "      \"loss\": 0.0507,\n",
      "      \"step\": 59040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7215098144238947,\n",
      "      \"grad_norm\": 0.008209459483623505,\n",
      "      \"learning_rate\": 3.14007561436673e-05,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 59060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.722770093575727,\n",
      "      \"grad_norm\": 0.007551342714577913,\n",
      "      \"learning_rate\": 3.1394454946439826e-05,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 59080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.724030372727559,\n",
      "      \"grad_norm\": 0.03966985270380974,\n",
      "      \"learning_rate\": 3.138815374921235e-05,\n",
      "      \"loss\": 0.1108,\n",
      "      \"step\": 59100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.725290651879391,\n",
      "      \"grad_norm\": 0.07114052772521973,\n",
      "      \"learning_rate\": 3.1381852551984884e-05,\n",
      "      \"loss\": 0.0298,\n",
      "      \"step\": 59120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7265509310312233,\n",
      "      \"grad_norm\": 0.09582805633544922,\n",
      "      \"learning_rate\": 3.1375551354757406e-05,\n",
      "      \"loss\": 0.0024,\n",
      "      \"step\": 59140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7278112101830554,\n",
      "      \"grad_norm\": 6.0251030921936035,\n",
      "      \"learning_rate\": 3.1369250157529935e-05,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 59160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7290714893348875,\n",
      "      \"grad_norm\": 0.013825888745486736,\n",
      "      \"learning_rate\": 3.136294896030246e-05,\n",
      "      \"loss\": 0.0245,\n",
      "      \"step\": 59180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7303317684867197,\n",
      "      \"grad_norm\": 0.018221404403448105,\n",
      "      \"learning_rate\": 3.1356647763074986e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 59200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.731592047638552,\n",
      "      \"grad_norm\": 0.0011565809836611152,\n",
      "      \"learning_rate\": 3.135034656584751e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 59220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.732852326790384,\n",
      "      \"grad_norm\": 0.0010362437460571527,\n",
      "      \"learning_rate\": 3.134404536862004e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 59240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.734112605942216,\n",
      "      \"grad_norm\": 0.10805324465036392,\n",
      "      \"learning_rate\": 3.133774417139257e-05,\n",
      "      \"loss\": 0.0418,\n",
      "      \"step\": 59260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.735372885094048,\n",
      "      \"grad_norm\": 0.001573770074173808,\n",
      "      \"learning_rate\": 3.1331442974165096e-05,\n",
      "      \"loss\": 0.0253,\n",
      "      \"step\": 59280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7366331642458803,\n",
      "      \"grad_norm\": 0.0072371503338217735,\n",
      "      \"learning_rate\": 3.1325141776937625e-05,\n",
      "      \"loss\": 0.0295,\n",
      "      \"step\": 59300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7378934433977125,\n",
      "      \"grad_norm\": 10.936264991760254,\n",
      "      \"learning_rate\": 3.131884057971015e-05,\n",
      "      \"loss\": 0.0508,\n",
      "      \"step\": 59320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7391537225495446,\n",
      "      \"grad_norm\": 0.4225282669067383,\n",
      "      \"learning_rate\": 3.1312539382482676e-05,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 59340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7404140017013767,\n",
      "      \"grad_norm\": 0.043945446610450745,\n",
      "      \"learning_rate\": 3.13062381852552e-05,\n",
      "      \"loss\": 0.0396,\n",
      "      \"step\": 59360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.741674280853209,\n",
      "      \"grad_norm\": 0.0018481885781511664,\n",
      "      \"learning_rate\": 3.129993698802773e-05,\n",
      "      \"loss\": 0.002,\n",
      "      \"step\": 59380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.742934560005041,\n",
      "      \"grad_norm\": 0.06990588456392288,\n",
      "      \"learning_rate\": 3.129363579080025e-05,\n",
      "      \"loss\": 0.0036,\n",
      "      \"step\": 59400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.744194839156873,\n",
      "      \"grad_norm\": 0.0030962377786636353,\n",
      "      \"learning_rate\": 3.128733459357278e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 59420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7454551183087053,\n",
      "      \"grad_norm\": 0.013667178340256214,\n",
      "      \"learning_rate\": 3.128103339634531e-05,\n",
      "      \"loss\": 0.0364,\n",
      "      \"step\": 59440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7467153974605374,\n",
      "      \"grad_norm\": 0.25304171442985535,\n",
      "      \"learning_rate\": 3.127473219911784e-05,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 59460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7479756766123695,\n",
      "      \"grad_norm\": 0.0008366785477846861,\n",
      "      \"learning_rate\": 3.126843100189036e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 59480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7492359557642017,\n",
      "      \"grad_norm\": 0.0037790474016219378,\n",
      "      \"learning_rate\": 3.126212980466289e-05,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 59500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.750496234916034,\n",
      "      \"grad_norm\": 0.1243334710597992,\n",
      "      \"learning_rate\": 3.125582860743542e-05,\n",
      "      \"loss\": 0.0468,\n",
      "      \"step\": 59520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.751756514067866,\n",
      "      \"grad_norm\": 0.0048600416630506516,\n",
      "      \"learning_rate\": 3.124952741020794e-05,\n",
      "      \"loss\": 0.0353,\n",
      "      \"step\": 59540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.753016793219698,\n",
      "      \"grad_norm\": 0.015675455331802368,\n",
      "      \"learning_rate\": 3.124322621298047e-05,\n",
      "      \"loss\": 0.0291,\n",
      "      \"step\": 59560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.75427707237153,\n",
      "      \"grad_norm\": 0.06863880157470703,\n",
      "      \"learning_rate\": 3.123692501575299e-05,\n",
      "      \"loss\": 0.0218,\n",
      "      \"step\": 59580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7555373515233623,\n",
      "      \"grad_norm\": 0.007028228137642145,\n",
      "      \"learning_rate\": 3.123062381852552e-05,\n",
      "      \"loss\": 0.0229,\n",
      "      \"step\": 59600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7567976306751945,\n",
      "      \"grad_norm\": 0.007081745192408562,\n",
      "      \"learning_rate\": 3.122432262129805e-05,\n",
      "      \"loss\": 0.0021,\n",
      "      \"step\": 59620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7580579098270266,\n",
      "      \"grad_norm\": 0.059440188109874725,\n",
      "      \"learning_rate\": 3.121802142407058e-05,\n",
      "      \"loss\": 0.0741,\n",
      "      \"step\": 59640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7593181889788587,\n",
      "      \"grad_norm\": 0.09147315472364426,\n",
      "      \"learning_rate\": 3.12117202268431e-05,\n",
      "      \"loss\": 0.058,\n",
      "      \"step\": 59660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.760578468130691,\n",
      "      \"grad_norm\": 0.004844717215746641,\n",
      "      \"learning_rate\": 3.120541902961563e-05,\n",
      "      \"loss\": 0.0027,\n",
      "      \"step\": 59680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.761838747282523,\n",
      "      \"grad_norm\": 0.08740417659282684,\n",
      "      \"learning_rate\": 3.119911783238815e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 59700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.763099026434355,\n",
      "      \"grad_norm\": 0.006441601552069187,\n",
      "      \"learning_rate\": 3.119281663516068e-05,\n",
      "      \"loss\": 0.0031,\n",
      "      \"step\": 59720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7643593055861873,\n",
      "      \"grad_norm\": 0.01212058961391449,\n",
      "      \"learning_rate\": 3.118651543793321e-05,\n",
      "      \"loss\": 0.0327,\n",
      "      \"step\": 59740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7656195847380194,\n",
      "      \"grad_norm\": 1.7830795049667358,\n",
      "      \"learning_rate\": 3.118021424070573e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 59760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7668798638898515,\n",
      "      \"grad_norm\": 0.0032862313091754913,\n",
      "      \"learning_rate\": 3.117391304347827e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 59780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7681401430416837,\n",
      "      \"grad_norm\": 0.011697176843881607,\n",
      "      \"learning_rate\": 3.116761184625079e-05,\n",
      "      \"loss\": 0.0105,\n",
      "      \"step\": 59800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.769400422193516,\n",
      "      \"grad_norm\": 0.0011637866264209151,\n",
      "      \"learning_rate\": 3.116131064902332e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 59820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.770660701345348,\n",
      "      \"grad_norm\": 0.17546984553337097,\n",
      "      \"learning_rate\": 3.115500945179584e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 59840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.77192098049718,\n",
      "      \"grad_norm\": 0.002428936306387186,\n",
      "      \"learning_rate\": 3.114870825456837e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 59860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.773181259649012,\n",
      "      \"grad_norm\": 0.0013080203207209706,\n",
      "      \"learning_rate\": 3.114240705734089e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 59880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7744415388008443,\n",
      "      \"grad_norm\": 0.0040781800635159016,\n",
      "      \"learning_rate\": 3.113610586011342e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 59900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7757018179526765,\n",
      "      \"grad_norm\": 0.0007468974217772484,\n",
      "      \"learning_rate\": 3.112980466288595e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 59920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7769620971045086,\n",
      "      \"grad_norm\": 0.0010519219795241952,\n",
      "      \"learning_rate\": 3.112350346565848e-05,\n",
      "      \"loss\": 0.0209,\n",
      "      \"step\": 59940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7782223762563407,\n",
      "      \"grad_norm\": 0.0010903727961704135,\n",
      "      \"learning_rate\": 3.111720226843101e-05,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 59960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.779482655408173,\n",
      "      \"grad_norm\": 0.001074223779141903,\n",
      "      \"learning_rate\": 3.111090107120353e-05,\n",
      "      \"loss\": 0.0529,\n",
      "      \"step\": 59980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.780742934560005,\n",
      "      \"grad_norm\": 0.3346211612224579,\n",
      "      \"learning_rate\": 3.110459987397606e-05,\n",
      "      \"loss\": 0.0553,\n",
      "      \"step\": 60000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.782003213711837,\n",
      "      \"grad_norm\": 0.009813074953854084,\n",
      "      \"learning_rate\": 3.109829867674858e-05,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 60020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7832634928636693,\n",
      "      \"grad_norm\": 0.0017422643722966313,\n",
      "      \"learning_rate\": 3.109199747952111e-05,\n",
      "      \"loss\": 0.0269,\n",
      "      \"step\": 60040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7845237720155014,\n",
      "      \"grad_norm\": 0.21090386807918549,\n",
      "      \"learning_rate\": 3.1085696282293634e-05,\n",
      "      \"loss\": 0.0332,\n",
      "      \"step\": 60060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7857840511673335,\n",
      "      \"grad_norm\": 0.002320896601304412,\n",
      "      \"learning_rate\": 3.107939508506616e-05,\n",
      "      \"loss\": 0.021,\n",
      "      \"step\": 60080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7870443303191657,\n",
      "      \"grad_norm\": 0.001732822973281145,\n",
      "      \"learning_rate\": 3.107309388783869e-05,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 60100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.788304609470998,\n",
      "      \"grad_norm\": 4.6929192543029785,\n",
      "      \"learning_rate\": 3.106679269061122e-05,\n",
      "      \"loss\": 0.0684,\n",
      "      \"step\": 60120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.78956488862283,\n",
      "      \"grad_norm\": 0.03341751545667648,\n",
      "      \"learning_rate\": 3.106049149338374e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 60140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.790825167774662,\n",
      "      \"grad_norm\": 0.007463294547051191,\n",
      "      \"learning_rate\": 3.105419029615627e-05,\n",
      "      \"loss\": 0.0445,\n",
      "      \"step\": 60160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.792085446926494,\n",
      "      \"grad_norm\": 0.14477363228797913,\n",
      "      \"learning_rate\": 3.10478890989288e-05,\n",
      "      \"loss\": 0.0099,\n",
      "      \"step\": 60180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7933457260783263,\n",
      "      \"grad_norm\": 0.2172735035419464,\n",
      "      \"learning_rate\": 3.104158790170132e-05,\n",
      "      \"loss\": 0.0211,\n",
      "      \"step\": 60200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7946060052301585,\n",
      "      \"grad_norm\": 8.48030948638916,\n",
      "      \"learning_rate\": 3.103528670447385e-05,\n",
      "      \"loss\": 0.0369,\n",
      "      \"step\": 60220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7958662843819906,\n",
      "      \"grad_norm\": 0.0016507715918123722,\n",
      "      \"learning_rate\": 3.1028985507246375e-05,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 60240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7971265635338227,\n",
      "      \"grad_norm\": 0.005745681468397379,\n",
      "      \"learning_rate\": 3.1022684310018904e-05,\n",
      "      \"loss\": 0.0045,\n",
      "      \"step\": 60260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.798386842685655,\n",
      "      \"grad_norm\": 4.958271026611328,\n",
      "      \"learning_rate\": 3.101638311279143e-05,\n",
      "      \"loss\": 0.0255,\n",
      "      \"step\": 60280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.799647121837487,\n",
      "      \"grad_norm\": 0.0016831087414175272,\n",
      "      \"learning_rate\": 3.101008191556396e-05,\n",
      "      \"loss\": 0.0307,\n",
      "      \"step\": 60300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.800907400989319,\n",
      "      \"grad_norm\": 0.004316834267228842,\n",
      "      \"learning_rate\": 3.1003780718336484e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 60320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8021676801411513,\n",
      "      \"grad_norm\": 0.2920458912849426,\n",
      "      \"learning_rate\": 3.099747952110901e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 60340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8034279592929834,\n",
      "      \"grad_norm\": 16.042606353759766,\n",
      "      \"learning_rate\": 3.0991178323881535e-05,\n",
      "      \"loss\": 0.0362,\n",
      "      \"step\": 60360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8046882384448155,\n",
      "      \"grad_norm\": 7.213622093200684,\n",
      "      \"learning_rate\": 3.0984877126654064e-05,\n",
      "      \"loss\": 0.0365,\n",
      "      \"step\": 60380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8059485175966477,\n",
      "      \"grad_norm\": 0.001892700558528304,\n",
      "      \"learning_rate\": 3.0978575929426587e-05,\n",
      "      \"loss\": 0.0292,\n",
      "      \"step\": 60400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.80720879674848,\n",
      "      \"grad_norm\": 0.0019022396299988031,\n",
      "      \"learning_rate\": 3.097227473219912e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 60420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.808469075900312,\n",
      "      \"grad_norm\": 0.03447318077087402,\n",
      "      \"learning_rate\": 3.096597353497165e-05,\n",
      "      \"loss\": 0.0042,\n",
      "      \"step\": 60440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.809729355052144,\n",
      "      \"grad_norm\": 23.25201416015625,\n",
      "      \"learning_rate\": 3.0959672337744174e-05,\n",
      "      \"loss\": 0.0211,\n",
      "      \"step\": 60460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.810989634203976,\n",
      "      \"grad_norm\": 0.007276453543454409,\n",
      "      \"learning_rate\": 3.09533711405167e-05,\n",
      "      \"loss\": 0.0208,\n",
      "      \"step\": 60480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8122499133558083,\n",
      "      \"grad_norm\": 0.7786283493041992,\n",
      "      \"learning_rate\": 3.0947069943289225e-05,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 60500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8135101925076405,\n",
      "      \"grad_norm\": 0.0017408161656931043,\n",
      "      \"learning_rate\": 3.0940768746061754e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 60520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8147704716594726,\n",
      "      \"grad_norm\": 0.0018107828218489885,\n",
      "      \"learning_rate\": 3.0934467548834276e-05,\n",
      "      \"loss\": 0.0324,\n",
      "      \"step\": 60540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8160307508113047,\n",
      "      \"grad_norm\": 0.23197683691978455,\n",
      "      \"learning_rate\": 3.0928166351606805e-05,\n",
      "      \"loss\": 0.0245,\n",
      "      \"step\": 60560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.817291029963137,\n",
      "      \"grad_norm\": 0.0013750448124483228,\n",
      "      \"learning_rate\": 3.0921865154379334e-05,\n",
      "      \"loss\": 0.0232,\n",
      "      \"step\": 60580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.818551309114969,\n",
      "      \"grad_norm\": 0.006735051050782204,\n",
      "      \"learning_rate\": 3.091556395715186e-05,\n",
      "      \"loss\": 0.0235,\n",
      "      \"step\": 60600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.819811588266801,\n",
      "      \"grad_norm\": 0.0021873379591852427,\n",
      "      \"learning_rate\": 3.0909262759924386e-05,\n",
      "      \"loss\": 0.0258,\n",
      "      \"step\": 60620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8210718674186332,\n",
      "      \"grad_norm\": 0.0044466122053563595,\n",
      "      \"learning_rate\": 3.0902961562696915e-05,\n",
      "      \"loss\": 0.0674,\n",
      "      \"step\": 60640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8223321465704654,\n",
      "      \"grad_norm\": 0.07053493708372116,\n",
      "      \"learning_rate\": 3.0896660365469444e-05,\n",
      "      \"loss\": 0.0083,\n",
      "      \"step\": 60660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8235924257222975,\n",
      "      \"grad_norm\": 0.002023172564804554,\n",
      "      \"learning_rate\": 3.0890359168241966e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 60680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8248527048741296,\n",
      "      \"grad_norm\": 0.001816184725612402,\n",
      "      \"learning_rate\": 3.0884057971014495e-05,\n",
      "      \"loss\": 0.037,\n",
      "      \"step\": 60700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.826112984025962,\n",
      "      \"grad_norm\": 0.004284143913537264,\n",
      "      \"learning_rate\": 3.087775677378702e-05,\n",
      "      \"loss\": 0.02,\n",
      "      \"step\": 60720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.827373263177794,\n",
      "      \"grad_norm\": 8.914834976196289,\n",
      "      \"learning_rate\": 3.0871455576559546e-05,\n",
      "      \"loss\": 0.02,\n",
      "      \"step\": 60740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.828633542329626,\n",
      "      \"grad_norm\": 1.7246028184890747,\n",
      "      \"learning_rate\": 3.0865154379332075e-05,\n",
      "      \"loss\": 0.0289,\n",
      "      \"step\": 60760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.829893821481458,\n",
      "      \"grad_norm\": 0.11716046929359436,\n",
      "      \"learning_rate\": 3.0858853182104604e-05,\n",
      "      \"loss\": 0.0488,\n",
      "      \"step\": 60780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8311541006332903,\n",
      "      \"grad_norm\": 0.6951552033424377,\n",
      "      \"learning_rate\": 3.085255198487713e-05,\n",
      "      \"loss\": 0.0382,\n",
      "      \"step\": 60800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8324143797851224,\n",
      "      \"grad_norm\": 0.005557483062148094,\n",
      "      \"learning_rate\": 3.0846250787649656e-05,\n",
      "      \"loss\": 0.0189,\n",
      "      \"step\": 60820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8336746589369546,\n",
      "      \"grad_norm\": 0.06326276808977127,\n",
      "      \"learning_rate\": 3.083994959042218e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 60840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8349349380887867,\n",
      "      \"grad_norm\": 0.03751911222934723,\n",
      "      \"learning_rate\": 3.083364839319471e-05,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 60860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.836195217240619,\n",
      "      \"grad_norm\": 0.09070418030023575,\n",
      "      \"learning_rate\": 3.0827347195967236e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 60880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.837455496392451,\n",
      "      \"grad_norm\": 0.006219813134521246,\n",
      "      \"learning_rate\": 3.082104599873976e-05,\n",
      "      \"loss\": 0.0105,\n",
      "      \"step\": 60900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.838715775544283,\n",
      "      \"grad_norm\": 0.01065763644874096,\n",
      "      \"learning_rate\": 3.0814744801512294e-05,\n",
      "      \"loss\": 0.0743,\n",
      "      \"step\": 60920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8399760546961152,\n",
      "      \"grad_norm\": 0.006251207552850246,\n",
      "      \"learning_rate\": 3.0808443604284816e-05,\n",
      "      \"loss\": 0.0569,\n",
      "      \"step\": 60940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8412363338479474,\n",
      "      \"grad_norm\": 0.09476717561483383,\n",
      "      \"learning_rate\": 3.0802142407057345e-05,\n",
      "      \"loss\": 0.0182,\n",
      "      \"step\": 60960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8424966129997795,\n",
      "      \"grad_norm\": 6.650217056274414,\n",
      "      \"learning_rate\": 3.079584120982987e-05,\n",
      "      \"loss\": 0.0221,\n",
      "      \"step\": 60980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8437568921516116,\n",
      "      \"grad_norm\": 5.776040554046631,\n",
      "      \"learning_rate\": 3.07895400126024e-05,\n",
      "      \"loss\": 0.036,\n",
      "      \"step\": 61000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.845017171303444,\n",
      "      \"grad_norm\": 0.003370904130861163,\n",
      "      \"learning_rate\": 3.078323881537492e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 61020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.846277450455276,\n",
      "      \"grad_norm\": 0.5784374475479126,\n",
      "      \"learning_rate\": 3.077693761814745e-05,\n",
      "      \"loss\": 0.0754,\n",
      "      \"step\": 61040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.847537729607108,\n",
      "      \"grad_norm\": 0.07964897155761719,\n",
      "      \"learning_rate\": 3.077063642091998e-05,\n",
      "      \"loss\": 0.0293,\n",
      "      \"step\": 61060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.84879800875894,\n",
      "      \"grad_norm\": 0.001136569306254387,\n",
      "      \"learning_rate\": 3.0764335223692506e-05,\n",
      "      \"loss\": 0.037,\n",
      "      \"step\": 61080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8500582879107723,\n",
      "      \"grad_norm\": 0.026087557896971703,\n",
      "      \"learning_rate\": 3.0758034026465035e-05,\n",
      "      \"loss\": 0.0309,\n",
      "      \"step\": 61100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8513185670626044,\n",
      "      \"grad_norm\": 0.015744926407933235,\n",
      "      \"learning_rate\": 3.075173282923756e-05,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 61120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8525788462144366,\n",
      "      \"grad_norm\": 0.0013065533712506294,\n",
      "      \"learning_rate\": 3.0745431632010086e-05,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 61140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8538391253662687,\n",
      "      \"grad_norm\": 9.444458961486816,\n",
      "      \"learning_rate\": 3.073913043478261e-05,\n",
      "      \"loss\": 0.0582,\n",
      "      \"step\": 61160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.855099404518101,\n",
      "      \"grad_norm\": 0.001335885259322822,\n",
      "      \"learning_rate\": 3.073282923755514e-05,\n",
      "      \"loss\": 0.0099,\n",
      "      \"step\": 61180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.856359683669933,\n",
      "      \"grad_norm\": 0.02000957354903221,\n",
      "      \"learning_rate\": 3.072652804032766e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 61200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.857619962821765,\n",
      "      \"grad_norm\": 0.004107920452952385,\n",
      "      \"learning_rate\": 3.072022684310019e-05,\n",
      "      \"loss\": 0.0484,\n",
      "      \"step\": 61220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8588802419735972,\n",
      "      \"grad_norm\": 4.790487289428711,\n",
      "      \"learning_rate\": 3.071392564587272e-05,\n",
      "      \"loss\": 0.0272,\n",
      "      \"step\": 61240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8601405211254294,\n",
      "      \"grad_norm\": 0.007956906221807003,\n",
      "      \"learning_rate\": 3.070762444864525e-05,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 61260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8614008002772615,\n",
      "      \"grad_norm\": 0.011302860453724861,\n",
      "      \"learning_rate\": 3.070132325141777e-05,\n",
      "      \"loss\": 0.0051,\n",
      "      \"step\": 61280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8626610794290936,\n",
      "      \"grad_norm\": 0.0006900310982018709,\n",
      "      \"learning_rate\": 3.06950220541903e-05,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 61300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.863921358580926,\n",
      "      \"grad_norm\": 0.004366486333310604,\n",
      "      \"learning_rate\": 3.068872085696283e-05,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 61320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.865181637732758,\n",
      "      \"grad_norm\": 0.001819987315684557,\n",
      "      \"learning_rate\": 3.068241965973535e-05,\n",
      "      \"loss\": 0.0157,\n",
      "      \"step\": 61340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.86644191688459,\n",
      "      \"grad_norm\": 0.02389262244105339,\n",
      "      \"learning_rate\": 3.067611846250788e-05,\n",
      "      \"loss\": 0.0293,\n",
      "      \"step\": 61360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.867702196036422,\n",
      "      \"grad_norm\": 0.0006408914923667908,\n",
      "      \"learning_rate\": 3.06698172652804e-05,\n",
      "      \"loss\": 0.0258,\n",
      "      \"step\": 61380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8689624751882543,\n",
      "      \"grad_norm\": 0.1070595234632492,\n",
      "      \"learning_rate\": 3.066351606805293e-05,\n",
      "      \"loss\": 0.0135,\n",
      "      \"step\": 61400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8702227543400864,\n",
      "      \"grad_norm\": 0.0005507428431883454,\n",
      "      \"learning_rate\": 3.065721487082546e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 61420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8714830334919186,\n",
      "      \"grad_norm\": 0.0015711718006059527,\n",
      "      \"learning_rate\": 3.065091367359799e-05,\n",
      "      \"loss\": 0.0399,\n",
      "      \"step\": 61440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8727433126437507,\n",
      "      \"grad_norm\": 4.138822555541992,\n",
      "      \"learning_rate\": 3.064461247637051e-05,\n",
      "      \"loss\": 0.034,\n",
      "      \"step\": 61460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.874003591795583,\n",
      "      \"grad_norm\": 1.0184686183929443,\n",
      "      \"learning_rate\": 3.063831127914304e-05,\n",
      "      \"loss\": 0.0161,\n",
      "      \"step\": 61480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.875263870947415,\n",
      "      \"grad_norm\": 0.026446130126714706,\n",
      "      \"learning_rate\": 3.063201008191556e-05,\n",
      "      \"loss\": 0.0029,\n",
      "      \"step\": 61500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.876524150099247,\n",
      "      \"grad_norm\": 0.14325366914272308,\n",
      "      \"learning_rate\": 3.062570888468809e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 61520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8777844292510792,\n",
      "      \"grad_norm\": 0.06286197155714035,\n",
      "      \"learning_rate\": 3.061940768746062e-05,\n",
      "      \"loss\": 0.0742,\n",
      "      \"step\": 61540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8790447084029114,\n",
      "      \"grad_norm\": 0.011295930482447147,\n",
      "      \"learning_rate\": 3.061310649023315e-05,\n",
      "      \"loss\": 0.0316,\n",
      "      \"step\": 61560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8803049875547435,\n",
      "      \"grad_norm\": 0.004802762996405363,\n",
      "      \"learning_rate\": 3.060680529300568e-05,\n",
      "      \"loss\": 0.0168,\n",
      "      \"step\": 61580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8815652667065756,\n",
      "      \"grad_norm\": 0.0038348922971636057,\n",
      "      \"learning_rate\": 3.06005040957782e-05,\n",
      "      \"loss\": 0.034,\n",
      "      \"step\": 61600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.882825545858408,\n",
      "      \"grad_norm\": 0.002043497981503606,\n",
      "      \"learning_rate\": 3.059420289855073e-05,\n",
      "      \"loss\": 0.0456,\n",
      "      \"step\": 61620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.88408582501024,\n",
      "      \"grad_norm\": 0.0024074965622276068,\n",
      "      \"learning_rate\": 3.058790170132325e-05,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 61640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.885346104162072,\n",
      "      \"grad_norm\": 7.423821926116943,\n",
      "      \"learning_rate\": 3.058160050409578e-05,\n",
      "      \"loss\": 0.0307,\n",
      "      \"step\": 61660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.886606383313904,\n",
      "      \"grad_norm\": 0.011427833698689938,\n",
      "      \"learning_rate\": 3.05752993068683e-05,\n",
      "      \"loss\": 0.005,\n",
      "      \"step\": 61680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8878666624657363,\n",
      "      \"grad_norm\": 0.030077653005719185,\n",
      "      \"learning_rate\": 3.056899810964083e-05,\n",
      "      \"loss\": 0.0425,\n",
      "      \"step\": 61700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8891269416175684,\n",
      "      \"grad_norm\": 0.1291709840297699,\n",
      "      \"learning_rate\": 3.056269691241336e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 61720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8903872207694006,\n",
      "      \"grad_norm\": 0.05969173088669777,\n",
      "      \"learning_rate\": 3.055639571518589e-05,\n",
      "      \"loss\": 0.0077,\n",
      "      \"step\": 61740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8916474999212327,\n",
      "      \"grad_norm\": 0.019935274496674538,\n",
      "      \"learning_rate\": 3.055009451795841e-05,\n",
      "      \"loss\": 0.0172,\n",
      "      \"step\": 61760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.892907779073065,\n",
      "      \"grad_norm\": 6.7365522384643555,\n",
      "      \"learning_rate\": 3.054379332073094e-05,\n",
      "      \"loss\": 0.0302,\n",
      "      \"step\": 61780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.894168058224897,\n",
      "      \"grad_norm\": 0.006107197143137455,\n",
      "      \"learning_rate\": 3.053749212350347e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 61800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.895428337376729,\n",
      "      \"grad_norm\": 0.02779165655374527,\n",
      "      \"learning_rate\": 3.053119092627599e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 61820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.896688616528561,\n",
      "      \"grad_norm\": 0.03678709641098976,\n",
      "      \"learning_rate\": 3.052488972904852e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 61840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8979488956803934,\n",
      "      \"grad_norm\": 4.103608131408691,\n",
      "      \"learning_rate\": 3.0518588531821044e-05,\n",
      "      \"loss\": 0.0365,\n",
      "      \"step\": 61860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.899209174832225,\n",
      "      \"grad_norm\": 0.03720473125576973,\n",
      "      \"learning_rate\": 3.0512287334593576e-05,\n",
      "      \"loss\": 0.0058,\n",
      "      \"step\": 61880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9004694539840576,\n",
      "      \"grad_norm\": 0.06157771125435829,\n",
      "      \"learning_rate\": 3.05059861373661e-05,\n",
      "      \"loss\": 0.0093,\n",
      "      \"step\": 61900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9017297331358893,\n",
      "      \"grad_norm\": 0.005085997749119997,\n",
      "      \"learning_rate\": 3.0499684940138628e-05,\n",
      "      \"loss\": 0.007,\n",
      "      \"step\": 61920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.902990012287722,\n",
      "      \"grad_norm\": 0.0033120603766292334,\n",
      "      \"learning_rate\": 3.0493383742911153e-05,\n",
      "      \"loss\": 0.0238,\n",
      "      \"step\": 61940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9042502914395536,\n",
      "      \"grad_norm\": 0.002900519175454974,\n",
      "      \"learning_rate\": 3.0487082545683682e-05,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 61960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.905510570591386,\n",
      "      \"grad_norm\": 0.032471999526023865,\n",
      "      \"learning_rate\": 3.0480781348456204e-05,\n",
      "      \"loss\": 0.0019,\n",
      "      \"step\": 61980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.906770849743218,\n",
      "      \"grad_norm\": 0.1804478019475937,\n",
      "      \"learning_rate\": 3.0474480151228734e-05,\n",
      "      \"loss\": 0.0319,\n",
      "      \"step\": 62000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9080311288950504,\n",
      "      \"grad_norm\": 10.3724946975708,\n",
      "      \"learning_rate\": 3.0468178954001263e-05,\n",
      "      \"loss\": 0.0414,\n",
      "      \"step\": 62020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.909291408046882,\n",
      "      \"grad_norm\": 0.000532419653609395,\n",
      "      \"learning_rate\": 3.0461877756773788e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 62040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9105516871987147,\n",
      "      \"grad_norm\": 0.0032771469559520483,\n",
      "      \"learning_rate\": 3.0455576559546317e-05,\n",
      "      \"loss\": 0.027,\n",
      "      \"step\": 62060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9118119663505464,\n",
      "      \"grad_norm\": 0.16545498371124268,\n",
      "      \"learning_rate\": 3.044927536231884e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 62080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.913072245502379,\n",
      "      \"grad_norm\": 0.12130662053823471,\n",
      "      \"learning_rate\": 3.0442974165091372e-05,\n",
      "      \"loss\": 0.0136,\n",
      "      \"step\": 62100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9143325246542107,\n",
      "      \"grad_norm\": 0.03002098761498928,\n",
      "      \"learning_rate\": 3.0436672967863894e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 62120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9155928038060432,\n",
      "      \"grad_norm\": 0.14673373103141785,\n",
      "      \"learning_rate\": 3.0430371770636423e-05,\n",
      "      \"loss\": 0.0931,\n",
      "      \"step\": 62140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.916853082957875,\n",
      "      \"grad_norm\": 0.05247732624411583,\n",
      "      \"learning_rate\": 3.0424070573408945e-05,\n",
      "      \"loss\": 0.0223,\n",
      "      \"step\": 62160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9181133621097075,\n",
      "      \"grad_norm\": 0.007356531452387571,\n",
      "      \"learning_rate\": 3.0417769376181478e-05,\n",
      "      \"loss\": 0.0278,\n",
      "      \"step\": 62180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.919373641261539,\n",
      "      \"grad_norm\": 0.02424977719783783,\n",
      "      \"learning_rate\": 3.0411468178954e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 62200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9206339204133718,\n",
      "      \"grad_norm\": 22.6508846282959,\n",
      "      \"learning_rate\": 3.040516698172653e-05,\n",
      "      \"loss\": 0.0345,\n",
      "      \"step\": 62220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9218941995652035,\n",
      "      \"grad_norm\": 0.07452577352523804,\n",
      "      \"learning_rate\": 3.0398865784499058e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 62240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.923154478717036,\n",
      "      \"grad_norm\": 0.07858661562204361,\n",
      "      \"learning_rate\": 3.0392564587271584e-05,\n",
      "      \"loss\": 0.0168,\n",
      "      \"step\": 62260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9244147578688677,\n",
      "      \"grad_norm\": 0.013835114426910877,\n",
      "      \"learning_rate\": 3.0386263390044113e-05,\n",
      "      \"loss\": 0.0341,\n",
      "      \"step\": 62280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9256750370207003,\n",
      "      \"grad_norm\": 0.0343702994287014,\n",
      "      \"learning_rate\": 3.0379962192816635e-05,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 62300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.926935316172532,\n",
      "      \"grad_norm\": 0.004403178580105305,\n",
      "      \"learning_rate\": 3.0373660995589164e-05,\n",
      "      \"loss\": 0.0217,\n",
      "      \"step\": 62320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9281955953243646,\n",
      "      \"grad_norm\": 0.045456912368535995,\n",
      "      \"learning_rate\": 3.036735979836169e-05,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 62340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9294558744761963,\n",
      "      \"grad_norm\": 0.0110357366502285,\n",
      "      \"learning_rate\": 3.036105860113422e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 62360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.930716153628029,\n",
      "      \"grad_norm\": 0.10266361385583878,\n",
      "      \"learning_rate\": 3.035475740390674e-05,\n",
      "      \"loss\": 0.0132,\n",
      "      \"step\": 62380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9319764327798605,\n",
      "      \"grad_norm\": 4.523881912231445,\n",
      "      \"learning_rate\": 3.034845620667927e-05,\n",
      "      \"loss\": 0.0549,\n",
      "      \"step\": 62400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.933236711931693,\n",
      "      \"grad_norm\": 0.005062466952949762,\n",
      "      \"learning_rate\": 3.0342155009451796e-05,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 62420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.934496991083525,\n",
      "      \"grad_norm\": 0.005847609136253595,\n",
      "      \"learning_rate\": 3.0335853812224325e-05,\n",
      "      \"loss\": 0.0365,\n",
      "      \"step\": 62440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9357572702353574,\n",
      "      \"grad_norm\": 0.022027386352419853,\n",
      "      \"learning_rate\": 3.0329552614996854e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 62460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.937017549387189,\n",
      "      \"grad_norm\": 0.006163300015032291,\n",
      "      \"learning_rate\": 3.0323251417769376e-05,\n",
      "      \"loss\": 0.0506,\n",
      "      \"step\": 62480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9382778285390216,\n",
      "      \"grad_norm\": 0.011620069853961468,\n",
      "      \"learning_rate\": 3.0317265280403277e-05,\n",
      "      \"loss\": 0.0302,\n",
      "      \"step\": 62500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9395381076908533,\n",
      "      \"grad_norm\": 0.008597868494689465,\n",
      "      \"learning_rate\": 3.0310964083175803e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 62520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.940798386842686,\n",
      "      \"grad_norm\": 0.019534502178430557,\n",
      "      \"learning_rate\": 3.0304662885948332e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 62540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9420586659945176,\n",
      "      \"grad_norm\": 0.00631875591352582,\n",
      "      \"learning_rate\": 3.029836168872086e-05,\n",
      "      \"loss\": 0.0393,\n",
      "      \"step\": 62560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.94331894514635,\n",
      "      \"grad_norm\": 3.823845148086548,\n",
      "      \"learning_rate\": 3.0292060491493383e-05,\n",
      "      \"loss\": 0.0264,\n",
      "      \"step\": 62580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.944579224298182,\n",
      "      \"grad_norm\": 28.516082763671875,\n",
      "      \"learning_rate\": 3.0285759294265916e-05,\n",
      "      \"loss\": 0.0047,\n",
      "      \"step\": 62600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9458395034500144,\n",
      "      \"grad_norm\": 0.0038094911724328995,\n",
      "      \"learning_rate\": 3.0279458097038438e-05,\n",
      "      \"loss\": 0.0018,\n",
      "      \"step\": 62620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.947099782601846,\n",
      "      \"grad_norm\": 0.0016602054238319397,\n",
      "      \"learning_rate\": 3.0273156899810967e-05,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 62640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9483600617536787,\n",
      "      \"grad_norm\": 0.011939013376832008,\n",
      "      \"learning_rate\": 3.0266855702583493e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 62660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9496203409055104,\n",
      "      \"grad_norm\": 0.006942244246602058,\n",
      "      \"learning_rate\": 3.026055450535602e-05,\n",
      "      \"loss\": 0.0322,\n",
      "      \"step\": 62680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.950880620057343,\n",
      "      \"grad_norm\": 0.057159386575222015,\n",
      "      \"learning_rate\": 3.0254253308128544e-05,\n",
      "      \"loss\": 0.026,\n",
      "      \"step\": 62700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9521408992091747,\n",
      "      \"grad_norm\": 0.00535010639578104,\n",
      "      \"learning_rate\": 3.0247952110901073e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 62720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9534011783610072,\n",
      "      \"grad_norm\": 3.6339304447174072,\n",
      "      \"learning_rate\": 3.02416509136736e-05,\n",
      "      \"loss\": 0.0537,\n",
      "      \"step\": 62740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.954661457512839,\n",
      "      \"grad_norm\": 0.006668159272521734,\n",
      "      \"learning_rate\": 3.0235349716446128e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 62760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.955921736664671,\n",
      "      \"grad_norm\": 0.004913896322250366,\n",
      "      \"learning_rate\": 3.0229048519218657e-05,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 62780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.957182015816503,\n",
      "      \"grad_norm\": 0.004131481517106295,\n",
      "      \"learning_rate\": 3.022274732199118e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 62800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9584422949683353,\n",
      "      \"grad_norm\": 0.006821820978075266,\n",
      "      \"learning_rate\": 3.0216446124763708e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 62820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9597025741201675,\n",
      "      \"grad_norm\": 0.0172260869294405,\n",
      "      \"learning_rate\": 3.0210144927536234e-05,\n",
      "      \"loss\": 0.0023,\n",
      "      \"step\": 62840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9609628532719996,\n",
      "      \"grad_norm\": 0.0033449844922870398,\n",
      "      \"learning_rate\": 3.0203843730308763e-05,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 62860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9622231324238317,\n",
      "      \"grad_norm\": 0.02240781858563423,\n",
      "      \"learning_rate\": 3.0197542533081285e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 62880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.963483411575664,\n",
      "      \"grad_norm\": 0.040510207414627075,\n",
      "      \"learning_rate\": 3.0191241335853814e-05,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 62900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.964743690727496,\n",
      "      \"grad_norm\": 0.0016300550196319818,\n",
      "      \"learning_rate\": 3.018494013862634e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 62920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.966003969879328,\n",
      "      \"grad_norm\": 0.02148476056754589,\n",
      "      \"learning_rate\": 3.017863894139887e-05,\n",
      "      \"loss\": 0.0034,\n",
      "      \"step\": 62940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9672642490311603,\n",
      "      \"grad_norm\": 0.04748229682445526,\n",
      "      \"learning_rate\": 3.017233774417139e-05,\n",
      "      \"loss\": 0.0552,\n",
      "      \"step\": 62960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9685245281829924,\n",
      "      \"grad_norm\": 0.002338410122320056,\n",
      "      \"learning_rate\": 3.016603654694392e-05,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 62980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9697848073348245,\n",
      "      \"grad_norm\": 0.00144309108145535,\n",
      "      \"learning_rate\": 3.0160050409577824e-05,\n",
      "      \"loss\": 0.0039,\n",
      "      \"step\": 63000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9710450864866567,\n",
      "      \"grad_norm\": 0.07081229239702225,\n",
      "      \"learning_rate\": 3.0153749212350346e-05,\n",
      "      \"loss\": 0.0257,\n",
      "      \"step\": 63020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.972305365638489,\n",
      "      \"grad_norm\": 12.194733619689941,\n",
      "      \"learning_rate\": 3.0147448015122875e-05,\n",
      "      \"loss\": 0.0067,\n",
      "      \"step\": 63040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.973565644790321,\n",
      "      \"grad_norm\": 0.006601800210773945,\n",
      "      \"learning_rate\": 3.0141146817895398e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 63060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.974825923942153,\n",
      "      \"grad_norm\": 11.803224563598633,\n",
      "      \"learning_rate\": 3.013484562066793e-05,\n",
      "      \"loss\": 0.0393,\n",
      "      \"step\": 63080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.976086203093985,\n",
      "      \"grad_norm\": 0.002537450287491083,\n",
      "      \"learning_rate\": 3.0128544423440452e-05,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 63100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9773464822458173,\n",
      "      \"grad_norm\": 0.00866558775305748,\n",
      "      \"learning_rate\": 3.012224322621298e-05,\n",
      "      \"loss\": 0.0268,\n",
      "      \"step\": 63120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9786067613976495,\n",
      "      \"grad_norm\": 0.09372498095035553,\n",
      "      \"learning_rate\": 3.011594202898551e-05,\n",
      "      \"loss\": 0.017,\n",
      "      \"step\": 63140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9798670405494816,\n",
      "      \"grad_norm\": 0.05931130796670914,\n",
      "      \"learning_rate\": 3.0109640831758036e-05,\n",
      "      \"loss\": 0.0308,\n",
      "      \"step\": 63160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9811273197013137,\n",
      "      \"grad_norm\": 0.01811293140053749,\n",
      "      \"learning_rate\": 3.0103339634530565e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 63180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.982387598853146,\n",
      "      \"grad_norm\": 0.003981099929660559,\n",
      "      \"learning_rate\": 3.0097038437303087e-05,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 63200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.983647878004978,\n",
      "      \"grad_norm\": 0.06039290875196457,\n",
      "      \"learning_rate\": 3.0090737240075616e-05,\n",
      "      \"loss\": 0.0407,\n",
      "      \"step\": 63220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.98490815715681,\n",
      "      \"grad_norm\": 0.016196612268686295,\n",
      "      \"learning_rate\": 3.0084436042848142e-05,\n",
      "      \"loss\": 0.0291,\n",
      "      \"step\": 63240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9861684363086423,\n",
      "      \"grad_norm\": 0.04816775023937225,\n",
      "      \"learning_rate\": 3.007813484562067e-05,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 63260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9874287154604744,\n",
      "      \"grad_norm\": 3.8592233657836914,\n",
      "      \"learning_rate\": 3.0071833648393193e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 63280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9886889946123065,\n",
      "      \"grad_norm\": 0.05059508606791496,\n",
      "      \"learning_rate\": 3.0065532451165722e-05,\n",
      "      \"loss\": 0.0533,\n",
      "      \"step\": 63300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9899492737641387,\n",
      "      \"grad_norm\": 8.816300392150879,\n",
      "      \"learning_rate\": 3.0059231253938248e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 63320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.991209552915971,\n",
      "      \"grad_norm\": 1.4019273519515991,\n",
      "      \"learning_rate\": 3.0052930056710777e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 63340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.992469832067803,\n",
      "      \"grad_norm\": 0.005884952377527952,\n",
      "      \"learning_rate\": 3.0046628859483306e-05,\n",
      "      \"loss\": 0.0467,\n",
      "      \"step\": 63360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.993730111219635,\n",
      "      \"grad_norm\": 0.0049116709269583225,\n",
      "      \"learning_rate\": 3.004032766225583e-05,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 63380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.994990390371467,\n",
      "      \"grad_norm\": 6.332178115844727,\n",
      "      \"learning_rate\": 3.0034026465028357e-05,\n",
      "      \"loss\": 0.0174,\n",
      "      \"step\": 63400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9962506695232993,\n",
      "      \"grad_norm\": 0.01105298288166523,\n",
      "      \"learning_rate\": 3.0027725267800883e-05,\n",
      "      \"loss\": 0.0397,\n",
      "      \"step\": 63420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9975109486751315,\n",
      "      \"grad_norm\": 5.70705509185791,\n",
      "      \"learning_rate\": 3.0021424070573412e-05,\n",
      "      \"loss\": 0.0506,\n",
      "      \"step\": 63440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9987712278269636,\n",
      "      \"grad_norm\": 0.1340138167142868,\n",
      "      \"learning_rate\": 3.0015122873345934e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 63460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.0,\n",
      "      \"grad_norm\": 0.0075993966311216354,\n",
      "      \"learning_rate\": 3.0008821676118463e-05,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 63480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.0,\n",
      "      \"eval_accuracy\": 0.9991808959737887,\n",
      "      \"eval_f1\": 0.9991813602015114,\n",
      "      \"eval_loss\": 0.003295167116448283,\n",
      "      \"eval_precision\": 0.998615394297942,\n",
      "      \"eval_recall\": 0.999747967991935,\n",
      "      \"eval_runtime\": 549.4621,\n",
      "      \"eval_samples_per_second\": 57.769,\n",
      "      \"eval_steps_per_second\": 7.222,\n",
      "      \"step\": 63480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.001260279151832,\n",
      "      \"grad_norm\": 0.02563372068107128,\n",
      "      \"learning_rate\": 3.000252047889099e-05,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 63500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.002520558303664,\n",
      "      \"grad_norm\": 0.009651023894548416,\n",
      "      \"learning_rate\": 2.9996219281663518e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 63520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.003780837455496,\n",
      "      \"grad_norm\": 0.03612048178911209,\n",
      "      \"learning_rate\": 2.998991808443604e-05,\n",
      "      \"loss\": 0.0216,\n",
      "      \"step\": 63540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.0050411166073285,\n",
      "      \"grad_norm\": 0.033398158848285675,\n",
      "      \"learning_rate\": 2.998361688720857e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 63560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.00630139575916,\n",
      "      \"grad_norm\": 0.27953922748565674,\n",
      "      \"learning_rate\": 2.9977315689981102e-05,\n",
      "      \"loss\": 0.0218,\n",
      "      \"step\": 63580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.007561674910993,\n",
      "      \"grad_norm\": 0.4775964617729187,\n",
      "      \"learning_rate\": 2.9971014492753624e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 63600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.0088219540628245,\n",
      "      \"grad_norm\": 0.02049681358039379,\n",
      "      \"learning_rate\": 2.9964713295526153e-05,\n",
      "      \"loss\": 0.0165,\n",
      "      \"step\": 63620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.010082233214657,\n",
      "      \"grad_norm\": 0.04405921325087547,\n",
      "      \"learning_rate\": 2.9958412098298675e-05,\n",
      "      \"loss\": 0.0248,\n",
      "      \"step\": 63640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.011342512366489,\n",
      "      \"grad_norm\": 8.546849250793457,\n",
      "      \"learning_rate\": 2.9952110901071208e-05,\n",
      "      \"loss\": 0.0276,\n",
      "      \"step\": 63660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.012602791518321,\n",
      "      \"grad_norm\": 0.021918099373579025,\n",
      "      \"learning_rate\": 2.994580970384373e-05,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 63680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.013863070670153,\n",
      "      \"grad_norm\": 0.019206400960683823,\n",
      "      \"learning_rate\": 2.993950850661626e-05,\n",
      "      \"loss\": 0.0353,\n",
      "      \"step\": 63700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.015123349821986,\n",
      "      \"grad_norm\": 0.10052139312028885,\n",
      "      \"learning_rate\": 2.9933207309388785e-05,\n",
      "      \"loss\": 0.0216,\n",
      "      \"step\": 63720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.016383628973817,\n",
      "      \"grad_norm\": 0.1815146803855896,\n",
      "      \"learning_rate\": 2.9926906112161314e-05,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 63740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.01764390812565,\n",
      "      \"grad_norm\": 0.4873565137386322,\n",
      "      \"learning_rate\": 2.9920604914933836e-05,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 63760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.0189041872774816,\n",
      "      \"grad_norm\": 0.0030318424105644226,\n",
      "      \"learning_rate\": 2.9914303717706365e-05,\n",
      "      \"loss\": 0.0372,\n",
      "      \"step\": 63780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.020164466429314,\n",
      "      \"grad_norm\": 0.002901832340285182,\n",
      "      \"learning_rate\": 2.9908002520478894e-05,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 63800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.021424745581146,\n",
      "      \"grad_norm\": 0.001588096609339118,\n",
      "      \"learning_rate\": 2.990170132325142e-05,\n",
      "      \"loss\": 0.0105,\n",
      "      \"step\": 63820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.022685024732978,\n",
      "      \"grad_norm\": 0.0007801471510902047,\n",
      "      \"learning_rate\": 2.989540012602395e-05,\n",
      "      \"loss\": 0.0131,\n",
      "      \"step\": 63840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.02394530388481,\n",
      "      \"grad_norm\": 0.000495042244438082,\n",
      "      \"learning_rate\": 2.988909892879647e-05,\n",
      "      \"loss\": 0.0328,\n",
      "      \"step\": 63860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.025205583036643,\n",
      "      \"grad_norm\": 0.0028577831108123064,\n",
      "      \"learning_rate\": 2.9882797731569e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 63880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.026465862188474,\n",
      "      \"grad_norm\": 0.015101773664355278,\n",
      "      \"learning_rate\": 2.9876496534341526e-05,\n",
      "      \"loss\": 0.0385,\n",
      "      \"step\": 63900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.027726141340307,\n",
      "      \"grad_norm\": 0.001624516909942031,\n",
      "      \"learning_rate\": 2.9870195337114055e-05,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 63920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.028986420492139,\n",
      "      \"grad_norm\": 0.020428797230124474,\n",
      "      \"learning_rate\": 2.9863894139886577e-05,\n",
      "      \"loss\": 0.0324,\n",
      "      \"step\": 63940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.030246699643971,\n",
      "      \"grad_norm\": 0.032988715916872025,\n",
      "      \"learning_rate\": 2.9857592942659106e-05,\n",
      "      \"loss\": 0.0213,\n",
      "      \"step\": 63960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.031506978795803,\n",
      "      \"grad_norm\": 0.0029852152802050114,\n",
      "      \"learning_rate\": 2.9851291745431632e-05,\n",
      "      \"loss\": 0.0289,\n",
      "      \"step\": 63980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.0327672579476355,\n",
      "      \"grad_norm\": 0.009117010980844498,\n",
      "      \"learning_rate\": 2.984499054820416e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 64000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.034027537099467,\n",
      "      \"grad_norm\": 0.0030609779059886932,\n",
      "      \"learning_rate\": 2.983868935097669e-05,\n",
      "      \"loss\": 0.0176,\n",
      "      \"step\": 64020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.0352878162513,\n",
      "      \"grad_norm\": 9.247361183166504,\n",
      "      \"learning_rate\": 2.9832388153749212e-05,\n",
      "      \"loss\": 0.0804,\n",
      "      \"step\": 64040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.036548095403131,\n",
      "      \"grad_norm\": 0.3632183074951172,\n",
      "      \"learning_rate\": 2.982608695652174e-05,\n",
      "      \"loss\": 0.0592,\n",
      "      \"step\": 64060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.037808374554964,\n",
      "      \"grad_norm\": 0.11496421694755554,\n",
      "      \"learning_rate\": 2.9819785759294267e-05,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 64080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.039068653706796,\n",
      "      \"grad_norm\": 0.2157420665025711,\n",
      "      \"learning_rate\": 2.9813484562066796e-05,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 64100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.040328932858628,\n",
      "      \"grad_norm\": 1.9653476476669312,\n",
      "      \"learning_rate\": 2.9807183364839318e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 64120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.04158921201046,\n",
      "      \"grad_norm\": 0.011513293720781803,\n",
      "      \"learning_rate\": 2.9800882167611847e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 64140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.0428494911622925,\n",
      "      \"grad_norm\": 6.877037525177002,\n",
      "      \"learning_rate\": 2.9794580970384373e-05,\n",
      "      \"loss\": 0.0332,\n",
      "      \"step\": 64160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.044109770314124,\n",
      "      \"grad_norm\": 0.00789628829807043,\n",
      "      \"learning_rate\": 2.9788279773156902e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 64180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.045370049465957,\n",
      "      \"grad_norm\": 0.7183530926704407,\n",
      "      \"learning_rate\": 2.9781978575929424e-05,\n",
      "      \"loss\": 0.0208,\n",
      "      \"step\": 64200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.0466303286177885,\n",
      "      \"grad_norm\": 0.10484565049409866,\n",
      "      \"learning_rate\": 2.9775677378701957e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 64220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.047890607769621,\n",
      "      \"grad_norm\": 13.602718353271484,\n",
      "      \"learning_rate\": 2.9769376181474486e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 64240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.049150886921453,\n",
      "      \"grad_norm\": 0.05345659703016281,\n",
      "      \"learning_rate\": 2.9763074984247008e-05,\n",
      "      \"loss\": 0.0552,\n",
      "      \"step\": 64260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.050411166073285,\n",
      "      \"grad_norm\": 0.004640619736164808,\n",
      "      \"learning_rate\": 2.9756773787019537e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 64280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.051671445225117,\n",
      "      \"grad_norm\": 0.0041552940383553505,\n",
      "      \"learning_rate\": 2.9750472589792063e-05,\n",
      "      \"loss\": 0.0283,\n",
      "      \"step\": 64300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.05293172437695,\n",
      "      \"grad_norm\": 3.896583080291748,\n",
      "      \"learning_rate\": 2.974417139256459e-05,\n",
      "      \"loss\": 0.0516,\n",
      "      \"step\": 64320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.054192003528781,\n",
      "      \"grad_norm\": 0.0032967731822282076,\n",
      "      \"learning_rate\": 2.9737870195337114e-05,\n",
      "      \"loss\": 0.0069,\n",
      "      \"step\": 64340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.055452282680614,\n",
      "      \"grad_norm\": 10.12458610534668,\n",
      "      \"learning_rate\": 2.9731568998109643e-05,\n",
      "      \"loss\": 0.0289,\n",
      "      \"step\": 64360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.0567125618324456,\n",
      "      \"grad_norm\": 0.664381206035614,\n",
      "      \"learning_rate\": 2.972526780088217e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 64380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.057972840984278,\n",
      "      \"grad_norm\": 4.072511672973633,\n",
      "      \"learning_rate\": 2.9718966603654698e-05,\n",
      "      \"loss\": 0.0612,\n",
      "      \"step\": 64400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.05923312013611,\n",
      "      \"grad_norm\": 13.972902297973633,\n",
      "      \"learning_rate\": 2.971266540642722e-05,\n",
      "      \"loss\": 0.0293,\n",
      "      \"step\": 64420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.060493399287942,\n",
      "      \"grad_norm\": 0.028754033148288727,\n",
      "      \"learning_rate\": 2.970636420919975e-05,\n",
      "      \"loss\": 0.0311,\n",
      "      \"step\": 64440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.061753678439774,\n",
      "      \"grad_norm\": 0.04766562953591347,\n",
      "      \"learning_rate\": 2.9700063011972278e-05,\n",
      "      \"loss\": 0.0344,\n",
      "      \"step\": 64460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.063013957591607,\n",
      "      \"grad_norm\": 0.11947529762983322,\n",
      "      \"learning_rate\": 2.9693761814744804e-05,\n",
      "      \"loss\": 0.0042,\n",
      "      \"step\": 64480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.064274236743438,\n",
      "      \"grad_norm\": 0.005678215995430946,\n",
      "      \"learning_rate\": 2.9687460617517333e-05,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 64500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.065534515895271,\n",
      "      \"grad_norm\": 0.11663603782653809,\n",
      "      \"learning_rate\": 2.9681159420289855e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 64520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.066794795047103,\n",
      "      \"grad_norm\": 0.0012845186283811927,\n",
      "      \"learning_rate\": 2.9674858223062384e-05,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 64540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.068055074198935,\n",
      "      \"grad_norm\": 0.0009206694085150957,\n",
      "      \"learning_rate\": 2.966855702583491e-05,\n",
      "      \"loss\": 0.0317,\n",
      "      \"step\": 64560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.069315353350767,\n",
      "      \"grad_norm\": 0.8471776843070984,\n",
      "      \"learning_rate\": 2.966225582860744e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 64580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.0705756325025995,\n",
      "      \"grad_norm\": 0.021177299320697784,\n",
      "      \"learning_rate\": 2.965595463137996e-05,\n",
      "      \"loss\": 0.0371,\n",
      "      \"step\": 64600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.071835911654431,\n",
      "      \"grad_norm\": 0.0141758406534791,\n",
      "      \"learning_rate\": 2.964965343415249e-05,\n",
      "      \"loss\": 0.0136,\n",
      "      \"step\": 64620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.073096190806264,\n",
      "      \"grad_norm\": 0.015928035601973534,\n",
      "      \"learning_rate\": 2.9643352236925016e-05,\n",
      "      \"loss\": 0.0076,\n",
      "      \"step\": 64640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.074356469958095,\n",
      "      \"grad_norm\": 0.006542056333273649,\n",
      "      \"learning_rate\": 2.9637051039697545e-05,\n",
      "      \"loss\": 0.0371,\n",
      "      \"step\": 64660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.075616749109928,\n",
      "      \"grad_norm\": 0.0008374507306143641,\n",
      "      \"learning_rate\": 2.9630749842470067e-05,\n",
      "      \"loss\": 0.0099,\n",
      "      \"step\": 64680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.07687702826176,\n",
      "      \"grad_norm\": 0.041173577308654785,\n",
      "      \"learning_rate\": 2.9624448645242596e-05,\n",
      "      \"loss\": 0.0041,\n",
      "      \"step\": 64700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.078137307413592,\n",
      "      \"grad_norm\": 0.14849598705768585,\n",
      "      \"learning_rate\": 2.9618147448015125e-05,\n",
      "      \"loss\": 0.0579,\n",
      "      \"step\": 64720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.079397586565424,\n",
      "      \"grad_norm\": 1.6285316944122314,\n",
      "      \"learning_rate\": 2.961184625078765e-05,\n",
      "      \"loss\": 0.0105,\n",
      "      \"step\": 64740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.0806578657172565,\n",
      "      \"grad_norm\": 0.0022999781649559736,\n",
      "      \"learning_rate\": 2.960554505356018e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 64760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.081918144869088,\n",
      "      \"grad_norm\": 0.001931333914399147,\n",
      "      \"learning_rate\": 2.9599243856332702e-05,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 64780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.083178424020921,\n",
      "      \"grad_norm\": 0.0017364353407174349,\n",
      "      \"learning_rate\": 2.9592942659105234e-05,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 64800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.0844387031727525,\n",
      "      \"grad_norm\": 0.032522015273571014,\n",
      "      \"learning_rate\": 2.9586641461877757e-05,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 64820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.085698982324585,\n",
      "      \"grad_norm\": 0.0005744901136495173,\n",
      "      \"learning_rate\": 2.9580340264650286e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 64840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.086959261476417,\n",
      "      \"grad_norm\": 0.0652061402797699,\n",
      "      \"learning_rate\": 2.9574039067422808e-05,\n",
      "      \"loss\": 0.0241,\n",
      "      \"step\": 64860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.088219540628249,\n",
      "      \"grad_norm\": 0.06455755233764648,\n",
      "      \"learning_rate\": 2.956773787019534e-05,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 64880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.089479819780081,\n",
      "      \"grad_norm\": 0.00996157992631197,\n",
      "      \"learning_rate\": 2.9561436672967863e-05,\n",
      "      \"loss\": 0.0221,\n",
      "      \"step\": 64900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.090740098931914,\n",
      "      \"grad_norm\": 0.0006031015072949231,\n",
      "      \"learning_rate\": 2.9555135475740392e-05,\n",
      "      \"loss\": 0.0297,\n",
      "      \"step\": 64920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.092000378083745,\n",
      "      \"grad_norm\": 0.0003875716938637197,\n",
      "      \"learning_rate\": 2.954883427851292e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 64940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.093260657235578,\n",
      "      \"grad_norm\": 0.0165080688893795,\n",
      "      \"learning_rate\": 2.9542533081285446e-05,\n",
      "      \"loss\": 0.0277,\n",
      "      \"step\": 64960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.0945209363874095,\n",
      "      \"grad_norm\": 0.030257532373070717,\n",
      "      \"learning_rate\": 2.9536231884057975e-05,\n",
      "      \"loss\": 0.0501,\n",
      "      \"step\": 64980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.095781215539242,\n",
      "      \"grad_norm\": 0.028217796236276627,\n",
      "      \"learning_rate\": 2.9529930686830498e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 65000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.097041494691074,\n",
      "      \"grad_norm\": 0.0067641823552548885,\n",
      "      \"learning_rate\": 2.9523629489603027e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 65020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.098301773842906,\n",
      "      \"grad_norm\": 0.02433108352124691,\n",
      "      \"learning_rate\": 2.9517328292375552e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 65040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.099562052994738,\n",
      "      \"grad_norm\": 0.001528608612716198,\n",
      "      \"learning_rate\": 2.951102709514808e-05,\n",
      "      \"loss\": 0.0171,\n",
      "      \"step\": 65060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.100822332146571,\n",
      "      \"grad_norm\": 0.000214795843930915,\n",
      "      \"learning_rate\": 2.9504725897920604e-05,\n",
      "      \"loss\": 0.0166,\n",
      "      \"step\": 65080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.102082611298402,\n",
      "      \"grad_norm\": 0.011913636699318886,\n",
      "      \"learning_rate\": 2.9498424700693133e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 65100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.103342890450235,\n",
      "      \"grad_norm\": 0.00678630918264389,\n",
      "      \"learning_rate\": 2.949212350346566e-05,\n",
      "      \"loss\": 0.0275,\n",
      "      \"step\": 65120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.104603169602067,\n",
      "      \"grad_norm\": 4.717810153961182,\n",
      "      \"learning_rate\": 2.9485822306238187e-05,\n",
      "      \"loss\": 0.037,\n",
      "      \"step\": 65140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.105863448753899,\n",
      "      \"grad_norm\": 0.003972586710005999,\n",
      "      \"learning_rate\": 2.9479521109010716e-05,\n",
      "      \"loss\": 0.0435,\n",
      "      \"step\": 65160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.107123727905731,\n",
      "      \"grad_norm\": 0.0018920234870165586,\n",
      "      \"learning_rate\": 2.947321991178324e-05,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 65180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.1083840070575635,\n",
      "      \"grad_norm\": 0.02884525991976261,\n",
      "      \"learning_rate\": 2.9466918714555768e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 65200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.109644286209395,\n",
      "      \"grad_norm\": 0.012824948877096176,\n",
      "      \"learning_rate\": 2.9460617517328293e-05,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 65220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.110904565361228,\n",
      "      \"grad_norm\": 0.00030174179119057953,\n",
      "      \"learning_rate\": 2.9454316320100822e-05,\n",
      "      \"loss\": 0.038,\n",
      "      \"step\": 65240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.112164844513059,\n",
      "      \"grad_norm\": 13.254246711730957,\n",
      "      \"learning_rate\": 2.9448015122873345e-05,\n",
      "      \"loss\": 0.0122,\n",
      "      \"step\": 65260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.113425123664892,\n",
      "      \"grad_norm\": 0.02633495256304741,\n",
      "      \"learning_rate\": 2.9441713925645874e-05,\n",
      "      \"loss\": 0.0424,\n",
      "      \"step\": 65280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.114685402816724,\n",
      "      \"grad_norm\": 0.0430520735681057,\n",
      "      \"learning_rate\": 2.94354127284184e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 65300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.115945681968556,\n",
      "      \"grad_norm\": 0.05273105204105377,\n",
      "      \"learning_rate\": 2.942911153119093e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 65320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.117205961120388,\n",
      "      \"grad_norm\": 0.09827153384685516,\n",
      "      \"learning_rate\": 2.942281033396345e-05,\n",
      "      \"loss\": 0.0189,\n",
      "      \"step\": 65340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.1184662402722205,\n",
      "      \"grad_norm\": 0.003006787970662117,\n",
      "      \"learning_rate\": 2.941650913673598e-05,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 65360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.119726519424052,\n",
      "      \"grad_norm\": 0.0017224712064489722,\n",
      "      \"learning_rate\": 2.9410207939508512e-05,\n",
      "      \"loss\": 0.0204,\n",
      "      \"step\": 65380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.120986798575885,\n",
      "      \"grad_norm\": 0.03280024603009224,\n",
      "      \"learning_rate\": 2.9403906742281034e-05,\n",
      "      \"loss\": 0.0273,\n",
      "      \"step\": 65400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.1222470777277165,\n",
      "      \"grad_norm\": 0.003750657197088003,\n",
      "      \"learning_rate\": 2.9397605545053563e-05,\n",
      "      \"loss\": 0.0077,\n",
      "      \"step\": 65420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.123507356879549,\n",
      "      \"grad_norm\": 0.004387788008898497,\n",
      "      \"learning_rate\": 2.939161940768746e-05,\n",
      "      \"loss\": 0.0323,\n",
      "      \"step\": 65440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.124767636031381,\n",
      "      \"grad_norm\": 0.01454863604158163,\n",
      "      \"learning_rate\": 2.938531821045999e-05,\n",
      "      \"loss\": 0.038,\n",
      "      \"step\": 65460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.126027915183213,\n",
      "      \"grad_norm\": 0.009301930665969849,\n",
      "      \"learning_rate\": 2.937901701323252e-05,\n",
      "      \"loss\": 0.0105,\n",
      "      \"step\": 65480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.127288194335045,\n",
      "      \"grad_norm\": 0.020590106025338173,\n",
      "      \"learning_rate\": 2.937271581600504e-05,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 65500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.128548473486878,\n",
      "      \"grad_norm\": 0.4349847733974457,\n",
      "      \"learning_rate\": 2.936641461877757e-05,\n",
      "      \"loss\": 0.0198,\n",
      "      \"step\": 65520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.129808752638709,\n",
      "      \"grad_norm\": 0.014598545618355274,\n",
      "      \"learning_rate\": 2.9360113421550096e-05,\n",
      "      \"loss\": 0.021,\n",
      "      \"step\": 65540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.131069031790542,\n",
      "      \"grad_norm\": 0.1474381685256958,\n",
      "      \"learning_rate\": 2.9353812224322625e-05,\n",
      "      \"loss\": 0.0331,\n",
      "      \"step\": 65560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.1323293109423735,\n",
      "      \"grad_norm\": 0.17139604687690735,\n",
      "      \"learning_rate\": 2.9347511027095147e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 65580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.133589590094206,\n",
      "      \"grad_norm\": 0.005975733511149883,\n",
      "      \"learning_rate\": 2.9341209829867676e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 65600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.134849869246038,\n",
      "      \"grad_norm\": 0.004578210413455963,\n",
      "      \"learning_rate\": 2.9334908632640202e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 65620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.13611014839787,\n",
      "      \"grad_norm\": 0.3926296532154083,\n",
      "      \"learning_rate\": 2.932860743541273e-05,\n",
      "      \"loss\": 0.036,\n",
      "      \"step\": 65640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.137370427549702,\n",
      "      \"grad_norm\": 0.0020910045132040977,\n",
      "      \"learning_rate\": 2.9322306238185253e-05,\n",
      "      \"loss\": 0.0238,\n",
      "      \"step\": 65660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.138630706701535,\n",
      "      \"grad_norm\": 0.016330575570464134,\n",
      "      \"learning_rate\": 2.9316005040957782e-05,\n",
      "      \"loss\": 0.0197,\n",
      "      \"step\": 65680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.139890985853366,\n",
      "      \"grad_norm\": 0.005154689773917198,\n",
      "      \"learning_rate\": 2.930970384373031e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 65700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.141151265005199,\n",
      "      \"grad_norm\": 0.006226749625056982,\n",
      "      \"learning_rate\": 2.9303402646502837e-05,\n",
      "      \"loss\": 0.0289,\n",
      "      \"step\": 65720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.142411544157031,\n",
      "      \"grad_norm\": 0.003916519228368998,\n",
      "      \"learning_rate\": 2.9297101449275366e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 65740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.143671823308863,\n",
      "      \"grad_norm\": 0.04312219098210335,\n",
      "      \"learning_rate\": 2.929080025204789e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 65760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.144932102460695,\n",
      "      \"grad_norm\": 0.0026621138677001,\n",
      "      \"learning_rate\": 2.9284499054820417e-05,\n",
      "      \"loss\": 0.0276,\n",
      "      \"step\": 65780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.1461923816125275,\n",
      "      \"grad_norm\": 0.0030124711338430643,\n",
      "      \"learning_rate\": 2.9278197857592943e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 65800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.147452660764359,\n",
      "      \"grad_norm\": 0.0006803930155001581,\n",
      "      \"learning_rate\": 2.9271896660365472e-05,\n",
      "      \"loss\": 0.0332,\n",
      "      \"step\": 65820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.148712939916192,\n",
      "      \"grad_norm\": 0.7416566014289856,\n",
      "      \"learning_rate\": 2.9265595463137994e-05,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 65840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.149973219068023,\n",
      "      \"grad_norm\": 0.0048318710178136826,\n",
      "      \"learning_rate\": 2.9259294265910527e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 65860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.151233498219856,\n",
      "      \"grad_norm\": 0.00045407144352793694,\n",
      "      \"learning_rate\": 2.925299306868305e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 65880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.152493777371688,\n",
      "      \"grad_norm\": 0.0006200492498464882,\n",
      "      \"learning_rate\": 2.9246691871455578e-05,\n",
      "      \"loss\": 0.0419,\n",
      "      \"step\": 65900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.15375405652352,\n",
      "      \"grad_norm\": 0.001567416125908494,\n",
      "      \"learning_rate\": 2.92403906742281e-05,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 65920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.155014335675352,\n",
      "      \"grad_norm\": 0.044816844165325165,\n",
      "      \"learning_rate\": 2.9234089477000633e-05,\n",
      "      \"loss\": 0.0635,\n",
      "      \"step\": 65940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.1562746148271845,\n",
      "      \"grad_norm\": 0.04424513503909111,\n",
      "      \"learning_rate\": 2.9227788279773162e-05,\n",
      "      \"loss\": 0.0531,\n",
      "      \"step\": 65960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.157534893979016,\n",
      "      \"grad_norm\": 0.32895538210868835,\n",
      "      \"learning_rate\": 2.9221487082545684e-05,\n",
      "      \"loss\": 0.0439,\n",
      "      \"step\": 65980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.158795173130849,\n",
      "      \"grad_norm\": 0.14059394598007202,\n",
      "      \"learning_rate\": 2.9215185885318213e-05,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 66000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.1600554522826805,\n",
      "      \"grad_norm\": 0.023860717192292213,\n",
      "      \"learning_rate\": 2.920888468809074e-05,\n",
      "      \"loss\": 0.0391,\n",
      "      \"step\": 66020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.161315731434513,\n",
      "      \"grad_norm\": 0.06890442967414856,\n",
      "      \"learning_rate\": 2.9202583490863268e-05,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 66040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.162576010586345,\n",
      "      \"grad_norm\": 0.0305822491645813,\n",
      "      \"learning_rate\": 2.919628229363579e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 66060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.163836289738177,\n",
      "      \"grad_norm\": 0.13652046024799347,\n",
      "      \"learning_rate\": 2.918998109640832e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 66080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.165096568890009,\n",
      "      \"grad_norm\": 0.0032481562811881304,\n",
      "      \"learning_rate\": 2.9183679899180845e-05,\n",
      "      \"loss\": 0.0366,\n",
      "      \"step\": 66100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.166356848041842,\n",
      "      \"grad_norm\": 0.006345316767692566,\n",
      "      \"learning_rate\": 2.9177378701953374e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 66120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.167617127193673,\n",
      "      \"grad_norm\": 11.896842956542969,\n",
      "      \"learning_rate\": 2.9171077504725896e-05,\n",
      "      \"loss\": 0.005,\n",
      "      \"step\": 66140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.168877406345506,\n",
      "      \"grad_norm\": 12.098638534545898,\n",
      "      \"learning_rate\": 2.9164776307498425e-05,\n",
      "      \"loss\": 0.0163,\n",
      "      \"step\": 66160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.1701376854973375,\n",
      "      \"grad_norm\": 11.339912414550781,\n",
      "      \"learning_rate\": 2.9158475110270954e-05,\n",
      "      \"loss\": 0.0592,\n",
      "      \"step\": 66180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.17139796464917,\n",
      "      \"grad_norm\": 0.0037441356107592583,\n",
      "      \"learning_rate\": 2.915217391304348e-05,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 66200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.172658243801002,\n",
      "      \"grad_norm\": 0.002985000843182206,\n",
      "      \"learning_rate\": 2.914587271581601e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 66220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.173918522952834,\n",
      "      \"grad_norm\": 0.05026256665587425,\n",
      "      \"learning_rate\": 2.913957151858853e-05,\n",
      "      \"loss\": 0.0191,\n",
      "      \"step\": 66240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.175178802104666,\n",
      "      \"grad_norm\": 0.003713351907208562,\n",
      "      \"learning_rate\": 2.913327032136106e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 66260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.176439081256499,\n",
      "      \"grad_norm\": 4.677281379699707,\n",
      "      \"learning_rate\": 2.9126969124133586e-05,\n",
      "      \"loss\": 0.0277,\n",
      "      \"step\": 66280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.17769936040833,\n",
      "      \"grad_norm\": 0.01245932187885046,\n",
      "      \"learning_rate\": 2.9120667926906115e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 66300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.178959639560163,\n",
      "      \"grad_norm\": 0.001702734618447721,\n",
      "      \"learning_rate\": 2.9114366729678637e-05,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 66320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.180219918711995,\n",
      "      \"grad_norm\": 0.0006615867023356259,\n",
      "      \"learning_rate\": 2.9108065532451166e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 66340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.181480197863827,\n",
      "      \"grad_norm\": 0.3367895483970642,\n",
      "      \"learning_rate\": 2.9101764335223692e-05,\n",
      "      \"loss\": 0.0453,\n",
      "      \"step\": 66360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.182740477015659,\n",
      "      \"grad_norm\": 0.002999373944476247,\n",
      "      \"learning_rate\": 2.909546313799622e-05,\n",
      "      \"loss\": 0.0389,\n",
      "      \"step\": 66380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.1840007561674915,\n",
      "      \"grad_norm\": 0.19252686202526093,\n",
      "      \"learning_rate\": 2.908916194076875e-05,\n",
      "      \"loss\": 0.0251,\n",
      "      \"step\": 66400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.185261035319323,\n",
      "      \"grad_norm\": 0.0690000057220459,\n",
      "      \"learning_rate\": 2.9082860743541272e-05,\n",
      "      \"loss\": 0.0601,\n",
      "      \"step\": 66420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.186521314471156,\n",
      "      \"grad_norm\": 0.8741313219070435,\n",
      "      \"learning_rate\": 2.9076559546313805e-05,\n",
      "      \"loss\": 0.0027,\n",
      "      \"step\": 66440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.187781593622987,\n",
      "      \"grad_norm\": 0.2565027177333832,\n",
      "      \"learning_rate\": 2.9070258349086327e-05,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 66460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.18904187277482,\n",
      "      \"grad_norm\": 4.967301845550537,\n",
      "      \"learning_rate\": 2.9063957151858856e-05,\n",
      "      \"loss\": 0.007,\n",
      "      \"step\": 66480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.190302151926652,\n",
      "      \"grad_norm\": 0.010878762230277061,\n",
      "      \"learning_rate\": 2.905765595463138e-05,\n",
      "      \"loss\": 0.0283,\n",
      "      \"step\": 66500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.191562431078484,\n",
      "      \"grad_norm\": 0.009859270416200161,\n",
      "      \"learning_rate\": 2.905135475740391e-05,\n",
      "      \"loss\": 0.0279,\n",
      "      \"step\": 66520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.192822710230316,\n",
      "      \"grad_norm\": 0.06624649465084076,\n",
      "      \"learning_rate\": 2.9045053560176433e-05,\n",
      "      \"loss\": 0.0287,\n",
      "      \"step\": 66540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.1940829893821485,\n",
      "      \"grad_norm\": 0.010137151926755905,\n",
      "      \"learning_rate\": 2.9038752362948962e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 66560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.19534326853398,\n",
      "      \"grad_norm\": 0.01374115888029337,\n",
      "      \"learning_rate\": 2.9032451165721487e-05,\n",
      "      \"loss\": 0.0374,\n",
      "      \"step\": 66580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.196603547685813,\n",
      "      \"grad_norm\": 0.16722343862056732,\n",
      "      \"learning_rate\": 2.9026149968494017e-05,\n",
      "      \"loss\": 0.0066,\n",
      "      \"step\": 66600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.1978638268376445,\n",
      "      \"grad_norm\": 0.0010439460165798664,\n",
      "      \"learning_rate\": 2.9019848771266546e-05,\n",
      "      \"loss\": 0.0251,\n",
      "      \"step\": 66620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.199124105989477,\n",
      "      \"grad_norm\": 0.001387096126563847,\n",
      "      \"learning_rate\": 2.9013547574039068e-05,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 66640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.200384385141309,\n",
      "      \"grad_norm\": 0.008276323787868023,\n",
      "      \"learning_rate\": 2.9007246376811597e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 66660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.201644664293141,\n",
      "      \"grad_norm\": 0.0009101714240387082,\n",
      "      \"learning_rate\": 2.9000945179584123e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 66680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.202904943444973,\n",
      "      \"grad_norm\": 0.0042539676651358604,\n",
      "      \"learning_rate\": 2.899464398235665e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 66700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.204165222596806,\n",
      "      \"grad_norm\": 0.0015723016113042831,\n",
      "      \"learning_rate\": 2.8988342785129174e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 66720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.205425501748637,\n",
      "      \"grad_norm\": 0.0005357885966077447,\n",
      "      \"learning_rate\": 2.8982041587901703e-05,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 66740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.20668578090047,\n",
      "      \"grad_norm\": 0.0017604450695216656,\n",
      "      \"learning_rate\": 2.897574039067423e-05,\n",
      "      \"loss\": 0.0611,\n",
      "      \"step\": 66760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.2079460600523015,\n",
      "      \"grad_norm\": 0.025027820840477943,\n",
      "      \"learning_rate\": 2.8969439193446758e-05,\n",
      "      \"loss\": 0.0293,\n",
      "      \"step\": 66780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.209206339204134,\n",
      "      \"grad_norm\": 0.002608735579997301,\n",
      "      \"learning_rate\": 2.896313799621928e-05,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 66800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.210466618355966,\n",
      "      \"grad_norm\": 0.011896241456270218,\n",
      "      \"learning_rate\": 2.895683679899181e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 66820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.211726897507798,\n",
      "      \"grad_norm\": 0.019556351006031036,\n",
      "      \"learning_rate\": 2.8950535601764338e-05,\n",
      "      \"loss\": 0.0162,\n",
      "      \"step\": 66840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.21298717665963,\n",
      "      \"grad_norm\": 0.0036251917481422424,\n",
      "      \"learning_rate\": 2.8944234404536864e-05,\n",
      "      \"loss\": 0.023,\n",
      "      \"step\": 66860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.214247455811463,\n",
      "      \"grad_norm\": 0.12444943189620972,\n",
      "      \"learning_rate\": 2.8937933207309393e-05,\n",
      "      \"loss\": 0.022,\n",
      "      \"step\": 66880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.215507734963294,\n",
      "      \"grad_norm\": 0.061899181455373764,\n",
      "      \"learning_rate\": 2.8931632010081915e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 66900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.216768014115127,\n",
      "      \"grad_norm\": 0.21199388802051544,\n",
      "      \"learning_rate\": 2.8925330812854444e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 66920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.218028293266959,\n",
      "      \"grad_norm\": 0.010395259596407413,\n",
      "      \"learning_rate\": 2.891902961562697e-05,\n",
      "      \"loss\": 0.0332,\n",
      "      \"step\": 66940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.219288572418791,\n",
      "      \"grad_norm\": 0.002929506590589881,\n",
      "      \"learning_rate\": 2.89127284183995e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 66960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.220548851570623,\n",
      "      \"grad_norm\": 0.006412523798644543,\n",
      "      \"learning_rate\": 2.890642722117202e-05,\n",
      "      \"loss\": 0.0311,\n",
      "      \"step\": 66980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.2218091307224554,\n",
      "      \"grad_norm\": 0.0020506810396909714,\n",
      "      \"learning_rate\": 2.890012602394455e-05,\n",
      "      \"loss\": 0.0055,\n",
      "      \"step\": 67000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.223069409874287,\n",
      "      \"grad_norm\": 0.006209480110555887,\n",
      "      \"learning_rate\": 2.8893824826717076e-05,\n",
      "      \"loss\": 0.0351,\n",
      "      \"step\": 67020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.224329689026119,\n",
      "      \"grad_norm\": 0.01895073615014553,\n",
      "      \"learning_rate\": 2.8887523629489605e-05,\n",
      "      \"loss\": 0.0232,\n",
      "      \"step\": 67040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.225589968177951,\n",
      "      \"grad_norm\": 0.0004474177840165794,\n",
      "      \"learning_rate\": 2.8881222432262134e-05,\n",
      "      \"loss\": 0.0311,\n",
      "      \"step\": 67060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.226850247329784,\n",
      "      \"grad_norm\": 0.0075683388859033585,\n",
      "      \"learning_rate\": 2.887492123503466e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 67080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.228110526481616,\n",
      "      \"grad_norm\": 0.0008966427412815392,\n",
      "      \"learning_rate\": 2.8868620037807188e-05,\n",
      "      \"loss\": 0.0243,\n",
      "      \"step\": 67100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.229370805633447,\n",
      "      \"grad_norm\": 0.011331556364893913,\n",
      "      \"learning_rate\": 2.886231884057971e-05,\n",
      "      \"loss\": 0.0594,\n",
      "      \"step\": 67120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.23063108478528,\n",
      "      \"grad_norm\": 0.00201033940538764,\n",
      "      \"learning_rate\": 2.885601764335224e-05,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 67140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.2318913639371125,\n",
      "      \"grad_norm\": 0.021398287266492844,\n",
      "      \"learning_rate\": 2.8849716446124765e-05,\n",
      "      \"loss\": 0.0256,\n",
      "      \"step\": 67160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.233151643088944,\n",
      "      \"grad_norm\": 0.005563502665609121,\n",
      "      \"learning_rate\": 2.8843415248897294e-05,\n",
      "      \"loss\": 0.0162,\n",
      "      \"step\": 67180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.234411922240776,\n",
      "      \"grad_norm\": 0.010208351537585258,\n",
      "      \"learning_rate\": 2.8837114051669817e-05,\n",
      "      \"loss\": 0.0378,\n",
      "      \"step\": 67200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.2356722013926085,\n",
      "      \"grad_norm\": 0.009366758167743683,\n",
      "      \"learning_rate\": 2.8830812854442346e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 67220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.236932480544441,\n",
      "      \"grad_norm\": 0.021737869828939438,\n",
      "      \"learning_rate\": 2.882451165721487e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 67240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.238192759696273,\n",
      "      \"grad_norm\": 0.0009962664917111397,\n",
      "      \"learning_rate\": 2.88182104599874e-05,\n",
      "      \"loss\": 0.0066,\n",
      "      \"step\": 67260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.239453038848104,\n",
      "      \"grad_norm\": 12.328391075134277,\n",
      "      \"learning_rate\": 2.881190926275993e-05,\n",
      "      \"loss\": 0.0259,\n",
      "      \"step\": 67280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.240713317999937,\n",
      "      \"grad_norm\": 0.010694940574467182,\n",
      "      \"learning_rate\": 2.880560806553245e-05,\n",
      "      \"loss\": 0.0159,\n",
      "      \"step\": 67300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.241973597151769,\n",
      "      \"grad_norm\": 0.002079802332445979,\n",
      "      \"learning_rate\": 2.879930686830498e-05,\n",
      "      \"loss\": 0.0165,\n",
      "      \"step\": 67320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.243233876303601,\n",
      "      \"grad_norm\": 1.760719656944275,\n",
      "      \"learning_rate\": 2.8793005671077506e-05,\n",
      "      \"loss\": 0.0283,\n",
      "      \"step\": 67340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.244494155455433,\n",
      "      \"grad_norm\": 0.22180752456188202,\n",
      "      \"learning_rate\": 2.8786704473850035e-05,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 67360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.2457544346072655,\n",
      "      \"grad_norm\": 0.01294457633048296,\n",
      "      \"learning_rate\": 2.8780403276622558e-05,\n",
      "      \"loss\": 0.0239,\n",
      "      \"step\": 67380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.247014713759097,\n",
      "      \"grad_norm\": 0.040118757635354996,\n",
      "      \"learning_rate\": 2.8774102079395087e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 67400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.24827499291093,\n",
      "      \"grad_norm\": 0.06436333060264587,\n",
      "      \"learning_rate\": 2.8767800882167612e-05,\n",
      "      \"loss\": 0.0171,\n",
      "      \"step\": 67420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.2495352720627615,\n",
      "      \"grad_norm\": 0.02401023358106613,\n",
      "      \"learning_rate\": 2.876149968494014e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 67440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.250795551214594,\n",
      "      \"grad_norm\": 0.000513060949742794,\n",
      "      \"learning_rate\": 2.8755198487712664e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 67460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.252055830366426,\n",
      "      \"grad_norm\": 0.015544076450169086,\n",
      "      \"learning_rate\": 2.8748897290485193e-05,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 67480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.253316109518258,\n",
      "      \"grad_norm\": 0.003956913016736507,\n",
      "      \"learning_rate\": 2.8742596093257718e-05,\n",
      "      \"loss\": 0.025,\n",
      "      \"step\": 67500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.25457638867009,\n",
      "      \"grad_norm\": 0.09959399700164795,\n",
      "      \"learning_rate\": 2.8736294896030247e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 67520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.255836667821923,\n",
      "      \"grad_norm\": 0.06296668201684952,\n",
      "      \"learning_rate\": 2.8729993698802776e-05,\n",
      "      \"loss\": 0.0601,\n",
      "      \"step\": 67540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.257096946973754,\n",
      "      \"grad_norm\": 0.005286658648401499,\n",
      "      \"learning_rate\": 2.87236925015753e-05,\n",
      "      \"loss\": 0.0018,\n",
      "      \"step\": 67560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.258357226125587,\n",
      "      \"grad_norm\": 0.007979951798915863,\n",
      "      \"learning_rate\": 2.8717391304347828e-05,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 67580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.259617505277419,\n",
      "      \"grad_norm\": 0.3399410843849182,\n",
      "      \"learning_rate\": 2.8711090107120353e-05,\n",
      "      \"loss\": 0.0055,\n",
      "      \"step\": 67600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.260877784429251,\n",
      "      \"grad_norm\": 0.11382346600294113,\n",
      "      \"learning_rate\": 2.8704788909892882e-05,\n",
      "      \"loss\": 0.0169,\n",
      "      \"step\": 67620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.262138063581083,\n",
      "      \"grad_norm\": 0.00341944326646626,\n",
      "      \"learning_rate\": 2.8698487712665405e-05,\n",
      "      \"loss\": 0.0093,\n",
      "      \"step\": 67640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.263398342732915,\n",
      "      \"grad_norm\": 0.0003103717172052711,\n",
      "      \"learning_rate\": 2.8692186515437937e-05,\n",
      "      \"loss\": 0.0122,\n",
      "      \"step\": 67660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.264658621884747,\n",
      "      \"grad_norm\": 0.009452704340219498,\n",
      "      \"learning_rate\": 2.868588531821046e-05,\n",
      "      \"loss\": 0.03,\n",
      "      \"step\": 67680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.26591890103658,\n",
      "      \"grad_norm\": 6.448324680328369,\n",
      "      \"learning_rate\": 2.867958412098299e-05,\n",
      "      \"loss\": 0.0247,\n",
      "      \"step\": 67700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.267179180188411,\n",
      "      \"grad_norm\": 0.15531328320503235,\n",
      "      \"learning_rate\": 2.8673282923755514e-05,\n",
      "      \"loss\": 0.0469,\n",
      "      \"step\": 67720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.268439459340244,\n",
      "      \"grad_norm\": 0.14464643597602844,\n",
      "      \"learning_rate\": 2.8666981726528043e-05,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 67740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.269699738492076,\n",
      "      \"grad_norm\": 0.1432085484266281,\n",
      "      \"learning_rate\": 2.8660680529300572e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 67760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.270960017643908,\n",
      "      \"grad_norm\": 0.05059355869889259,\n",
      "      \"learning_rate\": 2.8654379332073094e-05,\n",
      "      \"loss\": 0.0522,\n",
      "      \"step\": 67780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.27222029679574,\n",
      "      \"grad_norm\": 0.002846337389200926,\n",
      "      \"learning_rate\": 2.8648078134845623e-05,\n",
      "      \"loss\": 0.0478,\n",
      "      \"step\": 67800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.2734805759475725,\n",
      "      \"grad_norm\": 0.14759695529937744,\n",
      "      \"learning_rate\": 2.864177693761815e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 67820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.274740855099404,\n",
      "      \"grad_norm\": 0.0009681474766694009,\n",
      "      \"learning_rate\": 2.8635475740390678e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 67840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.276001134251237,\n",
      "      \"grad_norm\": 0.07017491012811661,\n",
      "      \"learning_rate\": 2.86291745431632e-05,\n",
      "      \"loss\": 0.0222,\n",
      "      \"step\": 67860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.277261413403068,\n",
      "      \"grad_norm\": 0.14245596528053284,\n",
      "      \"learning_rate\": 2.862287334593573e-05,\n",
      "      \"loss\": 0.0263,\n",
      "      \"step\": 67880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.278521692554901,\n",
      "      \"grad_norm\": 0.009922277182340622,\n",
      "      \"learning_rate\": 2.8616572148708255e-05,\n",
      "      \"loss\": 0.0046,\n",
      "      \"step\": 67900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.279781971706733,\n",
      "      \"grad_norm\": 0.10841879993677139,\n",
      "      \"learning_rate\": 2.8610270951480784e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 67920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.281042250858565,\n",
      "      \"grad_norm\": 0.005155559629201889,\n",
      "      \"learning_rate\": 2.8603969754253306e-05,\n",
      "      \"loss\": 0.0354,\n",
      "      \"step\": 67940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.282302530010397,\n",
      "      \"grad_norm\": 0.01613745465874672,\n",
      "      \"learning_rate\": 2.8597668557025835e-05,\n",
      "      \"loss\": 0.0304,\n",
      "      \"step\": 67960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.2835628091622295,\n",
      "      \"grad_norm\": 0.0014987721806392074,\n",
      "      \"learning_rate\": 2.8591367359798364e-05,\n",
      "      \"loss\": 0.0201,\n",
      "      \"step\": 67980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.284823088314061,\n",
      "      \"grad_norm\": 0.000943048216868192,\n",
      "      \"learning_rate\": 2.858506616257089e-05,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 68000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.286083367465894,\n",
      "      \"grad_norm\": 0.0028785935137420893,\n",
      "      \"learning_rate\": 2.857876496534342e-05,\n",
      "      \"loss\": 0.0312,\n",
      "      \"step\": 68020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.2873436466177255,\n",
      "      \"grad_norm\": 0.0007691900827921927,\n",
      "      \"learning_rate\": 2.857246376811594e-05,\n",
      "      \"loss\": 0.0227,\n",
      "      \"step\": 68040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.288603925769558,\n",
      "      \"grad_norm\": 0.002741551958024502,\n",
      "      \"learning_rate\": 2.856616257088847e-05,\n",
      "      \"loss\": 0.0286,\n",
      "      \"step\": 68060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.28986420492139,\n",
      "      \"grad_norm\": 0.0033302009105682373,\n",
      "      \"learning_rate\": 2.8559861373660996e-05,\n",
      "      \"loss\": 0.0287,\n",
      "      \"step\": 68080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.291124484073222,\n",
      "      \"grad_norm\": 0.5562200546264648,\n",
      "      \"learning_rate\": 2.8553560176433525e-05,\n",
      "      \"loss\": 0.0202,\n",
      "      \"step\": 68100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.292384763225054,\n",
      "      \"grad_norm\": 4.355289459228516,\n",
      "      \"learning_rate\": 2.8547258979206047e-05,\n",
      "      \"loss\": 0.0497,\n",
      "      \"step\": 68120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.293645042376887,\n",
      "      \"grad_norm\": 0.0583108626306057,\n",
      "      \"learning_rate\": 2.8540957781978576e-05,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 68140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.294905321528718,\n",
      "      \"grad_norm\": 0.007022962439805269,\n",
      "      \"learning_rate\": 2.8534656584751102e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 68160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.296165600680551,\n",
      "      \"grad_norm\": 4.857685089111328,\n",
      "      \"learning_rate\": 2.852835538752363e-05,\n",
      "      \"loss\": 0.0383,\n",
      "      \"step\": 68180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.2974258798323826,\n",
      "      \"grad_norm\": 0.021673470735549927,\n",
      "      \"learning_rate\": 2.852205419029616e-05,\n",
      "      \"loss\": 0.0163,\n",
      "      \"step\": 68200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.298686158984215,\n",
      "      \"grad_norm\": 0.0070260390639305115,\n",
      "      \"learning_rate\": 2.8515752993068682e-05,\n",
      "      \"loss\": 0.0064,\n",
      "      \"step\": 68220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.299946438136047,\n",
      "      \"grad_norm\": 0.5155923366546631,\n",
      "      \"learning_rate\": 2.8509451795841215e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 68240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.301206717287879,\n",
      "      \"grad_norm\": 0.008636322803795338,\n",
      "      \"learning_rate\": 2.8503150598613737e-05,\n",
      "      \"loss\": 0.039,\n",
      "      \"step\": 68260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.302466996439711,\n",
      "      \"grad_norm\": 0.0024741648230701685,\n",
      "      \"learning_rate\": 2.8496849401386266e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 68280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.303727275591544,\n",
      "      \"grad_norm\": 0.0005541547434404492,\n",
      "      \"learning_rate\": 2.8490548204158792e-05,\n",
      "      \"loss\": 0.0483,\n",
      "      \"step\": 68300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.304987554743375,\n",
      "      \"grad_norm\": 0.4637760818004608,\n",
      "      \"learning_rate\": 2.848424700693132e-05,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 68320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.306247833895208,\n",
      "      \"grad_norm\": 0.010801364667713642,\n",
      "      \"learning_rate\": 2.8477945809703843e-05,\n",
      "      \"loss\": 0.0173,\n",
      "      \"step\": 68340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.30750811304704,\n",
      "      \"grad_norm\": 0.0009082646574825048,\n",
      "      \"learning_rate\": 2.8471644612476372e-05,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 68360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.308768392198872,\n",
      "      \"grad_norm\": 0.07939263433218002,\n",
      "      \"learning_rate\": 2.8465343415248898e-05,\n",
      "      \"loss\": 0.005,\n",
      "      \"step\": 68380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.310028671350704,\n",
      "      \"grad_norm\": 0.002831180114299059,\n",
      "      \"learning_rate\": 2.8459042218021427e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 68400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.3112889505025365,\n",
      "      \"grad_norm\": 0.000855248945299536,\n",
      "      \"learning_rate\": 2.8452741020793956e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 68420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.312549229654368,\n",
      "      \"grad_norm\": 0.0008320932392962277,\n",
      "      \"learning_rate\": 2.8446439823566478e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 68440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.313809508806201,\n",
      "      \"grad_norm\": 0.0074949683621525764,\n",
      "      \"learning_rate\": 2.8440138626339007e-05,\n",
      "      \"loss\": 0.0052,\n",
      "      \"step\": 68460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.315069787958032,\n",
      "      \"grad_norm\": 0.1923106461763382,\n",
      "      \"learning_rate\": 2.8433837429111533e-05,\n",
      "      \"loss\": 0.0182,\n",
      "      \"step\": 68480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.316330067109865,\n",
      "      \"grad_norm\": 4.164570331573486,\n",
      "      \"learning_rate\": 2.8427536231884062e-05,\n",
      "      \"loss\": 0.0431,\n",
      "      \"step\": 68500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.317590346261697,\n",
      "      \"grad_norm\": 0.054965533316135406,\n",
      "      \"learning_rate\": 2.8421235034656584e-05,\n",
      "      \"loss\": 0.0066,\n",
      "      \"step\": 68520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.318850625413529,\n",
      "      \"grad_norm\": 0.0008941759006120265,\n",
      "      \"learning_rate\": 2.8414933837429113e-05,\n",
      "      \"loss\": 0.0083,\n",
      "      \"step\": 68540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.320110904565361,\n",
      "      \"grad_norm\": 0.004530345089733601,\n",
      "      \"learning_rate\": 2.840863264020164e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 68560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.3213711837171935,\n",
      "      \"grad_norm\": 0.0007400826434604824,\n",
      "      \"learning_rate\": 2.8402331442974168e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 68580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.322631462869025,\n",
      "      \"grad_norm\": 0.40838101506233215,\n",
      "      \"learning_rate\": 2.839603024574669e-05,\n",
      "      \"loss\": 0.0205,\n",
      "      \"step\": 68600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.323891742020858,\n",
      "      \"grad_norm\": 0.0013762302696704865,\n",
      "      \"learning_rate\": 2.838972904851922e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 68620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.3251520211726895,\n",
      "      \"grad_norm\": 0.03859194740653038,\n",
      "      \"learning_rate\": 2.8383427851291748e-05,\n",
      "      \"loss\": 0.0053,\n",
      "      \"step\": 68640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.326412300324522,\n",
      "      \"grad_norm\": 0.03526683524250984,\n",
      "      \"learning_rate\": 2.8377126654064274e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 68660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.327672579476354,\n",
      "      \"grad_norm\": 0.04240555316209793,\n",
      "      \"learning_rate\": 2.8370825456836803e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 68680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.328932858628186,\n",
      "      \"grad_norm\": 0.0003794356307480484,\n",
      "      \"learning_rate\": 2.8364524259609325e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 68700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.330193137780018,\n",
      "      \"grad_norm\": 0.029989520087838173,\n",
      "      \"learning_rate\": 2.8358223062381854e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 68720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.331453416931851,\n",
      "      \"grad_norm\": 6.094191551208496,\n",
      "      \"learning_rate\": 2.835192186515438e-05,\n",
      "      \"loss\": 0.06,\n",
      "      \"step\": 68740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.332713696083682,\n",
      "      \"grad_norm\": 0.010472077876329422,\n",
      "      \"learning_rate\": 2.834562066792691e-05,\n",
      "      \"loss\": 0.0363,\n",
      "      \"step\": 68760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.333973975235515,\n",
      "      \"grad_norm\": 0.010590183548629284,\n",
      "      \"learning_rate\": 2.833931947069943e-05,\n",
      "      \"loss\": 0.0268,\n",
      "      \"step\": 68780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.3352342543873466,\n",
      "      \"grad_norm\": 0.0020214130636304617,\n",
      "      \"learning_rate\": 2.833301827347196e-05,\n",
      "      \"loss\": 0.017,\n",
      "      \"step\": 68800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.336494533539179,\n",
      "      \"grad_norm\": 15.67264175415039,\n",
      "      \"learning_rate\": 2.8326717076244486e-05,\n",
      "      \"loss\": 0.0405,\n",
      "      \"step\": 68820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.337754812691011,\n",
      "      \"grad_norm\": 0.0060414476320147514,\n",
      "      \"learning_rate\": 2.8320415879017015e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 68840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.339015091842843,\n",
      "      \"grad_norm\": 0.006461614742875099,\n",
      "      \"learning_rate\": 2.8314114681789537e-05,\n",
      "      \"loss\": 0.0135,\n",
      "      \"step\": 68860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.340275370994675,\n",
      "      \"grad_norm\": 11.964496612548828,\n",
      "      \"learning_rate\": 2.830781348456207e-05,\n",
      "      \"loss\": 0.0073,\n",
      "      \"step\": 68880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.341535650146508,\n",
      "      \"grad_norm\": 0.002027650596573949,\n",
      "      \"learning_rate\": 2.83015122873346e-05,\n",
      "      \"loss\": 0.0222,\n",
      "      \"step\": 68900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.342795929298339,\n",
      "      \"grad_norm\": 0.1108492761850357,\n",
      "      \"learning_rate\": 2.829521109010712e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 68920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.344056208450172,\n",
      "      \"grad_norm\": 0.2819955050945282,\n",
      "      \"learning_rate\": 2.828890989287965e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 68940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.345316487602004,\n",
      "      \"grad_norm\": 0.0010277900146320462,\n",
      "      \"learning_rate\": 2.8282608695652176e-05,\n",
      "      \"loss\": 0.0308,\n",
      "      \"step\": 68960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.346576766753836,\n",
      "      \"grad_norm\": 5.6692681312561035,\n",
      "      \"learning_rate\": 2.8276307498424705e-05,\n",
      "      \"loss\": 0.0245,\n",
      "      \"step\": 68980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.347837045905668,\n",
      "      \"grad_norm\": 4.75492525100708,\n",
      "      \"learning_rate\": 2.8270006301197227e-05,\n",
      "      \"loss\": 0.028,\n",
      "      \"step\": 69000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.3490973250575005,\n",
      "      \"grad_norm\": 0.005901150405406952,\n",
      "      \"learning_rate\": 2.8263705103969756e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 69020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.350357604209332,\n",
      "      \"grad_norm\": 0.16859868168830872,\n",
      "      \"learning_rate\": 2.825740390674228e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 69040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.351617883361165,\n",
      "      \"grad_norm\": 0.036523692309856415,\n",
      "      \"learning_rate\": 2.825110270951481e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 69060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.352878162512996,\n",
      "      \"grad_norm\": 0.01667003706097603,\n",
      "      \"learning_rate\": 2.8244801512287333e-05,\n",
      "      \"loss\": 0.028,\n",
      "      \"step\": 69080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.354138441664829,\n",
      "      \"grad_norm\": 0.17193518579006195,\n",
      "      \"learning_rate\": 2.8238500315059862e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 69100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.355398720816661,\n",
      "      \"grad_norm\": 0.02089739963412285,\n",
      "      \"learning_rate\": 2.823219911783239e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 69120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.356658999968493,\n",
      "      \"grad_norm\": 0.0917966291308403,\n",
      "      \"learning_rate\": 2.822621298046629e-05,\n",
      "      \"loss\": 0.0662,\n",
      "      \"step\": 69140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.357919279120325,\n",
      "      \"grad_norm\": 0.10843725502490997,\n",
      "      \"learning_rate\": 2.8219911783238817e-05,\n",
      "      \"loss\": 0.0455,\n",
      "      \"step\": 69160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.3591795582721575,\n",
      "      \"grad_norm\": 0.02510090172290802,\n",
      "      \"learning_rate\": 2.821361058601134e-05,\n",
      "      \"loss\": 0.0221,\n",
      "      \"step\": 69180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.360439837423989,\n",
      "      \"grad_norm\": 0.22628562152385712,\n",
      "      \"learning_rate\": 2.820730938878387e-05,\n",
      "      \"loss\": 0.0028,\n",
      "      \"step\": 69200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.361700116575822,\n",
      "      \"grad_norm\": 0.009698654524981976,\n",
      "      \"learning_rate\": 2.82010081915564e-05,\n",
      "      \"loss\": 0.0382,\n",
      "      \"step\": 69220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.3629603957276535,\n",
      "      \"grad_norm\": 0.002412322210147977,\n",
      "      \"learning_rate\": 2.8194706994328923e-05,\n",
      "      \"loss\": 0.0048,\n",
      "      \"step\": 69240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.364220674879486,\n",
      "      \"grad_norm\": 0.005174947436898947,\n",
      "      \"learning_rate\": 2.8188405797101452e-05,\n",
      "      \"loss\": 0.0518,\n",
      "      \"step\": 69260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.365480954031318,\n",
      "      \"grad_norm\": 0.013041510246694088,\n",
      "      \"learning_rate\": 2.8182104599873975e-05,\n",
      "      \"loss\": 0.0322,\n",
      "      \"step\": 69280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.36674123318315,\n",
      "      \"grad_norm\": 0.012779856100678444,\n",
      "      \"learning_rate\": 2.8175803402646507e-05,\n",
      "      \"loss\": 0.0323,\n",
      "      \"step\": 69300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.368001512334982,\n",
      "      \"grad_norm\": 0.09882521629333496,\n",
      "      \"learning_rate\": 2.816950220541903e-05,\n",
      "      \"loss\": 0.0308,\n",
      "      \"step\": 69320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.369261791486815,\n",
      "      \"grad_norm\": 0.010539831593632698,\n",
      "      \"learning_rate\": 2.816320100819156e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 69340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.370522070638646,\n",
      "      \"grad_norm\": 0.032636608928442,\n",
      "      \"learning_rate\": 2.8156899810964084e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 69360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.371782349790479,\n",
      "      \"grad_norm\": 0.0018136035650968552,\n",
      "      \"learning_rate\": 2.8150598613736613e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 69380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.3730426289423106,\n",
      "      \"grad_norm\": 0.009797864593565464,\n",
      "      \"learning_rate\": 2.8144297416509135e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 69400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.374302908094143,\n",
      "      \"grad_norm\": 18.32673454284668,\n",
      "      \"learning_rate\": 2.8137996219281664e-05,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 69420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.375563187245975,\n",
      "      \"grad_norm\": 0.01549831684678793,\n",
      "      \"learning_rate\": 2.8131695022054193e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 69440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.376823466397807,\n",
      "      \"grad_norm\": 13.931199073791504,\n",
      "      \"learning_rate\": 2.812539382482672e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 69460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.378083745549639,\n",
      "      \"grad_norm\": 0.004174679052084684,\n",
      "      \"learning_rate\": 2.8119092627599248e-05,\n",
      "      \"loss\": 0.0099,\n",
      "      \"step\": 69480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.379344024701472,\n",
      "      \"grad_norm\": 0.003428971627727151,\n",
      "      \"learning_rate\": 2.811279143037177e-05,\n",
      "      \"loss\": 0.0316,\n",
      "      \"step\": 69500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.380604303853303,\n",
      "      \"grad_norm\": 0.00800031516700983,\n",
      "      \"learning_rate\": 2.81064902331443e-05,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 69520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.381864583005136,\n",
      "      \"grad_norm\": 6.031224727630615,\n",
      "      \"learning_rate\": 2.8100189035916825e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 69540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.383124862156968,\n",
      "      \"grad_norm\": 0.00896932277828455,\n",
      "      \"learning_rate\": 2.8093887838689354e-05,\n",
      "      \"loss\": 0.0564,\n",
      "      \"step\": 69560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.3843851413088,\n",
      "      \"grad_norm\": 0.0024703009985387325,\n",
      "      \"learning_rate\": 2.8087586641461876e-05,\n",
      "      \"loss\": 0.03,\n",
      "      \"step\": 69580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.385645420460632,\n",
      "      \"grad_norm\": 0.2472900003194809,\n",
      "      \"learning_rate\": 2.8081285444234405e-05,\n",
      "      \"loss\": 0.0496,\n",
      "      \"step\": 69600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.3869056996124645,\n",
      "      \"grad_norm\": 0.027267325669527054,\n",
      "      \"learning_rate\": 2.807498424700693e-05,\n",
      "      \"loss\": 0.0386,\n",
      "      \"step\": 69620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.388165978764296,\n",
      "      \"grad_norm\": 17.837772369384766,\n",
      "      \"learning_rate\": 2.806868304977946e-05,\n",
      "      \"loss\": 0.0429,\n",
      "      \"step\": 69640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.389426257916129,\n",
      "      \"grad_norm\": 0.10141008347272873,\n",
      "      \"learning_rate\": 2.806238185255199e-05,\n",
      "      \"loss\": 0.0305,\n",
      "      \"step\": 69660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.39068653706796,\n",
      "      \"grad_norm\": 0.04135948047041893,\n",
      "      \"learning_rate\": 2.805608065532451e-05,\n",
      "      \"loss\": 0.0252,\n",
      "      \"step\": 69680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.391946816219793,\n",
      "      \"grad_norm\": 0.04663743078708649,\n",
      "      \"learning_rate\": 2.804977945809704e-05,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 69700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.393207095371625,\n",
      "      \"grad_norm\": 0.09459248930215836,\n",
      "      \"learning_rate\": 2.8043478260869566e-05,\n",
      "      \"loss\": 0.0231,\n",
      "      \"step\": 69720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.394467374523457,\n",
      "      \"grad_norm\": 0.19815996289253235,\n",
      "      \"learning_rate\": 2.8037177063642095e-05,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 69740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.395727653675289,\n",
      "      \"grad_norm\": 3.2042667865753174,\n",
      "      \"learning_rate\": 2.8030875866414617e-05,\n",
      "      \"loss\": 0.0049,\n",
      "      \"step\": 69760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.3969879328271215,\n",
      "      \"grad_norm\": 0.1507461816072464,\n",
      "      \"learning_rate\": 2.8024574669187146e-05,\n",
      "      \"loss\": 0.0384,\n",
      "      \"step\": 69780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.398248211978953,\n",
      "      \"grad_norm\": 0.006015405524522066,\n",
      "      \"learning_rate\": 2.8018273471959672e-05,\n",
      "      \"loss\": 0.0385,\n",
      "      \"step\": 69800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.399508491130786,\n",
      "      \"grad_norm\": 0.016956055536866188,\n",
      "      \"learning_rate\": 2.80119722747322e-05,\n",
      "      \"loss\": 0.0474,\n",
      "      \"step\": 69820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.4007687702826175,\n",
      "      \"grad_norm\": 0.005051283165812492,\n",
      "      \"learning_rate\": 2.8005671077504723e-05,\n",
      "      \"loss\": 0.0334,\n",
      "      \"step\": 69840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.40202904943445,\n",
      "      \"grad_norm\": 0.0728142186999321,\n",
      "      \"learning_rate\": 2.7999369880277252e-05,\n",
      "      \"loss\": 0.0365,\n",
      "      \"step\": 69860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.403289328586282,\n",
      "      \"grad_norm\": 0.07587899267673492,\n",
      "      \"learning_rate\": 2.7993068683049785e-05,\n",
      "      \"loss\": 0.0271,\n",
      "      \"step\": 69880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.404549607738114,\n",
      "      \"grad_norm\": 0.0767192617058754,\n",
      "      \"learning_rate\": 2.7986767485822307e-05,\n",
      "      \"loss\": 0.0076,\n",
      "      \"step\": 69900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.405809886889946,\n",
      "      \"grad_norm\": 0.09054327011108398,\n",
      "      \"learning_rate\": 2.7980466288594836e-05,\n",
      "      \"loss\": 0.0021,\n",
      "      \"step\": 69920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.407070166041779,\n",
      "      \"grad_norm\": 0.039740901440382004,\n",
      "      \"learning_rate\": 2.7974165091367362e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 69940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.40833044519361,\n",
      "      \"grad_norm\": 0.0828423872590065,\n",
      "      \"learning_rate\": 2.796786389413989e-05,\n",
      "      \"loss\": 0.0215,\n",
      "      \"step\": 69960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.409590724345443,\n",
      "      \"grad_norm\": 0.067601777613163,\n",
      "      \"learning_rate\": 2.7961562696912413e-05,\n",
      "      \"loss\": 0.0283,\n",
      "      \"step\": 69980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.4108510034972745,\n",
      "      \"grad_norm\": 0.22303666174411774,\n",
      "      \"learning_rate\": 2.7955261499684942e-05,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 70000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.412111282649107,\n",
      "      \"grad_norm\": 4.545550346374512,\n",
      "      \"learning_rate\": 2.7948960302457468e-05,\n",
      "      \"loss\": 0.047,\n",
      "      \"step\": 70020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.413371561800939,\n",
      "      \"grad_norm\": 0.09977725148200989,\n",
      "      \"learning_rate\": 2.7942659105229997e-05,\n",
      "      \"loss\": 0.0537,\n",
      "      \"step\": 70040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.414631840952771,\n",
      "      \"grad_norm\": 0.0023053314071148634,\n",
      "      \"learning_rate\": 2.793635790800252e-05,\n",
      "      \"loss\": 0.0197,\n",
      "      \"step\": 70060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.415892120104603,\n",
      "      \"grad_norm\": 0.05897488072514534,\n",
      "      \"learning_rate\": 2.7930056710775048e-05,\n",
      "      \"loss\": 0.0056,\n",
      "      \"step\": 70080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.417152399256436,\n",
      "      \"grad_norm\": 0.004864010028541088,\n",
      "      \"learning_rate\": 2.7923755513547577e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 70100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.418412678408267,\n",
      "      \"grad_norm\": 0.02051733247935772,\n",
      "      \"learning_rate\": 2.7917454316320103e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 70120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.4196729575601,\n",
      "      \"grad_norm\": 0.08072549849748611,\n",
      "      \"learning_rate\": 2.7911153119092632e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 70140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.420933236711932,\n",
      "      \"grad_norm\": 0.020340602844953537,\n",
      "      \"learning_rate\": 2.7904851921865154e-05,\n",
      "      \"loss\": 0.0246,\n",
      "      \"step\": 70160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.422193515863764,\n",
      "      \"grad_norm\": 0.0009008186170831323,\n",
      "      \"learning_rate\": 2.7898550724637683e-05,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 70180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.423453795015596,\n",
      "      \"grad_norm\": 0.0022616861388087273,\n",
      "      \"learning_rate\": 2.789224952741021e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 70200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.4247140741674285,\n",
      "      \"grad_norm\": 0.0033236045856028795,\n",
      "      \"learning_rate\": 2.7885948330182738e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 70220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.42597435331926,\n",
      "      \"grad_norm\": 3.8902335166931152,\n",
      "      \"learning_rate\": 2.787964713295526e-05,\n",
      "      \"loss\": 0.0396,\n",
      "      \"step\": 70240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.427234632471093,\n",
      "      \"grad_norm\": 0.0032915479969233274,\n",
      "      \"learning_rate\": 2.787334593572779e-05,\n",
      "      \"loss\": 0.0059,\n",
      "      \"step\": 70260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.428494911622924,\n",
      "      \"grad_norm\": 0.0007896884926594794,\n",
      "      \"learning_rate\": 2.7867044738500315e-05,\n",
      "      \"loss\": 0.0256,\n",
      "      \"step\": 70280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.429755190774757,\n",
      "      \"grad_norm\": 0.019662879407405853,\n",
      "      \"learning_rate\": 2.7860743541272844e-05,\n",
      "      \"loss\": 0.0253,\n",
      "      \"step\": 70300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.431015469926589,\n",
      "      \"grad_norm\": 6.7138872146606445,\n",
      "      \"learning_rate\": 2.7854442344045366e-05,\n",
      "      \"loss\": 0.006,\n",
      "      \"step\": 70320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.432275749078421,\n",
      "      \"grad_norm\": 0.001477303565479815,\n",
      "      \"learning_rate\": 2.7848141146817895e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 70340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.433536028230253,\n",
      "      \"grad_norm\": 6.0794830322265625,\n",
      "      \"learning_rate\": 2.7841839949590424e-05,\n",
      "      \"loss\": 0.0306,\n",
      "      \"step\": 70360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.4347963073820855,\n",
      "      \"grad_norm\": 0.015637092292308807,\n",
      "      \"learning_rate\": 2.783553875236295e-05,\n",
      "      \"loss\": 0.0163,\n",
      "      \"step\": 70380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.436056586533917,\n",
      "      \"grad_norm\": 0.010369543917477131,\n",
      "      \"learning_rate\": 2.782923755513548e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 70400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.43731686568575,\n",
      "      \"grad_norm\": 0.2902579605579376,\n",
      "      \"learning_rate\": 2.7822936357908e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 70420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.4385771448375815,\n",
      "      \"grad_norm\": 6.690232753753662,\n",
      "      \"learning_rate\": 2.7816635160680534e-05,\n",
      "      \"loss\": 0.0268,\n",
      "      \"step\": 70440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.439837423989414,\n",
      "      \"grad_norm\": 0.8206266164779663,\n",
      "      \"learning_rate\": 2.7810333963453056e-05,\n",
      "      \"loss\": 0.0243,\n",
      "      \"step\": 70460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.441097703141246,\n",
      "      \"grad_norm\": 0.00040736477239988744,\n",
      "      \"learning_rate\": 2.7804032766225585e-05,\n",
      "      \"loss\": 0.045,\n",
      "      \"step\": 70480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.442357982293078,\n",
      "      \"grad_norm\": 5.69989538192749,\n",
      "      \"learning_rate\": 2.7797731568998107e-05,\n",
      "      \"loss\": 0.0382,\n",
      "      \"step\": 70500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.44361826144491,\n",
      "      \"grad_norm\": 0.010727247223258018,\n",
      "      \"learning_rate\": 2.779143037177064e-05,\n",
      "      \"loss\": 0.0178,\n",
      "      \"step\": 70520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.444878540596743,\n",
      "      \"grad_norm\": 2.423320770263672,\n",
      "      \"learning_rate\": 2.7785129174543162e-05,\n",
      "      \"loss\": 0.0169,\n",
      "      \"step\": 70540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.446138819748574,\n",
      "      \"grad_norm\": 0.02785125933587551,\n",
      "      \"learning_rate\": 2.777882797731569e-05,\n",
      "      \"loss\": 0.0216,\n",
      "      \"step\": 70560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.447399098900407,\n",
      "      \"grad_norm\": 0.012080790475010872,\n",
      "      \"learning_rate\": 2.777252678008822e-05,\n",
      "      \"loss\": 0.0464,\n",
      "      \"step\": 70580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.4486593780522385,\n",
      "      \"grad_norm\": 0.10114142298698425,\n",
      "      \"learning_rate\": 2.7766225582860746e-05,\n",
      "      \"loss\": 0.0338,\n",
      "      \"step\": 70600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.44991965720407,\n",
      "      \"grad_norm\": 0.27460670471191406,\n",
      "      \"learning_rate\": 2.7759924385633275e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 70620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.451179936355903,\n",
      "      \"grad_norm\": 0.14034172892570496,\n",
      "      \"learning_rate\": 2.7753623188405797e-05,\n",
      "      \"loss\": 0.0125,\n",
      "      \"step\": 70640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.452440215507735,\n",
      "      \"grad_norm\": 0.007409136742353439,\n",
      "      \"learning_rate\": 2.7747321991178326e-05,\n",
      "      \"loss\": 0.0242,\n",
      "      \"step\": 70660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.453700494659567,\n",
      "      \"grad_norm\": 0.011814923025667667,\n",
      "      \"learning_rate\": 2.774102079395085e-05,\n",
      "      \"loss\": 0.0093,\n",
      "      \"step\": 70680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.454960773811399,\n",
      "      \"grad_norm\": 0.009321236982941628,\n",
      "      \"learning_rate\": 2.773471959672338e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 70700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.456221052963231,\n",
      "      \"grad_norm\": 0.10586242377758026,\n",
      "      \"learning_rate\": 2.7728418399495903e-05,\n",
      "      \"loss\": 0.0324,\n",
      "      \"step\": 70720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.457481332115064,\n",
      "      \"grad_norm\": 0.07323053479194641,\n",
      "      \"learning_rate\": 2.7722117202268432e-05,\n",
      "      \"loss\": 0.0208,\n",
      "      \"step\": 70740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.458741611266896,\n",
      "      \"grad_norm\": 0.1140836551785469,\n",
      "      \"learning_rate\": 2.7715816005040958e-05,\n",
      "      \"loss\": 0.0163,\n",
      "      \"step\": 70760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.460001890418727,\n",
      "      \"grad_norm\": 0.0002980658027809113,\n",
      "      \"learning_rate\": 2.7709514807813487e-05,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 70780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.46126216957056,\n",
      "      \"grad_norm\": 0.06110216677188873,\n",
      "      \"learning_rate\": 2.7703213610586016e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 70800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.4625224487223925,\n",
      "      \"grad_norm\": 4.018832683563232,\n",
      "      \"learning_rate\": 2.7696912413358538e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 70820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.463782727874224,\n",
      "      \"grad_norm\": 0.00023292005062103271,\n",
      "      \"learning_rate\": 2.7690611216131067e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 70840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.465043007026056,\n",
      "      \"grad_norm\": 0.005648168735206127,\n",
      "      \"learning_rate\": 2.7684310018903593e-05,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 70860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.466303286177888,\n",
      "      \"grad_norm\": 0.09913431853055954,\n",
      "      \"learning_rate\": 2.767800882167612e-05,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 70880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.467563565329721,\n",
      "      \"grad_norm\": 0.00043920165626332164,\n",
      "      \"learning_rate\": 2.7671707624448644e-05,\n",
      "      \"loss\": 0.0376,\n",
      "      \"step\": 70900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.468823844481553,\n",
      "      \"grad_norm\": 0.000521922018378973,\n",
      "      \"learning_rate\": 2.7665406427221173e-05,\n",
      "      \"loss\": 0.0021,\n",
      "      \"step\": 70920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.470084123633384,\n",
      "      \"grad_norm\": 0.0009501941967755556,\n",
      "      \"learning_rate\": 2.76591052299937e-05,\n",
      "      \"loss\": 0.0272,\n",
      "      \"step\": 70940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.471344402785217,\n",
      "      \"grad_norm\": 0.0021041063591837883,\n",
      "      \"learning_rate\": 2.7652804032766228e-05,\n",
      "      \"loss\": 0.0064,\n",
      "      \"step\": 70960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.4726046819370495,\n",
      "      \"grad_norm\": 0.006354370154440403,\n",
      "      \"learning_rate\": 2.764650283553875e-05,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 70980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.473864961088881,\n",
      "      \"grad_norm\": 0.00022832643298897892,\n",
      "      \"learning_rate\": 2.764020163831128e-05,\n",
      "      \"loss\": 0.0326,\n",
      "      \"step\": 71000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.475125240240713,\n",
      "      \"grad_norm\": 8.930876731872559,\n",
      "      \"learning_rate\": 2.763390044108381e-05,\n",
      "      \"loss\": 0.0298,\n",
      "      \"step\": 71020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.4763855193925455,\n",
      "      \"grad_norm\": 0.018779899924993515,\n",
      "      \"learning_rate\": 2.7627599243856334e-05,\n",
      "      \"loss\": 0.0099,\n",
      "      \"step\": 71040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.477645798544377,\n",
      "      \"grad_norm\": 0.029882779344916344,\n",
      "      \"learning_rate\": 2.7621298046628863e-05,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 71060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.47890607769621,\n",
      "      \"grad_norm\": 0.1862182915210724,\n",
      "      \"learning_rate\": 2.7614996849401385e-05,\n",
      "      \"loss\": 0.0547,\n",
      "      \"step\": 71080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.480166356848041,\n",
      "      \"grad_norm\": 0.04172976687550545,\n",
      "      \"learning_rate\": 2.7608695652173917e-05,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 71100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.481426635999874,\n",
      "      \"grad_norm\": 0.0005936399102210999,\n",
      "      \"learning_rate\": 2.760239445494644e-05,\n",
      "      \"loss\": 0.0077,\n",
      "      \"step\": 71120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.482686915151706,\n",
      "      \"grad_norm\": 0.06850118935108185,\n",
      "      \"learning_rate\": 2.759609325771897e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 71140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.483947194303538,\n",
      "      \"grad_norm\": 0.00113948923535645,\n",
      "      \"learning_rate\": 2.7589792060491494e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 71160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.48520747345537,\n",
      "      \"grad_norm\": 0.11571355909109116,\n",
      "      \"learning_rate\": 2.7583490863264023e-05,\n",
      "      \"loss\": 0.0306,\n",
      "      \"step\": 71180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.4864677526072025,\n",
      "      \"grad_norm\": 0.002295122016221285,\n",
      "      \"learning_rate\": 2.7577189666036546e-05,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 71200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.487728031759034,\n",
      "      \"grad_norm\": 0.003307580016553402,\n",
      "      \"learning_rate\": 2.7570888468809075e-05,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 71220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.488988310910867,\n",
      "      \"grad_norm\": 0.012895381078124046,\n",
      "      \"learning_rate\": 2.7564587271581604e-05,\n",
      "      \"loss\": 0.044,\n",
      "      \"step\": 71240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.4902485900626985,\n",
      "      \"grad_norm\": 0.066978320479393,\n",
      "      \"learning_rate\": 2.755828607435413e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 71260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.491508869214531,\n",
      "      \"grad_norm\": 0.000860073952935636,\n",
      "      \"learning_rate\": 2.755198487712666e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 71280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.492769148366363,\n",
      "      \"grad_norm\": 18.00849723815918,\n",
      "      \"learning_rate\": 2.754568367989918e-05,\n",
      "      \"loss\": 0.0465,\n",
      "      \"step\": 71300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.494029427518195,\n",
      "      \"grad_norm\": 0.0008979253470897675,\n",
      "      \"learning_rate\": 2.753938248267171e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 71320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.495289706670027,\n",
      "      \"grad_norm\": 0.0018360281828790903,\n",
      "      \"learning_rate\": 2.7533081285444235e-05,\n",
      "      \"loss\": 0.0511,\n",
      "      \"step\": 71340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.49654998582186,\n",
      "      \"grad_norm\": 0.3817098140716553,\n",
      "      \"learning_rate\": 2.7526780088216764e-05,\n",
      "      \"loss\": 0.0292,\n",
      "      \"step\": 71360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.497810264973691,\n",
      "      \"grad_norm\": 0.7305157780647278,\n",
      "      \"learning_rate\": 2.7520478890989287e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 71380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.499070544125524,\n",
      "      \"grad_norm\": 0.36894047260284424,\n",
      "      \"learning_rate\": 2.7514177693761816e-05,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 71400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.500330823277356,\n",
      "      \"grad_norm\": 0.011637046933174133,\n",
      "      \"learning_rate\": 2.750787649653434e-05,\n",
      "      \"loss\": 0.0146,\n",
      "      \"step\": 71420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.501591102429188,\n",
      "      \"grad_norm\": 0.0004904477391391993,\n",
      "      \"learning_rate\": 2.750157529930687e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 71440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.50285138158102,\n",
      "      \"grad_norm\": 0.003268080996349454,\n",
      "      \"learning_rate\": 2.74952741020794e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 71460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.504111660732852,\n",
      "      \"grad_norm\": 0.0016906424425542355,\n",
      "      \"learning_rate\": 2.7488972904851922e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 71480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.505371939884684,\n",
      "      \"grad_norm\": 0.27132290601730347,\n",
      "      \"learning_rate\": 2.748267170762445e-05,\n",
      "      \"loss\": 0.0435,\n",
      "      \"step\": 71500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.506632219036517,\n",
      "      \"grad_norm\": 0.0011689227540045977,\n",
      "      \"learning_rate\": 2.7476370510396976e-05,\n",
      "      \"loss\": 0.0227,\n",
      "      \"step\": 71520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.507892498188348,\n",
      "      \"grad_norm\": 0.3756060004234314,\n",
      "      \"learning_rate\": 2.7470069313169505e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 71540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.509152777340181,\n",
      "      \"grad_norm\": 0.016128337010741234,\n",
      "      \"learning_rate\": 2.7463768115942028e-05,\n",
      "      \"loss\": 0.0389,\n",
      "      \"step\": 71560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.510413056492013,\n",
      "      \"grad_norm\": 0.5715398788452148,\n",
      "      \"learning_rate\": 2.7457466918714557e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 71580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.511673335643845,\n",
      "      \"grad_norm\": 0.000862422282807529,\n",
      "      \"learning_rate\": 2.7451165721487082e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 71600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.512933614795677,\n",
      "      \"grad_norm\": 0.0047273640520870686,\n",
      "      \"learning_rate\": 2.744486452425961e-05,\n",
      "      \"loss\": 0.0245,\n",
      "      \"step\": 71620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.5141938939475095,\n",
      "      \"grad_norm\": 0.011525326408445835,\n",
      "      \"learning_rate\": 2.7438563327032134e-05,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 71640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.515454173099341,\n",
      "      \"grad_norm\": 0.0019154212204739451,\n",
      "      \"learning_rate\": 2.7432262129804666e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 71660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.516714452251174,\n",
      "      \"grad_norm\": 0.036601852625608444,\n",
      "      \"learning_rate\": 2.742596093257719e-05,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 71680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.517974731403005,\n",
      "      \"grad_norm\": 0.0007633577915839851,\n",
      "      \"learning_rate\": 2.7419659735349717e-05,\n",
      "      \"loss\": 0.0287,\n",
      "      \"step\": 71700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.519235010554838,\n",
      "      \"grad_norm\": 0.025024035945534706,\n",
      "      \"learning_rate\": 2.7413358538122246e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 71720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.52049528970667,\n",
      "      \"grad_norm\": 0.23228435218334198,\n",
      "      \"learning_rate\": 2.7407057340894772e-05,\n",
      "      \"loss\": 0.0019,\n",
      "      \"step\": 71740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.521755568858502,\n",
      "      \"grad_norm\": 0.0019538854248821735,\n",
      "      \"learning_rate\": 2.74007561436673e-05,\n",
      "      \"loss\": 0.0217,\n",
      "      \"step\": 71760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.523015848010334,\n",
      "      \"grad_norm\": 0.002634946722537279,\n",
      "      \"learning_rate\": 2.7394454946439823e-05,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 71780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.5242761271621665,\n",
      "      \"grad_norm\": 0.05167040973901749,\n",
      "      \"learning_rate\": 2.7388153749212352e-05,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 71800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.525536406313998,\n",
      "      \"grad_norm\": 0.02906237542629242,\n",
      "      \"learning_rate\": 2.7381852551984878e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 71820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.526796685465831,\n",
      "      \"grad_norm\": 6.6076979637146,\n",
      "      \"learning_rate\": 2.7375551354757407e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 71840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.5280569646176625,\n",
      "      \"grad_norm\": 0.008249124512076378,\n",
      "      \"learning_rate\": 2.736925015752993e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 71860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.529317243769495,\n",
      "      \"grad_norm\": 0.011063747107982635,\n",
      "      \"learning_rate\": 2.736294896030246e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 71880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.530577522921327,\n",
      "      \"grad_norm\": 0.006500426214188337,\n",
      "      \"learning_rate\": 2.7356647763074984e-05,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 71900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.531837802073159,\n",
      "      \"grad_norm\": 0.003564271377399564,\n",
      "      \"learning_rate\": 2.7350346565847513e-05,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 71920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.533098081224991,\n",
      "      \"grad_norm\": 0.042769644409418106,\n",
      "      \"learning_rate\": 2.7344045368620042e-05,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 71940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.534358360376824,\n",
      "      \"grad_norm\": 0.012592226266860962,\n",
      "      \"learning_rate\": 2.7337744171392564e-05,\n",
      "      \"loss\": 0.0235,\n",
      "      \"step\": 71960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.535618639528655,\n",
      "      \"grad_norm\": 0.03820091485977173,\n",
      "      \"learning_rate\": 2.7331442974165093e-05,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 71980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.536878918680488,\n",
      "      \"grad_norm\": 0.09464562684297562,\n",
      "      \"learning_rate\": 2.732514177693762e-05,\n",
      "      \"loss\": 0.0269,\n",
      "      \"step\": 72000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.53813919783232,\n",
      "      \"grad_norm\": 0.03300880268216133,\n",
      "      \"learning_rate\": 2.7318840579710148e-05,\n",
      "      \"loss\": 0.0252,\n",
      "      \"step\": 72020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.539399476984152,\n",
      "      \"grad_norm\": 0.000740308896638453,\n",
      "      \"learning_rate\": 2.731253938248267e-05,\n",
      "      \"loss\": 0.0347,\n",
      "      \"step\": 72040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.540659756135984,\n",
      "      \"grad_norm\": 0.033973027020692825,\n",
      "      \"learning_rate\": 2.73062381852552e-05,\n",
      "      \"loss\": 0.0099,\n",
      "      \"step\": 72060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.541920035287816,\n",
      "      \"grad_norm\": 0.3326284885406494,\n",
      "      \"learning_rate\": 2.7299936988027725e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 72080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.543180314439648,\n",
      "      \"grad_norm\": 0.0730973333120346,\n",
      "      \"learning_rate\": 2.7293635790800254e-05,\n",
      "      \"loss\": 0.0489,\n",
      "      \"step\": 72100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.544440593591481,\n",
      "      \"grad_norm\": 0.01689477078616619,\n",
      "      \"learning_rate\": 2.7287334593572776e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 72120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.545700872743312,\n",
      "      \"grad_norm\": 0.025322167202830315,\n",
      "      \"learning_rate\": 2.7281033396345305e-05,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 72140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.546961151895145,\n",
      "      \"grad_norm\": 0.25292134284973145,\n",
      "      \"learning_rate\": 2.7274732199117835e-05,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 72160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.548221431046977,\n",
      "      \"grad_norm\": 0.003828803775832057,\n",
      "      \"learning_rate\": 2.726843100189036e-05,\n",
      "      \"loss\": 0.0099,\n",
      "      \"step\": 72180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.549481710198809,\n",
      "      \"grad_norm\": 0.07135678827762604,\n",
      "      \"learning_rate\": 2.726212980466289e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 72200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.550741989350641,\n",
      "      \"grad_norm\": 0.507063090801239,\n",
      "      \"learning_rate\": 2.725582860743541e-05,\n",
      "      \"loss\": 0.0252,\n",
      "      \"step\": 72220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.5520022685024735,\n",
      "      \"grad_norm\": 0.01488521508872509,\n",
      "      \"learning_rate\": 2.7249527410207944e-05,\n",
      "      \"loss\": 0.0279,\n",
      "      \"step\": 72240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.553262547654305,\n",
      "      \"grad_norm\": 0.00030395001522265375,\n",
      "      \"learning_rate\": 2.7243226212980466e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 72260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.554522826806138,\n",
      "      \"grad_norm\": 0.0005795477773062885,\n",
      "      \"learning_rate\": 2.7236925015752995e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 72280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.555783105957969,\n",
      "      \"grad_norm\": 0.00044214134686626494,\n",
      "      \"learning_rate\": 2.7230623818525517e-05,\n",
      "      \"loss\": 0.0272,\n",
      "      \"step\": 72300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.557043385109802,\n",
      "      \"grad_norm\": 0.006663477048277855,\n",
      "      \"learning_rate\": 2.722432262129805e-05,\n",
      "      \"loss\": 0.0136,\n",
      "      \"step\": 72320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.558303664261634,\n",
      "      \"grad_norm\": 0.0013783060712739825,\n",
      "      \"learning_rate\": 2.7218021424070572e-05,\n",
      "      \"loss\": 0.0115,\n",
      "      \"step\": 72340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.559563943413466,\n",
      "      \"grad_norm\": 0.06576185673475266,\n",
      "      \"learning_rate\": 2.72117202268431e-05,\n",
      "      \"loss\": 0.0429,\n",
      "      \"step\": 72360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.560824222565298,\n",
      "      \"grad_norm\": 0.4141690731048584,\n",
      "      \"learning_rate\": 2.720541902961563e-05,\n",
      "      \"loss\": 0.0245,\n",
      "      \"step\": 72380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.5620845017171305,\n",
      "      \"grad_norm\": 0.005197050515562296,\n",
      "      \"learning_rate\": 2.7199117832388156e-05,\n",
      "      \"loss\": 0.0246,\n",
      "      \"step\": 72400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.563344780868962,\n",
      "      \"grad_norm\": 0.001416908809915185,\n",
      "      \"learning_rate\": 2.7192816635160685e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 72420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.564605060020795,\n",
      "      \"grad_norm\": 0.001957091037184,\n",
      "      \"learning_rate\": 2.7186515437933207e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 72440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.5658653391726265,\n",
      "      \"grad_norm\": 0.039309870451688766,\n",
      "      \"learning_rate\": 2.7180214240705736e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 72460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.567125618324459,\n",
      "      \"grad_norm\": 3.953829765319824,\n",
      "      \"learning_rate\": 2.7173913043478262e-05,\n",
      "      \"loss\": 0.0253,\n",
      "      \"step\": 72480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.568385897476291,\n",
      "      \"grad_norm\": 0.0016156487399712205,\n",
      "      \"learning_rate\": 2.716761184625079e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 72500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.569646176628123,\n",
      "      \"grad_norm\": 0.01830952987074852,\n",
      "      \"learning_rate\": 2.7161310649023313e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 72520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.570906455779955,\n",
      "      \"grad_norm\": 8.05795669555664,\n",
      "      \"learning_rate\": 2.7155009451795842e-05,\n",
      "      \"loss\": 0.0162,\n",
      "      \"step\": 72540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.572166734931788,\n",
      "      \"grad_norm\": 0.0006413627997972071,\n",
      "      \"learning_rate\": 2.7148708254568368e-05,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 72560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.573427014083619,\n",
      "      \"grad_norm\": 0.0021320804953575134,\n",
      "      \"learning_rate\": 2.7142407057340897e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 72580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.574687293235452,\n",
      "      \"grad_norm\": 0.0407521054148674,\n",
      "      \"learning_rate\": 2.7136105860113426e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 72600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.575947572387284,\n",
      "      \"grad_norm\": 0.011710369028151035,\n",
      "      \"learning_rate\": 2.7129804662885948e-05,\n",
      "      \"loss\": 0.0248,\n",
      "      \"step\": 72620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.577207851539116,\n",
      "      \"grad_norm\": 0.047434672713279724,\n",
      "      \"learning_rate\": 2.7123503465658477e-05,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 72640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.578468130690948,\n",
      "      \"grad_norm\": 0.013377361930906773,\n",
      "      \"learning_rate\": 2.7117202268431003e-05,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 72660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.57972840984278,\n",
      "      \"grad_norm\": 0.03195112943649292,\n",
      "      \"learning_rate\": 2.7110901071203532e-05,\n",
      "      \"loss\": 0.0183,\n",
      "      \"step\": 72680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.580988688994612,\n",
      "      \"grad_norm\": 0.3173007667064667,\n",
      "      \"learning_rate\": 2.7104599873976054e-05,\n",
      "      \"loss\": 0.0287,\n",
      "      \"step\": 72700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.582248968146445,\n",
      "      \"grad_norm\": 4.736757755279541,\n",
      "      \"learning_rate\": 2.7098298676748583e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 72720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.583509247298276,\n",
      "      \"grad_norm\": 0.002875839127227664,\n",
      "      \"learning_rate\": 2.709199747952111e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 72740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.584769526450109,\n",
      "      \"grad_norm\": 0.021885277703404427,\n",
      "      \"learning_rate\": 2.7085696282293638e-05,\n",
      "      \"loss\": 0.0568,\n",
      "      \"step\": 72760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.586029805601941,\n",
      "      \"grad_norm\": 0.01039354968816042,\n",
      "      \"learning_rate\": 2.707939508506616e-05,\n",
      "      \"loss\": 0.0382,\n",
      "      \"step\": 72780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.587290084753773,\n",
      "      \"grad_norm\": 0.030167069286108017,\n",
      "      \"learning_rate\": 2.707309388783869e-05,\n",
      "      \"loss\": 0.0034,\n",
      "      \"step\": 72800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.588550363905605,\n",
      "      \"grad_norm\": 0.015967663377523422,\n",
      "      \"learning_rate\": 2.706679269061122e-05,\n",
      "      \"loss\": 0.0457,\n",
      "      \"step\": 72820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.5898106430574375,\n",
      "      \"grad_norm\": 0.005307397805154324,\n",
      "      \"learning_rate\": 2.7060491493383744e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 72840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.591070922209269,\n",
      "      \"grad_norm\": 0.009369360283017159,\n",
      "      \"learning_rate\": 2.7054190296156273e-05,\n",
      "      \"loss\": 0.0295,\n",
      "      \"step\": 72860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.592331201361102,\n",
      "      \"grad_norm\": 0.002803012728691101,\n",
      "      \"learning_rate\": 2.70478890989288e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 72880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.593591480512933,\n",
      "      \"grad_norm\": 0.008877026848495007,\n",
      "      \"learning_rate\": 2.7041587901701328e-05,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 72900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.594851759664766,\n",
      "      \"grad_norm\": 0.004379122518002987,\n",
      "      \"learning_rate\": 2.703528670447385e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 72920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.596112038816598,\n",
      "      \"grad_norm\": 38.75321578979492,\n",
      "      \"learning_rate\": 2.702898550724638e-05,\n",
      "      \"loss\": 0.0801,\n",
      "      \"step\": 72940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.59737231796843,\n",
      "      \"grad_norm\": 0.5329615473747253,\n",
      "      \"learning_rate\": 2.7022684310018905e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 72960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.598632597120262,\n",
      "      \"grad_norm\": 0.16243281960487366,\n",
      "      \"learning_rate\": 2.7016383112791434e-05,\n",
      "      \"loss\": 0.0183,\n",
      "      \"step\": 72980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.5998928762720945,\n",
      "      \"grad_norm\": 0.05071435868740082,\n",
      "      \"learning_rate\": 2.7010081915563956e-05,\n",
      "      \"loss\": 0.0246,\n",
      "      \"step\": 73000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.601153155423926,\n",
      "      \"grad_norm\": 0.005315368063747883,\n",
      "      \"learning_rate\": 2.7003780718336485e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 73020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.602413434575759,\n",
      "      \"grad_norm\": 0.18248969316482544,\n",
      "      \"learning_rate\": 2.699747952110901e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 73040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.6036737137275905,\n",
      "      \"grad_norm\": 0.009654749184846878,\n",
      "      \"learning_rate\": 2.699117832388154e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 73060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.604933992879423,\n",
      "      \"grad_norm\": 0.0009140155743807554,\n",
      "      \"learning_rate\": 2.698487712665407e-05,\n",
      "      \"loss\": 0.0168,\n",
      "      \"step\": 73080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.606194272031255,\n",
      "      \"grad_norm\": 0.10577467083930969,\n",
      "      \"learning_rate\": 2.697857592942659e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 73100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.607454551183087,\n",
      "      \"grad_norm\": 0.05219816416501999,\n",
      "      \"learning_rate\": 2.697227473219912e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 73120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.608714830334919,\n",
      "      \"grad_norm\": 0.0005485651199705899,\n",
      "      \"learning_rate\": 2.6965973534971646e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 73140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.609975109486752,\n",
      "      \"grad_norm\": 0.18674376606941223,\n",
      "      \"learning_rate\": 2.6959987397605547e-05,\n",
      "      \"loss\": 0.0348,\n",
      "      \"step\": 73160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.611235388638583,\n",
      "      \"grad_norm\": 0.004448567982763052,\n",
      "      \"learning_rate\": 2.6953686200378076e-05,\n",
      "      \"loss\": 0.0057,\n",
      "      \"step\": 73180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.612495667790416,\n",
      "      \"grad_norm\": 11.12244987487793,\n",
      "      \"learning_rate\": 2.6947385003150598e-05,\n",
      "      \"loss\": 0.0261,\n",
      "      \"step\": 73200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.6137559469422476,\n",
      "      \"grad_norm\": 0.045069869607686996,\n",
      "      \"learning_rate\": 2.6941083805923127e-05,\n",
      "      \"loss\": 0.0319,\n",
      "      \"step\": 73220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.61501622609408,\n",
      "      \"grad_norm\": 0.06396836787462234,\n",
      "      \"learning_rate\": 2.6934782608695653e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 73240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.616276505245912,\n",
      "      \"grad_norm\": 0.014643036760389805,\n",
      "      \"learning_rate\": 2.692848141146818e-05,\n",
      "      \"loss\": 0.04,\n",
      "      \"step\": 73260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.617536784397744,\n",
      "      \"grad_norm\": 5.1231303215026855,\n",
      "      \"learning_rate\": 2.6922180214240704e-05,\n",
      "      \"loss\": 0.0504,\n",
      "      \"step\": 73280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.618797063549576,\n",
      "      \"grad_norm\": 0.01799629256129265,\n",
      "      \"learning_rate\": 2.6915879017013236e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 73300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.620057342701409,\n",
      "      \"grad_norm\": 0.000652891700156033,\n",
      "      \"learning_rate\": 2.690957781978576e-05,\n",
      "      \"loss\": 0.0129,\n",
      "      \"step\": 73320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.62131762185324,\n",
      "      \"grad_norm\": 0.007879885844886303,\n",
      "      \"learning_rate\": 2.6903276622558288e-05,\n",
      "      \"loss\": 0.0416,\n",
      "      \"step\": 73340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.622577901005073,\n",
      "      \"grad_norm\": 0.35492008924484253,\n",
      "      \"learning_rate\": 2.689697542533081e-05,\n",
      "      \"loss\": 0.0326,\n",
      "      \"step\": 73360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.623838180156905,\n",
      "      \"grad_norm\": 0.05003349855542183,\n",
      "      \"learning_rate\": 2.6890674228103342e-05,\n",
      "      \"loss\": 0.0196,\n",
      "      \"step\": 73380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.625098459308737,\n",
      "      \"grad_norm\": 0.007916148751974106,\n",
      "      \"learning_rate\": 2.688437303087587e-05,\n",
      "      \"loss\": 0.0171,\n",
      "      \"step\": 73400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.626358738460569,\n",
      "      \"grad_norm\": 0.0014985332963988185,\n",
      "      \"learning_rate\": 2.6878071833648394e-05,\n",
      "      \"loss\": 0.0259,\n",
      "      \"step\": 73420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.6276190176124015,\n",
      "      \"grad_norm\": 0.001431826502084732,\n",
      "      \"learning_rate\": 2.6871770636420923e-05,\n",
      "      \"loss\": 0.0267,\n",
      "      \"step\": 73440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.628879296764233,\n",
      "      \"grad_norm\": 0.959781289100647,\n",
      "      \"learning_rate\": 2.6865469439193448e-05,\n",
      "      \"loss\": 0.0051,\n",
      "      \"step\": 73460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.630139575916066,\n",
      "      \"grad_norm\": 0.005236033815890551,\n",
      "      \"learning_rate\": 2.6859168241965977e-05,\n",
      "      \"loss\": 0.0298,\n",
      "      \"step\": 73480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.631399855067897,\n",
      "      \"grad_norm\": 0.1302349716424942,\n",
      "      \"learning_rate\": 2.68528670447385e-05,\n",
      "      \"loss\": 0.0477,\n",
      "      \"step\": 73500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.63266013421973,\n",
      "      \"grad_norm\": 0.0025578050408512354,\n",
      "      \"learning_rate\": 2.684656584751103e-05,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 73520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.633920413371562,\n",
      "      \"grad_norm\": 0.01704760082066059,\n",
      "      \"learning_rate\": 2.6840264650283554e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 73540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.635180692523394,\n",
      "      \"grad_norm\": 0.023052005097270012,\n",
      "      \"learning_rate\": 2.6833963453056083e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 73560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.636440971675226,\n",
      "      \"grad_norm\": 0.05366508662700653,\n",
      "      \"learning_rate\": 2.6827662255828606e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 73580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.6377012508270585,\n",
      "      \"grad_norm\": 0.003339990973472595,\n",
      "      \"learning_rate\": 2.6821361058601135e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 73600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.63896152997889,\n",
      "      \"grad_norm\": 0.0003926470526494086,\n",
      "      \"learning_rate\": 2.6815059861373664e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 73620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.640221809130723,\n",
      "      \"grad_norm\": 0.001610128441825509,\n",
      "      \"learning_rate\": 2.680875866414619e-05,\n",
      "      \"loss\": 0.0122,\n",
      "      \"step\": 73640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.6414820882825545,\n",
      "      \"grad_norm\": 0.0024967347271740437,\n",
      "      \"learning_rate\": 2.680245746691872e-05,\n",
      "      \"loss\": 0.036,\n",
      "      \"step\": 73660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.642742367434387,\n",
      "      \"grad_norm\": 0.2064417451620102,\n",
      "      \"learning_rate\": 2.679615626969124e-05,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 73680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.644002646586219,\n",
      "      \"grad_norm\": 0.027694523334503174,\n",
      "      \"learning_rate\": 2.678985507246377e-05,\n",
      "      \"loss\": 0.0135,\n",
      "      \"step\": 73700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.645262925738051,\n",
      "      \"grad_norm\": 5.03347635269165,\n",
      "      \"learning_rate\": 2.6783553875236295e-05,\n",
      "      \"loss\": 0.0238,\n",
      "      \"step\": 73720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.646523204889883,\n",
      "      \"grad_norm\": 0.004946321714669466,\n",
      "      \"learning_rate\": 2.6777252678008824e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 73740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.647783484041716,\n",
      "      \"grad_norm\": 0.0008020175155252218,\n",
      "      \"learning_rate\": 2.6770951480781347e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 73760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.649043763193547,\n",
      "      \"grad_norm\": 0.00034259865060448647,\n",
      "      \"learning_rate\": 2.6764650283553876e-05,\n",
      "      \"loss\": 0.0145,\n",
      "      \"step\": 73780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.65030404234538,\n",
      "      \"grad_norm\": 0.08622370660305023,\n",
      "      \"learning_rate\": 2.67583490863264e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 73800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.6515643214972116,\n",
      "      \"grad_norm\": 0.051380082964897156,\n",
      "      \"learning_rate\": 2.675204788909893e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 73820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.652824600649044,\n",
      "      \"grad_norm\": 4.510255336761475,\n",
      "      \"learning_rate\": 2.674574669187146e-05,\n",
      "      \"loss\": 0.0563,\n",
      "      \"step\": 73840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.654084879800876,\n",
      "      \"grad_norm\": 0.000591911724768579,\n",
      "      \"learning_rate\": 2.673944549464398e-05,\n",
      "      \"loss\": 0.0029,\n",
      "      \"step\": 73860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.655345158952708,\n",
      "      \"grad_norm\": 66.67544555664062,\n",
      "      \"learning_rate\": 2.6733144297416514e-05,\n",
      "      \"loss\": 0.0049,\n",
      "      \"step\": 73880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.65660543810454,\n",
      "      \"grad_norm\": 0.000333717733155936,\n",
      "      \"learning_rate\": 2.6726843100189036e-05,\n",
      "      \"loss\": 0.0221,\n",
      "      \"step\": 73900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.657865717256373,\n",
      "      \"grad_norm\": 0.00028243716224096715,\n",
      "      \"learning_rate\": 2.6720541902961565e-05,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 73920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.659125996408204,\n",
      "      \"grad_norm\": 1.0019420385360718,\n",
      "      \"learning_rate\": 2.671424070573409e-05,\n",
      "      \"loss\": 0.0191,\n",
      "      \"step\": 73940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.660386275560037,\n",
      "      \"grad_norm\": 0.00025268917670473456,\n",
      "      \"learning_rate\": 2.670793950850662e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 73960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.661646554711869,\n",
      "      \"grad_norm\": 0.001257267314940691,\n",
      "      \"learning_rate\": 2.6701638311279142e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 73980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.662906833863701,\n",
      "      \"grad_norm\": 0.05106845125555992,\n",
      "      \"learning_rate\": 2.669533711405167e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 74000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.664167113015533,\n",
      "      \"grad_norm\": 0.00039679763722233474,\n",
      "      \"learning_rate\": 2.6689035916824197e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 74020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.665427392167365,\n",
      "      \"grad_norm\": 13.541625022888184,\n",
      "      \"learning_rate\": 2.6682734719596726e-05,\n",
      "      \"loss\": 0.0231,\n",
      "      \"step\": 74040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.666687671319197,\n",
      "      \"grad_norm\": 0.0006881184526719153,\n",
      "      \"learning_rate\": 2.6676433522369255e-05,\n",
      "      \"loss\": 0.0723,\n",
      "      \"step\": 74060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.66794795047103,\n",
      "      \"grad_norm\": 0.007438955828547478,\n",
      "      \"learning_rate\": 2.6670132325141777e-05,\n",
      "      \"loss\": 0.0062,\n",
      "      \"step\": 74080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.669208229622861,\n",
      "      \"grad_norm\": 0.004346105270087719,\n",
      "      \"learning_rate\": 2.6663831127914306e-05,\n",
      "      \"loss\": 0.0213,\n",
      "      \"step\": 74100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.670468508774693,\n",
      "      \"grad_norm\": 0.18644018471240997,\n",
      "      \"learning_rate\": 2.6657529930686832e-05,\n",
      "      \"loss\": 0.0235,\n",
      "      \"step\": 74120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.671728787926526,\n",
      "      \"grad_norm\": 0.5778573751449585,\n",
      "      \"learning_rate\": 2.665122873345936e-05,\n",
      "      \"loss\": 0.025,\n",
      "      \"step\": 74140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.672989067078358,\n",
      "      \"grad_norm\": 0.007494122721254826,\n",
      "      \"learning_rate\": 2.6644927536231883e-05,\n",
      "      \"loss\": 0.0067,\n",
      "      \"step\": 74160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.67424934623019,\n",
      "      \"grad_norm\": 0.03903728350996971,\n",
      "      \"learning_rate\": 2.6638626339004412e-05,\n",
      "      \"loss\": 0.0338,\n",
      "      \"step\": 74180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.675509625382022,\n",
      "      \"grad_norm\": 0.002784537384286523,\n",
      "      \"learning_rate\": 2.6632325141776938e-05,\n",
      "      \"loss\": 0.0393,\n",
      "      \"step\": 74200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.676769904533854,\n",
      "      \"grad_norm\": 0.012371594086289406,\n",
      "      \"learning_rate\": 2.6626023944549467e-05,\n",
      "      \"loss\": 0.0388,\n",
      "      \"step\": 74220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.678030183685687,\n",
      "      \"grad_norm\": 0.04716303199529648,\n",
      "      \"learning_rate\": 2.661972274732199e-05,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 74240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.6792904628375185,\n",
      "      \"grad_norm\": 0.26172932982444763,\n",
      "      \"learning_rate\": 2.661342155009452e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 74260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.68055074198935,\n",
      "      \"grad_norm\": 0.0005435688653960824,\n",
      "      \"learning_rate\": 2.6607120352867047e-05,\n",
      "      \"loss\": 0.0067,\n",
      "      \"step\": 74280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.681811021141183,\n",
      "      \"grad_norm\": 0.03986625745892525,\n",
      "      \"learning_rate\": 2.6600819155639573e-05,\n",
      "      \"loss\": 0.0368,\n",
      "      \"step\": 74300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.683071300293015,\n",
      "      \"grad_norm\": 0.6077287793159485,\n",
      "      \"learning_rate\": 2.6594517958412102e-05,\n",
      "      \"loss\": 0.0529,\n",
      "      \"step\": 74320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.684331579444847,\n",
      "      \"grad_norm\": 11.11385726928711,\n",
      "      \"learning_rate\": 2.6588216761184624e-05,\n",
      "      \"loss\": 0.0426,\n",
      "      \"step\": 74340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.685591858596679,\n",
      "      \"grad_norm\": 0.03157292306423187,\n",
      "      \"learning_rate\": 2.6581915563957153e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 74360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.686852137748511,\n",
      "      \"grad_norm\": 0.06386951357126236,\n",
      "      \"learning_rate\": 2.657561436672968e-05,\n",
      "      \"loss\": 0.0264,\n",
      "      \"step\": 74380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.688112416900344,\n",
      "      \"grad_norm\": 0.0025276951491832733,\n",
      "      \"learning_rate\": 2.6569313169502208e-05,\n",
      "      \"loss\": 0.0324,\n",
      "      \"step\": 74400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.6893726960521755,\n",
      "      \"grad_norm\": 0.13917021453380585,\n",
      "      \"learning_rate\": 2.656301197227473e-05,\n",
      "      \"loss\": 0.0305,\n",
      "      \"step\": 74420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.690632975204007,\n",
      "      \"grad_norm\": 0.34615153074264526,\n",
      "      \"learning_rate\": 2.655671077504726e-05,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 74440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.69189325435584,\n",
      "      \"grad_norm\": 0.004303103778511286,\n",
      "      \"learning_rate\": 2.6550409577819785e-05,\n",
      "      \"loss\": 0.0369,\n",
      "      \"step\": 74460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.693153533507672,\n",
      "      \"grad_norm\": 11.690462112426758,\n",
      "      \"learning_rate\": 2.6544108380592314e-05,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 74480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.694413812659504,\n",
      "      \"grad_norm\": 0.0007031536661088467,\n",
      "      \"learning_rate\": 2.6537807183364836e-05,\n",
      "      \"loss\": 0.0099,\n",
      "      \"step\": 74500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.695674091811336,\n",
      "      \"grad_norm\": 0.01317709032446146,\n",
      "      \"learning_rate\": 2.653150598613737e-05,\n",
      "      \"loss\": 0.0458,\n",
      "      \"step\": 74520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.696934370963168,\n",
      "      \"grad_norm\": 0.035231076180934906,\n",
      "      \"learning_rate\": 2.6525204788909898e-05,\n",
      "      \"loss\": 0.0237,\n",
      "      \"step\": 74540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.698194650115001,\n",
      "      \"grad_norm\": 0.021876085549592972,\n",
      "      \"learning_rate\": 2.651890359168242e-05,\n",
      "      \"loss\": 0.0493,\n",
      "      \"step\": 74560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.699454929266833,\n",
      "      \"grad_norm\": 0.0876859650015831,\n",
      "      \"learning_rate\": 2.651260239445495e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 74580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.700715208418664,\n",
      "      \"grad_norm\": 0.01907515525817871,\n",
      "      \"learning_rate\": 2.6506616257088847e-05,\n",
      "      \"loss\": 0.0359,\n",
      "      \"step\": 74600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.701975487570497,\n",
      "      \"grad_norm\": 0.24122078716754913,\n",
      "      \"learning_rate\": 2.6500315059861376e-05,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 74620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.7032357667223295,\n",
      "      \"grad_norm\": 0.0027657062746584415,\n",
      "      \"learning_rate\": 2.6494013862633905e-05,\n",
      "      \"loss\": 0.0175,\n",
      "      \"step\": 74640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.704496045874161,\n",
      "      \"grad_norm\": 0.004813849460333586,\n",
      "      \"learning_rate\": 2.6487712665406427e-05,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 74660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.705756325025993,\n",
      "      \"grad_norm\": 0.0723280981183052,\n",
      "      \"learning_rate\": 2.6481411468178956e-05,\n",
      "      \"loss\": 0.0205,\n",
      "      \"step\": 74680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.707016604177825,\n",
      "      \"grad_norm\": 0.006935292389243841,\n",
      "      \"learning_rate\": 2.647511027095148e-05,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 74700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.708276883329658,\n",
      "      \"grad_norm\": 0.0014157664263620973,\n",
      "      \"learning_rate\": 2.646880907372401e-05,\n",
      "      \"loss\": 0.0319,\n",
      "      \"step\": 74720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.70953716248149,\n",
      "      \"grad_norm\": 0.021000344306230545,\n",
      "      \"learning_rate\": 2.6462507876496533e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 74740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.710797441633321,\n",
      "      \"grad_norm\": 0.04320303723216057,\n",
      "      \"learning_rate\": 2.6456206679269062e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 74760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.712057720785154,\n",
      "      \"grad_norm\": 0.024169713258743286,\n",
      "      \"learning_rate\": 2.6449905482041588e-05,\n",
      "      \"loss\": 0.0657,\n",
      "      \"step\": 74780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.7133179999369865,\n",
      "      \"grad_norm\": 0.0009361733100377023,\n",
      "      \"learning_rate\": 2.6443604284814117e-05,\n",
      "      \"loss\": 0.0275,\n",
      "      \"step\": 74800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.714578279088818,\n",
      "      \"grad_norm\": 0.010079550556838512,\n",
      "      \"learning_rate\": 2.643730308758664e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 74820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.71583855824065,\n",
      "      \"grad_norm\": 0.005457385443150997,\n",
      "      \"learning_rate\": 2.6431001890359168e-05,\n",
      "      \"loss\": 0.0256,\n",
      "      \"step\": 74840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.7170988373924825,\n",
      "      \"grad_norm\": 0.0013682030839845538,\n",
      "      \"learning_rate\": 2.64247006931317e-05,\n",
      "      \"loss\": 0.0384,\n",
      "      \"step\": 74860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.718359116544315,\n",
      "      \"grad_norm\": 8.335282325744629,\n",
      "      \"learning_rate\": 2.6418399495904223e-05,\n",
      "      \"loss\": 0.002,\n",
      "      \"step\": 74880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.719619395696147,\n",
      "      \"grad_norm\": 0.002134388778358698,\n",
      "      \"learning_rate\": 2.641209829867675e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 74900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.720879674847978,\n",
      "      \"grad_norm\": 0.004256082233041525,\n",
      "      \"learning_rate\": 2.6405797101449274e-05,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 74920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.722139953999811,\n",
      "      \"grad_norm\": 0.030631158500909805,\n",
      "      \"learning_rate\": 2.6399495904221806e-05,\n",
      "      \"loss\": 0.0331,\n",
      "      \"step\": 74940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.723400233151643,\n",
      "      \"grad_norm\": 1.587031602859497,\n",
      "      \"learning_rate\": 2.639319470699433e-05,\n",
      "      \"loss\": 0.0309,\n",
      "      \"step\": 74960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.724660512303475,\n",
      "      \"grad_norm\": 0.0034699363168329,\n",
      "      \"learning_rate\": 2.6386893509766858e-05,\n",
      "      \"loss\": 0.0041,\n",
      "      \"step\": 74980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.725920791455307,\n",
      "      \"grad_norm\": 0.010336029343307018,\n",
      "      \"learning_rate\": 2.6380592312539383e-05,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 75000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.7271810706071395,\n",
      "      \"grad_norm\": 7.584312915802002,\n",
      "      \"learning_rate\": 2.6374291115311912e-05,\n",
      "      \"loss\": 0.0076,\n",
      "      \"step\": 75020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.728441349758971,\n",
      "      \"grad_norm\": 0.0006831528153270483,\n",
      "      \"learning_rate\": 2.6367989918084435e-05,\n",
      "      \"loss\": 0.0248,\n",
      "      \"step\": 75040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.729701628910804,\n",
      "      \"grad_norm\": 0.02226456254720688,\n",
      "      \"learning_rate\": 2.6361688720856964e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 75060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.7309619080626355,\n",
      "      \"grad_norm\": 0.33433064818382263,\n",
      "      \"learning_rate\": 2.6355387523629493e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 75080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.732222187214468,\n",
      "      \"grad_norm\": 0.044597137719392776,\n",
      "      \"learning_rate\": 2.634908632640202e-05,\n",
      "      \"loss\": 0.0166,\n",
      "      \"step\": 75100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.7334824663663,\n",
      "      \"grad_norm\": 0.05417667329311371,\n",
      "      \"learning_rate\": 2.6342785129174547e-05,\n",
      "      \"loss\": 0.0221,\n",
      "      \"step\": 75120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.734742745518132,\n",
      "      \"grad_norm\": 0.011970879510045052,\n",
      "      \"learning_rate\": 2.633648393194707e-05,\n",
      "      \"loss\": 0.0279,\n",
      "      \"step\": 75140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.736003024669964,\n",
      "      \"grad_norm\": 0.13557304441928864,\n",
      "      \"learning_rate\": 2.63301827347196e-05,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 75160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.737263303821797,\n",
      "      \"grad_norm\": 0.001245254883542657,\n",
      "      \"learning_rate\": 2.6323881537492124e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 75180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.738523582973628,\n",
      "      \"grad_norm\": 0.09003230184316635,\n",
      "      \"learning_rate\": 2.6317580340264653e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 75200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.739783862125461,\n",
      "      \"grad_norm\": 0.0003988436656072736,\n",
      "      \"learning_rate\": 2.6311279143037176e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 75220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.741044141277293,\n",
      "      \"grad_norm\": 0.00023942870029713959,\n",
      "      \"learning_rate\": 2.6304977945809705e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 75240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.742304420429125,\n",
      "      \"grad_norm\": 0.021661778911948204,\n",
      "      \"learning_rate\": 2.629867674858223e-05,\n",
      "      \"loss\": 0.026,\n",
      "      \"step\": 75260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.743564699580957,\n",
      "      \"grad_norm\": 0.008197014220058918,\n",
      "      \"learning_rate\": 2.629237555135476e-05,\n",
      "      \"loss\": 0.0528,\n",
      "      \"step\": 75280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.744824978732789,\n",
      "      \"grad_norm\": 0.10846196860074997,\n",
      "      \"learning_rate\": 2.628607435412729e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 75300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.746085257884621,\n",
      "      \"grad_norm\": 0.08537527918815613,\n",
      "      \"learning_rate\": 2.627977315689981e-05,\n",
      "      \"loss\": 0.0267,\n",
      "      \"step\": 75320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.747345537036454,\n",
      "      \"grad_norm\": 0.004375996999442577,\n",
      "      \"learning_rate\": 2.627347195967234e-05,\n",
      "      \"loss\": 0.0061,\n",
      "      \"step\": 75340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.748605816188285,\n",
      "      \"grad_norm\": 0.0030561976600438356,\n",
      "      \"learning_rate\": 2.6267170762444865e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 75360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.749866095340118,\n",
      "      \"grad_norm\": 0.002540404675528407,\n",
      "      \"learning_rate\": 2.6260869565217394e-05,\n",
      "      \"loss\": 0.0302,\n",
      "      \"step\": 75380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.75112637449195,\n",
      "      \"grad_norm\": 0.016601338982582092,\n",
      "      \"learning_rate\": 2.6254568367989917e-05,\n",
      "      \"loss\": 0.0278,\n",
      "      \"step\": 75400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.752386653643782,\n",
      "      \"grad_norm\": 3.1775619983673096,\n",
      "      \"learning_rate\": 2.6248267170762446e-05,\n",
      "      \"loss\": 0.0775,\n",
      "      \"step\": 75420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.753646932795614,\n",
      "      \"grad_norm\": 5.8723249435424805,\n",
      "      \"learning_rate\": 2.624196597353497e-05,\n",
      "      \"loss\": 0.0282,\n",
      "      \"step\": 75440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.7549072119474465,\n",
      "      \"grad_norm\": 2.2162234783172607,\n",
      "      \"learning_rate\": 2.62356647763075e-05,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 75460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.756167491099278,\n",
      "      \"grad_norm\": 0.10754580795764923,\n",
      "      \"learning_rate\": 2.6229363579080023e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 75480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.757427770251111,\n",
      "      \"grad_norm\": 0.15394456684589386,\n",
      "      \"learning_rate\": 2.6223062381852552e-05,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 75500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.758688049402942,\n",
      "      \"grad_norm\": 0.010315944440662861,\n",
      "      \"learning_rate\": 2.6216761184625084e-05,\n",
      "      \"loss\": 0.02,\n",
      "      \"step\": 75520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.759948328554775,\n",
      "      \"grad_norm\": 0.4180658757686615,\n",
      "      \"learning_rate\": 2.6210459987397606e-05,\n",
      "      \"loss\": 0.034,\n",
      "      \"step\": 75540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.761208607706607,\n",
      "      \"grad_norm\": 0.0032406894024461508,\n",
      "      \"learning_rate\": 2.6204158790170135e-05,\n",
      "      \"loss\": 0.0173,\n",
      "      \"step\": 75560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.762468886858439,\n",
      "      \"grad_norm\": 0.012462936341762543,\n",
      "      \"learning_rate\": 2.619785759294266e-05,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 75580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.763729166010271,\n",
      "      \"grad_norm\": 0.003613463370129466,\n",
      "      \"learning_rate\": 2.619155639571519e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 75600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.7649894451621035,\n",
      "      \"grad_norm\": 0.10332510620355606,\n",
      "      \"learning_rate\": 2.6185255198487712e-05,\n",
      "      \"loss\": 0.0375,\n",
      "      \"step\": 75620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.766249724313935,\n",
      "      \"grad_norm\": 0.13686522841453552,\n",
      "      \"learning_rate\": 2.617895400126024e-05,\n",
      "      \"loss\": 0.0366,\n",
      "      \"step\": 75640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.767510003465768,\n",
      "      \"grad_norm\": 0.013573994860053062,\n",
      "      \"learning_rate\": 2.6172652804032767e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 75660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.7687702826175995,\n",
      "      \"grad_norm\": 0.12767904996871948,\n",
      "      \"learning_rate\": 2.6166351606805296e-05,\n",
      "      \"loss\": 0.0257,\n",
      "      \"step\": 75680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.770030561769432,\n",
      "      \"grad_norm\": 6.992685317993164,\n",
      "      \"learning_rate\": 2.616005040957782e-05,\n",
      "      \"loss\": 0.0222,\n",
      "      \"step\": 75700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.771290840921264,\n",
      "      \"grad_norm\": 0.09008616209030151,\n",
      "      \"learning_rate\": 2.6153749212350347e-05,\n",
      "      \"loss\": 0.0115,\n",
      "      \"step\": 75720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.772551120073096,\n",
      "      \"grad_norm\": 0.013829306699335575,\n",
      "      \"learning_rate\": 2.6147448015122876e-05,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 75740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.773811399224928,\n",
      "      \"grad_norm\": 0.003958710934966803,\n",
      "      \"learning_rate\": 2.6141146817895402e-05,\n",
      "      \"loss\": 0.0331,\n",
      "      \"step\": 75760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.775071678376761,\n",
      "      \"grad_norm\": 0.004756922367960215,\n",
      "      \"learning_rate\": 2.613484562066793e-05,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 75780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.776331957528592,\n",
      "      \"grad_norm\": 0.0030032796785235405,\n",
      "      \"learning_rate\": 2.6128544423440453e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 75800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.777592236680425,\n",
      "      \"grad_norm\": 0.1440538465976715,\n",
      "      \"learning_rate\": 2.6122243226212982e-05,\n",
      "      \"loss\": 0.0171,\n",
      "      \"step\": 75820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.778852515832257,\n",
      "      \"grad_norm\": 0.40839648246765137,\n",
      "      \"learning_rate\": 2.6115942028985508e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 75840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.780112794984089,\n",
      "      \"grad_norm\": 0.03394690901041031,\n",
      "      \"learning_rate\": 2.6109640831758037e-05,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 75860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.781373074135921,\n",
      "      \"grad_norm\": 0.00026795759913511574,\n",
      "      \"learning_rate\": 2.610333963453056e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 75880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.782633353287753,\n",
      "      \"grad_norm\": 0.725412130355835,\n",
      "      \"learning_rate\": 2.609703843730309e-05,\n",
      "      \"loss\": 0.0379,\n",
      "      \"step\": 75900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.783893632439585,\n",
      "      \"grad_norm\": 0.0029682465828955173,\n",
      "      \"learning_rate\": 2.6090737240075614e-05,\n",
      "      \"loss\": 0.0353,\n",
      "      \"step\": 75920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.785153911591418,\n",
      "      \"grad_norm\": 0.0007331332308240235,\n",
      "      \"learning_rate\": 2.6084436042848143e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 75940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.786414190743249,\n",
      "      \"grad_norm\": 1.0569424629211426,\n",
      "      \"learning_rate\": 2.6078134845620665e-05,\n",
      "      \"loss\": 0.0245,\n",
      "      \"step\": 75960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.787674469895082,\n",
      "      \"grad_norm\": 36.2298698425293,\n",
      "      \"learning_rate\": 2.6071833648393194e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 75980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.788934749046914,\n",
      "      \"grad_norm\": 0.0026736692525446415,\n",
      "      \"learning_rate\": 2.6065532451165723e-05,\n",
      "      \"loss\": 0.0765,\n",
      "      \"step\": 76000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.790195028198746,\n",
      "      \"grad_norm\": 0.11491447687149048,\n",
      "      \"learning_rate\": 2.605923125393825e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 76020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.791455307350578,\n",
      "      \"grad_norm\": 3.605926275253296,\n",
      "      \"learning_rate\": 2.6052930056710778e-05,\n",
      "      \"loss\": 0.0212,\n",
      "      \"step\": 76040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.7927155865024105,\n",
      "      \"grad_norm\": 12.321725845336914,\n",
      "      \"learning_rate\": 2.60466288594833e-05,\n",
      "      \"loss\": 0.026,\n",
      "      \"step\": 76060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.793975865654242,\n",
      "      \"grad_norm\": 0.0006866106414236128,\n",
      "      \"learning_rate\": 2.6040327662255833e-05,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 76080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.795236144806075,\n",
      "      \"grad_norm\": 0.0002590554649941623,\n",
      "      \"learning_rate\": 2.6034026465028355e-05,\n",
      "      \"loss\": 0.0036,\n",
      "      \"step\": 76100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.796496423957906,\n",
      "      \"grad_norm\": 0.024887321516871452,\n",
      "      \"learning_rate\": 2.6027725267800884e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 76120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.797756703109739,\n",
      "      \"grad_norm\": 0.0002147868217434734,\n",
      "      \"learning_rate\": 2.6021424070573406e-05,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 76140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.799016982261571,\n",
      "      \"grad_norm\": 0.0013338766293600202,\n",
      "      \"learning_rate\": 2.601512287334594e-05,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 76160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.800277261413403,\n",
      "      \"grad_norm\": 0.00023469669395126402,\n",
      "      \"learning_rate\": 2.600882167611846e-05,\n",
      "      \"loss\": 0.0125,\n",
      "      \"step\": 76180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.801537540565235,\n",
      "      \"grad_norm\": 0.026168782263994217,\n",
      "      \"learning_rate\": 2.600252047889099e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 76200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.8027978197170675,\n",
      "      \"grad_norm\": 0.00025391177041456103,\n",
      "      \"learning_rate\": 2.599621928166352e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 76220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.804058098868899,\n",
      "      \"grad_norm\": 0.0004505787219386548,\n",
      "      \"learning_rate\": 2.5989918084436045e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 76240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.805318378020732,\n",
      "      \"grad_norm\": 0.00037458137376233935,\n",
      "      \"learning_rate\": 2.5983616887208574e-05,\n",
      "      \"loss\": 0.0321,\n",
      "      \"step\": 76260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.8065786571725635,\n",
      "      \"grad_norm\": 0.0001747244386933744,\n",
      "      \"learning_rate\": 2.5977315689981096e-05,\n",
      "      \"loss\": 0.0241,\n",
      "      \"step\": 76280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.807838936324396,\n",
      "      \"grad_norm\": 0.0001566127029946074,\n",
      "      \"learning_rate\": 2.5971014492753625e-05,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 76300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.809099215476228,\n",
      "      \"grad_norm\": 5.607336044311523,\n",
      "      \"learning_rate\": 2.596471329552615e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 76320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.81035949462806,\n",
      "      \"grad_norm\": 0.0015772017650306225,\n",
      "      \"learning_rate\": 2.595841209829868e-05,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 76340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.811619773779892,\n",
      "      \"grad_norm\": 0.0003002874436788261,\n",
      "      \"learning_rate\": 2.5952110901071202e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 76360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.812880052931725,\n",
      "      \"grad_norm\": 0.00014786759857088327,\n",
      "      \"learning_rate\": 2.594580970384373e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 76380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.814140332083556,\n",
      "      \"grad_norm\": 0.00491114379838109,\n",
      "      \"learning_rate\": 2.5939508506616257e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 76400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.815400611235389,\n",
      "      \"grad_norm\": 0.050667546689510345,\n",
      "      \"learning_rate\": 2.5933207309388786e-05,\n",
      "      \"loss\": 0.0532,\n",
      "      \"step\": 76420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.816660890387221,\n",
      "      \"grad_norm\": 0.006005278788506985,\n",
      "      \"learning_rate\": 2.5926906112161315e-05,\n",
      "      \"loss\": 0.0191,\n",
      "      \"step\": 76440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.817921169539053,\n",
      "      \"grad_norm\": 0.038591258227825165,\n",
      "      \"learning_rate\": 2.5920604914933837e-05,\n",
      "      \"loss\": 0.0625,\n",
      "      \"step\": 76460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.819181448690885,\n",
      "      \"grad_norm\": 48.096954345703125,\n",
      "      \"learning_rate\": 2.5914303717706366e-05,\n",
      "      \"loss\": 0.0459,\n",
      "      \"step\": 76480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.820441727842717,\n",
      "      \"grad_norm\": 0.23383498191833496,\n",
      "      \"learning_rate\": 2.5908002520478892e-05,\n",
      "      \"loss\": 0.0166,\n",
      "      \"step\": 76500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.821702006994549,\n",
      "      \"grad_norm\": 0.00634965393692255,\n",
      "      \"learning_rate\": 2.590170132325142e-05,\n",
      "      \"loss\": 0.0451,\n",
      "      \"step\": 76520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.822962286146382,\n",
      "      \"grad_norm\": 0.001245074556209147,\n",
      "      \"learning_rate\": 2.5895400126023943e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 76540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.824222565298213,\n",
      "      \"grad_norm\": 3.8472886085510254,\n",
      "      \"learning_rate\": 2.5889098928796472e-05,\n",
      "      \"loss\": 0.0162,\n",
      "      \"step\": 76560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.825482844450046,\n",
      "      \"grad_norm\": 0.0011789801064878702,\n",
      "      \"learning_rate\": 2.5882797731568998e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 76580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.826743123601878,\n",
      "      \"grad_norm\": 0.021202117204666138,\n",
      "      \"learning_rate\": 2.5876496534341527e-05,\n",
      "      \"loss\": 0.0453,\n",
      "      \"step\": 76600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.82800340275371,\n",
      "      \"grad_norm\": 0.0730365440249443,\n",
      "      \"learning_rate\": 2.587019533711405e-05,\n",
      "      \"loss\": 0.0341,\n",
      "      \"step\": 76620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.829263681905542,\n",
      "      \"grad_norm\": 0.04069752246141434,\n",
      "      \"learning_rate\": 2.5863894139886578e-05,\n",
      "      \"loss\": 0.0476,\n",
      "      \"step\": 76640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.8305239610573745,\n",
      "      \"grad_norm\": 0.07725981622934341,\n",
      "      \"learning_rate\": 2.585759294265911e-05,\n",
      "      \"loss\": 0.0309,\n",
      "      \"step\": 76660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.831784240209206,\n",
      "      \"grad_norm\": 12.677987098693848,\n",
      "      \"learning_rate\": 2.5851291745431633e-05,\n",
      "      \"loss\": 0.0296,\n",
      "      \"step\": 76680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.833044519361039,\n",
      "      \"grad_norm\": 9.887280464172363,\n",
      "      \"learning_rate\": 2.5844990548204162e-05,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 76700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.83430479851287,\n",
      "      \"grad_norm\": 0.007393695879727602,\n",
      "      \"learning_rate\": 2.5838689350976684e-05,\n",
      "      \"loss\": 0.0444,\n",
      "      \"step\": 76720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.835565077664703,\n",
      "      \"grad_norm\": 0.010586131364107132,\n",
      "      \"learning_rate\": 2.5832388153749217e-05,\n",
      "      \"loss\": 0.0291,\n",
      "      \"step\": 76740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.836825356816535,\n",
      "      \"grad_norm\": 0.002750485436990857,\n",
      "      \"learning_rate\": 2.582608695652174e-05,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 76760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.838085635968367,\n",
      "      \"grad_norm\": 0.015038643963634968,\n",
      "      \"learning_rate\": 2.5819785759294268e-05,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 76780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.839345915120199,\n",
      "      \"grad_norm\": 0.0022046277299523354,\n",
      "      \"learning_rate\": 2.5813484562066794e-05,\n",
      "      \"loss\": 0.0353,\n",
      "      \"step\": 76800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.8406061942720315,\n",
      "      \"grad_norm\": 0.03082374483346939,\n",
      "      \"learning_rate\": 2.5807183364839323e-05,\n",
      "      \"loss\": 0.0523,\n",
      "      \"step\": 76820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.841866473423863,\n",
      "      \"grad_norm\": 0.010973082855343819,\n",
      "      \"learning_rate\": 2.5800882167611845e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 76840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.843126752575696,\n",
      "      \"grad_norm\": 0.06364282220602036,\n",
      "      \"learning_rate\": 2.5794580970384374e-05,\n",
      "      \"loss\": 0.0514,\n",
      "      \"step\": 76860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.8443870317275275,\n",
      "      \"grad_norm\": 0.014753196388483047,\n",
      "      \"learning_rate\": 2.5788279773156903e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 76880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.84564731087936,\n",
      "      \"grad_norm\": 0.016853736713528633,\n",
      "      \"learning_rate\": 2.578197857592943e-05,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 76900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.846907590031192,\n",
      "      \"grad_norm\": 0.025425460189580917,\n",
      "      \"learning_rate\": 2.5775677378701958e-05,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 76920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.848167869183024,\n",
      "      \"grad_norm\": 5.544378757476807,\n",
      "      \"learning_rate\": 2.576937618147448e-05,\n",
      "      \"loss\": 0.0446,\n",
      "      \"step\": 76940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.849428148334856,\n",
      "      \"grad_norm\": 0.05217714607715607,\n",
      "      \"learning_rate\": 2.576307498424701e-05,\n",
      "      \"loss\": 0.0345,\n",
      "      \"step\": 76960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.850688427486689,\n",
      "      \"grad_norm\": 0.008429622277617455,\n",
      "      \"learning_rate\": 2.5756773787019535e-05,\n",
      "      \"loss\": 0.0068,\n",
      "      \"step\": 76980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.85194870663852,\n",
      "      \"grad_norm\": 0.004174071829766035,\n",
      "      \"learning_rate\": 2.5750472589792064e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 77000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.853208985790353,\n",
      "      \"grad_norm\": 0.02386823296546936,\n",
      "      \"learning_rate\": 2.5744171392564586e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 77020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.854469264942185,\n",
      "      \"grad_norm\": 0.0170682854950428,\n",
      "      \"learning_rate\": 2.5737870195337115e-05,\n",
      "      \"loss\": 0.0385,\n",
      "      \"step\": 77040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.855729544094017,\n",
      "      \"grad_norm\": 0.044896069914102554,\n",
      "      \"learning_rate\": 2.573156899810964e-05,\n",
      "      \"loss\": 0.049,\n",
      "      \"step\": 77060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.856989823245849,\n",
      "      \"grad_norm\": 1.6359103918075562,\n",
      "      \"learning_rate\": 2.572526780088217e-05,\n",
      "      \"loss\": 0.005,\n",
      "      \"step\": 77080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.858250102397681,\n",
      "      \"grad_norm\": 0.13788579404354095,\n",
      "      \"learning_rate\": 2.57189666036547e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 77100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.859510381549513,\n",
      "      \"grad_norm\": 0.17279836535453796,\n",
      "      \"learning_rate\": 2.571266540642722e-05,\n",
      "      \"loss\": 0.0124,\n",
      "      \"step\": 77120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.860770660701346,\n",
      "      \"grad_norm\": 0.025902699679136276,\n",
      "      \"learning_rate\": 2.570636420919975e-05,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 77140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.862030939853177,\n",
      "      \"grad_norm\": 0.008130340836942196,\n",
      "      \"learning_rate\": 2.5700063011972276e-05,\n",
      "      \"loss\": 0.0166,\n",
      "      \"step\": 77160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.86329121900501,\n",
      "      \"grad_norm\": 0.04600914940237999,\n",
      "      \"learning_rate\": 2.5693761814744805e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 77180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.864551498156842,\n",
      "      \"grad_norm\": 0.9945614337921143,\n",
      "      \"learning_rate\": 2.5687460617517327e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 77200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.865811777308674,\n",
      "      \"grad_norm\": 0.0006495157722383738,\n",
      "      \"learning_rate\": 2.5681159420289856e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 77220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.867072056460506,\n",
      "      \"grad_norm\": 0.008532938547432423,\n",
      "      \"learning_rate\": 2.567485822306238e-05,\n",
      "      \"loss\": 0.0193,\n",
      "      \"step\": 77240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.8683323356123385,\n",
      "      \"grad_norm\": 0.001089276629500091,\n",
      "      \"learning_rate\": 2.566855702583491e-05,\n",
      "      \"loss\": 0.0055,\n",
      "      \"step\": 77260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.86959261476417,\n",
      "      \"grad_norm\": 0.0005678954184986651,\n",
      "      \"learning_rate\": 2.5662255828607433e-05,\n",
      "      \"loss\": 0.0198,\n",
      "      \"step\": 77280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.870852893916003,\n",
      "      \"grad_norm\": 0.003259817836806178,\n",
      "      \"learning_rate\": 2.5655954631379965e-05,\n",
      "      \"loss\": 0.0319,\n",
      "      \"step\": 77300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.872113173067834,\n",
      "      \"grad_norm\": 0.00045648388913832605,\n",
      "      \"learning_rate\": 2.5649653434152488e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 77320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.873373452219667,\n",
      "      \"grad_norm\": 0.000857802398968488,\n",
      "      \"learning_rate\": 2.5643352236925017e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 77340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.874633731371499,\n",
      "      \"grad_norm\": 0.05527951195836067,\n",
      "      \"learning_rate\": 2.5637051039697546e-05,\n",
      "      \"loss\": 0.0027,\n",
      "      \"step\": 77360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.875894010523331,\n",
      "      \"grad_norm\": 0.0010615610517561436,\n",
      "      \"learning_rate\": 2.563074984247007e-05,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 77380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.877154289675163,\n",
      "      \"grad_norm\": 0.008870085701346397,\n",
      "      \"learning_rate\": 2.56244486452426e-05,\n",
      "      \"loss\": 0.0077,\n",
      "      \"step\": 77400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.8784145688269955,\n",
      "      \"grad_norm\": 0.004647202789783478,\n",
      "      \"learning_rate\": 2.5618147448015123e-05,\n",
      "      \"loss\": 0.035,\n",
      "      \"step\": 77420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.879674847978827,\n",
      "      \"grad_norm\": 0.0013396220747381449,\n",
      "      \"learning_rate\": 2.5611846250787652e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 77440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.88093512713066,\n",
      "      \"grad_norm\": 0.0007376727880910039,\n",
      "      \"learning_rate\": 2.5605545053560177e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 77460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.8821954062824915,\n",
      "      \"grad_norm\": 0.0004165088466834277,\n",
      "      \"learning_rate\": 2.5599243856332706e-05,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 77480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.883455685434324,\n",
      "      \"grad_norm\": 0.010595382191240788,\n",
      "      \"learning_rate\": 2.559294265910523e-05,\n",
      "      \"loss\": 0.0326,\n",
      "      \"step\": 77500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.884715964586156,\n",
      "      \"grad_norm\": 0.0023389277048408985,\n",
      "      \"learning_rate\": 2.5586641461877758e-05,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 77520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.885976243737988,\n",
      "      \"grad_norm\": 0.00043239406659267843,\n",
      "      \"learning_rate\": 2.5580340264650283e-05,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 77540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.88723652288982,\n",
      "      \"grad_norm\": 0.02523372322320938,\n",
      "      \"learning_rate\": 2.5574039067422812e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 77560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.888496802041653,\n",
      "      \"grad_norm\": 0.0031473056878894567,\n",
      "      \"learning_rate\": 2.556773787019534e-05,\n",
      "      \"loss\": 0.0444,\n",
      "      \"step\": 77580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.889757081193484,\n",
      "      \"grad_norm\": 0.0008692408446222544,\n",
      "      \"learning_rate\": 2.5561436672967864e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 77600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.891017360345317,\n",
      "      \"grad_norm\": 0.03269890323281288,\n",
      "      \"learning_rate\": 2.5555135475740393e-05,\n",
      "      \"loss\": 0.0374,\n",
      "      \"step\": 77620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.8922776394971486,\n",
      "      \"grad_norm\": 0.006669057998806238,\n",
      "      \"learning_rate\": 2.554883427851292e-05,\n",
      "      \"loss\": 0.005,\n",
      "      \"step\": 77640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.893537918648981,\n",
      "      \"grad_norm\": 0.00045798279461450875,\n",
      "      \"learning_rate\": 2.5542533081285447e-05,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 77660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.894798197800813,\n",
      "      \"grad_norm\": 0.21386317908763885,\n",
      "      \"learning_rate\": 2.553623188405797e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 77680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.8960584769526445,\n",
      "      \"grad_norm\": 0.33738523721694946,\n",
      "      \"learning_rate\": 2.55299306868305e-05,\n",
      "      \"loss\": 0.0431,\n",
      "      \"step\": 77700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.897318756104477,\n",
      "      \"grad_norm\": 0.004232688806951046,\n",
      "      \"learning_rate\": 2.5523629489603024e-05,\n",
      "      \"loss\": 0.0194,\n",
      "      \"step\": 77720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.89857903525631,\n",
      "      \"grad_norm\": 0.04933858662843704,\n",
      "      \"learning_rate\": 2.5517328292375553e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 77740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.899839314408141,\n",
      "      \"grad_norm\": 0.0003944140626117587,\n",
      "      \"learning_rate\": 2.5511027095148076e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 77760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.901099593559973,\n",
      "      \"grad_norm\": 0.0003987878153566271,\n",
      "      \"learning_rate\": 2.5504725897920605e-05,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 77780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.902359872711806,\n",
      "      \"grad_norm\": 0.0340796560049057,\n",
      "      \"learning_rate\": 2.5498424700693134e-05,\n",
      "      \"loss\": 0.0156,\n",
      "      \"step\": 77800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.903620151863638,\n",
      "      \"grad_norm\": 0.00038052088348194957,\n",
      "      \"learning_rate\": 2.549212350346566e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 77820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.90488043101547,\n",
      "      \"grad_norm\": 0.0012537543661892414,\n",
      "      \"learning_rate\": 2.548582230623819e-05,\n",
      "      \"loss\": 0.0271,\n",
      "      \"step\": 77840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.906140710167302,\n",
      "      \"grad_norm\": 0.059458404779434204,\n",
      "      \"learning_rate\": 2.547952110901071e-05,\n",
      "      \"loss\": 0.005,\n",
      "      \"step\": 77860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.907400989319134,\n",
      "      \"grad_norm\": 0.0005510340561158955,\n",
      "      \"learning_rate\": 2.5473219911783243e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 77880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.908661268470967,\n",
      "      \"grad_norm\": 0.00851916242390871,\n",
      "      \"learning_rate\": 2.5466918714555765e-05,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 77900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.909921547622798,\n",
      "      \"grad_norm\": 0.0006736082723364234,\n",
      "      \"learning_rate\": 2.5460617517328294e-05,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 77920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.91118182677463,\n",
      "      \"grad_norm\": 0.0020558324176818132,\n",
      "      \"learning_rate\": 2.5454316320100817e-05,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 77940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.912442105926463,\n",
      "      \"grad_norm\": 0.005367637611925602,\n",
      "      \"learning_rate\": 2.544801512287335e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 77960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.913702385078295,\n",
      "      \"grad_norm\": 0.11021028459072113,\n",
      "      \"learning_rate\": 2.544171392564587e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 77980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.914962664230127,\n",
      "      \"grad_norm\": 0.0005711756530217826,\n",
      "      \"learning_rate\": 2.54354127284184e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 78000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.916222943381959,\n",
      "      \"grad_norm\": 0.0004932097508572042,\n",
      "      \"learning_rate\": 2.542911153119093e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 78020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.917483222533791,\n",
      "      \"grad_norm\": 0.022732313722372055,\n",
      "      \"learning_rate\": 2.5423125393824827e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 78040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.918743501685624,\n",
      "      \"grad_norm\": 0.11753605306148529,\n",
      "      \"learning_rate\": 2.5416824196597356e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 78060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.9200037808374555,\n",
      "      \"grad_norm\": 0.009374133311212063,\n",
      "      \"learning_rate\": 2.5410522999369878e-05,\n",
      "      \"loss\": 0.0494,\n",
      "      \"step\": 78080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.921264059989287,\n",
      "      \"grad_norm\": 0.0037744787987321615,\n",
      "      \"learning_rate\": 2.5404221802142407e-05,\n",
      "      \"loss\": 0.0359,\n",
      "      \"step\": 78100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.92252433914112,\n",
      "      \"grad_norm\": 0.0011135785607621074,\n",
      "      \"learning_rate\": 2.5397920604914936e-05,\n",
      "      \"loss\": 0.0145,\n",
      "      \"step\": 78120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.923784618292952,\n",
      "      \"grad_norm\": 0.1260630488395691,\n",
      "      \"learning_rate\": 2.5391619407687462e-05,\n",
      "      \"loss\": 0.022,\n",
      "      \"step\": 78140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.925044897444784,\n",
      "      \"grad_norm\": 0.015777062624692917,\n",
      "      \"learning_rate\": 2.538531821045999e-05,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 78160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.926305176596616,\n",
      "      \"grad_norm\": 0.011708471924066544,\n",
      "      \"learning_rate\": 2.5379017013232513e-05,\n",
      "      \"loss\": 0.0528,\n",
      "      \"step\": 78180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.927565455748448,\n",
      "      \"grad_norm\": 0.011519153602421284,\n",
      "      \"learning_rate\": 2.5372715816005042e-05,\n",
      "      \"loss\": 0.0216,\n",
      "      \"step\": 78200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.928825734900281,\n",
      "      \"grad_norm\": 0.152518630027771,\n",
      "      \"learning_rate\": 2.5366414618777568e-05,\n",
      "      \"loss\": 0.0361,\n",
      "      \"step\": 78220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.9300860140521126,\n",
      "      \"grad_norm\": 0.015115153975784779,\n",
      "      \"learning_rate\": 2.5360113421550097e-05,\n",
      "      \"loss\": 0.0487,\n",
      "      \"step\": 78240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.931346293203944,\n",
      "      \"grad_norm\": 0.0042234971188008785,\n",
      "      \"learning_rate\": 2.535381222432262e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 78260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.932606572355777,\n",
      "      \"grad_norm\": 0.0014201912563294172,\n",
      "      \"learning_rate\": 2.534751102709515e-05,\n",
      "      \"loss\": 0.0277,\n",
      "      \"step\": 78280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.933866851507609,\n",
      "      \"grad_norm\": 0.007712200749665499,\n",
      "      \"learning_rate\": 2.5341209829867674e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 78300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.935127130659441,\n",
      "      \"grad_norm\": 0.005021878518164158,\n",
      "      \"learning_rate\": 2.5334908632640203e-05,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 78320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.936387409811273,\n",
      "      \"grad_norm\": 0.004032772965729237,\n",
      "      \"learning_rate\": 2.5328607435412732e-05,\n",
      "      \"loss\": 0.0031,\n",
      "      \"step\": 78340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.937647688963105,\n",
      "      \"grad_norm\": 0.0211490411311388,\n",
      "      \"learning_rate\": 2.5322306238185258e-05,\n",
      "      \"loss\": 0.0392,\n",
      "      \"step\": 78360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.938907968114938,\n",
      "      \"grad_norm\": 0.010161460377275944,\n",
      "      \"learning_rate\": 2.5316005040957787e-05,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 78380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.94016824726677,\n",
      "      \"grad_norm\": 0.0030658601317554712,\n",
      "      \"learning_rate\": 2.530970384373031e-05,\n",
      "      \"loss\": 0.0396,\n",
      "      \"step\": 78400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.941428526418601,\n",
      "      \"grad_norm\": 0.0036629624664783478,\n",
      "      \"learning_rate\": 2.5303402646502838e-05,\n",
      "      \"loss\": 0.0305,\n",
      "      \"step\": 78420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.942688805570434,\n",
      "      \"grad_norm\": 0.08023574948310852,\n",
      "      \"learning_rate\": 2.5297101449275364e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 78440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.9439490847222665,\n",
      "      \"grad_norm\": 0.0024159902241081,\n",
      "      \"learning_rate\": 2.5290800252047893e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 78460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.945209363874098,\n",
      "      \"grad_norm\": 0.023058515042066574,\n",
      "      \"learning_rate\": 2.5284499054820415e-05,\n",
      "      \"loss\": 0.0529,\n",
      "      \"step\": 78480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.94646964302593,\n",
      "      \"grad_norm\": 0.006040588952600956,\n",
      "      \"learning_rate\": 2.5278197857592944e-05,\n",
      "      \"loss\": 0.0217,\n",
      "      \"step\": 78500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.947729922177762,\n",
      "      \"grad_norm\": 0.01426396518945694,\n",
      "      \"learning_rate\": 2.527189666036547e-05,\n",
      "      \"loss\": 0.0028,\n",
      "      \"step\": 78520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.948990201329595,\n",
      "      \"grad_norm\": 0.019525539129972458,\n",
      "      \"learning_rate\": 2.526591052299937e-05,\n",
      "      \"loss\": 0.047,\n",
      "      \"step\": 78540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.950250480481427,\n",
      "      \"grad_norm\": 0.002299741841852665,\n",
      "      \"learning_rate\": 2.52596093257719e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 78560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.951510759633258,\n",
      "      \"grad_norm\": 0.012616843916475773,\n",
      "      \"learning_rate\": 2.5253308128544422e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 78580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.952771038785091,\n",
      "      \"grad_norm\": 0.0027525611221790314,\n",
      "      \"learning_rate\": 2.524700693131695e-05,\n",
      "      \"loss\": 0.0166,\n",
      "      \"step\": 78600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.954031317936923,\n",
      "      \"grad_norm\": 0.020420560613274574,\n",
      "      \"learning_rate\": 2.5240705734089477e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 78620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.955291597088755,\n",
      "      \"grad_norm\": 0.005691684782505035,\n",
      "      \"learning_rate\": 2.5234404536862006e-05,\n",
      "      \"loss\": 0.0279,\n",
      "      \"step\": 78640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.956551876240587,\n",
      "      \"grad_norm\": 0.00484200194478035,\n",
      "      \"learning_rate\": 2.5228103339634535e-05,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 78660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.9578121553924195,\n",
      "      \"grad_norm\": 0.0008868658333085477,\n",
      "      \"learning_rate\": 2.5221802142407057e-05,\n",
      "      \"loss\": 0.0115,\n",
      "      \"step\": 78680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.959072434544251,\n",
      "      \"grad_norm\": 0.0038602144923061132,\n",
      "      \"learning_rate\": 2.5215500945179586e-05,\n",
      "      \"loss\": 0.0408,\n",
      "      \"step\": 78700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.960332713696084,\n",
      "      \"grad_norm\": 0.0021294006146490574,\n",
      "      \"learning_rate\": 2.520919974795211e-05,\n",
      "      \"loss\": 0.0189,\n",
      "      \"step\": 78720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.961592992847915,\n",
      "      \"grad_norm\": 0.0023747519589960575,\n",
      "      \"learning_rate\": 2.520289855072464e-05,\n",
      "      \"loss\": 0.0268,\n",
      "      \"step\": 78740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.962853271999748,\n",
      "      \"grad_norm\": 0.0024425366427749395,\n",
      "      \"learning_rate\": 2.5196597353497163e-05,\n",
      "      \"loss\": 0.0387,\n",
      "      \"step\": 78760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.96411355115158,\n",
      "      \"grad_norm\": 0.08610307425260544,\n",
      "      \"learning_rate\": 2.5190296156269695e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 78780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.965373830303412,\n",
      "      \"grad_norm\": 0.0019437108421698213,\n",
      "      \"learning_rate\": 2.5183994959042218e-05,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 78800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.966634109455244,\n",
      "      \"grad_norm\": 0.0009967851219698787,\n",
      "      \"learning_rate\": 2.5177693761814747e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 78820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.9678943886070766,\n",
      "      \"grad_norm\": 0.030768489465117455,\n",
      "      \"learning_rate\": 2.517139256458727e-05,\n",
      "      \"loss\": 0.0558,\n",
      "      \"step\": 78840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.969154667758908,\n",
      "      \"grad_norm\": 0.42839306592941284,\n",
      "      \"learning_rate\": 2.51650913673598e-05,\n",
      "      \"loss\": 0.0302,\n",
      "      \"step\": 78860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.970414946910741,\n",
      "      \"grad_norm\": 1.9086130857467651,\n",
      "      \"learning_rate\": 2.5158790170132324e-05,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 78880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.9716752260625725,\n",
      "      \"grad_norm\": 0.0009503629407845438,\n",
      "      \"learning_rate\": 2.5152488972904853e-05,\n",
      "      \"loss\": 0.005,\n",
      "      \"step\": 78900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.972935505214405,\n",
      "      \"grad_norm\": 0.010749071836471558,\n",
      "      \"learning_rate\": 2.5146187775677382e-05,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 78920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.974195784366237,\n",
      "      \"grad_norm\": 0.1665329486131668,\n",
      "      \"learning_rate\": 2.5139886578449907e-05,\n",
      "      \"loss\": 0.0395,\n",
      "      \"step\": 78940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.975456063518069,\n",
      "      \"grad_norm\": 0.001043507712893188,\n",
      "      \"learning_rate\": 2.5133585381222436e-05,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 78960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.976716342669901,\n",
      "      \"grad_norm\": 0.4667001962661743,\n",
      "      \"learning_rate\": 2.512728418399496e-05,\n",
      "      \"loss\": 0.0265,\n",
      "      \"step\": 78980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.977976621821734,\n",
      "      \"grad_norm\": 0.009852813556790352,\n",
      "      \"learning_rate\": 2.5120982986767488e-05,\n",
      "      \"loss\": 0.0428,\n",
      "      \"step\": 79000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.979236900973565,\n",
      "      \"grad_norm\": 9.744759559631348,\n",
      "      \"learning_rate\": 2.5114681789540013e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 79020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.980497180125398,\n",
      "      \"grad_norm\": 0.02381172589957714,\n",
      "      \"learning_rate\": 2.5108380592312542e-05,\n",
      "      \"loss\": 0.0242,\n",
      "      \"step\": 79040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.98175745927723,\n",
      "      \"grad_norm\": 0.008358485996723175,\n",
      "      \"learning_rate\": 2.5102079395085065e-05,\n",
      "      \"loss\": 0.0215,\n",
      "      \"step\": 79060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.983017738429062,\n",
      "      \"grad_norm\": 0.16867271065711975,\n",
      "      \"learning_rate\": 2.5095778197857594e-05,\n",
      "      \"loss\": 0.0131,\n",
      "      \"step\": 79080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.984278017580894,\n",
      "      \"grad_norm\": 0.017123552039265633,\n",
      "      \"learning_rate\": 2.508947700063012e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 79100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.985538296732726,\n",
      "      \"grad_norm\": 0.03777550533413887,\n",
      "      \"learning_rate\": 2.508317580340265e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 79120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.986798575884558,\n",
      "      \"grad_norm\": 14.363036155700684,\n",
      "      \"learning_rate\": 2.5076874606175177e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 79140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.988058855036391,\n",
      "      \"grad_norm\": 0.001003027311526239,\n",
      "      \"learning_rate\": 2.50705734089477e-05,\n",
      "      \"loss\": 0.0143,\n",
      "      \"step\": 79160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.989319134188222,\n",
      "      \"grad_norm\": 5.749880313873291,\n",
      "      \"learning_rate\": 2.506427221172023e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 79180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.990579413340055,\n",
      "      \"grad_norm\": 0.0016126750269904733,\n",
      "      \"learning_rate\": 2.5057971014492754e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 79200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.991839692491887,\n",
      "      \"grad_norm\": 0.0007408900419250131,\n",
      "      \"learning_rate\": 2.5051669817265283e-05,\n",
      "      \"loss\": 0.0176,\n",
      "      \"step\": 79220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.993099971643719,\n",
      "      \"grad_norm\": 0.001511109760031104,\n",
      "      \"learning_rate\": 2.5045368620037806e-05,\n",
      "      \"loss\": 0.036,\n",
      "      \"step\": 79240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.994360250795551,\n",
      "      \"grad_norm\": 0.007483936380594969,\n",
      "      \"learning_rate\": 2.5039067422810335e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 79260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.9956205299473835,\n",
      "      \"grad_norm\": 0.19954358041286469,\n",
      "      \"learning_rate\": 2.503276622558286e-05,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 79280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.996880809099215,\n",
      "      \"grad_norm\": 0.003076405031606555,\n",
      "      \"learning_rate\": 2.502646502835539e-05,\n",
      "      \"loss\": 0.0348,\n",
      "      \"step\": 79300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.998141088251048,\n",
      "      \"grad_norm\": 0.002360015641897917,\n",
      "      \"learning_rate\": 2.502016383112791e-05,\n",
      "      \"loss\": 0.0024,\n",
      "      \"step\": 79320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.999401367402879,\n",
      "      \"grad_norm\": 0.11880161613225937,\n",
      "      \"learning_rate\": 2.501386263390044e-05,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 79340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.0,\n",
      "      \"eval_accuracy\": 0.9994014239808455,\n",
      "      \"eval_f1\": 0.9994013296782934,\n",
      "      \"eval_loss\": 0.0034086694940924644,\n",
      "      \"eval_precision\": 0.9995588049918064,\n",
      "      \"eval_recall\": 0.999243903975805,\n",
      "      \"eval_runtime\": 565.4615,\n",
      "      \"eval_samples_per_second\": 56.135,\n",
      "      \"eval_steps_per_second\": 7.017,\n",
      "      \"step\": 79350\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.000630139575916,\n",
      "      \"grad_norm\": 0.008185388520359993,\n",
      "      \"learning_rate\": 2.5007561436672973e-05,\n",
      "      \"loss\": 0.0034,\n",
      "      \"step\": 79360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.001890418727748,\n",
      "      \"grad_norm\": 0.003336254507303238,\n",
      "      \"learning_rate\": 2.5001260239445495e-05,\n",
      "      \"loss\": 0.0267,\n",
      "      \"step\": 79380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.0031506978795806,\n",
      "      \"grad_norm\": 0.0028149872086942196,\n",
      "      \"learning_rate\": 2.499495904221802e-05,\n",
      "      \"loss\": 0.0307,\n",
      "      \"step\": 79400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.004410977031412,\n",
      "      \"grad_norm\": 0.004008937627077103,\n",
      "      \"learning_rate\": 2.498865784499055e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 79420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.005671256183245,\n",
      "      \"grad_norm\": 0.0033352551981806755,\n",
      "      \"learning_rate\": 2.4982356647763076e-05,\n",
      "      \"loss\": 0.0183,\n",
      "      \"step\": 79440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.0069315353350765,\n",
      "      \"grad_norm\": 0.001338784582912922,\n",
      "      \"learning_rate\": 2.4976055450535605e-05,\n",
      "      \"loss\": 0.0033,\n",
      "      \"step\": 79460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.008191814486909,\n",
      "      \"grad_norm\": 0.013979557901620865,\n",
      "      \"learning_rate\": 2.496975425330813e-05,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 79480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.009452093638741,\n",
      "      \"grad_norm\": 0.002749023027718067,\n",
      "      \"learning_rate\": 2.4963453056080656e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 79500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.010712372790573,\n",
      "      \"grad_norm\": 0.0008876990177668631,\n",
      "      \"learning_rate\": 2.4957151858853185e-05,\n",
      "      \"loss\": 0.0037,\n",
      "      \"step\": 79520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.011972651942405,\n",
      "      \"grad_norm\": 0.0016544447280466557,\n",
      "      \"learning_rate\": 2.495085066162571e-05,\n",
      "      \"loss\": 0.0273,\n",
      "      \"step\": 79540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.013232931094238,\n",
      "      \"grad_norm\": 0.058405302464962006,\n",
      "      \"learning_rate\": 2.4944549464398236e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 79560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.014493210246069,\n",
      "      \"grad_norm\": 5.268632411956787,\n",
      "      \"learning_rate\": 2.4938248267170762e-05,\n",
      "      \"loss\": 0.0358,\n",
      "      \"step\": 79580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.015753489397902,\n",
      "      \"grad_norm\": 0.013116029091179371,\n",
      "      \"learning_rate\": 2.493194706994329e-05,\n",
      "      \"loss\": 0.0145,\n",
      "      \"step\": 79600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.017013768549734,\n",
      "      \"grad_norm\": 0.014454159885644913,\n",
      "      \"learning_rate\": 2.4925645872715817e-05,\n",
      "      \"loss\": 0.0083,\n",
      "      \"step\": 79620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.018274047701566,\n",
      "      \"grad_norm\": 7.927215576171875,\n",
      "      \"learning_rate\": 2.4919344675488342e-05,\n",
      "      \"loss\": 0.0356,\n",
      "      \"step\": 79640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.019534326853398,\n",
      "      \"grad_norm\": 0.0028148675337433815,\n",
      "      \"learning_rate\": 2.4913043478260868e-05,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 79660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.02079460600523,\n",
      "      \"grad_norm\": 0.09717649221420288,\n",
      "      \"learning_rate\": 2.49067422810334e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 79680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.022054885157062,\n",
      "      \"grad_norm\": 0.13341765105724335,\n",
      "      \"learning_rate\": 2.4900441083805926e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 79700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.023315164308895,\n",
      "      \"grad_norm\": 0.6565231680870056,\n",
      "      \"learning_rate\": 2.4894139886578452e-05,\n",
      "      \"loss\": 0.0099,\n",
      "      \"step\": 79720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.024575443460726,\n",
      "      \"grad_norm\": 0.002785125281661749,\n",
      "      \"learning_rate\": 2.4887838689350977e-05,\n",
      "      \"loss\": 0.0695,\n",
      "      \"step\": 79740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.025835722612559,\n",
      "      \"grad_norm\": 0.019408181309700012,\n",
      "      \"learning_rate\": 2.4881537492123506e-05,\n",
      "      \"loss\": 0.0452,\n",
      "      \"step\": 79760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.027096001764391,\n",
      "      \"grad_norm\": 0.011007041670382023,\n",
      "      \"learning_rate\": 2.4875236294896032e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 79780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.028356280916223,\n",
      "      \"grad_norm\": 0.4144282341003418,\n",
      "      \"learning_rate\": 2.4868935097668558e-05,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 79800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.029616560068055,\n",
      "      \"grad_norm\": 0.0034032745752483606,\n",
      "      \"learning_rate\": 2.4862633900441083e-05,\n",
      "      \"loss\": 0.0037,\n",
      "      \"step\": 79820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.0308768392198875,\n",
      "      \"grad_norm\": 0.002300455467775464,\n",
      "      \"learning_rate\": 2.4856332703213612e-05,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 79840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.032137118371719,\n",
      "      \"grad_norm\": 7.155287265777588,\n",
      "      \"learning_rate\": 2.4850031505986138e-05,\n",
      "      \"loss\": 0.0337,\n",
      "      \"step\": 79860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.033397397523552,\n",
      "      \"grad_norm\": 0.32666268944740295,\n",
      "      \"learning_rate\": 2.4843730308758664e-05,\n",
      "      \"loss\": 0.0311,\n",
      "      \"step\": 79880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.034657676675383,\n",
      "      \"grad_norm\": 0.005959245841950178,\n",
      "      \"learning_rate\": 2.483742911153119e-05,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 79900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.035917955827216,\n",
      "      \"grad_norm\": 0.04242866113781929,\n",
      "      \"learning_rate\": 2.483112791430372e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 79920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.037178234979048,\n",
      "      \"grad_norm\": 0.005123460665345192,\n",
      "      \"learning_rate\": 2.4824826717076248e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 79940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.03843851413088,\n",
      "      \"grad_norm\": 0.0467446930706501,\n",
      "      \"learning_rate\": 2.4818525519848773e-05,\n",
      "      \"loss\": 0.0262,\n",
      "      \"step\": 79960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.039698793282712,\n",
      "      \"grad_norm\": 0.008339756168425083,\n",
      "      \"learning_rate\": 2.48122243226213e-05,\n",
      "      \"loss\": 0.0305,\n",
      "      \"step\": 79980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.0409590724345446,\n",
      "      \"grad_norm\": 0.005818616133183241,\n",
      "      \"learning_rate\": 2.4805923125393828e-05,\n",
      "      \"loss\": 0.02,\n",
      "      \"step\": 80000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.042219351586376,\n",
      "      \"grad_norm\": 0.022806433960795403,\n",
      "      \"learning_rate\": 2.4799621928166354e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 80020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.043479630738209,\n",
      "      \"grad_norm\": 0.01499315444380045,\n",
      "      \"learning_rate\": 2.479332073093888e-05,\n",
      "      \"loss\": 0.0272,\n",
      "      \"step\": 80040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.0447399098900405,\n",
      "      \"grad_norm\": 0.00587173318490386,\n",
      "      \"learning_rate\": 2.4787019533711405e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 80060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.046000189041873,\n",
      "      \"grad_norm\": 0.0026606752071529627,\n",
      "      \"learning_rate\": 2.4780718336483934e-05,\n",
      "      \"loss\": 0.0311,\n",
      "      \"step\": 80080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.047260468193705,\n",
      "      \"grad_norm\": 0.007120296824723482,\n",
      "      \"learning_rate\": 2.477441713925646e-05,\n",
      "      \"loss\": 0.0521,\n",
      "      \"step\": 80100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.048520747345537,\n",
      "      \"grad_norm\": 0.05939015746116638,\n",
      "      \"learning_rate\": 2.4768115942028985e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 80120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.049781026497369,\n",
      "      \"grad_norm\": 0.004397923592478037,\n",
      "      \"learning_rate\": 2.4761814744801514e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 80140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.051041305649202,\n",
      "      \"grad_norm\": 6.399450302124023,\n",
      "      \"learning_rate\": 2.475551354757404e-05,\n",
      "      \"loss\": 0.0508,\n",
      "      \"step\": 80160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.052301584801033,\n",
      "      \"grad_norm\": 0.004465557634830475,\n",
      "      \"learning_rate\": 2.474921235034657e-05,\n",
      "      \"loss\": 0.0034,\n",
      "      \"step\": 80180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.053561863952866,\n",
      "      \"grad_norm\": 0.02477520890533924,\n",
      "      \"learning_rate\": 2.4742911153119095e-05,\n",
      "      \"loss\": 0.0324,\n",
      "      \"step\": 80200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.054822143104698,\n",
      "      \"grad_norm\": 0.9390979409217834,\n",
      "      \"learning_rate\": 2.473660995589162e-05,\n",
      "      \"loss\": 0.0295,\n",
      "      \"step\": 80220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.05608242225653,\n",
      "      \"grad_norm\": 2.6351213455200195,\n",
      "      \"learning_rate\": 2.4730308758664146e-05,\n",
      "      \"loss\": 0.0212,\n",
      "      \"step\": 80240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.057342701408362,\n",
      "      \"grad_norm\": 0.05043405666947365,\n",
      "      \"learning_rate\": 2.4724007561436675e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 80260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.058602980560194,\n",
      "      \"grad_norm\": 0.03462120145559311,\n",
      "      \"learning_rate\": 2.47177063642092e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 80280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.059863259712026,\n",
      "      \"grad_norm\": 0.0007707857294008136,\n",
      "      \"learning_rate\": 2.4711405166981726e-05,\n",
      "      \"loss\": 0.019,\n",
      "      \"step\": 80300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.061123538863859,\n",
      "      \"grad_norm\": 1.1303012371063232,\n",
      "      \"learning_rate\": 2.4705103969754255e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 80320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.06238381801569,\n",
      "      \"grad_norm\": 0.004233086947351694,\n",
      "      \"learning_rate\": 2.469880277252678e-05,\n",
      "      \"loss\": 0.0204,\n",
      "      \"step\": 80340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.063644097167523,\n",
      "      \"grad_norm\": 0.3970435857772827,\n",
      "      \"learning_rate\": 2.469250157529931e-05,\n",
      "      \"loss\": 0.0284,\n",
      "      \"step\": 80360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.064904376319355,\n",
      "      \"grad_norm\": 0.0019322352018207312,\n",
      "      \"learning_rate\": 2.4686200378071836e-05,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 80380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.066164655471187,\n",
      "      \"grad_norm\": 0.008532020263373852,\n",
      "      \"learning_rate\": 2.467989918084436e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 80400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.067424934623019,\n",
      "      \"grad_norm\": 0.0018945800838992,\n",
      "      \"learning_rate\": 2.467359798361689e-05,\n",
      "      \"loss\": 0.0174,\n",
      "      \"step\": 80420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.0686852137748515,\n",
      "      \"grad_norm\": 0.007468394003808498,\n",
      "      \"learning_rate\": 2.4667296786389416e-05,\n",
      "      \"loss\": 0.019,\n",
      "      \"step\": 80440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.069945492926683,\n",
      "      \"grad_norm\": 0.0012392837088555098,\n",
      "      \"learning_rate\": 2.466099558916194e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 80460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.071205772078516,\n",
      "      \"grad_norm\": 5.325293064117432,\n",
      "      \"learning_rate\": 2.4654694391934467e-05,\n",
      "      \"loss\": 0.0586,\n",
      "      \"step\": 80480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.072466051230347,\n",
      "      \"grad_norm\": 0.003317094873636961,\n",
      "      \"learning_rate\": 2.4648393194706996e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 80500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.07372633038218,\n",
      "      \"grad_norm\": 0.024166246876120567,\n",
      "      \"learning_rate\": 2.4642091997479522e-05,\n",
      "      \"loss\": 0.0209,\n",
      "      \"step\": 80520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.074986609534012,\n",
      "      \"grad_norm\": 0.026918690651655197,\n",
      "      \"learning_rate\": 2.4635790800252048e-05,\n",
      "      \"loss\": 0.0279,\n",
      "      \"step\": 80540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.076246888685844,\n",
      "      \"grad_norm\": 0.0057044439017772675,\n",
      "      \"learning_rate\": 2.4629489603024573e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 80560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.077507167837676,\n",
      "      \"grad_norm\": 0.03104439005255699,\n",
      "      \"learning_rate\": 2.4623188405797102e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 80580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.0787674469895085,\n",
      "      \"grad_norm\": 3.1006298065185547,\n",
      "      \"learning_rate\": 2.461688720856963e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 80600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.08002772614134,\n",
      "      \"grad_norm\": 0.002625425346195698,\n",
      "      \"learning_rate\": 2.4610586011342157e-05,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 80620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.081288005293173,\n",
      "      \"grad_norm\": 0.05110383778810501,\n",
      "      \"learning_rate\": 2.4604284814114683e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 80640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.0825482844450045,\n",
      "      \"grad_norm\": 0.01723361946642399,\n",
      "      \"learning_rate\": 2.459798361688721e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 80660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.083808563596837,\n",
      "      \"grad_norm\": 0.0006416331743821502,\n",
      "      \"learning_rate\": 2.4591682419659737e-05,\n",
      "      \"loss\": 0.0333,\n",
      "      \"step\": 80680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.085068842748669,\n",
      "      \"grad_norm\": 0.007445104885846376,\n",
      "      \"learning_rate\": 2.4585381222432263e-05,\n",
      "      \"loss\": 0.0233,\n",
      "      \"step\": 80700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.086329121900501,\n",
      "      \"grad_norm\": 0.0009494887781329453,\n",
      "      \"learning_rate\": 2.457908002520479e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 80720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.087589401052333,\n",
      "      \"grad_norm\": 1.4707586765289307,\n",
      "      \"learning_rate\": 2.4572778827977318e-05,\n",
      "      \"loss\": 0.0294,\n",
      "      \"step\": 80740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.088849680204166,\n",
      "      \"grad_norm\": 0.4678839445114136,\n",
      "      \"learning_rate\": 2.4566477630749843e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 80760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.090109959355997,\n",
      "      \"grad_norm\": 0.057356297969818115,\n",
      "      \"learning_rate\": 2.456017643352237e-05,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 80780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.09137023850783,\n",
      "      \"grad_norm\": 0.03903897851705551,\n",
      "      \"learning_rate\": 2.4553875236294895e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 80800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.092630517659662,\n",
      "      \"grad_norm\": 0.07176125794649124,\n",
      "      \"learning_rate\": 2.4547574039067424e-05,\n",
      "      \"loss\": 0.0305,\n",
      "      \"step\": 80820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.093890796811494,\n",
      "      \"grad_norm\": 0.0012841656571254134,\n",
      "      \"learning_rate\": 2.4541272841839953e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 80840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.095151075963326,\n",
      "      \"grad_norm\": 0.027776887640357018,\n",
      "      \"learning_rate\": 2.4534971644612478e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 80860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.096411355115158,\n",
      "      \"grad_norm\": 0.006637475453317165,\n",
      "      \"learning_rate\": 2.4528670447385004e-05,\n",
      "      \"loss\": 0.0288,\n",
      "      \"step\": 80880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.09767163426699,\n",
      "      \"grad_norm\": 0.024627096951007843,\n",
      "      \"learning_rate\": 2.4522369250157533e-05,\n",
      "      \"loss\": 0.0038,\n",
      "      \"step\": 80900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.098931913418823,\n",
      "      \"grad_norm\": 0.06441425532102585,\n",
      "      \"learning_rate\": 2.451606805293006e-05,\n",
      "      \"loss\": 0.0018,\n",
      "      \"step\": 80920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.100192192570654,\n",
      "      \"grad_norm\": 0.000985645572654903,\n",
      "      \"learning_rate\": 2.4509766855702584e-05,\n",
      "      \"loss\": 0.0362,\n",
      "      \"step\": 80940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.101452471722487,\n",
      "      \"grad_norm\": 0.000782791874371469,\n",
      "      \"learning_rate\": 2.450346565847511e-05,\n",
      "      \"loss\": 0.0066,\n",
      "      \"step\": 80960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.102712750874319,\n",
      "      \"grad_norm\": 0.0038849872071295977,\n",
      "      \"learning_rate\": 2.449716446124764e-05,\n",
      "      \"loss\": 0.0299,\n",
      "      \"step\": 80980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.103973030026151,\n",
      "      \"grad_norm\": 0.0034429861698299646,\n",
      "      \"learning_rate\": 2.4490863264020165e-05,\n",
      "      \"loss\": 0.0283,\n",
      "      \"step\": 81000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.105233309177983,\n",
      "      \"grad_norm\": 0.001979570370167494,\n",
      "      \"learning_rate\": 2.448456206679269e-05,\n",
      "      \"loss\": 0.0068,\n",
      "      \"step\": 81020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.1064935883298155,\n",
      "      \"grad_norm\": 0.0006449646898545325,\n",
      "      \"learning_rate\": 2.447826086956522e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 81040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.107753867481647,\n",
      "      \"grad_norm\": 0.005888547282665968,\n",
      "      \"learning_rate\": 2.4471959672337745e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 81060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.10901414663348,\n",
      "      \"grad_norm\": 0.005988613702356815,\n",
      "      \"learning_rate\": 2.4465658475110274e-05,\n",
      "      \"loss\": 0.0314,\n",
      "      \"step\": 81080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.110274425785311,\n",
      "      \"grad_norm\": 0.0011306059313938022,\n",
      "      \"learning_rate\": 2.44593572778828e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 81100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.111534704937144,\n",
      "      \"grad_norm\": 0.004991122055798769,\n",
      "      \"learning_rate\": 2.4453056080655325e-05,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 81120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.112794984088976,\n",
      "      \"grad_norm\": 0.0011547452304512262,\n",
      "      \"learning_rate\": 2.444675488342785e-05,\n",
      "      \"loss\": 0.0157,\n",
      "      \"step\": 81140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.114055263240807,\n",
      "      \"grad_norm\": 0.005895090755075216,\n",
      "      \"learning_rate\": 2.444045368620038e-05,\n",
      "      \"loss\": 0.0501,\n",
      "      \"step\": 81160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.11531554239264,\n",
      "      \"grad_norm\": 0.02255493588745594,\n",
      "      \"learning_rate\": 2.4434152488972906e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 81180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.1165758215444725,\n",
      "      \"grad_norm\": 0.012662994675338268,\n",
      "      \"learning_rate\": 2.442785129174543e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 81200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.117836100696304,\n",
      "      \"grad_norm\": 8.657190322875977,\n",
      "      \"learning_rate\": 2.442155009451796e-05,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 81220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.119096379848136,\n",
      "      \"grad_norm\": 0.0024892918299883604,\n",
      "      \"learning_rate\": 2.4415248897290486e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 81240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.1203566589999685,\n",
      "      \"grad_norm\": 0.0012566088698804379,\n",
      "      \"learning_rate\": 2.440894770006301e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 81260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.1216169381518,\n",
      "      \"grad_norm\": 0.009816987439990044,\n",
      "      \"learning_rate\": 2.440264650283554e-05,\n",
      "      \"loss\": 0.0206,\n",
      "      \"step\": 81280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.122877217303633,\n",
      "      \"grad_norm\": 0.007488802075386047,\n",
      "      \"learning_rate\": 2.4396345305608066e-05,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 81300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.1241374964554645,\n",
      "      \"grad_norm\": 0.0021305731497704983,\n",
      "      \"learning_rate\": 2.4390044108380595e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 81320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.125397775607297,\n",
      "      \"grad_norm\": 0.0006370721966959536,\n",
      "      \"learning_rate\": 2.438374291115312e-05,\n",
      "      \"loss\": 0.0099,\n",
      "      \"step\": 81340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.126658054759129,\n",
      "      \"grad_norm\": 12.390130996704102,\n",
      "      \"learning_rate\": 2.4377441713925647e-05,\n",
      "      \"loss\": 0.0304,\n",
      "      \"step\": 81360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.127918333910961,\n",
      "      \"grad_norm\": 0.00037928883102722466,\n",
      "      \"learning_rate\": 2.4371140516698172e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 81380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.129178613062793,\n",
      "      \"grad_norm\": 0.219321146607399,\n",
      "      \"learning_rate\": 2.43648393194707e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 81400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.130438892214626,\n",
      "      \"grad_norm\": 0.0005122240982018411,\n",
      "      \"learning_rate\": 2.4358538122243227e-05,\n",
      "      \"loss\": 0.0135,\n",
      "      \"step\": 81420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.131699171366457,\n",
      "      \"grad_norm\": 8.107872009277344,\n",
      "      \"learning_rate\": 2.4352236925015753e-05,\n",
      "      \"loss\": 0.0388,\n",
      "      \"step\": 81440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.13295945051829,\n",
      "      \"grad_norm\": 0.010063452646136284,\n",
      "      \"learning_rate\": 2.434593572778828e-05,\n",
      "      \"loss\": 0.0031,\n",
      "      \"step\": 81460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.1342197296701215,\n",
      "      \"grad_norm\": 0.012544830329716206,\n",
      "      \"learning_rate\": 2.4339634530560807e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 81480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.135480008821954,\n",
      "      \"grad_norm\": 0.09590185433626175,\n",
      "      \"learning_rate\": 2.4333333333333336e-05,\n",
      "      \"loss\": 0.0244,\n",
      "      \"step\": 81500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.136740287973786,\n",
      "      \"grad_norm\": 0.013483124785125256,\n",
      "      \"learning_rate\": 2.4327032136105862e-05,\n",
      "      \"loss\": 0.025,\n",
      "      \"step\": 81520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.138000567125618,\n",
      "      \"grad_norm\": 0.0014701500767841935,\n",
      "      \"learning_rate\": 2.4320730938878388e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 81540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.13926084627745,\n",
      "      \"grad_norm\": 0.012258918024599552,\n",
      "      \"learning_rate\": 2.4314429741650917e-05,\n",
      "      \"loss\": 0.0719,\n",
      "      \"step\": 81560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.140521125429283,\n",
      "      \"grad_norm\": 0.6162412166595459,\n",
      "      \"learning_rate\": 2.4308128544423442e-05,\n",
      "      \"loss\": 0.043,\n",
      "      \"step\": 81580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.141781404581114,\n",
      "      \"grad_norm\": 0.004592706449329853,\n",
      "      \"learning_rate\": 2.4301827347195968e-05,\n",
      "      \"loss\": 0.016,\n",
      "      \"step\": 81600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.143041683732947,\n",
      "      \"grad_norm\": 0.022281305864453316,\n",
      "      \"learning_rate\": 2.4295526149968494e-05,\n",
      "      \"loss\": 0.0231,\n",
      "      \"step\": 81620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.144301962884779,\n",
      "      \"grad_norm\": 0.02731296978890896,\n",
      "      \"learning_rate\": 2.4289224952741023e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 81640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.145562242036611,\n",
      "      \"grad_norm\": 0.02471317909657955,\n",
      "      \"learning_rate\": 2.428292375551355e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 81660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.146822521188443,\n",
      "      \"grad_norm\": 0.003860325086861849,\n",
      "      \"learning_rate\": 2.4276622558286074e-05,\n",
      "      \"loss\": 0.0276,\n",
      "      \"step\": 81680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.148082800340275,\n",
      "      \"grad_norm\": 0.0010490014683455229,\n",
      "      \"learning_rate\": 2.42703213610586e-05,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 81700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.149343079492107,\n",
      "      \"grad_norm\": 4.333734035491943,\n",
      "      \"learning_rate\": 2.426402016383113e-05,\n",
      "      \"loss\": 0.0211,\n",
      "      \"step\": 81720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.15060335864394,\n",
      "      \"grad_norm\": 0.08279024064540863,\n",
      "      \"learning_rate\": 2.4257718966603658e-05,\n",
      "      \"loss\": 0.0156,\n",
      "      \"step\": 81740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.151863637795771,\n",
      "      \"grad_norm\": 0.004146849270910025,\n",
      "      \"learning_rate\": 2.4251417769376183e-05,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 81760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.153123916947604,\n",
      "      \"grad_norm\": 0.020296232774853706,\n",
      "      \"learning_rate\": 2.424511657214871e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 81780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.154384196099436,\n",
      "      \"grad_norm\": 0.054533734917640686,\n",
      "      \"learning_rate\": 2.4238815374921238e-05,\n",
      "      \"loss\": 0.0116,\n",
      "      \"step\": 81800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.155644475251268,\n",
      "      \"grad_norm\": 0.01777423545718193,\n",
      "      \"learning_rate\": 2.4232514177693764e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 81820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.1569047544031,\n",
      "      \"grad_norm\": 0.0019628393929451704,\n",
      "      \"learning_rate\": 2.422621298046629e-05,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 81840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.1581650335549325,\n",
      "      \"grad_norm\": 0.002138984389603138,\n",
      "      \"learning_rate\": 2.4219911783238815e-05,\n",
      "      \"loss\": 0.0019,\n",
      "      \"step\": 81860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.159425312706764,\n",
      "      \"grad_norm\": 30.04208755493164,\n",
      "      \"learning_rate\": 2.4213610586011344e-05,\n",
      "      \"loss\": 0.0532,\n",
      "      \"step\": 81880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.160685591858597,\n",
      "      \"grad_norm\": 0.005226557608693838,\n",
      "      \"learning_rate\": 2.420730938878387e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 81900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.1619458710104285,\n",
      "      \"grad_norm\": 0.000832719961181283,\n",
      "      \"learning_rate\": 2.4201008191556395e-05,\n",
      "      \"loss\": 0.0375,\n",
      "      \"step\": 81920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.163206150162261,\n",
      "      \"grad_norm\": 0.8144594430923462,\n",
      "      \"learning_rate\": 2.419470699432892e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 81940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.164466429314093,\n",
      "      \"grad_norm\": 0.0007768792565912008,\n",
      "      \"learning_rate\": 2.418840579710145e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 81960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.165726708465925,\n",
      "      \"grad_norm\": 0.02438262663781643,\n",
      "      \"learning_rate\": 2.418210459987398e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 81980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.166986987617757,\n",
      "      \"grad_norm\": 0.006687922403216362,\n",
      "      \"learning_rate\": 2.4175803402646505e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 82000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.16824726676959,\n",
      "      \"grad_norm\": 0.00344814988784492,\n",
      "      \"learning_rate\": 2.416950220541903e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 82020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.169507545921421,\n",
      "      \"grad_norm\": 4.952227592468262,\n",
      "      \"learning_rate\": 2.4163201008191556e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 82040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.170767825073254,\n",
      "      \"grad_norm\": 0.000933665141928941,\n",
      "      \"learning_rate\": 2.4156899810964085e-05,\n",
      "      \"loss\": 0.0294,\n",
      "      \"step\": 82060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.1720281042250855,\n",
      "      \"grad_norm\": 0.24346670508384705,\n",
      "      \"learning_rate\": 2.415059861373661e-05,\n",
      "      \"loss\": 0.0542,\n",
      "      \"step\": 82080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.173288383376918,\n",
      "      \"grad_norm\": 0.0036054053343832493,\n",
      "      \"learning_rate\": 2.4144297416509136e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 82100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.17454866252875,\n",
      "      \"grad_norm\": 0.009872996248304844,\n",
      "      \"learning_rate\": 2.4137996219281665e-05,\n",
      "      \"loss\": 0.0162,\n",
      "      \"step\": 82120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.175808941680582,\n",
      "      \"grad_norm\": 0.01677737385034561,\n",
      "      \"learning_rate\": 2.413169502205419e-05,\n",
      "      \"loss\": 0.0368,\n",
      "      \"step\": 82140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.177069220832414,\n",
      "      \"grad_norm\": 0.0013838917948305607,\n",
      "      \"learning_rate\": 2.4125393824826717e-05,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 82160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.178329499984247,\n",
      "      \"grad_norm\": 0.01074856799095869,\n",
      "      \"learning_rate\": 2.4119092627599246e-05,\n",
      "      \"loss\": 0.0317,\n",
      "      \"step\": 82180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.179589779136078,\n",
      "      \"grad_norm\": 0.0034465380012989044,\n",
      "      \"learning_rate\": 2.411279143037177e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 82200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.180850058287911,\n",
      "      \"grad_norm\": 2.877734422683716,\n",
      "      \"learning_rate\": 2.41064902331443e-05,\n",
      "      \"loss\": 0.0029,\n",
      "      \"step\": 82220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.182110337439743,\n",
      "      \"grad_norm\": 0.08017250895500183,\n",
      "      \"learning_rate\": 2.4100189035916826e-05,\n",
      "      \"loss\": 0.0356,\n",
      "      \"step\": 82240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.183370616591575,\n",
      "      \"grad_norm\": 0.011694262735545635,\n",
      "      \"learning_rate\": 2.4093887838689352e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 82260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.184630895743407,\n",
      "      \"grad_norm\": 0.024116074666380882,\n",
      "      \"learning_rate\": 2.4087586641461877e-05,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 82280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.185891174895239,\n",
      "      \"grad_norm\": 0.12896959483623505,\n",
      "      \"learning_rate\": 2.4081285444234406e-05,\n",
      "      \"loss\": 0.0465,\n",
      "      \"step\": 82300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.187151454047071,\n",
      "      \"grad_norm\": 0.011901967227458954,\n",
      "      \"learning_rate\": 2.4074984247006932e-05,\n",
      "      \"loss\": 0.0116,\n",
      "      \"step\": 82320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.188411733198904,\n",
      "      \"grad_norm\": 0.004128039348870516,\n",
      "      \"learning_rate\": 2.4068683049779458e-05,\n",
      "      \"loss\": 0.0145,\n",
      "      \"step\": 82340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.189672012350735,\n",
      "      \"grad_norm\": 0.02126981131732464,\n",
      "      \"learning_rate\": 2.4062381852551983e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 82360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.190932291502568,\n",
      "      \"grad_norm\": 10.276575088500977,\n",
      "      \"learning_rate\": 2.4056080655324512e-05,\n",
      "      \"loss\": 0.0064,\n",
      "      \"step\": 82380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.1921925706544,\n",
      "      \"grad_norm\": 0.002245113719254732,\n",
      "      \"learning_rate\": 2.404977945809704e-05,\n",
      "      \"loss\": 0.013,\n",
      "      \"step\": 82400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.193452849806232,\n",
      "      \"grad_norm\": 0.011128976941108704,\n",
      "      \"learning_rate\": 2.4043478260869567e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 82420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.194713128958064,\n",
      "      \"grad_norm\": 0.010773662477731705,\n",
      "      \"learning_rate\": 2.4037177063642093e-05,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 82440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.1959734081098965,\n",
      "      \"grad_norm\": 0.002131800167262554,\n",
      "      \"learning_rate\": 2.4030875866414622e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 82460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.197233687261728,\n",
      "      \"grad_norm\": 0.008387369103729725,\n",
      "      \"learning_rate\": 2.4024574669187148e-05,\n",
      "      \"loss\": 0.0178,\n",
      "      \"step\": 82480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.198493966413561,\n",
      "      \"grad_norm\": 0.023850755766034126,\n",
      "      \"learning_rate\": 2.4018273471959673e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 82500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.1997542455653925,\n",
      "      \"grad_norm\": 0.05058182775974274,\n",
      "      \"learning_rate\": 2.40119722747322e-05,\n",
      "      \"loss\": 0.0357,\n",
      "      \"step\": 82520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.201014524717225,\n",
      "      \"grad_norm\": 0.0036762524396181107,\n",
      "      \"learning_rate\": 2.4005671077504728e-05,\n",
      "      \"loss\": 0.0354,\n",
      "      \"step\": 82540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.202274803869057,\n",
      "      \"grad_norm\": 0.0820944756269455,\n",
      "      \"learning_rate\": 2.3999369880277254e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 82560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.203535083020889,\n",
      "      \"grad_norm\": 0.0031050422694534063,\n",
      "      \"learning_rate\": 2.399306868304978e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 82580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.204795362172721,\n",
      "      \"grad_norm\": 0.0006013695965521038,\n",
      "      \"learning_rate\": 2.3986767485822305e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 82600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.206055641324554,\n",
      "      \"grad_norm\": 8.571696281433105,\n",
      "      \"learning_rate\": 2.3980466288594834e-05,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 82620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.207315920476385,\n",
      "      \"grad_norm\": 0.001610941719263792,\n",
      "      \"learning_rate\": 2.3974165091367363e-05,\n",
      "      \"loss\": 0.0042,\n",
      "      \"step\": 82640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.208576199628218,\n",
      "      \"grad_norm\": 0.04651960730552673,\n",
      "      \"learning_rate\": 2.396786389413989e-05,\n",
      "      \"loss\": 0.0393,\n",
      "      \"step\": 82660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.2098364787800495,\n",
      "      \"grad_norm\": 0.004945557564496994,\n",
      "      \"learning_rate\": 2.3961562696912414e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 82680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.211096757931882,\n",
      "      \"grad_norm\": 0.003232858842238784,\n",
      "      \"learning_rate\": 2.3955261499684943e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 82700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.212357037083714,\n",
      "      \"grad_norm\": 0.0018436971586197615,\n",
      "      \"learning_rate\": 2.394896030245747e-05,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 82720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.213617316235546,\n",
      "      \"grad_norm\": 3.7167675495147705,\n",
      "      \"learning_rate\": 2.3942659105229995e-05,\n",
      "      \"loss\": 0.0458,\n",
      "      \"step\": 82740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.214877595387378,\n",
      "      \"grad_norm\": 0.012348907068371773,\n",
      "      \"learning_rate\": 2.393635790800252e-05,\n",
      "      \"loss\": 0.0271,\n",
      "      \"step\": 82760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.216137874539211,\n",
      "      \"grad_norm\": 0.021502846851944923,\n",
      "      \"learning_rate\": 2.393005671077505e-05,\n",
      "      \"loss\": 0.044,\n",
      "      \"step\": 82780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.217398153691042,\n",
      "      \"grad_norm\": 0.025183171033859253,\n",
      "      \"learning_rate\": 2.3923755513547575e-05,\n",
      "      \"loss\": 0.0145,\n",
      "      \"step\": 82800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.218658432842875,\n",
      "      \"grad_norm\": 0.5209406614303589,\n",
      "      \"learning_rate\": 2.39174543163201e-05,\n",
      "      \"loss\": 0.0189,\n",
      "      \"step\": 82820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.219918711994707,\n",
      "      \"grad_norm\": 0.35059159994125366,\n",
      "      \"learning_rate\": 2.3911153119092626e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 82840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.221178991146539,\n",
      "      \"grad_norm\": 0.005400823894888163,\n",
      "      \"learning_rate\": 2.3904851921865155e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 82860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.222439270298371,\n",
      "      \"grad_norm\": 0.13981056213378906,\n",
      "      \"learning_rate\": 2.3898550724637684e-05,\n",
      "      \"loss\": 0.019,\n",
      "      \"step\": 82880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.223699549450203,\n",
      "      \"grad_norm\": 0.0375690720975399,\n",
      "      \"learning_rate\": 2.389224952741021e-05,\n",
      "      \"loss\": 0.0173,\n",
      "      \"step\": 82900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.224959828602035,\n",
      "      \"grad_norm\": 0.003317613620311022,\n",
      "      \"learning_rate\": 2.3885948330182736e-05,\n",
      "      \"loss\": 0.0373,\n",
      "      \"step\": 82920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.226220107753868,\n",
      "      \"grad_norm\": 0.0026605327147990465,\n",
      "      \"learning_rate\": 2.387964713295526e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 82940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.227480386905699,\n",
      "      \"grad_norm\": 10.315479278564453,\n",
      "      \"learning_rate\": 2.387334593572779e-05,\n",
      "      \"loss\": 0.016,\n",
      "      \"step\": 82960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.228740666057532,\n",
      "      \"grad_norm\": 8.926589012145996,\n",
      "      \"learning_rate\": 2.3867044738500316e-05,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 82980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.230000945209364,\n",
      "      \"grad_norm\": 0.0022281224373728037,\n",
      "      \"learning_rate\": 2.386074354127284e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 83000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.231261224361196,\n",
      "      \"grad_norm\": 0.2697591483592987,\n",
      "      \"learning_rate\": 2.385444234404537e-05,\n",
      "      \"loss\": 0.0219,\n",
      "      \"step\": 83020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.232521503513028,\n",
      "      \"grad_norm\": 0.05008843168616295,\n",
      "      \"learning_rate\": 2.3848141146817896e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 83040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.2337817826648605,\n",
      "      \"grad_norm\": 0.012833363376557827,\n",
      "      \"learning_rate\": 2.3841839949590422e-05,\n",
      "      \"loss\": 0.0367,\n",
      "      \"step\": 83060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.235042061816692,\n",
      "      \"grad_norm\": 0.04473831504583359,\n",
      "      \"learning_rate\": 2.383553875236295e-05,\n",
      "      \"loss\": 0.0521,\n",
      "      \"step\": 83080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.236302340968525,\n",
      "      \"grad_norm\": 3.9896342754364014,\n",
      "      \"learning_rate\": 2.3829237555135477e-05,\n",
      "      \"loss\": 0.0032,\n",
      "      \"step\": 83100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.2375626201203564,\n",
      "      \"grad_norm\": 0.006809869781136513,\n",
      "      \"learning_rate\": 2.3822936357908006e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 83120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.238822899272189,\n",
      "      \"grad_norm\": 0.002050332259386778,\n",
      "      \"learning_rate\": 2.381663516068053e-05,\n",
      "      \"loss\": 0.006,\n",
      "      \"step\": 83140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.240083178424021,\n",
      "      \"grad_norm\": 0.0047536734491586685,\n",
      "      \"learning_rate\": 2.3810333963453057e-05,\n",
      "      \"loss\": 0.0397,\n",
      "      \"step\": 83160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.241343457575853,\n",
      "      \"grad_norm\": 0.010392732918262482,\n",
      "      \"learning_rate\": 2.3804032766225583e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 83180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.242603736727685,\n",
      "      \"grad_norm\": 0.014855213463306427,\n",
      "      \"learning_rate\": 2.379773156899811e-05,\n",
      "      \"loss\": 0.0339,\n",
      "      \"step\": 83200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.243864015879518,\n",
      "      \"grad_norm\": 0.0025324004236608744,\n",
      "      \"learning_rate\": 2.3791430371770637e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 83220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.245124295031349,\n",
      "      \"grad_norm\": 4.176977634429932,\n",
      "      \"learning_rate\": 2.3785129174543163e-05,\n",
      "      \"loss\": 0.0382,\n",
      "      \"step\": 83240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.246384574183182,\n",
      "      \"grad_norm\": 1.7829333543777466,\n",
      "      \"learning_rate\": 2.377882797731569e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 83260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.2476448533350135,\n",
      "      \"grad_norm\": 0.022946208715438843,\n",
      "      \"learning_rate\": 2.3772526780088218e-05,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 83280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.248905132486846,\n",
      "      \"grad_norm\": 0.008571004495024681,\n",
      "      \"learning_rate\": 2.3766225582860743e-05,\n",
      "      \"loss\": 0.0042,\n",
      "      \"step\": 83300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.250165411638678,\n",
      "      \"grad_norm\": 0.008875864557921886,\n",
      "      \"learning_rate\": 2.3759924385633272e-05,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 83320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.25142569079051,\n",
      "      \"grad_norm\": 0.006220362614840269,\n",
      "      \"learning_rate\": 2.3753623188405798e-05,\n",
      "      \"loss\": 0.0319,\n",
      "      \"step\": 83340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.252685969942342,\n",
      "      \"grad_norm\": 0.003611057996749878,\n",
      "      \"learning_rate\": 2.3747321991178327e-05,\n",
      "      \"loss\": 0.0426,\n",
      "      \"step\": 83360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.253946249094175,\n",
      "      \"grad_norm\": 0.23933997750282288,\n",
      "      \"learning_rate\": 2.3741020793950853e-05,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 83380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.255206528246006,\n",
      "      \"grad_norm\": 11.301862716674805,\n",
      "      \"learning_rate\": 2.3734719596723378e-05,\n",
      "      \"loss\": 0.0062,\n",
      "      \"step\": 83400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.256466807397839,\n",
      "      \"grad_norm\": 0.006632012315094471,\n",
      "      \"learning_rate\": 2.3728418399495904e-05,\n",
      "      \"loss\": 0.0233,\n",
      "      \"step\": 83420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.257727086549671,\n",
      "      \"grad_norm\": 0.0019974634051322937,\n",
      "      \"learning_rate\": 2.3722117202268433e-05,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 83440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.258987365701503,\n",
      "      \"grad_norm\": 0.004864959511905909,\n",
      "      \"learning_rate\": 2.371581600504096e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 83460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.260247644853335,\n",
      "      \"grad_norm\": 0.00197796244174242,\n",
      "      \"learning_rate\": 2.370982986767486e-05,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 83480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.261507924005167,\n",
      "      \"grad_norm\": 0.3095659017562866,\n",
      "      \"learning_rate\": 2.3703843730308757e-05,\n",
      "      \"loss\": 0.0058,\n",
      "      \"step\": 83500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.262768203156999,\n",
      "      \"grad_norm\": 0.1937735378742218,\n",
      "      \"learning_rate\": 2.369754253308129e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 83520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.264028482308832,\n",
      "      \"grad_norm\": 0.0008352154400199652,\n",
      "      \"learning_rate\": 2.3691241335853815e-05,\n",
      "      \"loss\": 0.0174,\n",
      "      \"step\": 83540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.265288761460663,\n",
      "      \"grad_norm\": 0.0036372726317495108,\n",
      "      \"learning_rate\": 2.368494013862634e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 83560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.266549040612496,\n",
      "      \"grad_norm\": 0.0018360280664637685,\n",
      "      \"learning_rate\": 2.3678638941398866e-05,\n",
      "      \"loss\": 0.0159,\n",
      "      \"step\": 83580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.267809319764328,\n",
      "      \"grad_norm\": 0.050337351858615875,\n",
      "      \"learning_rate\": 2.3672337744171395e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 83600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.26906959891616,\n",
      "      \"grad_norm\": 0.03229456767439842,\n",
      "      \"learning_rate\": 2.366603654694392e-05,\n",
      "      \"loss\": 0.039,\n",
      "      \"step\": 83620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.270329878067992,\n",
      "      \"grad_norm\": 0.002344343811273575,\n",
      "      \"learning_rate\": 2.3659735349716447e-05,\n",
      "      \"loss\": 0.0384,\n",
      "      \"step\": 83640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.2715901572198245,\n",
      "      \"grad_norm\": 3.553612232208252,\n",
      "      \"learning_rate\": 2.3653434152488972e-05,\n",
      "      \"loss\": 0.0324,\n",
      "      \"step\": 83660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.272850436371656,\n",
      "      \"grad_norm\": 0.00794168934226036,\n",
      "      \"learning_rate\": 2.36471329552615e-05,\n",
      "      \"loss\": 0.0232,\n",
      "      \"step\": 83680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.274110715523489,\n",
      "      \"grad_norm\": 14.103944778442383,\n",
      "      \"learning_rate\": 2.3640831758034027e-05,\n",
      "      \"loss\": 0.0223,\n",
      "      \"step\": 83700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.2753709946753204,\n",
      "      \"grad_norm\": 0.0012360626133158803,\n",
      "      \"learning_rate\": 2.3634530560806553e-05,\n",
      "      \"loss\": 0.04,\n",
      "      \"step\": 83720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.276631273827153,\n",
      "      \"grad_norm\": 0.5046100616455078,\n",
      "      \"learning_rate\": 2.3628229363579082e-05,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 83740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.277891552978985,\n",
      "      \"grad_norm\": 0.0014949473552405834,\n",
      "      \"learning_rate\": 2.3621928166351607e-05,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 83760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.279151832130817,\n",
      "      \"grad_norm\": 0.00131521699950099,\n",
      "      \"learning_rate\": 2.3615626969124136e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 83780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.280412111282649,\n",
      "      \"grad_norm\": 0.0008095631492324173,\n",
      "      \"learning_rate\": 2.3609325771896662e-05,\n",
      "      \"loss\": 0.0202,\n",
      "      \"step\": 83800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.281672390434482,\n",
      "      \"grad_norm\": 0.09555062651634216,\n",
      "      \"learning_rate\": 2.3603024574669188e-05,\n",
      "      \"loss\": 0.0039,\n",
      "      \"step\": 83820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.282932669586313,\n",
      "      \"grad_norm\": 0.0008657661965116858,\n",
      "      \"learning_rate\": 2.3596723377441713e-05,\n",
      "      \"loss\": 0.0143,\n",
      "      \"step\": 83840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.284192948738146,\n",
      "      \"grad_norm\": 0.0035948799923062325,\n",
      "      \"learning_rate\": 2.3590422180214242e-05,\n",
      "      \"loss\": 0.0252,\n",
      "      \"step\": 83860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.2854532278899775,\n",
      "      \"grad_norm\": 0.005359618458896875,\n",
      "      \"learning_rate\": 2.3584120982986768e-05,\n",
      "      \"loss\": 0.0301,\n",
      "      \"step\": 83880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.28671350704181,\n",
      "      \"grad_norm\": 0.010193627327680588,\n",
      "      \"learning_rate\": 2.3577819785759294e-05,\n",
      "      \"loss\": 0.0055,\n",
      "      \"step\": 83900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.287973786193642,\n",
      "      \"grad_norm\": 0.020772667601704597,\n",
      "      \"learning_rate\": 2.3571518588531823e-05,\n",
      "      \"loss\": 0.0064,\n",
      "      \"step\": 83920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.289234065345474,\n",
      "      \"grad_norm\": 0.00965371448546648,\n",
      "      \"learning_rate\": 2.356521739130435e-05,\n",
      "      \"loss\": 0.013,\n",
      "      \"step\": 83940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.290494344497306,\n",
      "      \"grad_norm\": 0.010522636584937572,\n",
      "      \"learning_rate\": 2.3558916194076878e-05,\n",
      "      \"loss\": 0.0479,\n",
      "      \"step\": 83960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.291754623649139,\n",
      "      \"grad_norm\": 0.00362101080827415,\n",
      "      \"learning_rate\": 2.3552614996849403e-05,\n",
      "      \"loss\": 0.0348,\n",
      "      \"step\": 83980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.29301490280097,\n",
      "      \"grad_norm\": 0.0014496449148282409,\n",
      "      \"learning_rate\": 2.354631379962193e-05,\n",
      "      \"loss\": 0.0282,\n",
      "      \"step\": 84000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.294275181952803,\n",
      "      \"grad_norm\": 0.030102016404271126,\n",
      "      \"learning_rate\": 2.3540012602394458e-05,\n",
      "      \"loss\": 0.0019,\n",
      "      \"step\": 84020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.295535461104635,\n",
      "      \"grad_norm\": 0.011565773747861385,\n",
      "      \"learning_rate\": 2.3533711405166984e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 84040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.296795740256467,\n",
      "      \"grad_norm\": 0.010541313327848911,\n",
      "      \"learning_rate\": 2.352741020793951e-05,\n",
      "      \"loss\": 0.0136,\n",
      "      \"step\": 84060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.298056019408299,\n",
      "      \"grad_norm\": 0.06942039728164673,\n",
      "      \"learning_rate\": 2.3521109010712035e-05,\n",
      "      \"loss\": 0.0041,\n",
      "      \"step\": 84080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.299316298560131,\n",
      "      \"grad_norm\": 0.0011185731273144484,\n",
      "      \"learning_rate\": 2.3514807813484564e-05,\n",
      "      \"loss\": 0.0037,\n",
      "      \"step\": 84100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.300576577711963,\n",
      "      \"grad_norm\": 0.00793964508920908,\n",
      "      \"learning_rate\": 2.350850661625709e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 84120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.301836856863796,\n",
      "      \"grad_norm\": 0.12175828963518143,\n",
      "      \"learning_rate\": 2.3502205419029615e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 84140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.303097136015627,\n",
      "      \"grad_norm\": 0.0009378021350130439,\n",
      "      \"learning_rate\": 2.349590422180214e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 84160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.30435741516746,\n",
      "      \"grad_norm\": 0.000360706151695922,\n",
      "      \"learning_rate\": 2.348960302457467e-05,\n",
      "      \"loss\": 0.0143,\n",
      "      \"step\": 84180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.305617694319292,\n",
      "      \"grad_norm\": 0.0004651897179428488,\n",
      "      \"learning_rate\": 2.34833018273472e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 84200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.306877973471124,\n",
      "      \"grad_norm\": 9.259549140930176,\n",
      "      \"learning_rate\": 2.3477000630119725e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 84220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.308138252622956,\n",
      "      \"grad_norm\": 0.011801374144852161,\n",
      "      \"learning_rate\": 2.347069943289225e-05,\n",
      "      \"loss\": 0.0248,\n",
      "      \"step\": 84240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.3093985317747885,\n",
      "      \"grad_norm\": 0.4130842685699463,\n",
      "      \"learning_rate\": 2.346439823566478e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 84260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.31065881092662,\n",
      "      \"grad_norm\": 0.04181987792253494,\n",
      "      \"learning_rate\": 2.3458097038437305e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 84280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.311919090078453,\n",
      "      \"grad_norm\": 0.025852981954813004,\n",
      "      \"learning_rate\": 2.345179584120983e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 84300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.313179369230284,\n",
      "      \"grad_norm\": 16.95008087158203,\n",
      "      \"learning_rate\": 2.3445494643982356e-05,\n",
      "      \"loss\": 0.064,\n",
      "      \"step\": 84320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.314439648382117,\n",
      "      \"grad_norm\": 0.10771259665489197,\n",
      "      \"learning_rate\": 2.3439193446754885e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 84340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.315699927533949,\n",
      "      \"grad_norm\": 18.147916793823242,\n",
      "      \"learning_rate\": 2.343289224952741e-05,\n",
      "      \"loss\": 0.0252,\n",
      "      \"step\": 84360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.316960206685781,\n",
      "      \"grad_norm\": 0.0009041758021339774,\n",
      "      \"learning_rate\": 2.3426591052299937e-05,\n",
      "      \"loss\": 0.0159,\n",
      "      \"step\": 84380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.318220485837613,\n",
      "      \"grad_norm\": 0.003992210607975721,\n",
      "      \"learning_rate\": 2.3420289855072462e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 84400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.3194807649894456,\n",
      "      \"grad_norm\": 0.023170538246631622,\n",
      "      \"learning_rate\": 2.3413988657844995e-05,\n",
      "      \"loss\": 0.0456,\n",
      "      \"step\": 84420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.320741044141277,\n",
      "      \"grad_norm\": 0.011827301234006882,\n",
      "      \"learning_rate\": 2.340768746061752e-05,\n",
      "      \"loss\": 0.0061,\n",
      "      \"step\": 84440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.32200132329311,\n",
      "      \"grad_norm\": 0.0016204486601054668,\n",
      "      \"learning_rate\": 2.3401386263390046e-05,\n",
      "      \"loss\": 0.0367,\n",
      "      \"step\": 84460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.3232616024449415,\n",
      "      \"grad_norm\": 0.005431188270449638,\n",
      "      \"learning_rate\": 2.339508506616257e-05,\n",
      "      \"loss\": 0.0365,\n",
      "      \"step\": 84480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.324521881596774,\n",
      "      \"grad_norm\": 0.005720659624785185,\n",
      "      \"learning_rate\": 2.33887838689351e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 84500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.325782160748606,\n",
      "      \"grad_norm\": 0.07899793237447739,\n",
      "      \"learning_rate\": 2.3382482671707626e-05,\n",
      "      \"loss\": 0.0541,\n",
      "      \"step\": 84520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.327042439900438,\n",
      "      \"grad_norm\": 0.21910642087459564,\n",
      "      \"learning_rate\": 2.3376181474480152e-05,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 84540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.32830271905227,\n",
      "      \"grad_norm\": 0.016033977270126343,\n",
      "      \"learning_rate\": 2.3369880277252678e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 84560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.329562998204103,\n",
      "      \"grad_norm\": 0.0008356604957953095,\n",
      "      \"learning_rate\": 2.3363579080025207e-05,\n",
      "      \"loss\": 0.0196,\n",
      "      \"step\": 84580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.330823277355934,\n",
      "      \"grad_norm\": 16.811365127563477,\n",
      "      \"learning_rate\": 2.3357277882797732e-05,\n",
      "      \"loss\": 0.0216,\n",
      "      \"step\": 84600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.332083556507767,\n",
      "      \"grad_norm\": 0.0019646266009658575,\n",
      "      \"learning_rate\": 2.3350976685570258e-05,\n",
      "      \"loss\": 0.0143,\n",
      "      \"step\": 84620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.333343835659599,\n",
      "      \"grad_norm\": 0.0008764279773458838,\n",
      "      \"learning_rate\": 2.3344675488342787e-05,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 84640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.33460411481143,\n",
      "      \"grad_norm\": 0.0007651488995179534,\n",
      "      \"learning_rate\": 2.3338374291115313e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 84660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.335864393963263,\n",
      "      \"grad_norm\": 6.7257232666015625,\n",
      "      \"learning_rate\": 2.333207309388784e-05,\n",
      "      \"loss\": 0.0298,\n",
      "      \"step\": 84680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.337124673115095,\n",
      "      \"grad_norm\": 0.026165876537561417,\n",
      "      \"learning_rate\": 2.3325771896660367e-05,\n",
      "      \"loss\": 0.0334,\n",
      "      \"step\": 84700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.338384952266927,\n",
      "      \"grad_norm\": 0.049498300999403,\n",
      "      \"learning_rate\": 2.3319470699432893e-05,\n",
      "      \"loss\": 0.0349,\n",
      "      \"step\": 84720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.339645231418759,\n",
      "      \"grad_norm\": 0.0010268492624163628,\n",
      "      \"learning_rate\": 2.3313169502205422e-05,\n",
      "      \"loss\": 0.0184,\n",
      "      \"step\": 84740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.340905510570591,\n",
      "      \"grad_norm\": 0.016755225136876106,\n",
      "      \"learning_rate\": 2.3306868304977948e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 84760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.342165789722424,\n",
      "      \"grad_norm\": 0.006165320053696632,\n",
      "      \"learning_rate\": 2.3300567107750473e-05,\n",
      "      \"loss\": 0.0188,\n",
      "      \"step\": 84780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.343426068874256,\n",
      "      \"grad_norm\": 6.777063369750977,\n",
      "      \"learning_rate\": 2.3294265910523e-05,\n",
      "      \"loss\": 0.0058,\n",
      "      \"step\": 84800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.344686348026087,\n",
      "      \"grad_norm\": 0.028492437675595284,\n",
      "      \"learning_rate\": 2.3287964713295528e-05,\n",
      "      \"loss\": 0.0174,\n",
      "      \"step\": 84820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.34594662717792,\n",
      "      \"grad_norm\": 0.1319873034954071,\n",
      "      \"learning_rate\": 2.3281663516068054e-05,\n",
      "      \"loss\": 0.0462,\n",
      "      \"step\": 84840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.3472069063297525,\n",
      "      \"grad_norm\": 0.0027963542379438877,\n",
      "      \"learning_rate\": 2.327536231884058e-05,\n",
      "      \"loss\": 0.0274,\n",
      "      \"step\": 84860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.348467185481584,\n",
      "      \"grad_norm\": 0.009105570614337921,\n",
      "      \"learning_rate\": 2.3269061121613108e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 84880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.349727464633416,\n",
      "      \"grad_norm\": 0.07087116688489914,\n",
      "      \"learning_rate\": 2.3262759924385634e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 84900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.350987743785248,\n",
      "      \"grad_norm\": 0.35353800654411316,\n",
      "      \"learning_rate\": 2.3256458727158163e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 84920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.352248022937081,\n",
      "      \"grad_norm\": 0.0004298600251786411,\n",
      "      \"learning_rate\": 2.325015752993069e-05,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 84940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.353508302088913,\n",
      "      \"grad_norm\": 0.04871227964758873,\n",
      "      \"learning_rate\": 2.3243856332703214e-05,\n",
      "      \"loss\": 0.0056,\n",
      "      \"step\": 84960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.354768581240744,\n",
      "      \"grad_norm\": 0.005586165469139814,\n",
      "      \"learning_rate\": 2.323755513547574e-05,\n",
      "      \"loss\": 0.0142,\n",
      "      \"step\": 84980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.356028860392577,\n",
      "      \"grad_norm\": 0.07739666104316711,\n",
      "      \"learning_rate\": 2.323125393824827e-05,\n",
      "      \"loss\": 0.0495,\n",
      "      \"step\": 85000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.3572891395444096,\n",
      "      \"grad_norm\": 0.0007928722188808024,\n",
      "      \"learning_rate\": 2.3224952741020795e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 85020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.358549418696241,\n",
      "      \"grad_norm\": 0.0005838940851390362,\n",
      "      \"learning_rate\": 2.321865154379332e-05,\n",
      "      \"loss\": 0.0436,\n",
      "      \"step\": 85040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.359809697848073,\n",
      "      \"grad_norm\": 0.016110097989439964,\n",
      "      \"learning_rate\": 2.3212350346565846e-05,\n",
      "      \"loss\": 0.0208,\n",
      "      \"step\": 85060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.3610699769999055,\n",
      "      \"grad_norm\": 0.0022749800700694323,\n",
      "      \"learning_rate\": 2.3206049149338375e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 85080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.362330256151737,\n",
      "      \"grad_norm\": 0.0028829167131334543,\n",
      "      \"learning_rate\": 2.3199747952110904e-05,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 85100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.36359053530357,\n",
      "      \"grad_norm\": 0.03126812353730202,\n",
      "      \"learning_rate\": 2.319344675488343e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 85120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.3648508144554015,\n",
      "      \"grad_norm\": 5.87888240814209,\n",
      "      \"learning_rate\": 2.3187145557655955e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 85140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.366111093607234,\n",
      "      \"grad_norm\": 0.001789239002391696,\n",
      "      \"learning_rate\": 2.3180844360428484e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 85160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.367371372759066,\n",
      "      \"grad_norm\": 0.8645703792572021,\n",
      "      \"learning_rate\": 2.317454316320101e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 85180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.368631651910898,\n",
      "      \"grad_norm\": 8.412240028381348,\n",
      "      \"learning_rate\": 2.3168241965973536e-05,\n",
      "      \"loss\": 0.0183,\n",
      "      \"step\": 85200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.36989193106273,\n",
      "      \"grad_norm\": 0.0026461670640856028,\n",
      "      \"learning_rate\": 2.316194076874606e-05,\n",
      "      \"loss\": 0.0494,\n",
      "      \"step\": 85220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.371152210214563,\n",
      "      \"grad_norm\": 0.0019548479467630386,\n",
      "      \"learning_rate\": 2.315563957151859e-05,\n",
      "      \"loss\": 0.029,\n",
      "      \"step\": 85240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.372412489366394,\n",
      "      \"grad_norm\": 0.004068815149366856,\n",
      "      \"learning_rate\": 2.3149338374291116e-05,\n",
      "      \"loss\": 0.0173,\n",
      "      \"step\": 85260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.373672768518227,\n",
      "      \"grad_norm\": 0.02870008908212185,\n",
      "      \"learning_rate\": 2.314303717706364e-05,\n",
      "      \"loss\": 0.0122,\n",
      "      \"step\": 85280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.3749330476700585,\n",
      "      \"grad_norm\": 0.009319869801402092,\n",
      "      \"learning_rate\": 2.3136735979836167e-05,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 85300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.376193326821891,\n",
      "      \"grad_norm\": 0.29057547450065613,\n",
      "      \"learning_rate\": 2.31304347826087e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 85320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.377453605973723,\n",
      "      \"grad_norm\": 0.0009090827079489827,\n",
      "      \"learning_rate\": 2.3124133585381225e-05,\n",
      "      \"loss\": 0.0122,\n",
      "      \"step\": 85340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.378713885125555,\n",
      "      \"grad_norm\": 0.001646307180635631,\n",
      "      \"learning_rate\": 2.311783238815375e-05,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 85360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.379974164277387,\n",
      "      \"grad_norm\": 0.0018855798989534378,\n",
      "      \"learning_rate\": 2.3111531190926277e-05,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 85380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.38123444342922,\n",
      "      \"grad_norm\": 0.004705094266682863,\n",
      "      \"learning_rate\": 2.3105229993698806e-05,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 85400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.382494722581051,\n",
      "      \"grad_norm\": 0.0016188498120754957,\n",
      "      \"learning_rate\": 2.309892879647133e-05,\n",
      "      \"loss\": 0.0276,\n",
      "      \"step\": 85420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.383755001732884,\n",
      "      \"grad_norm\": 0.00937514379620552,\n",
      "      \"learning_rate\": 2.3092627599243857e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 85440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.385015280884716,\n",
      "      \"grad_norm\": 0.0005420050001703203,\n",
      "      \"learning_rate\": 2.3086326402016383e-05,\n",
      "      \"loss\": 0.0411,\n",
      "      \"step\": 85460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.386275560036548,\n",
      "      \"grad_norm\": 0.0010555500630289316,\n",
      "      \"learning_rate\": 2.3080025204788912e-05,\n",
      "      \"loss\": 0.0341,\n",
      "      \"step\": 85480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.38753583918838,\n",
      "      \"grad_norm\": 0.002130316337570548,\n",
      "      \"learning_rate\": 2.3073724007561437e-05,\n",
      "      \"loss\": 0.0183,\n",
      "      \"step\": 85500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.388796118340212,\n",
      "      \"grad_norm\": 0.006175072863698006,\n",
      "      \"learning_rate\": 2.3067422810333963e-05,\n",
      "      \"loss\": 0.0034,\n",
      "      \"step\": 85520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.390056397492044,\n",
      "      \"grad_norm\": 20.53880500793457,\n",
      "      \"learning_rate\": 2.306112161310649e-05,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 85540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.391316676643877,\n",
      "      \"grad_norm\": 0.07922857254743576,\n",
      "      \"learning_rate\": 2.3054820415879018e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 85560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.392576955795708,\n",
      "      \"grad_norm\": 0.006704352796077728,\n",
      "      \"learning_rate\": 2.3048519218651547e-05,\n",
      "      \"loss\": 0.0202,\n",
      "      \"step\": 85580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.393837234947541,\n",
      "      \"grad_norm\": 0.0014638148713856936,\n",
      "      \"learning_rate\": 2.3042218021424072e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 85600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.395097514099373,\n",
      "      \"grad_norm\": 0.0006779681425541639,\n",
      "      \"learning_rate\": 2.3035916824196598e-05,\n",
      "      \"loss\": 0.0322,\n",
      "      \"step\": 85620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.396357793251205,\n",
      "      \"grad_norm\": 0.0005926936864852905,\n",
      "      \"learning_rate\": 2.3029615626969127e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 85640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.397618072403037,\n",
      "      \"grad_norm\": 0.04655135050415993,\n",
      "      \"learning_rate\": 2.3023314429741653e-05,\n",
      "      \"loss\": 0.0533,\n",
      "      \"step\": 85660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.3988783515548695,\n",
      "      \"grad_norm\": 0.004362482577562332,\n",
      "      \"learning_rate\": 2.301701323251418e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 85680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.400138630706701,\n",
      "      \"grad_norm\": 0.000772452971432358,\n",
      "      \"learning_rate\": 2.3010712035286704e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 85700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.401398909858534,\n",
      "      \"grad_norm\": 0.09690328687429428,\n",
      "      \"learning_rate\": 2.3004410838059233e-05,\n",
      "      \"loss\": 0.0233,\n",
      "      \"step\": 85720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.4026591890103655,\n",
      "      \"grad_norm\": 0.0008948735194280744,\n",
      "      \"learning_rate\": 2.299810964083176e-05,\n",
      "      \"loss\": 0.0132,\n",
      "      \"step\": 85740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.403919468162198,\n",
      "      \"grad_norm\": 0.21135644614696503,\n",
      "      \"learning_rate\": 2.2991808443604284e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 85760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.40517974731403,\n",
      "      \"grad_norm\": 0.013822422362864017,\n",
      "      \"learning_rate\": 2.2985507246376813e-05,\n",
      "      \"loss\": 0.0075,\n",
      "      \"step\": 85780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.406440026465862,\n",
      "      \"grad_norm\": 0.031080085784196854,\n",
      "      \"learning_rate\": 2.297920604914934e-05,\n",
      "      \"loss\": 0.0436,\n",
      "      \"step\": 85800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.407700305617694,\n",
      "      \"grad_norm\": 0.0021775655914098024,\n",
      "      \"learning_rate\": 2.2972904851921868e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 85820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.408960584769527,\n",
      "      \"grad_norm\": 0.022006457671523094,\n",
      "      \"learning_rate\": 2.2966603654694394e-05,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 85840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.410220863921358,\n",
      "      \"grad_norm\": 0.00511700427159667,\n",
      "      \"learning_rate\": 2.296030245746692e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 85860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.411481143073191,\n",
      "      \"grad_norm\": 0.14670194685459137,\n",
      "      \"learning_rate\": 2.2954001260239445e-05,\n",
      "      \"loss\": 0.0032,\n",
      "      \"step\": 85880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.4127414222250225,\n",
      "      \"grad_norm\": 0.007180942688137293,\n",
      "      \"learning_rate\": 2.2947700063011974e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 85900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.414001701376855,\n",
      "      \"grad_norm\": 0.0007076745969243348,\n",
      "      \"learning_rate\": 2.29413988657845e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 85920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.415261980528687,\n",
      "      \"grad_norm\": 0.0007213577046059072,\n",
      "      \"learning_rate\": 2.2935097668557025e-05,\n",
      "      \"loss\": 0.0227,\n",
      "      \"step\": 85940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.416522259680519,\n",
      "      \"grad_norm\": 0.0010178874945268035,\n",
      "      \"learning_rate\": 2.2928796471329554e-05,\n",
      "      \"loss\": 0.0341,\n",
      "      \"step\": 85960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.417782538832351,\n",
      "      \"grad_norm\": 0.0029887764248996973,\n",
      "      \"learning_rate\": 2.292249527410208e-05,\n",
      "      \"loss\": 0.038,\n",
      "      \"step\": 85980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.419042817984184,\n",
      "      \"grad_norm\": 0.10312183946371078,\n",
      "      \"learning_rate\": 2.291619407687461e-05,\n",
      "      \"loss\": 0.027,\n",
      "      \"step\": 86000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.420303097136015,\n",
      "      \"grad_norm\": 0.0024791364558041096,\n",
      "      \"learning_rate\": 2.2909892879647135e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 86020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.421563376287848,\n",
      "      \"grad_norm\": 0.0029920951928943396,\n",
      "      \"learning_rate\": 2.290359168241966e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 86040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.42282365543968,\n",
      "      \"grad_norm\": 0.003252253867685795,\n",
      "      \"learning_rate\": 2.289729048519219e-05,\n",
      "      \"loss\": 0.0355,\n",
      "      \"step\": 86060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.424083934591512,\n",
      "      \"grad_norm\": 0.0035246156621724367,\n",
      "      \"learning_rate\": 2.2890989287964715e-05,\n",
      "      \"loss\": 0.0204,\n",
      "      \"step\": 86080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.425344213743344,\n",
      "      \"grad_norm\": 0.003171778516843915,\n",
      "      \"learning_rate\": 2.288468809073724e-05,\n",
      "      \"loss\": 0.0032,\n",
      "      \"step\": 86100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.426604492895176,\n",
      "      \"grad_norm\": 0.001594375935383141,\n",
      "      \"learning_rate\": 2.2878386893509766e-05,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 86120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.427864772047008,\n",
      "      \"grad_norm\": 0.00496600242331624,\n",
      "      \"learning_rate\": 2.2872085696282295e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 86140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.429125051198841,\n",
      "      \"grad_norm\": 0.019978445023298264,\n",
      "      \"learning_rate\": 2.286578449905482e-05,\n",
      "      \"loss\": 0.0377,\n",
      "      \"step\": 86160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.430385330350672,\n",
      "      \"grad_norm\": 0.011827589944005013,\n",
      "      \"learning_rate\": 2.2859483301827347e-05,\n",
      "      \"loss\": 0.0075,\n",
      "      \"step\": 86180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.431645609502505,\n",
      "      \"grad_norm\": 0.010100946761667728,\n",
      "      \"learning_rate\": 2.2853182104599872e-05,\n",
      "      \"loss\": 0.0243,\n",
      "      \"step\": 86200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.432905888654337,\n",
      "      \"grad_norm\": 0.1366000920534134,\n",
      "      \"learning_rate\": 2.28468809073724e-05,\n",
      "      \"loss\": 0.0183,\n",
      "      \"step\": 86220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.434166167806169,\n",
      "      \"grad_norm\": 0.09395290166139603,\n",
      "      \"learning_rate\": 2.284057971014493e-05,\n",
      "      \"loss\": 0.0264,\n",
      "      \"step\": 86240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.435426446958001,\n",
      "      \"grad_norm\": 0.0037815694231539965,\n",
      "      \"learning_rate\": 2.2834278512917456e-05,\n",
      "      \"loss\": 0.0498,\n",
      "      \"step\": 86260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.4366867261098335,\n",
      "      \"grad_norm\": 0.025669962167739868,\n",
      "      \"learning_rate\": 2.2827977315689982e-05,\n",
      "      \"loss\": 0.0323,\n",
      "      \"step\": 86280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.437947005261665,\n",
      "      \"grad_norm\": 0.004048263654112816,\n",
      "      \"learning_rate\": 2.282167611846251e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 86300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.439207284413498,\n",
      "      \"grad_norm\": 11.852398872375488,\n",
      "      \"learning_rate\": 2.2815374921235036e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 86320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.4404675635653295,\n",
      "      \"grad_norm\": 0.49117517471313477,\n",
      "      \"learning_rate\": 2.2809073724007562e-05,\n",
      "      \"loss\": 0.0267,\n",
      "      \"step\": 86340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.441727842717162,\n",
      "      \"grad_norm\": 0.0012512950925156474,\n",
      "      \"learning_rate\": 2.2802772526780088e-05,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 86360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.442988121868994,\n",
      "      \"grad_norm\": 0.001864260295405984,\n",
      "      \"learning_rate\": 2.2796471329552617e-05,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 86380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.444248401020826,\n",
      "      \"grad_norm\": 0.07196753472089767,\n",
      "      \"learning_rate\": 2.2790170132325142e-05,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 86400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.445508680172658,\n",
      "      \"grad_norm\": 0.034530069679021835,\n",
      "      \"learning_rate\": 2.2783868935097668e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 86420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.446768959324491,\n",
      "      \"grad_norm\": 0.00046138331526890397,\n",
      "      \"learning_rate\": 2.2777567737870194e-05,\n",
      "      \"loss\": 0.0031,\n",
      "      \"step\": 86440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.448029238476322,\n",
      "      \"grad_norm\": 0.032663051038980484,\n",
      "      \"learning_rate\": 2.2771266540642723e-05,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 86460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.449289517628155,\n",
      "      \"grad_norm\": 0.0016480999765917659,\n",
      "      \"learning_rate\": 2.2764965343415252e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 86480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.4505497967799865,\n",
      "      \"grad_norm\": 0.0004659947007894516,\n",
      "      \"learning_rate\": 2.2758664146187778e-05,\n",
      "      \"loss\": 0.0157,\n",
      "      \"step\": 86500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.451810075931819,\n",
      "      \"grad_norm\": 0.027052944526076317,\n",
      "      \"learning_rate\": 2.2752362948960303e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 86520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.453070355083651,\n",
      "      \"grad_norm\": 8.272047996520996,\n",
      "      \"learning_rate\": 2.2746061751732832e-05,\n",
      "      \"loss\": 0.0374,\n",
      "      \"step\": 86540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.454330634235483,\n",
      "      \"grad_norm\": 0.0009064013138413429,\n",
      "      \"learning_rate\": 2.2739760554505358e-05,\n",
      "      \"loss\": 0.0287,\n",
      "      \"step\": 86560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.455590913387315,\n",
      "      \"grad_norm\": 4.481412410736084,\n",
      "      \"learning_rate\": 2.2733459357277884e-05,\n",
      "      \"loss\": 0.0237,\n",
      "      \"step\": 86580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.456851192539148,\n",
      "      \"grad_norm\": 12.338025093078613,\n",
      "      \"learning_rate\": 2.272715816005041e-05,\n",
      "      \"loss\": 0.0642,\n",
      "      \"step\": 86600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.458111471690979,\n",
      "      \"grad_norm\": 0.02966887317597866,\n",
      "      \"learning_rate\": 2.2720856962822938e-05,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 86620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.459371750842812,\n",
      "      \"grad_norm\": 0.09120000153779984,\n",
      "      \"learning_rate\": 2.2714555765595464e-05,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 86640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.460632029994644,\n",
      "      \"grad_norm\": 0.0035224182065576315,\n",
      "      \"learning_rate\": 2.270825456836799e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 86660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.461892309146476,\n",
      "      \"grad_norm\": 0.004641466774046421,\n",
      "      \"learning_rate\": 2.270195337114052e-05,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 86680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.463152588298308,\n",
      "      \"grad_norm\": 0.004048832692205906,\n",
      "      \"learning_rate\": 2.2695652173913044e-05,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 86700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.46441286745014,\n",
      "      \"grad_norm\": 0.001125489128753543,\n",
      "      \"learning_rate\": 2.2689350976685573e-05,\n",
      "      \"loss\": 0.006,\n",
      "      \"step\": 86720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.465673146601972,\n",
      "      \"grad_norm\": 0.000732273852918297,\n",
      "      \"learning_rate\": 2.26830497794581e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 86740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.466933425753805,\n",
      "      \"grad_norm\": 0.40933921933174133,\n",
      "      \"learning_rate\": 2.2676748582230625e-05,\n",
      "      \"loss\": 0.0298,\n",
      "      \"step\": 86760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.468193704905636,\n",
      "      \"grad_norm\": 0.007265307474881411,\n",
      "      \"learning_rate\": 2.267044738500315e-05,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 86780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.469453984057469,\n",
      "      \"grad_norm\": 0.0053413063287734985,\n",
      "      \"learning_rate\": 2.266414618777568e-05,\n",
      "      \"loss\": 0.0019,\n",
      "      \"step\": 86800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.470714263209301,\n",
      "      \"grad_norm\": 0.014482172206044197,\n",
      "      \"learning_rate\": 2.2657844990548205e-05,\n",
      "      \"loss\": 0.0464,\n",
      "      \"step\": 86820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.471974542361133,\n",
      "      \"grad_norm\": 5.843480587005615,\n",
      "      \"learning_rate\": 2.265154379332073e-05,\n",
      "      \"loss\": 0.0491,\n",
      "      \"step\": 86840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.473234821512965,\n",
      "      \"grad_norm\": 0.0037736922968178988,\n",
      "      \"learning_rate\": 2.264524259609326e-05,\n",
      "      \"loss\": 0.0261,\n",
      "      \"step\": 86860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.4744951006647975,\n",
      "      \"grad_norm\": 0.022730093449354172,\n",
      "      \"learning_rate\": 2.2638941398865785e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 86880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.475755379816629,\n",
      "      \"grad_norm\": 0.0025984575040638447,\n",
      "      \"learning_rate\": 2.263264020163831e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 86900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.477015658968462,\n",
      "      \"grad_norm\": 0.043770186603069305,\n",
      "      \"learning_rate\": 2.262633900441084e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 86920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.4782759381202935,\n",
      "      \"grad_norm\": 0.0117409722879529,\n",
      "      \"learning_rate\": 2.2620037807183366e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 86940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.479536217272126,\n",
      "      \"grad_norm\": 0.013199751265347004,\n",
      "      \"learning_rate\": 2.2613736609955895e-05,\n",
      "      \"loss\": 0.0142,\n",
      "      \"step\": 86960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.480796496423958,\n",
      "      \"grad_norm\": 0.016390003263950348,\n",
      "      \"learning_rate\": 2.260743541272842e-05,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 86980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.48205677557579,\n",
      "      \"grad_norm\": 0.14619392156600952,\n",
      "      \"learning_rate\": 2.2601134215500946e-05,\n",
      "      \"loss\": 0.0121,\n",
      "      \"step\": 87000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.483317054727622,\n",
      "      \"grad_norm\": 0.0004999220254831016,\n",
      "      \"learning_rate\": 2.259483301827347e-05,\n",
      "      \"loss\": 0.0132,\n",
      "      \"step\": 87020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.484577333879455,\n",
      "      \"grad_norm\": 0.022878967225551605,\n",
      "      \"learning_rate\": 2.2588531821046e-05,\n",
      "      \"loss\": 0.0135,\n",
      "      \"step\": 87040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.485837613031286,\n",
      "      \"grad_norm\": 0.0006393283838406205,\n",
      "      \"learning_rate\": 2.2582230623818526e-05,\n",
      "      \"loss\": 0.0115,\n",
      "      \"step\": 87060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.487097892183119,\n",
      "      \"grad_norm\": 0.0006202819640748203,\n",
      "      \"learning_rate\": 2.2575929426591052e-05,\n",
      "      \"loss\": 0.0206,\n",
      "      \"step\": 87080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.4883581713349505,\n",
      "      \"grad_norm\": 0.061533983796834946,\n",
      "      \"learning_rate\": 2.2569628229363578e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 87100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.489618450486783,\n",
      "      \"grad_norm\": 0.007822615094482899,\n",
      "      \"learning_rate\": 2.2563327032136107e-05,\n",
      "      \"loss\": 0.029,\n",
      "      \"step\": 87120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.490878729638615,\n",
      "      \"grad_norm\": 5.394133567810059,\n",
      "      \"learning_rate\": 2.2557025834908636e-05,\n",
      "      \"loss\": 0.0161,\n",
      "      \"step\": 87140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.492139008790447,\n",
      "      \"grad_norm\": 0.008713532239198685,\n",
      "      \"learning_rate\": 2.255072463768116e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 87160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.493399287942279,\n",
      "      \"grad_norm\": 0.0010259801056236029,\n",
      "      \"learning_rate\": 2.2544423440453687e-05,\n",
      "      \"loss\": 0.0132,\n",
      "      \"step\": 87180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.494659567094112,\n",
      "      \"grad_norm\": 0.03147391602396965,\n",
      "      \"learning_rate\": 2.2538122243226216e-05,\n",
      "      \"loss\": 0.0165,\n",
      "      \"step\": 87200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.495919846245943,\n",
      "      \"grad_norm\": 0.0030310829170048237,\n",
      "      \"learning_rate\": 2.253182104599874e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 87220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.497180125397776,\n",
      "      \"grad_norm\": 0.0007144596893340349,\n",
      "      \"learning_rate\": 2.2525519848771267e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 87240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.498440404549608,\n",
      "      \"grad_norm\": 0.0025494673755019903,\n",
      "      \"learning_rate\": 2.2519218651543793e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 87260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.49970068370144,\n",
      "      \"grad_norm\": 0.010234605520963669,\n",
      "      \"learning_rate\": 2.2512917454316322e-05,\n",
      "      \"loss\": 0.0116,\n",
      "      \"step\": 87280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.500960962853272,\n",
      "      \"grad_norm\": 0.01067332737147808,\n",
      "      \"learning_rate\": 2.2506616257088848e-05,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 87300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.502221242005104,\n",
      "      \"grad_norm\": 0.00026471467572264373,\n",
      "      \"learning_rate\": 2.2500315059861373e-05,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 87320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.503481521156936,\n",
      "      \"grad_norm\": 7.4567952156066895,\n",
      "      \"learning_rate\": 2.24940138626339e-05,\n",
      "      \"loss\": 0.036,\n",
      "      \"step\": 87340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.504741800308769,\n",
      "      \"grad_norm\": 5.333784103393555,\n",
      "      \"learning_rate\": 2.2487712665406428e-05,\n",
      "      \"loss\": 0.0374,\n",
      "      \"step\": 87360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.5060020794606,\n",
      "      \"grad_norm\": 0.13276255130767822,\n",
      "      \"learning_rate\": 2.2481411468178957e-05,\n",
      "      \"loss\": 0.071,\n",
      "      \"step\": 87380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.507262358612433,\n",
      "      \"grad_norm\": 8.774532318115234,\n",
      "      \"learning_rate\": 2.2475110270951483e-05,\n",
      "      \"loss\": 0.0279,\n",
      "      \"step\": 87400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.508522637764265,\n",
      "      \"grad_norm\": 0.19475606083869934,\n",
      "      \"learning_rate\": 2.2468809073724008e-05,\n",
      "      \"loss\": 0.0019,\n",
      "      \"step\": 87420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.509782916916097,\n",
      "      \"grad_norm\": 0.15355946123600006,\n",
      "      \"learning_rate\": 2.2462507876496537e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 87440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.511043196067929,\n",
      "      \"grad_norm\": 0.0907512828707695,\n",
      "      \"learning_rate\": 2.2456206679269063e-05,\n",
      "      \"loss\": 0.0131,\n",
      "      \"step\": 87460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.5123034752197615,\n",
      "      \"grad_norm\": 0.030716070905327797,\n",
      "      \"learning_rate\": 2.244990548204159e-05,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 87480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.513563754371593,\n",
      "      \"grad_norm\": 0.22119376063346863,\n",
      "      \"learning_rate\": 2.2443604284814114e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 87500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.514824033523426,\n",
      "      \"grad_norm\": 0.12696385383605957,\n",
      "      \"learning_rate\": 2.2437303087586643e-05,\n",
      "      \"loss\": 0.0295,\n",
      "      \"step\": 87520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.5160843126752575,\n",
      "      \"grad_norm\": 0.10477084666490555,\n",
      "      \"learning_rate\": 2.243100189035917e-05,\n",
      "      \"loss\": 0.0165,\n",
      "      \"step\": 87540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.51734459182709,\n",
      "      \"grad_norm\": 0.8977871537208557,\n",
      "      \"learning_rate\": 2.2424700693131695e-05,\n",
      "      \"loss\": 0.0341,\n",
      "      \"step\": 87560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.518604870978922,\n",
      "      \"grad_norm\": 0.0985146313905716,\n",
      "      \"learning_rate\": 2.241839949590422e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 87580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.519865150130754,\n",
      "      \"grad_norm\": 0.0010025561787188053,\n",
      "      \"learning_rate\": 2.241209829867675e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 87600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.521125429282586,\n",
      "      \"grad_norm\": 0.002639858052134514,\n",
      "      \"learning_rate\": 2.240579710144928e-05,\n",
      "      \"loss\": 0.0228,\n",
      "      \"step\": 87620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.522385708434419,\n",
      "      \"grad_norm\": 0.0004063453816343099,\n",
      "      \"learning_rate\": 2.2399495904221804e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 87640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.52364598758625,\n",
      "      \"grad_norm\": 0.0013875466538593173,\n",
      "      \"learning_rate\": 2.239319470699433e-05,\n",
      "      \"loss\": 0.0246,\n",
      "      \"step\": 87660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.524906266738083,\n",
      "      \"grad_norm\": 0.006752534303814173,\n",
      "      \"learning_rate\": 2.2386893509766855e-05,\n",
      "      \"loss\": 0.0341,\n",
      "      \"step\": 87680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.5261665458899145,\n",
      "      \"grad_norm\": 0.0008394370670430362,\n",
      "      \"learning_rate\": 2.2380592312539384e-05,\n",
      "      \"loss\": 0.0218,\n",
      "      \"step\": 87700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.527426825041747,\n",
      "      \"grad_norm\": 0.01563749648630619,\n",
      "      \"learning_rate\": 2.237429111531191e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 87720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.528687104193579,\n",
      "      \"grad_norm\": 0.07494352012872696,\n",
      "      \"learning_rate\": 2.2367989918084436e-05,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 87740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.529947383345411,\n",
      "      \"grad_norm\": Infinity,\n",
      "      \"learning_rate\": 2.2361688720856965e-05,\n",
      "      \"loss\": 0.0293,\n",
      "      \"step\": 87760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.531207662497243,\n",
      "      \"grad_norm\": 0.0008818074129521847,\n",
      "      \"learning_rate\": 2.2355702583490866e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 87780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.532467941649076,\n",
      "      \"grad_norm\": 0.0004102113889530301,\n",
      "      \"learning_rate\": 2.234940138626339e-05,\n",
      "      \"loss\": 0.0237,\n",
      "      \"step\": 87800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.533728220800907,\n",
      "      \"grad_norm\": 0.0005215381388552487,\n",
      "      \"learning_rate\": 2.2343100189035917e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 87820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.53498849995274,\n",
      "      \"grad_norm\": 0.009191603399813175,\n",
      "      \"learning_rate\": 2.2336798991808443e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 87840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.536248779104572,\n",
      "      \"grad_norm\": 0.21802674233913422,\n",
      "      \"learning_rate\": 2.233049779458097e-05,\n",
      "      \"loss\": 0.0193,\n",
      "      \"step\": 87860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.537509058256404,\n",
      "      \"grad_norm\": 0.0007624211721122265,\n",
      "      \"learning_rate\": 2.2324196597353497e-05,\n",
      "      \"loss\": 0.035,\n",
      "      \"step\": 87880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.538769337408236,\n",
      "      \"grad_norm\": 0.7738772034645081,\n",
      "      \"learning_rate\": 2.2317895400126023e-05,\n",
      "      \"loss\": 0.0355,\n",
      "      \"step\": 87900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.540029616560068,\n",
      "      \"grad_norm\": 9.095802307128906,\n",
      "      \"learning_rate\": 2.2311594202898552e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 87920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.5412898957119,\n",
      "      \"grad_norm\": 0.0005831395392306149,\n",
      "      \"learning_rate\": 2.230529300567108e-05,\n",
      "      \"loss\": 0.0228,\n",
      "      \"step\": 87940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.542550174863733,\n",
      "      \"grad_norm\": 0.002997764851897955,\n",
      "      \"learning_rate\": 2.2298991808443607e-05,\n",
      "      \"loss\": 0.0243,\n",
      "      \"step\": 87960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.543810454015564,\n",
      "      \"grad_norm\": 0.01706809177994728,\n",
      "      \"learning_rate\": 2.2292690611216132e-05,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 87980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.545070733167397,\n",
      "      \"grad_norm\": 6.088263988494873,\n",
      "      \"learning_rate\": 2.2286389413988658e-05,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 88000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.546331012319229,\n",
      "      \"grad_norm\": 0.00043680774979293346,\n",
      "      \"learning_rate\": 2.2280088216761187e-05,\n",
      "      \"loss\": 0.0288,\n",
      "      \"step\": 88020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.547591291471061,\n",
      "      \"grad_norm\": 0.0055307731963694096,\n",
      "      \"learning_rate\": 2.2273787019533713e-05,\n",
      "      \"loss\": 0.022,\n",
      "      \"step\": 88040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.548851570622893,\n",
      "      \"grad_norm\": 0.0008848334546200931,\n",
      "      \"learning_rate\": 2.2267485822306238e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 88060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.550111849774725,\n",
      "      \"grad_norm\": 0.037207458168268204,\n",
      "      \"learning_rate\": 2.2261184625078764e-05,\n",
      "      \"loss\": 0.0189,\n",
      "      \"step\": 88080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.551372128926557,\n",
      "      \"grad_norm\": 0.002997569739818573,\n",
      "      \"learning_rate\": 2.2254883427851293e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 88100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.55263240807839,\n",
      "      \"grad_norm\": 8.758753776550293,\n",
      "      \"learning_rate\": 2.224858223062382e-05,\n",
      "      \"loss\": 0.017,\n",
      "      \"step\": 88120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.5538926872302214,\n",
      "      \"grad_norm\": 0.008079273626208305,\n",
      "      \"learning_rate\": 2.2242281033396348e-05,\n",
      "      \"loss\": 0.0068,\n",
      "      \"step\": 88140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.555152966382053,\n",
      "      \"grad_norm\": 0.05811134725809097,\n",
      "      \"learning_rate\": 2.2235979836168873e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 88160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.556413245533886,\n",
      "      \"grad_norm\": 0.0008962758583948016,\n",
      "      \"learning_rate\": 2.2229678638941402e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 88180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.557673524685718,\n",
      "      \"grad_norm\": 0.2160744071006775,\n",
      "      \"learning_rate\": 2.2223377441713928e-05,\n",
      "      \"loss\": 0.0019,\n",
      "      \"step\": 88200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.55893380383755,\n",
      "      \"grad_norm\": 0.0004022277134936303,\n",
      "      \"learning_rate\": 2.2217076244486454e-05,\n",
      "      \"loss\": 0.0266,\n",
      "      \"step\": 88220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.560194082989382,\n",
      "      \"grad_norm\": 0.12953633069992065,\n",
      "      \"learning_rate\": 2.221077504725898e-05,\n",
      "      \"loss\": 0.0115,\n",
      "      \"step\": 88240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.561454362141214,\n",
      "      \"grad_norm\": 4.627294540405273,\n",
      "      \"learning_rate\": 2.220447385003151e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 88260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.562714641293047,\n",
      "      \"grad_norm\": 0.0026213359087705612,\n",
      "      \"learning_rate\": 2.2198172652804034e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 88280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.5639749204448785,\n",
      "      \"grad_norm\": 0.00479287700727582,\n",
      "      \"learning_rate\": 2.219187145557656e-05,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 88300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.56523519959671,\n",
      "      \"grad_norm\": 0.009098971262574196,\n",
      "      \"learning_rate\": 2.2185570258349085e-05,\n",
      "      \"loss\": 0.0193,\n",
      "      \"step\": 88320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.566495478748543,\n",
      "      \"grad_norm\": 0.0004065465764142573,\n",
      "      \"learning_rate\": 2.2179269061121614e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 88340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.567755757900375,\n",
      "      \"grad_norm\": 0.005771948024630547,\n",
      "      \"learning_rate\": 2.217296786389414e-05,\n",
      "      \"loss\": 0.0358,\n",
      "      \"step\": 88360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.569016037052207,\n",
      "      \"grad_norm\": 62.86954116821289,\n",
      "      \"learning_rate\": 2.216666666666667e-05,\n",
      "      \"loss\": 0.0124,\n",
      "      \"step\": 88380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.570276316204039,\n",
      "      \"grad_norm\": 0.06311667710542679,\n",
      "      \"learning_rate\": 2.2160365469439195e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 88400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.571536595355871,\n",
      "      \"grad_norm\": 0.0014118868857622147,\n",
      "      \"learning_rate\": 2.215406427221172e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 88420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.572796874507704,\n",
      "      \"grad_norm\": 0.1878189891576767,\n",
      "      \"learning_rate\": 2.214776307498425e-05,\n",
      "      \"loss\": 0.0146,\n",
      "      \"step\": 88440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.574057153659536,\n",
      "      \"grad_norm\": 0.0004371710238046944,\n",
      "      \"learning_rate\": 2.2141461877756775e-05,\n",
      "      \"loss\": 0.0526,\n",
      "      \"step\": 88460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.575317432811367,\n",
      "      \"grad_norm\": 0.07144084572792053,\n",
      "      \"learning_rate\": 2.21351606805293e-05,\n",
      "      \"loss\": 0.0277,\n",
      "      \"step\": 88480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.5765777119632,\n",
      "      \"grad_norm\": 0.014873387292027473,\n",
      "      \"learning_rate\": 2.212885948330183e-05,\n",
      "      \"loss\": 0.058,\n",
      "      \"step\": 88500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.577837991115032,\n",
      "      \"grad_norm\": 0.053159695118665695,\n",
      "      \"learning_rate\": 2.2122558286074355e-05,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 88520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.579098270266864,\n",
      "      \"grad_norm\": 0.0022517009638249874,\n",
      "      \"learning_rate\": 2.211625708884688e-05,\n",
      "      \"loss\": 0.0249,\n",
      "      \"step\": 88540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.580358549418696,\n",
      "      \"grad_norm\": 0.10894040018320084,\n",
      "      \"learning_rate\": 2.2109955891619407e-05,\n",
      "      \"loss\": 0.036,\n",
      "      \"step\": 88560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.581618828570528,\n",
      "      \"grad_norm\": 0.013406872749328613,\n",
      "      \"learning_rate\": 2.2103654694391936e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 88580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.582879107722361,\n",
      "      \"grad_norm\": 0.0009883036836981773,\n",
      "      \"learning_rate\": 2.2097353497164465e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 88600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.584139386874193,\n",
      "      \"grad_norm\": 0.0064421147108078,\n",
      "      \"learning_rate\": 2.209105229993699e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 88620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.585399666026024,\n",
      "      \"grad_norm\": 0.001461166888475418,\n",
      "      \"learning_rate\": 2.2084751102709516e-05,\n",
      "      \"loss\": 0.0157,\n",
      "      \"step\": 88640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.586659945177857,\n",
      "      \"grad_norm\": 0.004658362362533808,\n",
      "      \"learning_rate\": 2.207844990548204e-05,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 88660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.5879202243296895,\n",
      "      \"grad_norm\": 0.009150723926723003,\n",
      "      \"learning_rate\": 2.207214870825457e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 88680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.589180503481521,\n",
      "      \"grad_norm\": 0.0033993185497820377,\n",
      "      \"learning_rate\": 2.2065847511027096e-05,\n",
      "      \"loss\": 0.0211,\n",
      "      \"step\": 88700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.590440782633353,\n",
      "      \"grad_norm\": 0.024902060627937317,\n",
      "      \"learning_rate\": 2.2059546313799622e-05,\n",
      "      \"loss\": 0.039,\n",
      "      \"step\": 88720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.5917010617851854,\n",
      "      \"grad_norm\": 0.008801868185400963,\n",
      "      \"learning_rate\": 2.2053245116572148e-05,\n",
      "      \"loss\": 0.0222,\n",
      "      \"step\": 88740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.592961340937018,\n",
      "      \"grad_norm\": 0.004781896714121103,\n",
      "      \"learning_rate\": 2.2046943919344677e-05,\n",
      "      \"loss\": 0.016,\n",
      "      \"step\": 88760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.59422162008885,\n",
      "      \"grad_norm\": 0.004450109787285328,\n",
      "      \"learning_rate\": 2.2040642722117202e-05,\n",
      "      \"loss\": 0.0201,\n",
      "      \"step\": 88780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.595481899240681,\n",
      "      \"grad_norm\": 0.0012564328499138355,\n",
      "      \"learning_rate\": 2.2034341524889728e-05,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 88800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.596742178392514,\n",
      "      \"grad_norm\": 0.0011434165062382817,\n",
      "      \"learning_rate\": 2.2028040327662257e-05,\n",
      "      \"loss\": 0.0202,\n",
      "      \"step\": 88820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.5980024575443466,\n",
      "      \"grad_norm\": 0.012830328196287155,\n",
      "      \"learning_rate\": 2.2021739130434786e-05,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 88840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.599262736696178,\n",
      "      \"grad_norm\": 0.1144847497344017,\n",
      "      \"learning_rate\": 2.2015437933207312e-05,\n",
      "      \"loss\": 0.0316,\n",
      "      \"step\": 88860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.60052301584801,\n",
      "      \"grad_norm\": 0.01979069970548153,\n",
      "      \"learning_rate\": 2.2009136735979837e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 88880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.6017832949998425,\n",
      "      \"grad_norm\": 0.08075204491615295,\n",
      "      \"learning_rate\": 2.2002835538752363e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 88900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.603043574151675,\n",
      "      \"grad_norm\": 0.012187761254608631,\n",
      "      \"learning_rate\": 2.1996534341524892e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 88920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.604303853303507,\n",
      "      \"grad_norm\": 4.238460540771484,\n",
      "      \"learning_rate\": 2.1990233144297418e-05,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 88940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.6055641324553385,\n",
      "      \"grad_norm\": 0.010641280561685562,\n",
      "      \"learning_rate\": 2.1983931947069943e-05,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 88960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.606824411607171,\n",
      "      \"grad_norm\": 0.003678516950458288,\n",
      "      \"learning_rate\": 2.197763074984247e-05,\n",
      "      \"loss\": 0.0024,\n",
      "      \"step\": 88980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.608084690759003,\n",
      "      \"grad_norm\": 0.005250292364507914,\n",
      "      \"learning_rate\": 2.1971329552614998e-05,\n",
      "      \"loss\": 0.0531,\n",
      "      \"step\": 89000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.609344969910835,\n",
      "      \"grad_norm\": 0.3000529408454895,\n",
      "      \"learning_rate\": 2.1965028355387524e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 89020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.610605249062667,\n",
      "      \"grad_norm\": 0.0008617820567451417,\n",
      "      \"learning_rate\": 2.195872715816005e-05,\n",
      "      \"loss\": 0.0099,\n",
      "      \"step\": 89040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.6118655282145,\n",
      "      \"grad_norm\": 0.026401080191135406,\n",
      "      \"learning_rate\": 2.195242596093258e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 89060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.613125807366331,\n",
      "      \"grad_norm\": 0.0027670839335769415,\n",
      "      \"learning_rate\": 2.1946124763705107e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 89080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.614386086518164,\n",
      "      \"grad_norm\": 5.268978118896484,\n",
      "      \"learning_rate\": 2.1939823566477633e-05,\n",
      "      \"loss\": 0.0188,\n",
      "      \"step\": 89100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.6156463656699955,\n",
      "      \"grad_norm\": 0.002714843489229679,\n",
      "      \"learning_rate\": 2.193352236925016e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 89120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.616906644821828,\n",
      "      \"grad_norm\": 0.006954410579055548,\n",
      "      \"learning_rate\": 2.1927221172022684e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 89140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.61816692397366,\n",
      "      \"grad_norm\": 0.03297043964266777,\n",
      "      \"learning_rate\": 2.1920919974795213e-05,\n",
      "      \"loss\": 0.0364,\n",
      "      \"step\": 89160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.619427203125492,\n",
      "      \"grad_norm\": 0.13122457265853882,\n",
      "      \"learning_rate\": 2.191461877756774e-05,\n",
      "      \"loss\": 0.0193,\n",
      "      \"step\": 89180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.620687482277324,\n",
      "      \"grad_norm\": 0.18584930896759033,\n",
      "      \"learning_rate\": 2.1908317580340265e-05,\n",
      "      \"loss\": 0.0061,\n",
      "      \"step\": 89200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.621947761429157,\n",
      "      \"grad_norm\": 0.012313352897763252,\n",
      "      \"learning_rate\": 2.190201638311279e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 89220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.623208040580988,\n",
      "      \"grad_norm\": 0.0015262203523889184,\n",
      "      \"learning_rate\": 2.189571518588532e-05,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 89240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.624468319732821,\n",
      "      \"grad_norm\": 0.40031397342681885,\n",
      "      \"learning_rate\": 2.1889413988657845e-05,\n",
      "      \"loss\": 0.0163,\n",
      "      \"step\": 89260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.625728598884653,\n",
      "      \"grad_norm\": 0.022708816453814507,\n",
      "      \"learning_rate\": 2.1883112791430374e-05,\n",
      "      \"loss\": 0.0255,\n",
      "      \"step\": 89280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.626988878036485,\n",
      "      \"grad_norm\": 0.0042179361917078495,\n",
      "      \"learning_rate\": 2.18768115942029e-05,\n",
      "      \"loss\": 0.0323,\n",
      "      \"step\": 89300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.628249157188317,\n",
      "      \"grad_norm\": 0.00985352136194706,\n",
      "      \"learning_rate\": 2.1870510396975425e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 89320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.629509436340149,\n",
      "      \"grad_norm\": 0.0015478301793336868,\n",
      "      \"learning_rate\": 2.1864209199747954e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 89340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.630769715491981,\n",
      "      \"grad_norm\": 0.03479568287730217,\n",
      "      \"learning_rate\": 2.185790800252048e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 89360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.632029994643814,\n",
      "      \"grad_norm\": 0.011248916387557983,\n",
      "      \"learning_rate\": 2.1851606805293006e-05,\n",
      "      \"loss\": 0.0059,\n",
      "      \"step\": 89380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.633290273795645,\n",
      "      \"grad_norm\": 0.30809879302978516,\n",
      "      \"learning_rate\": 2.1845305608065535e-05,\n",
      "      \"loss\": 0.0066,\n",
      "      \"step\": 89400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.634550552947478,\n",
      "      \"grad_norm\": 0.0004098239296581596,\n",
      "      \"learning_rate\": 2.183900441083806e-05,\n",
      "      \"loss\": 0.0255,\n",
      "      \"step\": 89420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.63581083209931,\n",
      "      \"grad_norm\": 0.0002888111921492964,\n",
      "      \"learning_rate\": 2.1832703213610586e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 89440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.637071111251142,\n",
      "      \"grad_norm\": 1.4537038803100586,\n",
      "      \"learning_rate\": 2.1826402016383112e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 89460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.638331390402974,\n",
      "      \"grad_norm\": 0.007165833842009306,\n",
      "      \"learning_rate\": 2.182010081915564e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 89480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.6395916695548065,\n",
      "      \"grad_norm\": 0.0032667196355760098,\n",
      "      \"learning_rate\": 2.181379962192817e-05,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 89500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.640851948706638,\n",
      "      \"grad_norm\": 10.251762390136719,\n",
      "      \"learning_rate\": 2.1807498424700696e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 89520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.642112227858471,\n",
      "      \"grad_norm\": 0.0014005947159603238,\n",
      "      \"learning_rate\": 2.180119722747322e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 89540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.6433725070103025,\n",
      "      \"grad_norm\": 0.13311049342155457,\n",
      "      \"learning_rate\": 2.1794896030245747e-05,\n",
      "      \"loss\": 0.0243,\n",
      "      \"step\": 89560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.644632786162135,\n",
      "      \"grad_norm\": 0.001399155124090612,\n",
      "      \"learning_rate\": 2.1788594833018276e-05,\n",
      "      \"loss\": 0.0121,\n",
      "      \"step\": 89580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.645893065313967,\n",
      "      \"grad_norm\": 0.004019257612526417,\n",
      "      \"learning_rate\": 2.17822936357908e-05,\n",
      "      \"loss\": 0.0136,\n",
      "      \"step\": 89600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.647153344465799,\n",
      "      \"grad_norm\": 0.0026828621048480272,\n",
      "      \"learning_rate\": 2.1775992438563327e-05,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 89620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.648413623617631,\n",
      "      \"grad_norm\": 61.130348205566406,\n",
      "      \"learning_rate\": 2.1769691241335853e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 89640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.649673902769464,\n",
      "      \"grad_norm\": 0.004127220716327429,\n",
      "      \"learning_rate\": 2.1763390044108382e-05,\n",
      "      \"loss\": 0.0143,\n",
      "      \"step\": 89660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.650934181921295,\n",
      "      \"grad_norm\": 0.002229532226920128,\n",
      "      \"learning_rate\": 2.1757088846880907e-05,\n",
      "      \"loss\": 0.0345,\n",
      "      \"step\": 89680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.652194461073128,\n",
      "      \"grad_norm\": 0.004535903222858906,\n",
      "      \"learning_rate\": 2.1750787649653433e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 89700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.6534547402249595,\n",
      "      \"grad_norm\": 0.0006014027749188244,\n",
      "      \"learning_rate\": 2.1744486452425962e-05,\n",
      "      \"loss\": 0.0172,\n",
      "      \"step\": 89720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.654715019376792,\n",
      "      \"grad_norm\": 0.39642319083213806,\n",
      "      \"learning_rate\": 2.173818525519849e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 89740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.655975298528624,\n",
      "      \"grad_norm\": 0.0029442517552524805,\n",
      "      \"learning_rate\": 2.1731884057971017e-05,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 89760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.657235577680456,\n",
      "      \"grad_norm\": 0.09083540737628937,\n",
      "      \"learning_rate\": 2.1725582860743543e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 89780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.658495856832288,\n",
      "      \"grad_norm\": 5.981989860534668,\n",
      "      \"learning_rate\": 2.1719281663516068e-05,\n",
      "      \"loss\": 0.0411,\n",
      "      \"step\": 89800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.659756135984121,\n",
      "      \"grad_norm\": 0.01939665526151657,\n",
      "      \"learning_rate\": 2.1712980466288597e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 89820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.661016415135952,\n",
      "      \"grad_norm\": 0.0005129076889716089,\n",
      "      \"learning_rate\": 2.1706679269061123e-05,\n",
      "      \"loss\": 0.0073,\n",
      "      \"step\": 89840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.662276694287785,\n",
      "      \"grad_norm\": 14.812917709350586,\n",
      "      \"learning_rate\": 2.170037807183365e-05,\n",
      "      \"loss\": 0.0274,\n",
      "      \"step\": 89860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.663536973439617,\n",
      "      \"grad_norm\": 0.0008073535282164812,\n",
      "      \"learning_rate\": 2.1694076874606174e-05,\n",
      "      \"loss\": 0.0205,\n",
      "      \"step\": 89880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.664797252591449,\n",
      "      \"grad_norm\": 0.00832973700016737,\n",
      "      \"learning_rate\": 2.1687775677378703e-05,\n",
      "      \"loss\": 0.0166,\n",
      "      \"step\": 89900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.666057531743281,\n",
      "      \"grad_norm\": 0.021835381165146828,\n",
      "      \"learning_rate\": 2.168147448015123e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 89920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.667317810895113,\n",
      "      \"grad_norm\": 0.0028798216953873634,\n",
      "      \"learning_rate\": 2.1675173282923755e-05,\n",
      "      \"loss\": 0.0265,\n",
      "      \"step\": 89940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.668578090046945,\n",
      "      \"grad_norm\": 0.0021801821421831846,\n",
      "      \"learning_rate\": 2.1668872085696284e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 89960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.669838369198778,\n",
      "      \"grad_norm\": 0.0016565819969400764,\n",
      "      \"learning_rate\": 2.1662570888468813e-05,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 89980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.671098648350609,\n",
      "      \"grad_norm\": 0.0004484151431825012,\n",
      "      \"learning_rate\": 2.1656269691241338e-05,\n",
      "      \"loss\": 0.0398,\n",
      "      \"step\": 90000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.672358927502442,\n",
      "      \"grad_norm\": 3.577631711959839,\n",
      "      \"learning_rate\": 2.1649968494013864e-05,\n",
      "      \"loss\": 0.0129,\n",
      "      \"step\": 90020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.673619206654274,\n",
      "      \"grad_norm\": 0.0030069437343627214,\n",
      "      \"learning_rate\": 2.164366729678639e-05,\n",
      "      \"loss\": 0.0247,\n",
      "      \"step\": 90040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.674879485806106,\n",
      "      \"grad_norm\": 0.018165213987231255,\n",
      "      \"learning_rate\": 2.163736609955892e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 90060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.676139764957938,\n",
      "      \"grad_norm\": 0.0017747191013768315,\n",
      "      \"learning_rate\": 2.1631064902331444e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 90080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.6774000441097705,\n",
      "      \"grad_norm\": 0.01720343343913555,\n",
      "      \"learning_rate\": 2.1625078764965345e-05,\n",
      "      \"loss\": 0.0313,\n",
      "      \"step\": 90100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.678660323261602,\n",
      "      \"grad_norm\": 0.004627051297575235,\n",
      "      \"learning_rate\": 2.161877756773787e-05,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 90120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.679920602413435,\n",
      "      \"grad_norm\": 0.0681881532073021,\n",
      "      \"learning_rate\": 2.16124763705104e-05,\n",
      "      \"loss\": 0.0037,\n",
      "      \"step\": 90140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.6811808815652665,\n",
      "      \"grad_norm\": 0.007222834508866072,\n",
      "      \"learning_rate\": 2.1606175173282925e-05,\n",
      "      \"loss\": 0.0077,\n",
      "      \"step\": 90160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.682441160717099,\n",
      "      \"grad_norm\": 0.0014333565486595035,\n",
      "      \"learning_rate\": 2.159987397605545e-05,\n",
      "      \"loss\": 0.0313,\n",
      "      \"step\": 90180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.683701439868931,\n",
      "      \"grad_norm\": 0.0854588970541954,\n",
      "      \"learning_rate\": 2.1593572778827977e-05,\n",
      "      \"loss\": 0.0255,\n",
      "      \"step\": 90200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.684961719020763,\n",
      "      \"grad_norm\": 0.0004948092973791063,\n",
      "      \"learning_rate\": 2.1587271581600506e-05,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 90220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.686221998172595,\n",
      "      \"grad_norm\": 0.012698143720626831,\n",
      "      \"learning_rate\": 2.158097038437303e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 90240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.687482277324428,\n",
      "      \"grad_norm\": 0.0003542900085449219,\n",
      "      \"learning_rate\": 2.1574669187145557e-05,\n",
      "      \"loss\": 0.0206,\n",
      "      \"step\": 90260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.688742556476259,\n",
      "      \"grad_norm\": 0.1448441743850708,\n",
      "      \"learning_rate\": 2.1568367989918086e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 90280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.690002835628092,\n",
      "      \"grad_norm\": 0.006329777650535107,\n",
      "      \"learning_rate\": 2.1562066792690612e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 90300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.6912631147799235,\n",
      "      \"grad_norm\": 2.3069374561309814,\n",
      "      \"learning_rate\": 2.155576559546314e-05,\n",
      "      \"loss\": 0.0093,\n",
      "      \"step\": 90320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.692523393931756,\n",
      "      \"grad_norm\": 0.0015278069768100977,\n",
      "      \"learning_rate\": 2.1549464398235667e-05,\n",
      "      \"loss\": 0.0447,\n",
      "      \"step\": 90340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.693783673083588,\n",
      "      \"grad_norm\": 0.04917238652706146,\n",
      "      \"learning_rate\": 2.1543163201008192e-05,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 90360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.69504395223542,\n",
      "      \"grad_norm\": 5.634705543518066,\n",
      "      \"learning_rate\": 2.1536862003780718e-05,\n",
      "      \"loss\": 0.0301,\n",
      "      \"step\": 90380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.696304231387252,\n",
      "      \"grad_norm\": 0.013619180768728256,\n",
      "      \"learning_rate\": 2.1530560806553247e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 90400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.697564510539085,\n",
      "      \"grad_norm\": 0.006551254540681839,\n",
      "      \"learning_rate\": 2.1524259609325772e-05,\n",
      "      \"loss\": 0.0669,\n",
      "      \"step\": 90420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.698824789690916,\n",
      "      \"grad_norm\": 0.08411075919866562,\n",
      "      \"learning_rate\": 2.1517958412098298e-05,\n",
      "      \"loss\": 0.0077,\n",
      "      \"step\": 90440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.700085068842749,\n",
      "      \"grad_norm\": 0.004587078932672739,\n",
      "      \"learning_rate\": 2.1511657214870827e-05,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 90460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.701345347994581,\n",
      "      \"grad_norm\": 0.0023514078930020332,\n",
      "      \"learning_rate\": 2.1505356017643353e-05,\n",
      "      \"loss\": 0.033,\n",
      "      \"step\": 90480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.702605627146413,\n",
      "      \"grad_norm\": 0.007044608239084482,\n",
      "      \"learning_rate\": 2.149905482041588e-05,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 90500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.703865906298245,\n",
      "      \"grad_norm\": 0.025174971669912338,\n",
      "      \"learning_rate\": 2.1492753623188408e-05,\n",
      "      \"loss\": 0.022,\n",
      "      \"step\": 90520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.705126185450077,\n",
      "      \"grad_norm\": 0.003481842577457428,\n",
      "      \"learning_rate\": 2.1486452425960933e-05,\n",
      "      \"loss\": 0.0262,\n",
      "      \"step\": 90540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.706386464601909,\n",
      "      \"grad_norm\": 0.03544461727142334,\n",
      "      \"learning_rate\": 2.1480151228733462e-05,\n",
      "      \"loss\": 0.03,\n",
      "      \"step\": 90560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.707646743753742,\n",
      "      \"grad_norm\": 0.0023114020004868507,\n",
      "      \"learning_rate\": 2.1473850031505988e-05,\n",
      "      \"loss\": 0.032,\n",
      "      \"step\": 90580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.708907022905573,\n",
      "      \"grad_norm\": 0.0019669118337333202,\n",
      "      \"learning_rate\": 2.1467548834278514e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 90600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.710167302057406,\n",
      "      \"grad_norm\": 0.026615018025040627,\n",
      "      \"learning_rate\": 2.146124763705104e-05,\n",
      "      \"loss\": 0.0212,\n",
      "      \"step\": 90620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.711427581209238,\n",
      "      \"grad_norm\": 0.011651314795017242,\n",
      "      \"learning_rate\": 2.1454946439823568e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 90640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.71268786036107,\n",
      "      \"grad_norm\": 0.01234869658946991,\n",
      "      \"learning_rate\": 2.1448645242596094e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 90660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.713948139512902,\n",
      "      \"grad_norm\": 0.2022220343351364,\n",
      "      \"learning_rate\": 2.144234404536862e-05,\n",
      "      \"loss\": 0.0545,\n",
      "      \"step\": 90680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.7152084186647345,\n",
      "      \"grad_norm\": 0.008065946400165558,\n",
      "      \"learning_rate\": 2.1436042848141145e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 90700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.716468697816566,\n",
      "      \"grad_norm\": 0.002318622777238488,\n",
      "      \"learning_rate\": 2.1429741650913674e-05,\n",
      "      \"loss\": 0.0299,\n",
      "      \"step\": 90720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.717728976968399,\n",
      "      \"grad_norm\": 0.016905799508094788,\n",
      "      \"learning_rate\": 2.1423440453686203e-05,\n",
      "      \"loss\": 0.0129,\n",
      "      \"step\": 90740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.7189892561202305,\n",
      "      \"grad_norm\": 0.07748225331306458,\n",
      "      \"learning_rate\": 2.141713925645873e-05,\n",
      "      \"loss\": 0.0239,\n",
      "      \"step\": 90760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.720249535272063,\n",
      "      \"grad_norm\": 0.012199090793728828,\n",
      "      \"learning_rate\": 2.1410838059231255e-05,\n",
      "      \"loss\": 0.0281,\n",
      "      \"step\": 90780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.721509814423895,\n",
      "      \"grad_norm\": 0.005572786554694176,\n",
      "      \"learning_rate\": 2.1404536862003784e-05,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 90800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.722770093575727,\n",
      "      \"grad_norm\": 0.07335954904556274,\n",
      "      \"learning_rate\": 2.139823566477631e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 90820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.724030372727559,\n",
      "      \"grad_norm\": 0.01062002032995224,\n",
      "      \"learning_rate\": 2.1391934467548835e-05,\n",
      "      \"loss\": 0.0193,\n",
      "      \"step\": 90840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.725290651879392,\n",
      "      \"grad_norm\": 0.12438920140266418,\n",
      "      \"learning_rate\": 2.138563327032136e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 90860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.726550931031223,\n",
      "      \"grad_norm\": 0.04316873475909233,\n",
      "      \"learning_rate\": 2.137933207309389e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 90880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.727811210183056,\n",
      "      \"grad_norm\": 0.0012975373538210988,\n",
      "      \"learning_rate\": 2.1373030875866415e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 90900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.7290714893348875,\n",
      "      \"grad_norm\": 0.025002870708703995,\n",
      "      \"learning_rate\": 2.136672967863894e-05,\n",
      "      \"loss\": 0.0448,\n",
      "      \"step\": 90920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.73033176848672,\n",
      "      \"grad_norm\": 0.016705172136425972,\n",
      "      \"learning_rate\": 2.1360428481411467e-05,\n",
      "      \"loss\": 0.0474,\n",
      "      \"step\": 90940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.731592047638552,\n",
      "      \"grad_norm\": 1.6483269929885864,\n",
      "      \"learning_rate\": 2.1354127284184e-05,\n",
      "      \"loss\": 0.0076,\n",
      "      \"step\": 90960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.732852326790384,\n",
      "      \"grad_norm\": 0.008092633448541164,\n",
      "      \"learning_rate\": 2.1347826086956525e-05,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 90980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.734112605942216,\n",
      "      \"grad_norm\": 0.01894056797027588,\n",
      "      \"learning_rate\": 2.134152488972905e-05,\n",
      "      \"loss\": 0.0259,\n",
      "      \"step\": 91000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.735372885094049,\n",
      "      \"grad_norm\": 0.06724096834659576,\n",
      "      \"learning_rate\": 2.1335223692501576e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 91020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.73663316424588,\n",
      "      \"grad_norm\": 0.1261599361896515,\n",
      "      \"learning_rate\": 2.1328922495274105e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 91040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.737893443397713,\n",
      "      \"grad_norm\": 0.0010146036511287093,\n",
      "      \"learning_rate\": 2.132262129804663e-05,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 91060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.739153722549545,\n",
      "      \"grad_norm\": 0.03273843601346016,\n",
      "      \"learning_rate\": 2.1316320100819156e-05,\n",
      "      \"loss\": 0.0294,\n",
      "      \"step\": 91080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.740414001701377,\n",
      "      \"grad_norm\": 0.000632098235655576,\n",
      "      \"learning_rate\": 2.1310018903591682e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 91100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.741674280853209,\n",
      "      \"grad_norm\": 0.36098045110702515,\n",
      "      \"learning_rate\": 2.130371770636421e-05,\n",
      "      \"loss\": 0.0277,\n",
      "      \"step\": 91120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.742934560005041,\n",
      "      \"grad_norm\": 2.8877017498016357,\n",
      "      \"learning_rate\": 2.1297416509136737e-05,\n",
      "      \"loss\": 0.0184,\n",
      "      \"step\": 91140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.744194839156873,\n",
      "      \"grad_norm\": 0.035157423466444016,\n",
      "      \"learning_rate\": 2.1291115311909262e-05,\n",
      "      \"loss\": 0.0406,\n",
      "      \"step\": 91160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.745455118308706,\n",
      "      \"grad_norm\": 0.5851600766181946,\n",
      "      \"learning_rate\": 2.1284814114681788e-05,\n",
      "      \"loss\": 0.0435,\n",
      "      \"step\": 91180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.746715397460537,\n",
      "      \"grad_norm\": 0.010672837495803833,\n",
      "      \"learning_rate\": 2.1278512917454317e-05,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 91200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.74797567661237,\n",
      "      \"grad_norm\": 0.002293405355885625,\n",
      "      \"learning_rate\": 2.1272211720226846e-05,\n",
      "      \"loss\": 0.0353,\n",
      "      \"step\": 91220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.749235955764202,\n",
      "      \"grad_norm\": 0.022778034210205078,\n",
      "      \"learning_rate\": 2.126591052299937e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 91240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.750496234916034,\n",
      "      \"grad_norm\": 0.0003252472379244864,\n",
      "      \"learning_rate\": 2.1259609325771897e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 91260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.751756514067866,\n",
      "      \"grad_norm\": 0.07929284125566483,\n",
      "      \"learning_rate\": 2.1253308128544426e-05,\n",
      "      \"loss\": 0.0722,\n",
      "      \"step\": 91280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.7530167932196985,\n",
      "      \"grad_norm\": 0.0009781772969290614,\n",
      "      \"learning_rate\": 2.1247006931316952e-05,\n",
      "      \"loss\": 0.047,\n",
      "      \"step\": 91300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.75427707237153,\n",
      "      \"grad_norm\": 0.010150669142603874,\n",
      "      \"learning_rate\": 2.1240705734089478e-05,\n",
      "      \"loss\": 0.035,\n",
      "      \"step\": 91320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.755537351523363,\n",
      "      \"grad_norm\": 0.0008968804031610489,\n",
      "      \"learning_rate\": 2.1234404536862003e-05,\n",
      "      \"loss\": 0.0169,\n",
      "      \"step\": 91340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.7567976306751945,\n",
      "      \"grad_norm\": 0.005420254543423653,\n",
      "      \"learning_rate\": 2.1228103339634532e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 91360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.758057909827027,\n",
      "      \"grad_norm\": 0.0008462357800453901,\n",
      "      \"learning_rate\": 2.1221802142407058e-05,\n",
      "      \"loss\": 0.0201,\n",
      "      \"step\": 91380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.759318188978859,\n",
      "      \"grad_norm\": 0.4866454303264618,\n",
      "      \"learning_rate\": 2.1215500945179584e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 91400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.760578468130691,\n",
      "      \"grad_norm\": 0.7191217541694641,\n",
      "      \"learning_rate\": 2.1209199747952113e-05,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 91420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.761838747282523,\n",
      "      \"grad_norm\": 0.0034074997529387474,\n",
      "      \"learning_rate\": 2.120289855072464e-05,\n",
      "      \"loss\": 0.0257,\n",
      "      \"step\": 91440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.763099026434356,\n",
      "      \"grad_norm\": 0.030223742127418518,\n",
      "      \"learning_rate\": 2.1196597353497167e-05,\n",
      "      \"loss\": 0.0029,\n",
      "      \"step\": 91460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.764359305586187,\n",
      "      \"grad_norm\": 0.0008400653023272753,\n",
      "      \"learning_rate\": 2.1190296156269693e-05,\n",
      "      \"loss\": 0.0034,\n",
      "      \"step\": 91480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.76561958473802,\n",
      "      \"grad_norm\": 8.172046661376953,\n",
      "      \"learning_rate\": 2.118399495904222e-05,\n",
      "      \"loss\": 0.0028,\n",
      "      \"step\": 91500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.7668798638898515,\n",
      "      \"grad_norm\": 0.0005568115157075226,\n",
      "      \"learning_rate\": 2.1177693761814744e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 91520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.768140143041684,\n",
      "      \"grad_norm\": 0.04464452713727951,\n",
      "      \"learning_rate\": 2.1171392564587273e-05,\n",
      "      \"loss\": 0.0345,\n",
      "      \"step\": 91540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.769400422193516,\n",
      "      \"grad_norm\": 0.00892338901758194,\n",
      "      \"learning_rate\": 2.11650913673598e-05,\n",
      "      \"loss\": 0.0156,\n",
      "      \"step\": 91560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.770660701345348,\n",
      "      \"grad_norm\": 7.559555530548096,\n",
      "      \"learning_rate\": 2.1158790170132325e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 91580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.77192098049718,\n",
      "      \"grad_norm\": 14.884723663330078,\n",
      "      \"learning_rate\": 2.115248897290485e-05,\n",
      "      \"loss\": 0.0369,\n",
      "      \"step\": 91600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.773181259649013,\n",
      "      \"grad_norm\": 1.0627260208129883,\n",
      "      \"learning_rate\": 2.114618777567738e-05,\n",
      "      \"loss\": 0.0069,\n",
      "      \"step\": 91620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.774441538800844,\n",
      "      \"grad_norm\": 0.2408386468887329,\n",
      "      \"learning_rate\": 2.113988657844991e-05,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 91640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.775701817952677,\n",
      "      \"grad_norm\": 0.0009156368323601782,\n",
      "      \"learning_rate\": 2.1133585381222434e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 91660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.776962097104509,\n",
      "      \"grad_norm\": 0.00039023757562972605,\n",
      "      \"learning_rate\": 2.112728418399496e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 91680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.778222376256341,\n",
      "      \"grad_norm\": 0.000621047627646476,\n",
      "      \"learning_rate\": 2.112098298676749e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 91700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.779482655408173,\n",
      "      \"grad_norm\": 7.863494873046875,\n",
      "      \"learning_rate\": 2.1114681789540014e-05,\n",
      "      \"loss\": 0.0476,\n",
      "      \"step\": 91720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.7807429345600045,\n",
      "      \"grad_norm\": 0.004447347950190306,\n",
      "      \"learning_rate\": 2.110838059231254e-05,\n",
      "      \"loss\": 0.0259,\n",
      "      \"step\": 91740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.782003213711837,\n",
      "      \"grad_norm\": 0.05246535688638687,\n",
      "      \"learning_rate\": 2.1102079395085066e-05,\n",
      "      \"loss\": 0.0122,\n",
      "      \"step\": 91760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.78326349286367,\n",
      "      \"grad_norm\": 0.021313119679689407,\n",
      "      \"learning_rate\": 2.1095778197857595e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 91780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.784523772015501,\n",
      "      \"grad_norm\": 0.001899908878840506,\n",
      "      \"learning_rate\": 2.108947700063012e-05,\n",
      "      \"loss\": 0.0172,\n",
      "      \"step\": 91800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.785784051167333,\n",
      "      \"grad_norm\": 0.006365633569657803,\n",
      "      \"learning_rate\": 2.1083175803402646e-05,\n",
      "      \"loss\": 0.0208,\n",
      "      \"step\": 91820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.787044330319166,\n",
      "      \"grad_norm\": 0.439250648021698,\n",
      "      \"learning_rate\": 2.107687460617517e-05,\n",
      "      \"loss\": 0.0026,\n",
      "      \"step\": 91840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.788304609470998,\n",
      "      \"grad_norm\": 0.0018964109476655722,\n",
      "      \"learning_rate\": 2.10705734089477e-05,\n",
      "      \"loss\": 0.0356,\n",
      "      \"step\": 91860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.78956488862283,\n",
      "      \"grad_norm\": 0.010914347134530544,\n",
      "      \"learning_rate\": 2.106427221172023e-05,\n",
      "      \"loss\": 0.0129,\n",
      "      \"step\": 91880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.790825167774662,\n",
      "      \"grad_norm\": 0.009210634045302868,\n",
      "      \"learning_rate\": 2.1057971014492755e-05,\n",
      "      \"loss\": 0.002,\n",
      "      \"step\": 91900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.792085446926494,\n",
      "      \"grad_norm\": 0.017820509150624275,\n",
      "      \"learning_rate\": 2.105166981726528e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 91920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.793345726078327,\n",
      "      \"grad_norm\": 0.0006234709871932864,\n",
      "      \"learning_rate\": 2.104536862003781e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 91940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.7946060052301585,\n",
      "      \"grad_norm\": 0.0018794414354488254,\n",
      "      \"learning_rate\": 2.1039067422810336e-05,\n",
      "      \"loss\": 0.0125,\n",
      "      \"step\": 91960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.79586628438199,\n",
      "      \"grad_norm\": 0.00048191394307650626,\n",
      "      \"learning_rate\": 2.103276622558286e-05,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 91980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.797126563533823,\n",
      "      \"grad_norm\": 0.13631317019462585,\n",
      "      \"learning_rate\": 2.1026465028355387e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 92000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.798386842685655,\n",
      "      \"grad_norm\": 0.00418099807575345,\n",
      "      \"learning_rate\": 2.1020163831127916e-05,\n",
      "      \"loss\": 0.0019,\n",
      "      \"step\": 92020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.799647121837487,\n",
      "      \"grad_norm\": 0.004890156909823418,\n",
      "      \"learning_rate\": 2.1013862633900442e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 92040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.800907400989319,\n",
      "      \"grad_norm\": 0.20326130092144012,\n",
      "      \"learning_rate\": 2.1007561436672967e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 92060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.802167680141151,\n",
      "      \"grad_norm\": 0.0010978764621540904,\n",
      "      \"learning_rate\": 2.1001260239445493e-05,\n",
      "      \"loss\": 0.0307,\n",
      "      \"step\": 92080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.803427959292984,\n",
      "      \"grad_norm\": 0.0042140064761042595,\n",
      "      \"learning_rate\": 2.0994959042218022e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 92100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.8046882384448155,\n",
      "      \"grad_norm\": 0.005237726494669914,\n",
      "      \"learning_rate\": 2.098865784499055e-05,\n",
      "      \"loss\": 0.0333,\n",
      "      \"step\": 92120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.805948517596647,\n",
      "      \"grad_norm\": 0.03337867558002472,\n",
      "      \"learning_rate\": 2.0982356647763077e-05,\n",
      "      \"loss\": 0.0073,\n",
      "      \"step\": 92140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.80720879674848,\n",
      "      \"grad_norm\": 4.019516944885254,\n",
      "      \"learning_rate\": 2.0976055450535602e-05,\n",
      "      \"loss\": 0.0201,\n",
      "      \"step\": 92160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.808469075900312,\n",
      "      \"grad_norm\": 5.069073677062988,\n",
      "      \"learning_rate\": 2.096975425330813e-05,\n",
      "      \"loss\": 0.0283,\n",
      "      \"step\": 92180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.809729355052144,\n",
      "      \"grad_norm\": 0.12856309115886688,\n",
      "      \"learning_rate\": 2.0963453056080657e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 92200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.810989634203976,\n",
      "      \"grad_norm\": 0.007228868547827005,\n",
      "      \"learning_rate\": 2.0957151858853183e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 92220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.812249913355808,\n",
      "      \"grad_norm\": 0.003789077280089259,\n",
      "      \"learning_rate\": 2.095085066162571e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 92240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.813510192507641,\n",
      "      \"grad_norm\": 0.04125075414776802,\n",
      "      \"learning_rate\": 2.0944549464398237e-05,\n",
      "      \"loss\": 0.007,\n",
      "      \"step\": 92260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.814770471659473,\n",
      "      \"grad_norm\": 0.008685531094670296,\n",
      "      \"learning_rate\": 2.0938248267170763e-05,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 92280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.816030750811304,\n",
      "      \"grad_norm\": 0.00142277788836509,\n",
      "      \"learning_rate\": 2.093194706994329e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 92300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.817291029963137,\n",
      "      \"grad_norm\": 0.0010533756576478481,\n",
      "      \"learning_rate\": 2.0925645872715818e-05,\n",
      "      \"loss\": 0.0176,\n",
      "      \"step\": 92320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.818551309114969,\n",
      "      \"grad_norm\": 0.014508336782455444,\n",
      "      \"learning_rate\": 2.0919344675488343e-05,\n",
      "      \"loss\": 0.0287,\n",
      "      \"step\": 92340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.819811588266801,\n",
      "      \"grad_norm\": 0.1529337465763092,\n",
      "      \"learning_rate\": 2.0913043478260872e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 92360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.821071867418633,\n",
      "      \"grad_norm\": 0.009476843290030956,\n",
      "      \"learning_rate\": 2.0906742281033398e-05,\n",
      "      \"loss\": 0.0202,\n",
      "      \"step\": 92380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.822332146570465,\n",
      "      \"grad_norm\": 0.0009701515664346516,\n",
      "      \"learning_rate\": 2.0900441083805924e-05,\n",
      "      \"loss\": 0.0355,\n",
      "      \"step\": 92400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.823592425722298,\n",
      "      \"grad_norm\": 0.0016647782176733017,\n",
      "      \"learning_rate\": 2.089413988657845e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 92420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.82485270487413,\n",
      "      \"grad_norm\": 0.0012210409622639418,\n",
      "      \"learning_rate\": 2.088783868935098e-05,\n",
      "      \"loss\": 0.0202,\n",
      "      \"step\": 92440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.826112984025961,\n",
      "      \"grad_norm\": 0.0036471292842179537,\n",
      "      \"learning_rate\": 2.0881537492123504e-05,\n",
      "      \"loss\": 0.0213,\n",
      "      \"step\": 92460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.827373263177794,\n",
      "      \"grad_norm\": 0.02123773656785488,\n",
      "      \"learning_rate\": 2.087523629489603e-05,\n",
      "      \"loss\": 0.0142,\n",
      "      \"step\": 92480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.8286335423296265,\n",
      "      \"grad_norm\": 0.002993575995787978,\n",
      "      \"learning_rate\": 2.086925015752993e-05,\n",
      "      \"loss\": 0.0051,\n",
      "      \"step\": 92500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.829893821481458,\n",
      "      \"grad_norm\": 0.13213039934635162,\n",
      "      \"learning_rate\": 2.086294896030246e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 92520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.83115410063329,\n",
      "      \"grad_norm\": 0.027569154277443886,\n",
      "      \"learning_rate\": 2.0856647763074985e-05,\n",
      "      \"loss\": 0.0028,\n",
      "      \"step\": 92540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.8324143797851224,\n",
      "      \"grad_norm\": 0.0009021622245199978,\n",
      "      \"learning_rate\": 2.085034656584751e-05,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 92560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.833674658936955,\n",
      "      \"grad_norm\": 0.038875870406627655,\n",
      "      \"learning_rate\": 2.0844045368620037e-05,\n",
      "      \"loss\": 0.0294,\n",
      "      \"step\": 92580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.834934938088787,\n",
      "      \"grad_norm\": 0.006331312004476786,\n",
      "      \"learning_rate\": 2.0837744171392566e-05,\n",
      "      \"loss\": 0.0105,\n",
      "      \"step\": 92600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.836195217240618,\n",
      "      \"grad_norm\": 0.004966855514794588,\n",
      "      \"learning_rate\": 2.083144297416509e-05,\n",
      "      \"loss\": 0.006,\n",
      "      \"step\": 92620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.837455496392451,\n",
      "      \"grad_norm\": 0.0008491533808410168,\n",
      "      \"learning_rate\": 2.0825141776937617e-05,\n",
      "      \"loss\": 0.0029,\n",
      "      \"step\": 92640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.838715775544283,\n",
      "      \"grad_norm\": 0.08109662681818008,\n",
      "      \"learning_rate\": 2.0818840579710146e-05,\n",
      "      \"loss\": 0.0076,\n",
      "      \"step\": 92660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.839976054696115,\n",
      "      \"grad_norm\": 0.0005670796963386238,\n",
      "      \"learning_rate\": 2.0812539382482675e-05,\n",
      "      \"loss\": 0.0259,\n",
      "      \"step\": 92680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.841236333847947,\n",
      "      \"grad_norm\": 0.000581531785428524,\n",
      "      \"learning_rate\": 2.08062381852552e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 92700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.8424966129997795,\n",
      "      \"grad_norm\": 0.0009013333474285901,\n",
      "      \"learning_rate\": 2.0799936988027726e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 92720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.843756892151611,\n",
      "      \"grad_norm\": 0.028349734842777252,\n",
      "      \"learning_rate\": 2.0793635790800252e-05,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 92740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.845017171303444,\n",
      "      \"grad_norm\": 7.279160499572754,\n",
      "      \"learning_rate\": 2.078733459357278e-05,\n",
      "      \"loss\": 0.0605,\n",
      "      \"step\": 92760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.8462774504552755,\n",
      "      \"grad_norm\": 0.21699219942092896,\n",
      "      \"learning_rate\": 2.0781033396345307e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 92780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.847537729607108,\n",
      "      \"grad_norm\": 0.004065995570272207,\n",
      "      \"learning_rate\": 2.0774732199117832e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 92800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.84879800875894,\n",
      "      \"grad_norm\": 0.0006524272612296045,\n",
      "      \"learning_rate\": 2.0768431001890358e-05,\n",
      "      \"loss\": 0.0232,\n",
      "      \"step\": 92820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.850058287910772,\n",
      "      \"grad_norm\": 0.002173197688534856,\n",
      "      \"learning_rate\": 2.0762129804662887e-05,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 92840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.851318567062604,\n",
      "      \"grad_norm\": 0.2880403995513916,\n",
      "      \"learning_rate\": 2.0755828607435413e-05,\n",
      "      \"loss\": 0.0395,\n",
      "      \"step\": 92860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.852578846214437,\n",
      "      \"grad_norm\": 0.16258853673934937,\n",
      "      \"learning_rate\": 2.0749527410207942e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 92880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.853839125366268,\n",
      "      \"grad_norm\": 0.0006850630161352456,\n",
      "      \"learning_rate\": 2.0743226212980467e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 92900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.855099404518101,\n",
      "      \"grad_norm\": 0.005226000212132931,\n",
      "      \"learning_rate\": 2.0736925015752996e-05,\n",
      "      \"loss\": 0.0068,\n",
      "      \"step\": 92920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.8563596836699325,\n",
      "      \"grad_norm\": 0.03125975653529167,\n",
      "      \"learning_rate\": 2.0730623818525522e-05,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 92940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.857619962821765,\n",
      "      \"grad_norm\": 0.0006750249885953963,\n",
      "      \"learning_rate\": 2.0724322621298048e-05,\n",
      "      \"loss\": 0.0276,\n",
      "      \"step\": 92960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.858880241973597,\n",
      "      \"grad_norm\": 0.0817907527089119,\n",
      "      \"learning_rate\": 2.0718021424070573e-05,\n",
      "      \"loss\": 0.0238,\n",
      "      \"step\": 92980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.860140521125429,\n",
      "      \"grad_norm\": 32.13873291015625,\n",
      "      \"learning_rate\": 2.0711720226843102e-05,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 93000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.861400800277261,\n",
      "      \"grad_norm\": 0.0016365408664569259,\n",
      "      \"learning_rate\": 2.0705419029615628e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 93020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.862661079429094,\n",
      "      \"grad_norm\": 0.0014973173383623362,\n",
      "      \"learning_rate\": 2.0699117832388154e-05,\n",
      "      \"loss\": 0.0266,\n",
      "      \"step\": 93040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.863921358580925,\n",
      "      \"grad_norm\": 0.00046232977183535695,\n",
      "      \"learning_rate\": 2.069281663516068e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 93060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.865181637732758,\n",
      "      \"grad_norm\": 0.0003575302835088223,\n",
      "      \"learning_rate\": 2.068651543793321e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 93080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.86644191688459,\n",
      "      \"grad_norm\": 0.00039024752913974226,\n",
      "      \"learning_rate\": 2.0680214240705737e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 93100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.867702196036422,\n",
      "      \"grad_norm\": 0.005172511097043753,\n",
      "      \"learning_rate\": 2.0673913043478263e-05,\n",
      "      \"loss\": 0.0174,\n",
      "      \"step\": 93120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.868962475188254,\n",
      "      \"grad_norm\": 0.04502952843904495,\n",
      "      \"learning_rate\": 2.066761184625079e-05,\n",
      "      \"loss\": 0.0248,\n",
      "      \"step\": 93140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.8702227543400864,\n",
      "      \"grad_norm\": 0.11395702511072159,\n",
      "      \"learning_rate\": 2.0661310649023314e-05,\n",
      "      \"loss\": 0.033,\n",
      "      \"step\": 93160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.871483033491918,\n",
      "      \"grad_norm\": 0.1519106924533844,\n",
      "      \"learning_rate\": 2.0655009451795843e-05,\n",
      "      \"loss\": 0.0232,\n",
      "      \"step\": 93180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.872743312643751,\n",
      "      \"grad_norm\": 0.0031758914701640606,\n",
      "      \"learning_rate\": 2.064870825456837e-05,\n",
      "      \"loss\": 0.0273,\n",
      "      \"step\": 93200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.874003591795582,\n",
      "      \"grad_norm\": 0.0007478517945855856,\n",
      "      \"learning_rate\": 2.0642407057340895e-05,\n",
      "      \"loss\": 0.0038,\n",
      "      \"step\": 93220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.875263870947415,\n",
      "      \"grad_norm\": 0.0003815965319518,\n",
      "      \"learning_rate\": 2.0636105860113424e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 93240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.876524150099247,\n",
      "      \"grad_norm\": 0.00046237983042374253,\n",
      "      \"learning_rate\": 2.062980466288595e-05,\n",
      "      \"loss\": 0.0171,\n",
      "      \"step\": 93260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.877784429251079,\n",
      "      \"grad_norm\": 0.0006716265925206244,\n",
      "      \"learning_rate\": 2.0623503465658475e-05,\n",
      "      \"loss\": 0.0352,\n",
      "      \"step\": 93280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.879044708402911,\n",
      "      \"grad_norm\": 0.031088367104530334,\n",
      "      \"learning_rate\": 2.0617202268431e-05,\n",
      "      \"loss\": 0.0584,\n",
      "      \"step\": 93300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.8803049875547435,\n",
      "      \"grad_norm\": 0.09187937527894974,\n",
      "      \"learning_rate\": 2.061090107120353e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 93320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.881565266706575,\n",
      "      \"grad_norm\": 0.12397577613592148,\n",
      "      \"learning_rate\": 2.060459987397606e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 93340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.882825545858408,\n",
      "      \"grad_norm\": 0.00045068710460327566,\n",
      "      \"learning_rate\": 2.0598298676748584e-05,\n",
      "      \"loss\": 0.031,\n",
      "      \"step\": 93360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.8840858250102395,\n",
      "      \"grad_norm\": 0.10136755555868149,\n",
      "      \"learning_rate\": 2.059199747952111e-05,\n",
      "      \"loss\": 0.0168,\n",
      "      \"step\": 93380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.885346104162072,\n",
      "      \"grad_norm\": 0.0012810751795768738,\n",
      "      \"learning_rate\": 2.0585696282293636e-05,\n",
      "      \"loss\": 0.0157,\n",
      "      \"step\": 93400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.886606383313904,\n",
      "      \"grad_norm\": 0.0004723728634417057,\n",
      "      \"learning_rate\": 2.0579395085066165e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 93420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.887866662465736,\n",
      "      \"grad_norm\": 0.0004787115612998605,\n",
      "      \"learning_rate\": 2.057309388783869e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 93440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.889126941617568,\n",
      "      \"grad_norm\": 0.0008943201974034309,\n",
      "      \"learning_rate\": 2.0566792690611216e-05,\n",
      "      \"loss\": 0.0157,\n",
      "      \"step\": 93460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.890387220769401,\n",
      "      \"grad_norm\": 0.004050950054079294,\n",
      "      \"learning_rate\": 2.0560491493383742e-05,\n",
      "      \"loss\": 0.0361,\n",
      "      \"step\": 93480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.891647499921232,\n",
      "      \"grad_norm\": 0.0006602669018320739,\n",
      "      \"learning_rate\": 2.055419029615627e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 93500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.892907779073065,\n",
      "      \"grad_norm\": 0.0015809760661795735,\n",
      "      \"learning_rate\": 2.0547889098928796e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 93520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.8941680582248965,\n",
      "      \"grad_norm\": 0.04993579909205437,\n",
      "      \"learning_rate\": 2.0541587901701322e-05,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 93540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.895428337376729,\n",
      "      \"grad_norm\": 0.007406031247228384,\n",
      "      \"learning_rate\": 2.053528670447385e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 93560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.896688616528561,\n",
      "      \"grad_norm\": 0.009221654385328293,\n",
      "      \"learning_rate\": 2.052898550724638e-05,\n",
      "      \"loss\": 0.0178,\n",
      "      \"step\": 93580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.897948895680393,\n",
      "      \"grad_norm\": 0.002920893020927906,\n",
      "      \"learning_rate\": 2.0522684310018906e-05,\n",
      "      \"loss\": 0.0396,\n",
      "      \"step\": 93600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.899209174832225,\n",
      "      \"grad_norm\": 0.025584707036614418,\n",
      "      \"learning_rate\": 2.051638311279143e-05,\n",
      "      \"loss\": 0.0332,\n",
      "      \"step\": 93620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.900469453984058,\n",
      "      \"grad_norm\": 0.004194523673504591,\n",
      "      \"learning_rate\": 2.0510081915563957e-05,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 93640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.901729733135889,\n",
      "      \"grad_norm\": 8.37618350982666,\n",
      "      \"learning_rate\": 2.0503780718336486e-05,\n",
      "      \"loss\": 0.0282,\n",
      "      \"step\": 93660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.902990012287722,\n",
      "      \"grad_norm\": 0.037177398800849915,\n",
      "      \"learning_rate\": 2.0497479521109012e-05,\n",
      "      \"loss\": 0.0347,\n",
      "      \"step\": 93680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.904250291439554,\n",
      "      \"grad_norm\": 0.7708628177642822,\n",
      "      \"learning_rate\": 2.0491178323881538e-05,\n",
      "      \"loss\": 0.0492,\n",
      "      \"step\": 93700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.905510570591386,\n",
      "      \"grad_norm\": 0.04893790930509567,\n",
      "      \"learning_rate\": 2.0484877126654063e-05,\n",
      "      \"loss\": 0.0204,\n",
      "      \"step\": 93720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.906770849743218,\n",
      "      \"grad_norm\": 0.004158579744398594,\n",
      "      \"learning_rate\": 2.0478575929426592e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 93740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.90803112889505,\n",
      "      \"grad_norm\": 0.03452114015817642,\n",
      "      \"learning_rate\": 2.0472274732199118e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 93760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.909291408046882,\n",
      "      \"grad_norm\": 0.0034330016933381557,\n",
      "      \"learning_rate\": 2.0465973534971647e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 93780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.910551687198715,\n",
      "      \"grad_norm\": 0.017369063571095467,\n",
      "      \"learning_rate\": 2.0459672337744173e-05,\n",
      "      \"loss\": 0.0121,\n",
      "      \"step\": 93800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.911811966350546,\n",
      "      \"grad_norm\": 0.0015663678059354424,\n",
      "      \"learning_rate\": 2.04533711405167e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 93820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.913072245502379,\n",
      "      \"grad_norm\": 0.0036220161709934473,\n",
      "      \"learning_rate\": 2.0447069943289227e-05,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 93840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.914332524654211,\n",
      "      \"grad_norm\": 0.0030786225106567144,\n",
      "      \"learning_rate\": 2.0440768746061753e-05,\n",
      "      \"loss\": 0.0389,\n",
      "      \"step\": 93860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.915592803806043,\n",
      "      \"grad_norm\": 0.002110406057909131,\n",
      "      \"learning_rate\": 2.043446754883428e-05,\n",
      "      \"loss\": 0.0064,\n",
      "      \"step\": 93880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.916853082957875,\n",
      "      \"grad_norm\": 0.012388411909341812,\n",
      "      \"learning_rate\": 2.0428166351606808e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 93900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.9181133621097075,\n",
      "      \"grad_norm\": 0.014555452391505241,\n",
      "      \"learning_rate\": 2.0421865154379333e-05,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 93920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.919373641261539,\n",
      "      \"grad_norm\": 0.014986783266067505,\n",
      "      \"learning_rate\": 2.041556395715186e-05,\n",
      "      \"loss\": 0.0036,\n",
      "      \"step\": 93940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.920633920413372,\n",
      "      \"grad_norm\": 5.5743513107299805,\n",
      "      \"learning_rate\": 2.0409262759924385e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 93960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.9218941995652035,\n",
      "      \"grad_norm\": 0.0013166108401492238,\n",
      "      \"learning_rate\": 2.0402961562696914e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 93980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.923154478717036,\n",
      "      \"grad_norm\": 0.00828543584793806,\n",
      "      \"learning_rate\": 2.0396975425330814e-05,\n",
      "      \"loss\": 0.0038,\n",
      "      \"step\": 94000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.924414757868868,\n",
      "      \"grad_norm\": 8.370017051696777,\n",
      "      \"learning_rate\": 2.039067422810334e-05,\n",
      "      \"loss\": 0.0156,\n",
      "      \"step\": 94020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.9256750370207,\n",
      "      \"grad_norm\": 0.04648313671350479,\n",
      "      \"learning_rate\": 2.0384373030875866e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 94040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.926935316172532,\n",
      "      \"grad_norm\": 0.0359245203435421,\n",
      "      \"learning_rate\": 2.0378071833648395e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 94060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.928195595324365,\n",
      "      \"grad_norm\": 0.0012911228695884347,\n",
      "      \"learning_rate\": 2.037177063642092e-05,\n",
      "      \"loss\": 0.0235,\n",
      "      \"step\": 94080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.929455874476196,\n",
      "      \"grad_norm\": 0.0015887768240645528,\n",
      "      \"learning_rate\": 2.0365469439193446e-05,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 94100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.930716153628029,\n",
      "      \"grad_norm\": 0.08072473108768463,\n",
      "      \"learning_rate\": 2.0359168241965975e-05,\n",
      "      \"loss\": 0.0191,\n",
      "      \"step\": 94120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.9319764327798605,\n",
      "      \"grad_norm\": 7.1938910484313965,\n",
      "      \"learning_rate\": 2.03528670447385e-05,\n",
      "      \"loss\": 0.0122,\n",
      "      \"step\": 94140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.933236711931693,\n",
      "      \"grad_norm\": 0.7763176560401917,\n",
      "      \"learning_rate\": 2.034656584751103e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 94160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.934496991083525,\n",
      "      \"grad_norm\": 0.0037893743719905615,\n",
      "      \"learning_rate\": 2.0340264650283555e-05,\n",
      "      \"loss\": 0.0247,\n",
      "      \"step\": 94180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.935757270235357,\n",
      "      \"grad_norm\": 0.14025312662124634,\n",
      "      \"learning_rate\": 2.033396345305608e-05,\n",
      "      \"loss\": 0.0067,\n",
      "      \"step\": 94200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.937017549387189,\n",
      "      \"grad_norm\": 0.0029650311917066574,\n",
      "      \"learning_rate\": 2.0327662255828607e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 94220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.938277828539022,\n",
      "      \"grad_norm\": 0.0038661942817270756,\n",
      "      \"learning_rate\": 2.0321361058601136e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 94240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.939538107690853,\n",
      "      \"grad_norm\": 0.0014082688139751554,\n",
      "      \"learning_rate\": 2.031505986137366e-05,\n",
      "      \"loss\": 0.0355,\n",
      "      \"step\": 94260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.940798386842686,\n",
      "      \"grad_norm\": 0.01618494838476181,\n",
      "      \"learning_rate\": 2.0308758664146187e-05,\n",
      "      \"loss\": 0.0182,\n",
      "      \"step\": 94280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.942058665994518,\n",
      "      \"grad_norm\": 0.004805692937225103,\n",
      "      \"learning_rate\": 2.0302457466918716e-05,\n",
      "      \"loss\": 0.0048,\n",
      "      \"step\": 94300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.94331894514635,\n",
      "      \"grad_norm\": 0.0028050767723470926,\n",
      "      \"learning_rate\": 2.0296156269691242e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 94320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.944579224298182,\n",
      "      \"grad_norm\": 0.003780259285122156,\n",
      "      \"learning_rate\": 2.028985507246377e-05,\n",
      "      \"loss\": 0.0029,\n",
      "      \"step\": 94340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.945839503450014,\n",
      "      \"grad_norm\": 0.02062457986176014,\n",
      "      \"learning_rate\": 2.0283553875236297e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 94360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.947099782601846,\n",
      "      \"grad_norm\": 6.430177688598633,\n",
      "      \"learning_rate\": 2.0277252678008822e-05,\n",
      "      \"loss\": 0.0147,\n",
      "      \"step\": 94380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.948360061753679,\n",
      "      \"grad_norm\": 0.0010926955146715045,\n",
      "      \"learning_rate\": 2.027095148078135e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 94400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.94962034090551,\n",
      "      \"grad_norm\": 0.0013149183942005038,\n",
      "      \"learning_rate\": 2.0264650283553877e-05,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 94420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.950880620057343,\n",
      "      \"grad_norm\": 0.001967388205230236,\n",
      "      \"learning_rate\": 2.0258349086326403e-05,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 94440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.952140899209175,\n",
      "      \"grad_norm\": 0.004681423772126436,\n",
      "      \"learning_rate\": 2.0252047889098928e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 94460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.953401178361007,\n",
      "      \"grad_norm\": 0.0030553617980331182,\n",
      "      \"learning_rate\": 2.0245746691871457e-05,\n",
      "      \"loss\": 0.0116,\n",
      "      \"step\": 94480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.954661457512839,\n",
      "      \"grad_norm\": 0.002942095510661602,\n",
      "      \"learning_rate\": 2.0239445494643983e-05,\n",
      "      \"loss\": 0.0143,\n",
      "      \"step\": 94500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.9559217366646715,\n",
      "      \"grad_norm\": 8.719033241271973,\n",
      "      \"learning_rate\": 2.023314429741651e-05,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 94520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.957182015816503,\n",
      "      \"grad_norm\": 0.008253826759755611,\n",
      "      \"learning_rate\": 2.0226843100189034e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 94540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.958442294968336,\n",
      "      \"grad_norm\": 0.0008780359639786184,\n",
      "      \"learning_rate\": 2.0220541902961567e-05,\n",
      "      \"loss\": 0.0529,\n",
      "      \"step\": 94560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.9597025741201675,\n",
      "      \"grad_norm\": 0.03960675373673439,\n",
      "      \"learning_rate\": 2.0214240705734092e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 94580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.960962853272,\n",
      "      \"grad_norm\": 0.0007708218181505799,\n",
      "      \"learning_rate\": 2.0207939508506618e-05,\n",
      "      \"loss\": 0.0161,\n",
      "      \"step\": 94600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.962223132423832,\n",
      "      \"grad_norm\": 0.027733681723475456,\n",
      "      \"learning_rate\": 2.0201638311279144e-05,\n",
      "      \"loss\": 0.0124,\n",
      "      \"step\": 94620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.963483411575664,\n",
      "      \"grad_norm\": 0.000653766852337867,\n",
      "      \"learning_rate\": 2.0195337114051673e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 94640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.964743690727496,\n",
      "      \"grad_norm\": 0.006299401633441448,\n",
      "      \"learning_rate\": 2.0189035916824198e-05,\n",
      "      \"loss\": 0.0396,\n",
      "      \"step\": 94660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.966003969879329,\n",
      "      \"grad_norm\": 0.3008798360824585,\n",
      "      \"learning_rate\": 2.0182734719596724e-05,\n",
      "      \"loss\": 0.0208,\n",
      "      \"step\": 94680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.96726424903116,\n",
      "      \"grad_norm\": 0.0032911195885390043,\n",
      "      \"learning_rate\": 2.017643352236925e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 94700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.968524528182993,\n",
      "      \"grad_norm\": 0.001991530181840062,\n",
      "      \"learning_rate\": 2.017013232514178e-05,\n",
      "      \"loss\": 0.0197,\n",
      "      \"step\": 94720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.9697848073348245,\n",
      "      \"grad_norm\": 0.015262135304510593,\n",
      "      \"learning_rate\": 2.0163831127914304e-05,\n",
      "      \"loss\": 0.0059,\n",
      "      \"step\": 94740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.971045086486657,\n",
      "      \"grad_norm\": 0.06585772335529327,\n",
      "      \"learning_rate\": 2.015752993068683e-05,\n",
      "      \"loss\": 0.043,\n",
      "      \"step\": 94760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.972305365638489,\n",
      "      \"grad_norm\": 0.0037043027114123106,\n",
      "      \"learning_rate\": 2.0151228733459356e-05,\n",
      "      \"loss\": 0.0032,\n",
      "      \"step\": 94780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.973565644790321,\n",
      "      \"grad_norm\": 0.019988926127552986,\n",
      "      \"learning_rate\": 2.0144927536231885e-05,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 94800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.974825923942153,\n",
      "      \"grad_norm\": 0.008974059484899044,\n",
      "      \"learning_rate\": 2.0138626339004414e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 94820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.976086203093986,\n",
      "      \"grad_norm\": 12.129912376403809,\n",
      "      \"learning_rate\": 2.013232514177694e-05,\n",
      "      \"loss\": 0.0075,\n",
      "      \"step\": 94840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.977346482245817,\n",
      "      \"grad_norm\": 0.004634350072592497,\n",
      "      \"learning_rate\": 2.0126023944549465e-05,\n",
      "      \"loss\": 0.0514,\n",
      "      \"step\": 94860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.97860676139765,\n",
      "      \"grad_norm\": 0.004874435719102621,\n",
      "      \"learning_rate\": 2.0119722747321994e-05,\n",
      "      \"loss\": 0.022,\n",
      "      \"step\": 94880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.979867040549482,\n",
      "      \"grad_norm\": 0.00852267351001501,\n",
      "      \"learning_rate\": 2.011342155009452e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 94900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.981127319701314,\n",
      "      \"grad_norm\": 0.0032075154595077038,\n",
      "      \"learning_rate\": 2.0107120352867045e-05,\n",
      "      \"loss\": 0.0176,\n",
      "      \"step\": 94920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.982387598853146,\n",
      "      \"grad_norm\": 0.0011905106948688626,\n",
      "      \"learning_rate\": 2.010081915563957e-05,\n",
      "      \"loss\": 0.0282,\n",
      "      \"step\": 94940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.983647878004978,\n",
      "      \"grad_norm\": 0.16145576536655426,\n",
      "      \"learning_rate\": 2.00945179584121e-05,\n",
      "      \"loss\": 0.0042,\n",
      "      \"step\": 94960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.98490815715681,\n",
      "      \"grad_norm\": 0.004026712384074926,\n",
      "      \"learning_rate\": 2.0088216761184626e-05,\n",
      "      \"loss\": 0.0021,\n",
      "      \"step\": 94980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.986168436308643,\n",
      "      \"grad_norm\": 0.1167670339345932,\n",
      "      \"learning_rate\": 2.008191556395715e-05,\n",
      "      \"loss\": 0.0245,\n",
      "      \"step\": 95000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.987428715460474,\n",
      "      \"grad_norm\": 0.0023470334708690643,\n",
      "      \"learning_rate\": 2.007561436672968e-05,\n",
      "      \"loss\": 0.0204,\n",
      "      \"step\": 95020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.988688994612307,\n",
      "      \"grad_norm\": 0.009594812989234924,\n",
      "      \"learning_rate\": 2.0069313169502206e-05,\n",
      "      \"loss\": 0.0294,\n",
      "      \"step\": 95040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.989949273764139,\n",
      "      \"grad_norm\": 4.833985805511475,\n",
      "      \"learning_rate\": 2.0063011972274735e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 95060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.991209552915971,\n",
      "      \"grad_norm\": 0.25908395648002625,\n",
      "      \"learning_rate\": 2.005671077504726e-05,\n",
      "      \"loss\": 0.0269,\n",
      "      \"step\": 95080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.992469832067803,\n",
      "      \"grad_norm\": 1.0248427391052246,\n",
      "      \"learning_rate\": 2.0050409577819786e-05,\n",
      "      \"loss\": 0.0019,\n",
      "      \"step\": 95100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.9937301112196355,\n",
      "      \"grad_norm\": 0.012908673845231533,\n",
      "      \"learning_rate\": 2.0044108380592312e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 95120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.994990390371467,\n",
      "      \"grad_norm\": 0.3249235451221466,\n",
      "      \"learning_rate\": 2.003780718336484e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 95140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.9962506695233,\n",
      "      \"grad_norm\": 3.8392183780670166,\n",
      "      \"learning_rate\": 2.0031505986137367e-05,\n",
      "      \"loss\": 0.039,\n",
      "      \"step\": 95160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.9975109486751315,\n",
      "      \"grad_norm\": 0.02316298522055149,\n",
      "      \"learning_rate\": 2.0025204788909892e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 95180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.998771227826964,\n",
      "      \"grad_norm\": 0.011142313480377197,\n",
      "      \"learning_rate\": 2.001890359168242e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 95200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.0,\n",
      "      \"grad_norm\": 0.0021752347238361835,\n",
      "      \"learning_rate\": 2.0012602394454947e-05,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 95220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.0,\n",
      "      \"eval_accuracy\": 0.9994014239808455,\n",
      "      \"eval_f1\": 0.9994013296782934,\n",
      "      \"eval_loss\": 0.0020127154421061277,\n",
      "      \"eval_precision\": 0.9995588049918064,\n",
      "      \"eval_recall\": 0.999243903975805,\n",
      "      \"eval_runtime\": 582.6315,\n",
      "      \"eval_samples_per_second\": 54.48,\n",
      "      \"eval_steps_per_second\": 6.81,\n",
      "      \"step\": 95220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.001260279151832,\n",
      "      \"grad_norm\": 0.026166897267103195,\n",
      "      \"learning_rate\": 2.0006301197227476e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 95240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.002520558303664,\n",
      "      \"grad_norm\": 0.01783393882215023,\n",
      "      \"learning_rate\": 2e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 95260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.003780837455496,\n",
      "      \"grad_norm\": 0.038320522755384445,\n",
      "      \"learning_rate\": 1.9993698802772527e-05,\n",
      "      \"loss\": 0.0174,\n",
      "      \"step\": 95280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.0050411166073285,\n",
      "      \"grad_norm\": 0.010600398294627666,\n",
      "      \"learning_rate\": 1.9987397605545056e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 95300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.00630139575916,\n",
      "      \"grad_norm\": 0.002033966826274991,\n",
      "      \"learning_rate\": 1.9981096408317582e-05,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 95320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.007561674910993,\n",
      "      \"grad_norm\": 0.000747930200304836,\n",
      "      \"learning_rate\": 1.9974795211090108e-05,\n",
      "      \"loss\": 0.0391,\n",
      "      \"step\": 95340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.0088219540628245,\n",
      "      \"grad_norm\": 0.0033101823646575212,\n",
      "      \"learning_rate\": 1.9968494013862633e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 95360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.010082233214657,\n",
      "      \"grad_norm\": 0.01142377220094204,\n",
      "      \"learning_rate\": 1.9962192816635162e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 95380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.011342512366489,\n",
      "      \"grad_norm\": 0.0027791426982730627,\n",
      "      \"learning_rate\": 1.9955891619407688e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 95400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.012602791518321,\n",
      "      \"grad_norm\": 0.03373774513602257,\n",
      "      \"learning_rate\": 1.9949590422180214e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 95420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.013863070670153,\n",
      "      \"grad_norm\": 0.007660856004804373,\n",
      "      \"learning_rate\": 1.994328922495274e-05,\n",
      "      \"loss\": 0.0059,\n",
      "      \"step\": 95440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.015123349821986,\n",
      "      \"grad_norm\": 0.04818492755293846,\n",
      "      \"learning_rate\": 1.993698802772527e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 95460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.016383628973817,\n",
      "      \"grad_norm\": 0.0034123496152460575,\n",
      "      \"learning_rate\": 1.9930686830497797e-05,\n",
      "      \"loss\": 0.0019,\n",
      "      \"step\": 95480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.01764390812565,\n",
      "      \"grad_norm\": 0.021290047094225883,\n",
      "      \"learning_rate\": 1.9924385633270323e-05,\n",
      "      \"loss\": 0.0453,\n",
      "      \"step\": 95500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.0189041872774816,\n",
      "      \"grad_norm\": 0.15964196622371674,\n",
      "      \"learning_rate\": 1.991808443604285e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 95520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.020164466429314,\n",
      "      \"grad_norm\": 0.0018916316330432892,\n",
      "      \"learning_rate\": 1.9911783238815378e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 95540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.021424745581146,\n",
      "      \"grad_norm\": 0.0021665466483682394,\n",
      "      \"learning_rate\": 1.9905482041587903e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 95560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.022685024732978,\n",
      "      \"grad_norm\": 0.002794594271108508,\n",
      "      \"learning_rate\": 1.989918084436043e-05,\n",
      "      \"loss\": 0.0245,\n",
      "      \"step\": 95580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.02394530388481,\n",
      "      \"grad_norm\": 4.434595584869385,\n",
      "      \"learning_rate\": 1.9892879647132955e-05,\n",
      "      \"loss\": 0.0517,\n",
      "      \"step\": 95600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.025205583036643,\n",
      "      \"grad_norm\": 0.16680391132831573,\n",
      "      \"learning_rate\": 1.9886578449905484e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 95620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.026465862188474,\n",
      "      \"grad_norm\": 0.010089723393321037,\n",
      "      \"learning_rate\": 1.988027725267801e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 95640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.027726141340307,\n",
      "      \"grad_norm\": 0.002625678665935993,\n",
      "      \"learning_rate\": 1.9873976055450535e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 95660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.028986420492139,\n",
      "      \"grad_norm\": 0.011584078893065453,\n",
      "      \"learning_rate\": 1.986767485822306e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 95680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.030246699643971,\n",
      "      \"grad_norm\": 0.005413162987679243,\n",
      "      \"learning_rate\": 1.9861373660995593e-05,\n",
      "      \"loss\": 0.0061,\n",
      "      \"step\": 95700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.031506978795803,\n",
      "      \"grad_norm\": 0.031857024878263474,\n",
      "      \"learning_rate\": 1.985507246376812e-05,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 95720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.0327672579476355,\n",
      "      \"grad_norm\": 0.08797810226678848,\n",
      "      \"learning_rate\": 1.9848771266540644e-05,\n",
      "      \"loss\": 0.0221,\n",
      "      \"step\": 95740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.034027537099467,\n",
      "      \"grad_norm\": 0.0021461688447743654,\n",
      "      \"learning_rate\": 1.984247006931317e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 95760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.0352878162513,\n",
      "      \"grad_norm\": 0.012638438493013382,\n",
      "      \"learning_rate\": 1.98361688720857e-05,\n",
      "      \"loss\": 0.0228,\n",
      "      \"step\": 95780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.036548095403131,\n",
      "      \"grad_norm\": 0.008097153156995773,\n",
      "      \"learning_rate\": 1.9829867674858225e-05,\n",
      "      \"loss\": 0.0351,\n",
      "      \"step\": 95800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.037808374554964,\n",
      "      \"grad_norm\": 0.04201024770736694,\n",
      "      \"learning_rate\": 1.982356647763075e-05,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 95820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.039068653706796,\n",
      "      \"grad_norm\": 0.004436004441231489,\n",
      "      \"learning_rate\": 1.9817265280403276e-05,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 95840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.040328932858628,\n",
      "      \"grad_norm\": 0.0034514055587351322,\n",
      "      \"learning_rate\": 1.9810964083175805e-05,\n",
      "      \"loss\": 0.0184,\n",
      "      \"step\": 95860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.04158921201046,\n",
      "      \"grad_norm\": 0.01233554258942604,\n",
      "      \"learning_rate\": 1.980466288594833e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 95880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.0428494911622925,\n",
      "      \"grad_norm\": 0.011116152629256248,\n",
      "      \"learning_rate\": 1.9798361688720856e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 95900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.044109770314124,\n",
      "      \"grad_norm\": 0.08637601882219315,\n",
      "      \"learning_rate\": 1.9792060491493385e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 95920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.045370049465957,\n",
      "      \"grad_norm\": 0.12948906421661377,\n",
      "      \"learning_rate\": 1.978575929426591e-05,\n",
      "      \"loss\": 0.0237,\n",
      "      \"step\": 95940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.0466303286177885,\n",
      "      \"grad_norm\": 0.00304963206872344,\n",
      "      \"learning_rate\": 1.977945809703844e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 95960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.047890607769621,\n",
      "      \"grad_norm\": 0.00667373463511467,\n",
      "      \"learning_rate\": 1.9773156899810966e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 95980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.049150886921453,\n",
      "      \"grad_norm\": 0.0009979449678212404,\n",
      "      \"learning_rate\": 1.976685570258349e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 96000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.050411166073285,\n",
      "      \"grad_norm\": 0.0028025240171700716,\n",
      "      \"learning_rate\": 1.9760554505356017e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 96020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.051671445225117,\n",
      "      \"grad_norm\": 0.0031520500779151917,\n",
      "      \"learning_rate\": 1.9754253308128546e-05,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 96040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.05293172437695,\n",
      "      \"grad_norm\": 7.695467948913574,\n",
      "      \"learning_rate\": 1.9747952110901072e-05,\n",
      "      \"loss\": 0.0064,\n",
      "      \"step\": 96060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.054192003528781,\n",
      "      \"grad_norm\": 0.003460574196651578,\n",
      "      \"learning_rate\": 1.9741650913673597e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 96080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.055452282680614,\n",
      "      \"grad_norm\": 0.004089470487087965,\n",
      "      \"learning_rate\": 1.9735349716446126e-05,\n",
      "      \"loss\": 0.0215,\n",
      "      \"step\": 96100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.0567125618324456,\n",
      "      \"grad_norm\": 0.007398703135550022,\n",
      "      \"learning_rate\": 1.9729048519218652e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 96120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.057972840984278,\n",
      "      \"grad_norm\": 0.0006866825860925019,\n",
      "      \"learning_rate\": 1.9722747321991178e-05,\n",
      "      \"loss\": 0.0129,\n",
      "      \"step\": 96140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.05923312013611,\n",
      "      \"grad_norm\": 0.003949364181607962,\n",
      "      \"learning_rate\": 1.9716446124763707e-05,\n",
      "      \"loss\": 0.0083,\n",
      "      \"step\": 96160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.060493399287942,\n",
      "      \"grad_norm\": 0.0006557846791110933,\n",
      "      \"learning_rate\": 1.9710144927536232e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 96180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.061753678439774,\n",
      "      \"grad_norm\": 0.05210588872432709,\n",
      "      \"learning_rate\": 1.970384373030876e-05,\n",
      "      \"loss\": 0.0268,\n",
      "      \"step\": 96200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.063013957591607,\n",
      "      \"grad_norm\": 0.002592681208625436,\n",
      "      \"learning_rate\": 1.9697542533081287e-05,\n",
      "      \"loss\": 0.0481,\n",
      "      \"step\": 96220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.064274236743438,\n",
      "      \"grad_norm\": 0.06276755034923553,\n",
      "      \"learning_rate\": 1.9691241335853813e-05,\n",
      "      \"loss\": 0.0038,\n",
      "      \"step\": 96240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.065534515895271,\n",
      "      \"grad_norm\": 0.006621942855417728,\n",
      "      \"learning_rate\": 1.968494013862634e-05,\n",
      "      \"loss\": 0.0205,\n",
      "      \"step\": 96260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.066794795047103,\n",
      "      \"grad_norm\": 0.00747602479532361,\n",
      "      \"learning_rate\": 1.9678638941398867e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 96280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.068055074198935,\n",
      "      \"grad_norm\": 0.0022117095068097115,\n",
      "      \"learning_rate\": 1.9672337744171393e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 96300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.069315353350767,\n",
      "      \"grad_norm\": 0.004004557617008686,\n",
      "      \"learning_rate\": 1.966603654694392e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 96320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.0705756325025995,\n",
      "      \"grad_norm\": 0.005299472715705633,\n",
      "      \"learning_rate\": 1.9659735349716444e-05,\n",
      "      \"loss\": 0.035,\n",
      "      \"step\": 96340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.071835911654431,\n",
      "      \"grad_norm\": 0.0031934394501149654,\n",
      "      \"learning_rate\": 1.9653434152488973e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 96360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.073096190806264,\n",
      "      \"grad_norm\": 0.005877162795513868,\n",
      "      \"learning_rate\": 1.9647132955261502e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 96380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.074356469958095,\n",
      "      \"grad_norm\": 0.025952773168683052,\n",
      "      \"learning_rate\": 1.9640831758034028e-05,\n",
      "      \"loss\": 0.007,\n",
      "      \"step\": 96400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.075616749109928,\n",
      "      \"grad_norm\": 0.1024666503071785,\n",
      "      \"learning_rate\": 1.9634530560806554e-05,\n",
      "      \"loss\": 0.0319,\n",
      "      \"step\": 96420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.07687702826176,\n",
      "      \"grad_norm\": 0.004136526957154274,\n",
      "      \"learning_rate\": 1.9628229363579083e-05,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 96440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.078137307413592,\n",
      "      \"grad_norm\": 0.015002850443124771,\n",
      "      \"learning_rate\": 1.962192816635161e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 96460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.079397586565424,\n",
      "      \"grad_norm\": 0.0033859009854495525,\n",
      "      \"learning_rate\": 1.9615626969124134e-05,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 96480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.0806578657172565,\n",
      "      \"grad_norm\": 0.021217212080955505,\n",
      "      \"learning_rate\": 1.960932577189666e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 96500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.081918144869088,\n",
      "      \"grad_norm\": 0.02541998215019703,\n",
      "      \"learning_rate\": 1.960302457466919e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 96520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.083178424020921,\n",
      "      \"grad_norm\": 0.0044214315712451935,\n",
      "      \"learning_rate\": 1.9596723377441714e-05,\n",
      "      \"loss\": 0.0548,\n",
      "      \"step\": 96540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.0844387031727525,\n",
      "      \"grad_norm\": 0.0017994894878938794,\n",
      "      \"learning_rate\": 1.959042218021424e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 96560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.085698982324585,\n",
      "      \"grad_norm\": 0.08743378520011902,\n",
      "      \"learning_rate\": 1.9584120982986766e-05,\n",
      "      \"loss\": 0.0449,\n",
      "      \"step\": 96580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.086959261476417,\n",
      "      \"grad_norm\": 0.3669660687446594,\n",
      "      \"learning_rate\": 1.9577819785759298e-05,\n",
      "      \"loss\": 0.0161,\n",
      "      \"step\": 96600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.088219540628249,\n",
      "      \"grad_norm\": 0.7172430157661438,\n",
      "      \"learning_rate\": 1.9571518588531824e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 96620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.089479819780081,\n",
      "      \"grad_norm\": 4.522134304046631,\n",
      "      \"learning_rate\": 1.956521739130435e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 96640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.090740098931914,\n",
      "      \"grad_norm\": 0.0032384973019361496,\n",
      "      \"learning_rate\": 1.9558916194076875e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 96660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.092000378083745,\n",
      "      \"grad_norm\": 0.00134247203823179,\n",
      "      \"learning_rate\": 1.9552614996849404e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 96680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.093260657235578,\n",
      "      \"grad_norm\": 0.024892229586839676,\n",
      "      \"learning_rate\": 1.954631379962193e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 96700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.0945209363874095,\n",
      "      \"grad_norm\": 0.3976236879825592,\n",
      "      \"learning_rate\": 1.9540012602394455e-05,\n",
      "      \"loss\": 0.0052,\n",
      "      \"step\": 96720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.095781215539242,\n",
      "      \"grad_norm\": 0.0023902764078229666,\n",
      "      \"learning_rate\": 1.953371140516698e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 96740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.097041494691074,\n",
      "      \"grad_norm\": 0.006163441576063633,\n",
      "      \"learning_rate\": 1.952741020793951e-05,\n",
      "      \"loss\": 0.0146,\n",
      "      \"step\": 96760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.098301773842906,\n",
      "      \"grad_norm\": 0.027553027495741844,\n",
      "      \"learning_rate\": 1.9521109010712036e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 96780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.099562052994738,\n",
      "      \"grad_norm\": 0.0007040132186375558,\n",
      "      \"learning_rate\": 1.951480781348456e-05,\n",
      "      \"loss\": 0.0369,\n",
      "      \"step\": 96800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.100822332146571,\n",
      "      \"grad_norm\": 0.010255705565214157,\n",
      "      \"learning_rate\": 1.9508506616257087e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 96820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.102082611298402,\n",
      "      \"grad_norm\": 0.000920686055906117,\n",
      "      \"learning_rate\": 1.9502205419029616e-05,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 96840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.103342890450235,\n",
      "      \"grad_norm\": 0.03985142335295677,\n",
      "      \"learning_rate\": 1.9495904221802145e-05,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 96860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.104603169602067,\n",
      "      \"grad_norm\": 0.001191257848404348,\n",
      "      \"learning_rate\": 1.948960302457467e-05,\n",
      "      \"loss\": 0.0397,\n",
      "      \"step\": 96880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.105863448753899,\n",
      "      \"grad_norm\": 0.000670366920530796,\n",
      "      \"learning_rate\": 1.9483301827347197e-05,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 96900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.107123727905731,\n",
      "      \"grad_norm\": 0.010789872147142887,\n",
      "      \"learning_rate\": 1.9477000630119726e-05,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 96920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.1083840070575635,\n",
      "      \"grad_norm\": 0.006870730314403772,\n",
      "      \"learning_rate\": 1.947069943289225e-05,\n",
      "      \"loss\": 0.059,\n",
      "      \"step\": 96940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.109644286209395,\n",
      "      \"grad_norm\": 0.0032116700895130634,\n",
      "      \"learning_rate\": 1.9464398235664777e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 96960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.110904565361228,\n",
      "      \"grad_norm\": 0.0013307532062754035,\n",
      "      \"learning_rate\": 1.9458097038437303e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 96980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.112164844513059,\n",
      "      \"grad_norm\": 0.05611501634120941,\n",
      "      \"learning_rate\": 1.945179584120983e-05,\n",
      "      \"loss\": 0.0146,\n",
      "      \"step\": 97000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.113425123664892,\n",
      "      \"grad_norm\": 0.0018561758333817124,\n",
      "      \"learning_rate\": 1.9445494643982357e-05,\n",
      "      \"loss\": 0.0266,\n",
      "      \"step\": 97020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.114685402816724,\n",
      "      \"grad_norm\": 0.007536259479820728,\n",
      "      \"learning_rate\": 1.9439193446754883e-05,\n",
      "      \"loss\": 0.0263,\n",
      "      \"step\": 97040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.115945681968556,\n",
      "      \"grad_norm\": 7.887012004852295,\n",
      "      \"learning_rate\": 1.9432892249527412e-05,\n",
      "      \"loss\": 0.0281,\n",
      "      \"step\": 97060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.117205961120388,\n",
      "      \"grad_norm\": 0.0028132819570600986,\n",
      "      \"learning_rate\": 1.9426591052299938e-05,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 97080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.1184662402722205,\n",
      "      \"grad_norm\": 0.008404999040067196,\n",
      "      \"learning_rate\": 1.9420289855072467e-05,\n",
      "      \"loss\": 0.013,\n",
      "      \"step\": 97100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.119726519424052,\n",
      "      \"grad_norm\": 0.047854967415332794,\n",
      "      \"learning_rate\": 1.9413988657844992e-05,\n",
      "      \"loss\": 0.0069,\n",
      "      \"step\": 97120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.120986798575885,\n",
      "      \"grad_norm\": 0.01040016207844019,\n",
      "      \"learning_rate\": 1.9407687460617518e-05,\n",
      "      \"loss\": 0.0452,\n",
      "      \"step\": 97140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.1222470777277165,\n",
      "      \"grad_norm\": 0.015493283048272133,\n",
      "      \"learning_rate\": 1.9401386263390044e-05,\n",
      "      \"loss\": 0.0157,\n",
      "      \"step\": 97160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.123507356879549,\n",
      "      \"grad_norm\": 0.0017628155183047056,\n",
      "      \"learning_rate\": 1.9395085066162573e-05,\n",
      "      \"loss\": 0.0256,\n",
      "      \"step\": 97180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.124767636031381,\n",
      "      \"grad_norm\": 0.02756291627883911,\n",
      "      \"learning_rate\": 1.9388783868935098e-05,\n",
      "      \"loss\": 0.0229,\n",
      "      \"step\": 97200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.126027915183213,\n",
      "      \"grad_norm\": 0.0059087867848575115,\n",
      "      \"learning_rate\": 1.9382482671707624e-05,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 97220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.127288194335045,\n",
      "      \"grad_norm\": 5.761173248291016,\n",
      "      \"learning_rate\": 1.937618147448015e-05,\n",
      "      \"loss\": 0.0189,\n",
      "      \"step\": 97240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.128548473486878,\n",
      "      \"grad_norm\": 1.758857250213623,\n",
      "      \"learning_rate\": 1.936988027725268e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 97260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.129808752638709,\n",
      "      \"grad_norm\": 0.0008386163390241563,\n",
      "      \"learning_rate\": 1.9363579080025208e-05,\n",
      "      \"loss\": 0.0251,\n",
      "      \"step\": 97280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.131069031790542,\n",
      "      \"grad_norm\": 0.04068249091506004,\n",
      "      \"learning_rate\": 1.9357277882797733e-05,\n",
      "      \"loss\": 0.0172,\n",
      "      \"step\": 97300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.1323293109423735,\n",
      "      \"grad_norm\": 0.2778061330318451,\n",
      "      \"learning_rate\": 1.935097668557026e-05,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 97320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.133589590094206,\n",
      "      \"grad_norm\": 0.000912342919036746,\n",
      "      \"learning_rate\": 1.9344675488342788e-05,\n",
      "      \"loss\": 0.0235,\n",
      "      \"step\": 97340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.134849869246038,\n",
      "      \"grad_norm\": 21.5783748626709,\n",
      "      \"learning_rate\": 1.9338374291115314e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 97360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.13611014839787,\n",
      "      \"grad_norm\": 0.05688021332025528,\n",
      "      \"learning_rate\": 1.933207309388784e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 97380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.137370427549702,\n",
      "      \"grad_norm\": 0.0013036220334470272,\n",
      "      \"learning_rate\": 1.9325771896660365e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 97400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.138630706701535,\n",
      "      \"grad_norm\": 0.0004678131954278797,\n",
      "      \"learning_rate\": 1.9319470699432894e-05,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 97420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.139890985853366,\n",
      "      \"grad_norm\": 0.0013354620896279812,\n",
      "      \"learning_rate\": 1.931316950220542e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 97440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.141151265005199,\n",
      "      \"grad_norm\": 0.003178610233590007,\n",
      "      \"learning_rate\": 1.9306868304977945e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 97460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.142411544157031,\n",
      "      \"grad_norm\": 0.0006856477120891213,\n",
      "      \"learning_rate\": 1.930056710775047e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 97480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.143671823308863,\n",
      "      \"grad_norm\": 0.002379068871960044,\n",
      "      \"learning_rate\": 1.9294265910523e-05,\n",
      "      \"loss\": 0.0357,\n",
      "      \"step\": 97500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.144932102460695,\n",
      "      \"grad_norm\": 9.95698356628418,\n",
      "      \"learning_rate\": 1.928796471329553e-05,\n",
      "      \"loss\": 0.0631,\n",
      "      \"step\": 97520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.1461923816125275,\n",
      "      \"grad_norm\": 0.007731057703495026,\n",
      "      \"learning_rate\": 1.9281663516068055e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 97540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.147452660764359,\n",
      "      \"grad_norm\": 0.0019480495247989893,\n",
      "      \"learning_rate\": 1.927536231884058e-05,\n",
      "      \"loss\": 0.0255,\n",
      "      \"step\": 97560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.148712939916192,\n",
      "      \"grad_norm\": 0.0008196349372155964,\n",
      "      \"learning_rate\": 1.926906112161311e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 97580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.149973219068023,\n",
      "      \"grad_norm\": 0.31856095790863037,\n",
      "      \"learning_rate\": 1.9262759924385635e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 97600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.151233498219856,\n",
      "      \"grad_norm\": 6.447207927703857,\n",
      "      \"learning_rate\": 1.925645872715816e-05,\n",
      "      \"loss\": 0.0211,\n",
      "      \"step\": 97620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.152493777371688,\n",
      "      \"grad_norm\": 0.034513916820287704,\n",
      "      \"learning_rate\": 1.9250157529930686e-05,\n",
      "      \"loss\": 0.0244,\n",
      "      \"step\": 97640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.15375405652352,\n",
      "      \"grad_norm\": 0.005320241209119558,\n",
      "      \"learning_rate\": 1.9243856332703215e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 97660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.155014335675352,\n",
      "      \"grad_norm\": 0.003880945732817054,\n",
      "      \"learning_rate\": 1.923755513547574e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 97680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.1562746148271845,\n",
      "      \"grad_norm\": 0.013680403120815754,\n",
      "      \"learning_rate\": 1.9231253938248267e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 97700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.157534893979016,\n",
      "      \"grad_norm\": 0.011167814955115318,\n",
      "      \"learning_rate\": 1.9224952741020792e-05,\n",
      "      \"loss\": 0.0193,\n",
      "      \"step\": 97720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.158795173130849,\n",
      "      \"grad_norm\": 0.05707041919231415,\n",
      "      \"learning_rate\": 1.921865154379332e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 97740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.1600554522826805,\n",
      "      \"grad_norm\": 0.004567865282297134,\n",
      "      \"learning_rate\": 1.921235034656585e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 97760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.161315731434513,\n",
      "      \"grad_norm\": 0.02474444918334484,\n",
      "      \"learning_rate\": 1.9206049149338376e-05,\n",
      "      \"loss\": 0.0287,\n",
      "      \"step\": 97780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.162576010586345,\n",
      "      \"grad_norm\": 0.0008437026408500969,\n",
      "      \"learning_rate\": 1.91997479521109e-05,\n",
      "      \"loss\": 0.0241,\n",
      "      \"step\": 97800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.163836289738177,\n",
      "      \"grad_norm\": 0.001388623146340251,\n",
      "      \"learning_rate\": 1.919344675488343e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 97820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.165096568890009,\n",
      "      \"grad_norm\": 0.0007843346684239805,\n",
      "      \"learning_rate\": 1.9187145557655956e-05,\n",
      "      \"loss\": 0.0243,\n",
      "      \"step\": 97840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.166356848041842,\n",
      "      \"grad_norm\": 0.40252286195755005,\n",
      "      \"learning_rate\": 1.9180844360428482e-05,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 97860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.167617127193673,\n",
      "      \"grad_norm\": 0.5485747456550598,\n",
      "      \"learning_rate\": 1.9174543163201008e-05,\n",
      "      \"loss\": 0.0075,\n",
      "      \"step\": 97880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.168877406345506,\n",
      "      \"grad_norm\": 0.0019225702853873372,\n",
      "      \"learning_rate\": 1.9168241965973537e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 97900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.1701376854973375,\n",
      "      \"grad_norm\": 0.009627383202314377,\n",
      "      \"learning_rate\": 1.9161940768746062e-05,\n",
      "      \"loss\": 0.0345,\n",
      "      \"step\": 97920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.17139796464917,\n",
      "      \"grad_norm\": 0.023547682911157608,\n",
      "      \"learning_rate\": 1.9155639571518588e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 97940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.172658243801002,\n",
      "      \"grad_norm\": 0.019519764930009842,\n",
      "      \"learning_rate\": 1.9149338374291117e-05,\n",
      "      \"loss\": 0.0623,\n",
      "      \"step\": 97960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.173918522952834,\n",
      "      \"grad_norm\": 0.010738380253314972,\n",
      "      \"learning_rate\": 1.9143037177063643e-05,\n",
      "      \"loss\": 0.0064,\n",
      "      \"step\": 97980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.175178802104666,\n",
      "      \"grad_norm\": 0.24240203201770782,\n",
      "      \"learning_rate\": 1.9136735979836172e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 98000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.176439081256499,\n",
      "      \"grad_norm\": 0.2555523216724396,\n",
      "      \"learning_rate\": 1.9130434782608697e-05,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 98020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.17769936040833,\n",
      "      \"grad_norm\": 0.00822798628360033,\n",
      "      \"learning_rate\": 1.9124133585381223e-05,\n",
      "      \"loss\": 0.0178,\n",
      "      \"step\": 98040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.178959639560163,\n",
      "      \"grad_norm\": 0.027330419048666954,\n",
      "      \"learning_rate\": 1.911783238815375e-05,\n",
      "      \"loss\": 0.0316,\n",
      "      \"step\": 98060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.180219918711995,\n",
      "      \"grad_norm\": 0.018704403191804886,\n",
      "      \"learning_rate\": 1.9111531190926278e-05,\n",
      "      \"loss\": 0.0178,\n",
      "      \"step\": 98080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.181480197863827,\n",
      "      \"grad_norm\": 0.024008959531784058,\n",
      "      \"learning_rate\": 1.9105229993698803e-05,\n",
      "      \"loss\": 0.002,\n",
      "      \"step\": 98100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.182740477015659,\n",
      "      \"grad_norm\": 0.0012737407814711332,\n",
      "      \"learning_rate\": 1.909892879647133e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 98120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.1840007561674915,\n",
      "      \"grad_norm\": 0.003424046328291297,\n",
      "      \"learning_rate\": 1.9092627599243858e-05,\n",
      "      \"loss\": 0.0171,\n",
      "      \"step\": 98140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.185261035319323,\n",
      "      \"grad_norm\": 0.003142725443467498,\n",
      "      \"learning_rate\": 1.9086326402016384e-05,\n",
      "      \"loss\": 0.0076,\n",
      "      \"step\": 98160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.186521314471156,\n",
      "      \"grad_norm\": 0.0038293979596346617,\n",
      "      \"learning_rate\": 1.908002520478891e-05,\n",
      "      \"loss\": 0.0229,\n",
      "      \"step\": 98180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.187781593622987,\n",
      "      \"grad_norm\": 0.006390674971044064,\n",
      "      \"learning_rate\": 1.907372400756144e-05,\n",
      "      \"loss\": 0.0317,\n",
      "      \"step\": 98200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.18904187277482,\n",
      "      \"grad_norm\": 0.05476810783147812,\n",
      "      \"learning_rate\": 1.9067422810333964e-05,\n",
      "      \"loss\": 0.0209,\n",
      "      \"step\": 98220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.190302151926652,\n",
      "      \"grad_norm\": 0.002996716182678938,\n",
      "      \"learning_rate\": 1.9061121613106493e-05,\n",
      "      \"loss\": 0.0265,\n",
      "      \"step\": 98240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.191562431078484,\n",
      "      \"grad_norm\": 0.11033805459737778,\n",
      "      \"learning_rate\": 1.905482041587902e-05,\n",
      "      \"loss\": 0.0252,\n",
      "      \"step\": 98260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.192822710230316,\n",
      "      \"grad_norm\": 0.15493303537368774,\n",
      "      \"learning_rate\": 1.9048519218651544e-05,\n",
      "      \"loss\": 0.0189,\n",
      "      \"step\": 98280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.1940829893821485,\n",
      "      \"grad_norm\": 0.01347010675817728,\n",
      "      \"learning_rate\": 1.904221802142407e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 98300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.19534326853398,\n",
      "      \"grad_norm\": 0.007033472880721092,\n",
      "      \"learning_rate\": 1.90359168241966e-05,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 98320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.196603547685813,\n",
      "      \"grad_norm\": 0.09711702913045883,\n",
      "      \"learning_rate\": 1.9029615626969125e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 98340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.1978638268376445,\n",
      "      \"grad_norm\": 0.00648734113201499,\n",
      "      \"learning_rate\": 1.902331442974165e-05,\n",
      "      \"loss\": 0.0334,\n",
      "      \"step\": 98360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.199124105989477,\n",
      "      \"grad_norm\": 0.13552889227867126,\n",
      "      \"learning_rate\": 1.9017013232514176e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 98380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.200384385141309,\n",
      "      \"grad_norm\": 0.03820763900876045,\n",
      "      \"learning_rate\": 1.9010712035286705e-05,\n",
      "      \"loss\": 0.0161,\n",
      "      \"step\": 98400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.201644664293141,\n",
      "      \"grad_norm\": 0.15126684308052063,\n",
      "      \"learning_rate\": 1.9004410838059234e-05,\n",
      "      \"loss\": 0.0162,\n",
      "      \"step\": 98420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.202904943444973,\n",
      "      \"grad_norm\": 7.022610664367676,\n",
      "      \"learning_rate\": 1.899810964083176e-05,\n",
      "      \"loss\": 0.0208,\n",
      "      \"step\": 98440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.204165222596806,\n",
      "      \"grad_norm\": 0.22092808783054352,\n",
      "      \"learning_rate\": 1.8991808443604285e-05,\n",
      "      \"loss\": 0.0353,\n",
      "      \"step\": 98460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.205425501748637,\n",
      "      \"grad_norm\": 14.781670570373535,\n",
      "      \"learning_rate\": 1.8985507246376814e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 98480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.20668578090047,\n",
      "      \"grad_norm\": 0.0027239012997597456,\n",
      "      \"learning_rate\": 1.897920604914934e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 98500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.2079460600523015,\n",
      "      \"grad_norm\": 0.0032806876115500927,\n",
      "      \"learning_rate\": 1.8972904851921866e-05,\n",
      "      \"loss\": 0.0169,\n",
      "      \"step\": 98520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.209206339204134,\n",
      "      \"grad_norm\": 0.00227144593372941,\n",
      "      \"learning_rate\": 1.896660365469439e-05,\n",
      "      \"loss\": 0.0176,\n",
      "      \"step\": 98540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.210466618355966,\n",
      "      \"grad_norm\": 5.529057025909424,\n",
      "      \"learning_rate\": 1.896030245746692e-05,\n",
      "      \"loss\": 0.0305,\n",
      "      \"step\": 98560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.211726897507798,\n",
      "      \"grad_norm\": 0.0902923047542572,\n",
      "      \"learning_rate\": 1.8954001260239446e-05,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 98580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.21298717665963,\n",
      "      \"grad_norm\": 0.009063039906322956,\n",
      "      \"learning_rate\": 1.8947700063011972e-05,\n",
      "      \"loss\": 0.019,\n",
      "      \"step\": 98600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.214247455811463,\n",
      "      \"grad_norm\": 0.11303038150072098,\n",
      "      \"learning_rate\": 1.8941398865784497e-05,\n",
      "      \"loss\": 0.0044,\n",
      "      \"step\": 98620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.215507734963294,\n",
      "      \"grad_norm\": 0.005958612076938152,\n",
      "      \"learning_rate\": 1.8935097668557026e-05,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 98640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.216768014115127,\n",
      "      \"grad_norm\": 0.10295776277780533,\n",
      "      \"learning_rate\": 1.8928796471329555e-05,\n",
      "      \"loss\": 0.02,\n",
      "      \"step\": 98660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.218028293266959,\n",
      "      \"grad_norm\": 0.002107444452121854,\n",
      "      \"learning_rate\": 1.892249527410208e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 98680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.219288572418791,\n",
      "      \"grad_norm\": 0.06498631089925766,\n",
      "      \"learning_rate\": 1.8916194076874607e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 98700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.220548851570623,\n",
      "      \"grad_norm\": 0.2579774260520935,\n",
      "      \"learning_rate\": 1.8909892879647136e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 98720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.2218091307224554,\n",
      "      \"grad_norm\": 0.000977764604613185,\n",
      "      \"learning_rate\": 1.890359168241966e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 98740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.223069409874287,\n",
      "      \"grad_norm\": 0.010587982833385468,\n",
      "      \"learning_rate\": 1.8897290485192187e-05,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 98760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.224329689026119,\n",
      "      \"grad_norm\": 0.0008537365356460214,\n",
      "      \"learning_rate\": 1.8890989287964713e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 98780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.225589968177951,\n",
      "      \"grad_norm\": 0.002067714696750045,\n",
      "      \"learning_rate\": 1.8884688090737242e-05,\n",
      "      \"loss\": 0.0324,\n",
      "      \"step\": 98800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.226850247329784,\n",
      "      \"grad_norm\": 0.01746520958840847,\n",
      "      \"learning_rate\": 1.8878386893509767e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 98820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.228110526481616,\n",
      "      \"grad_norm\": 0.005313987843692303,\n",
      "      \"learning_rate\": 1.8872085696282293e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 98840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.229370805633447,\n",
      "      \"grad_norm\": 0.0054491725750267506,\n",
      "      \"learning_rate\": 1.8865784499054822e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 98860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.23063108478528,\n",
      "      \"grad_norm\": 0.0007461006753146648,\n",
      "      \"learning_rate\": 1.8859483301827348e-05,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 98880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.2318913639371125,\n",
      "      \"grad_norm\": 0.0004121437086723745,\n",
      "      \"learning_rate\": 1.8853182104599877e-05,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 98900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.233151643088944,\n",
      "      \"grad_norm\": 0.24306470155715942,\n",
      "      \"learning_rate\": 1.8846880907372402e-05,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 98920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.234411922240776,\n",
      "      \"grad_norm\": 0.31937310099601746,\n",
      "      \"learning_rate\": 1.8840579710144928e-05,\n",
      "      \"loss\": 0.039,\n",
      "      \"step\": 98940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.2356722013926085,\n",
      "      \"grad_norm\": 0.014724968932569027,\n",
      "      \"learning_rate\": 1.8834278512917454e-05,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 98960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.236932480544441,\n",
      "      \"grad_norm\": 0.00044322802568785846,\n",
      "      \"learning_rate\": 1.8827977315689983e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 98980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.238192759696273,\n",
      "      \"grad_norm\": 0.011363200843334198,\n",
      "      \"learning_rate\": 1.882167611846251e-05,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 99000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.239453038848104,\n",
      "      \"grad_norm\": 0.0039640506729483604,\n",
      "      \"learning_rate\": 1.8815374921235034e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 99020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.240713317999937,\n",
      "      \"grad_norm\": 21.817270278930664,\n",
      "      \"learning_rate\": 1.8809073724007563e-05,\n",
      "      \"loss\": 0.0045,\n",
      "      \"step\": 99040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.241973597151769,\n",
      "      \"grad_norm\": 0.006754690781235695,\n",
      "      \"learning_rate\": 1.880277252678009e-05,\n",
      "      \"loss\": 0.0175,\n",
      "      \"step\": 99060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.243233876303601,\n",
      "      \"grad_norm\": 0.3633774518966675,\n",
      "      \"learning_rate\": 1.8796471329552614e-05,\n",
      "      \"loss\": 0.0173,\n",
      "      \"step\": 99080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.244494155455433,\n",
      "      \"grad_norm\": 0.00032781405025161803,\n",
      "      \"learning_rate\": 1.8790170132325144e-05,\n",
      "      \"loss\": 0.0193,\n",
      "      \"step\": 99100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.2457544346072655,\n",
      "      \"grad_norm\": 0.00034116068854928017,\n",
      "      \"learning_rate\": 1.878386893509767e-05,\n",
      "      \"loss\": 0.028,\n",
      "      \"step\": 99120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.247014713759097,\n",
      "      \"grad_norm\": 0.00041522138053551316,\n",
      "      \"learning_rate\": 1.8777567737870198e-05,\n",
      "      \"loss\": 0.0021,\n",
      "      \"step\": 99140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.24827499291093,\n",
      "      \"grad_norm\": 0.0014575195964425802,\n",
      "      \"learning_rate\": 1.8771266540642724e-05,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 99160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.2495352720627615,\n",
      "      \"grad_norm\": 7.214636325836182,\n",
      "      \"learning_rate\": 1.876496534341525e-05,\n",
      "      \"loss\": 0.0121,\n",
      "      \"step\": 99180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.250795551214594,\n",
      "      \"grad_norm\": 0.0031918450258672237,\n",
      "      \"learning_rate\": 1.8758664146187775e-05,\n",
      "      \"loss\": 0.0045,\n",
      "      \"step\": 99200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.252055830366426,\n",
      "      \"grad_norm\": 0.0002957023971248418,\n",
      "      \"learning_rate\": 1.8752362948960304e-05,\n",
      "      \"loss\": 0.0121,\n",
      "      \"step\": 99220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.253316109518258,\n",
      "      \"grad_norm\": 0.004401827231049538,\n",
      "      \"learning_rate\": 1.874606175173283e-05,\n",
      "      \"loss\": 0.0132,\n",
      "      \"step\": 99240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.25457638867009,\n",
      "      \"grad_norm\": 0.0010873248102143407,\n",
      "      \"learning_rate\": 1.8739760554505356e-05,\n",
      "      \"loss\": 0.0218,\n",
      "      \"step\": 99260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.255836667821923,\n",
      "      \"grad_norm\": 0.0004720582510344684,\n",
      "      \"learning_rate\": 1.873345935727788e-05,\n",
      "      \"loss\": 0.0274,\n",
      "      \"step\": 99280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.257096946973754,\n",
      "      \"grad_norm\": 0.00026228197384625673,\n",
      "      \"learning_rate\": 1.872715816005041e-05,\n",
      "      \"loss\": 0.0049,\n",
      "      \"step\": 99300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.258357226125587,\n",
      "      \"grad_norm\": 0.014556320384144783,\n",
      "      \"learning_rate\": 1.872085696282294e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 99320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.259617505277419,\n",
      "      \"grad_norm\": 0.042096320539712906,\n",
      "      \"learning_rate\": 1.8714555765595465e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 99340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.260877784429251,\n",
      "      \"grad_norm\": 0.10672043263912201,\n",
      "      \"learning_rate\": 1.870825456836799e-05,\n",
      "      \"loss\": 0.0342,\n",
      "      \"step\": 99360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.262138063581083,\n",
      "      \"grad_norm\": 0.005158634856343269,\n",
      "      \"learning_rate\": 1.870195337114052e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 99380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.263398342732915,\n",
      "      \"grad_norm\": 0.07812982052564621,\n",
      "      \"learning_rate\": 1.8695652173913045e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 99400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.264658621884747,\n",
      "      \"grad_norm\": 0.0005911724292673171,\n",
      "      \"learning_rate\": 1.868935097668557e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 99420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.26591890103658,\n",
      "      \"grad_norm\": 0.007953961379826069,\n",
      "      \"learning_rate\": 1.8683049779458097e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 99440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.267179180188411,\n",
      "      \"grad_norm\": 0.0003404788440093398,\n",
      "      \"learning_rate\": 1.8676748582230626e-05,\n",
      "      \"loss\": 0.0308,\n",
      "      \"step\": 99460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.268439459340244,\n",
      "      \"grad_norm\": 0.00035106451832689345,\n",
      "      \"learning_rate\": 1.867044738500315e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 99480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.269699738492076,\n",
      "      \"grad_norm\": 0.0026272512041032314,\n",
      "      \"learning_rate\": 1.8664146187775677e-05,\n",
      "      \"loss\": 0.0505,\n",
      "      \"step\": 99500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.270960017643908,\n",
      "      \"grad_norm\": 0.0020299293100833893,\n",
      "      \"learning_rate\": 1.8657844990548203e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 99520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.27222029679574,\n",
      "      \"grad_norm\": 0.0008345997193828225,\n",
      "      \"learning_rate\": 1.865154379332073e-05,\n",
      "      \"loss\": 0.0304,\n",
      "      \"step\": 99540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.2734805759475725,\n",
      "      \"grad_norm\": 0.7662627100944519,\n",
      "      \"learning_rate\": 1.864524259609326e-05,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 99560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.274740855099404,\n",
      "      \"grad_norm\": 0.1373540312051773,\n",
      "      \"learning_rate\": 1.8638941398865786e-05,\n",
      "      \"loss\": 0.0044,\n",
      "      \"step\": 99580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.276001134251237,\n",
      "      \"grad_norm\": 0.0009911895031109452,\n",
      "      \"learning_rate\": 1.8632640201638312e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 99600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.277261413403068,\n",
      "      \"grad_norm\": 0.025173548609018326,\n",
      "      \"learning_rate\": 1.862633900441084e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 99620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.278521692554901,\n",
      "      \"grad_norm\": 0.0013203086564317346,\n",
      "      \"learning_rate\": 1.8620037807183367e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 99640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.279781971706733,\n",
      "      \"grad_norm\": 0.000669827510137111,\n",
      "      \"learning_rate\": 1.8613736609955892e-05,\n",
      "      \"loss\": 0.0281,\n",
      "      \"step\": 99660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.281042250858565,\n",
      "      \"grad_norm\": 0.0005184055771678686,\n",
      "      \"learning_rate\": 1.8607435412728418e-05,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 99680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.282302530010397,\n",
      "      \"grad_norm\": 0.004187091253697872,\n",
      "      \"learning_rate\": 1.8601134215500947e-05,\n",
      "      \"loss\": 0.0182,\n",
      "      \"step\": 99700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.2835628091622295,\n",
      "      \"grad_norm\": 0.04909689351916313,\n",
      "      \"learning_rate\": 1.8594833018273473e-05,\n",
      "      \"loss\": 0.003,\n",
      "      \"step\": 99720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.284823088314061,\n",
      "      \"grad_norm\": 0.0009401628049090505,\n",
      "      \"learning_rate\": 1.8588531821045998e-05,\n",
      "      \"loss\": 0.0253,\n",
      "      \"step\": 99740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.286083367465894,\n",
      "      \"grad_norm\": 0.012269851751625538,\n",
      "      \"learning_rate\": 1.8582230623818524e-05,\n",
      "      \"loss\": 0.0194,\n",
      "      \"step\": 99760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.2873436466177255,\n",
      "      \"grad_norm\": 0.00454463716596365,\n",
      "      \"learning_rate\": 1.8575929426591053e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 99780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.288603925769558,\n",
      "      \"grad_norm\": 0.0005651118117384613,\n",
      "      \"learning_rate\": 1.8569628229363582e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 99800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.28986420492139,\n",
      "      \"grad_norm\": 0.08443145453929901,\n",
      "      \"learning_rate\": 1.8563327032136108e-05,\n",
      "      \"loss\": 0.0424,\n",
      "      \"step\": 99820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.291124484073222,\n",
      "      \"grad_norm\": 0.001130671240389347,\n",
      "      \"learning_rate\": 1.8557025834908633e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 99840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.292384763225054,\n",
      "      \"grad_norm\": 0.008925117552280426,\n",
      "      \"learning_rate\": 1.855072463768116e-05,\n",
      "      \"loss\": 0.0058,\n",
      "      \"step\": 99860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.293645042376887,\n",
      "      \"grad_norm\": 0.04055025428533554,\n",
      "      \"learning_rate\": 1.8544423440453688e-05,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 99880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.294905321528718,\n",
      "      \"grad_norm\": 0.006222055293619633,\n",
      "      \"learning_rate\": 1.8538122243226214e-05,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 99900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.296165600680551,\n",
      "      \"grad_norm\": 0.25746867060661316,\n",
      "      \"learning_rate\": 1.853182104599874e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 99920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.2974258798323826,\n",
      "      \"grad_norm\": 0.0670011043548584,\n",
      "      \"learning_rate\": 1.8525519848771268e-05,\n",
      "      \"loss\": 0.0065,\n",
      "      \"step\": 99940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.298686158984215,\n",
      "      \"grad_norm\": 0.0011929350439459085,\n",
      "      \"learning_rate\": 1.8519218651543794e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 99960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.299946438136047,\n",
      "      \"grad_norm\": 0.002156828762963414,\n",
      "      \"learning_rate\": 1.851291745431632e-05,\n",
      "      \"loss\": 0.0211,\n",
      "      \"step\": 99980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.301206717287879,\n",
      "      \"grad_norm\": 0.008029406890273094,\n",
      "      \"learning_rate\": 1.850661625708885e-05,\n",
      "      \"loss\": 0.0047,\n",
      "      \"step\": 100000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.302466996439711,\n",
      "      \"grad_norm\": 0.0015965240309014916,\n",
      "      \"learning_rate\": 1.8500315059861374e-05,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 100020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.303727275591544,\n",
      "      \"grad_norm\": 0.02791311778128147,\n",
      "      \"learning_rate\": 1.8494013862633903e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 100040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.304987554743375,\n",
      "      \"grad_norm\": 0.005010474473237991,\n",
      "      \"learning_rate\": 1.848771266540643e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 100060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.306247833895208,\n",
      "      \"grad_norm\": 0.0017732811393216252,\n",
      "      \"learning_rate\": 1.8481411468178955e-05,\n",
      "      \"loss\": 0.0221,\n",
      "      \"step\": 100080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.30750811304704,\n",
      "      \"grad_norm\": 8.869351387023926,\n",
      "      \"learning_rate\": 1.847511027095148e-05,\n",
      "      \"loss\": 0.0265,\n",
      "      \"step\": 100100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.308768392198872,\n",
      "      \"grad_norm\": 0.006530539132654667,\n",
      "      \"learning_rate\": 1.846880907372401e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 100120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.310028671350704,\n",
      "      \"grad_norm\": 0.0009153690771199763,\n",
      "      \"learning_rate\": 1.8462507876496535e-05,\n",
      "      \"loss\": 0.0455,\n",
      "      \"step\": 100140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.3112889505025365,\n",
      "      \"grad_norm\": 0.0013952727895230055,\n",
      "      \"learning_rate\": 1.845620667926906e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 100160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.312549229654368,\n",
      "      \"grad_norm\": 0.003192928619682789,\n",
      "      \"learning_rate\": 1.8449905482041586e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 100180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.313809508806201,\n",
      "      \"grad_norm\": 0.0036064544692635536,\n",
      "      \"learning_rate\": 1.8443604284814115e-05,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 100200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.315069787958032,\n",
      "      \"grad_norm\": 0.0007338512223213911,\n",
      "      \"learning_rate\": 1.8437303087586644e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 100220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.316330067109865,\n",
      "      \"grad_norm\": 0.01562703587114811,\n",
      "      \"learning_rate\": 1.843100189035917e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 100240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.317590346261697,\n",
      "      \"grad_norm\": 0.0031224465928971767,\n",
      "      \"learning_rate\": 1.8424700693131696e-05,\n",
      "      \"loss\": 0.0171,\n",
      "      \"step\": 100260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.318850625413529,\n",
      "      \"grad_norm\": 0.04135574400424957,\n",
      "      \"learning_rate\": 1.8418399495904225e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 100280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.320110904565361,\n",
      "      \"grad_norm\": 0.0008539826376363635,\n",
      "      \"learning_rate\": 1.8412413358538122e-05,\n",
      "      \"loss\": 0.0037,\n",
      "      \"step\": 100300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.3213711837171935,\n",
      "      \"grad_norm\": 0.011994494125247002,\n",
      "      \"learning_rate\": 1.8406112161310648e-05,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 100320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.322631462869025,\n",
      "      \"grad_norm\": 0.03256167843937874,\n",
      "      \"learning_rate\": 1.8399810964083177e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 100340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.323891742020858,\n",
      "      \"grad_norm\": 0.0051827216520905495,\n",
      "      \"learning_rate\": 1.8393509766855706e-05,\n",
      "      \"loss\": 0.043,\n",
      "      \"step\": 100360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.3251520211726895,\n",
      "      \"grad_norm\": 0.027403298765420914,\n",
      "      \"learning_rate\": 1.838720856962823e-05,\n",
      "      \"loss\": 0.052,\n",
      "      \"step\": 100380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.326412300324522,\n",
      "      \"grad_norm\": 0.004749712068587542,\n",
      "      \"learning_rate\": 1.8380907372400757e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 100400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.327672579476354,\n",
      "      \"grad_norm\": 2.3906264305114746,\n",
      "      \"learning_rate\": 1.8374606175173283e-05,\n",
      "      \"loss\": 0.0021,\n",
      "      \"step\": 100420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.328932858628186,\n",
      "      \"grad_norm\": 0.029915938153862953,\n",
      "      \"learning_rate\": 1.8368304977945812e-05,\n",
      "      \"loss\": 0.0196,\n",
      "      \"step\": 100440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.330193137780018,\n",
      "      \"grad_norm\": 0.003282796125859022,\n",
      "      \"learning_rate\": 1.8362003780718338e-05,\n",
      "      \"loss\": 0.0026,\n",
      "      \"step\": 100460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.331453416931851,\n",
      "      \"grad_norm\": 0.0325777605175972,\n",
      "      \"learning_rate\": 1.8355702583490863e-05,\n",
      "      \"loss\": 0.0125,\n",
      "      \"step\": 100480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.332713696083682,\n",
      "      \"grad_norm\": 0.010068291798233986,\n",
      "      \"learning_rate\": 1.834940138626339e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 100500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.333973975235515,\n",
      "      \"grad_norm\": 4.942373752593994,\n",
      "      \"learning_rate\": 1.8343100189035918e-05,\n",
      "      \"loss\": 0.0142,\n",
      "      \"step\": 100520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.3352342543873466,\n",
      "      \"grad_norm\": 0.009148292243480682,\n",
      "      \"learning_rate\": 1.8336798991808444e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 100540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.336494533539179,\n",
      "      \"grad_norm\": 0.007038512732833624,\n",
      "      \"learning_rate\": 1.8330497794580973e-05,\n",
      "      \"loss\": 0.023,\n",
      "      \"step\": 100560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.337754812691011,\n",
      "      \"grad_norm\": 0.012364744208753109,\n",
      "      \"learning_rate\": 1.8324196597353498e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 100580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.339015091842843,\n",
      "      \"grad_norm\": 0.629047691822052,\n",
      "      \"learning_rate\": 1.8317895400126024e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 100600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.340275370994675,\n",
      "      \"grad_norm\": 0.0026681534945964813,\n",
      "      \"learning_rate\": 1.8311594202898553e-05,\n",
      "      \"loss\": 0.0043,\n",
      "      \"step\": 100620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.341535650146508,\n",
      "      \"grad_norm\": 0.08225005865097046,\n",
      "      \"learning_rate\": 1.830529300567108e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 100640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.342795929298339,\n",
      "      \"grad_norm\": 0.025649087503552437,\n",
      "      \"learning_rate\": 1.8298991808443604e-05,\n",
      "      \"loss\": 0.0062,\n",
      "      \"step\": 100660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.344056208450172,\n",
      "      \"grad_norm\": 0.0020459170918911695,\n",
      "      \"learning_rate\": 1.8292690611216133e-05,\n",
      "      \"loss\": 0.0191,\n",
      "      \"step\": 100680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.345316487602004,\n",
      "      \"grad_norm\": 0.14966867864131927,\n",
      "      \"learning_rate\": 1.828638941398866e-05,\n",
      "      \"loss\": 0.0455,\n",
      "      \"step\": 100700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.346576766753836,\n",
      "      \"grad_norm\": 0.00043946155346930027,\n",
      "      \"learning_rate\": 1.8280088216761185e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 100720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.347837045905668,\n",
      "      \"grad_norm\": 0.025365246459841728,\n",
      "      \"learning_rate\": 1.827378701953371e-05,\n",
      "      \"loss\": 0.0342,\n",
      "      \"step\": 100740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.3490973250575005,\n",
      "      \"grad_norm\": 0.03583134338259697,\n",
      "      \"learning_rate\": 1.826748582230624e-05,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 100760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.350357604209332,\n",
      "      \"grad_norm\": 0.06785817444324493,\n",
      "      \"learning_rate\": 1.826118462507877e-05,\n",
      "      \"loss\": 0.0147,\n",
      "      \"step\": 100780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.351617883361165,\n",
      "      \"grad_norm\": 0.003004316473379731,\n",
      "      \"learning_rate\": 1.8254883427851294e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 100800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.352878162512996,\n",
      "      \"grad_norm\": 0.05349787697196007,\n",
      "      \"learning_rate\": 1.824858223062382e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 100820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.354138441664829,\n",
      "      \"grad_norm\": 0.000410734792239964,\n",
      "      \"learning_rate\": 1.8242281033396345e-05,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 100840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.355398720816661,\n",
      "      \"grad_norm\": 0.016307028010487556,\n",
      "      \"learning_rate\": 1.8235979836168874e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 100860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.356658999968493,\n",
      "      \"grad_norm\": 0.0004993955371901393,\n",
      "      \"learning_rate\": 1.82296786389414e-05,\n",
      "      \"loss\": 0.0297,\n",
      "      \"step\": 100880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.357919279120325,\n",
      "      \"grad_norm\": 0.001257493393495679,\n",
      "      \"learning_rate\": 1.8223377441713926e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 100900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.3591795582721575,\n",
      "      \"grad_norm\": 0.9096136689186096,\n",
      "      \"learning_rate\": 1.821707624448645e-05,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 100920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.360439837423989,\n",
      "      \"grad_norm\": 9.585600852966309,\n",
      "      \"learning_rate\": 1.821077504725898e-05,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 100940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.361700116575822,\n",
      "      \"grad_norm\": 0.0005201363819651306,\n",
      "      \"learning_rate\": 1.8204473850031506e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 100960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.3629603957276535,\n",
      "      \"grad_norm\": 0.006649205926805735,\n",
      "      \"learning_rate\": 1.819817265280403e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 100980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.364220674879486,\n",
      "      \"grad_norm\": 0.0014485535211861134,\n",
      "      \"learning_rate\": 1.819187145557656e-05,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 101000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.365480954031318,\n",
      "      \"grad_norm\": 0.0002456723595969379,\n",
      "      \"learning_rate\": 1.818557025834909e-05,\n",
      "      \"loss\": 0.0146,\n",
      "      \"step\": 101020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.36674123318315,\n",
      "      \"grad_norm\": 7.801548004150391,\n",
      "      \"learning_rate\": 1.8179269061121615e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 101040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.368001512334982,\n",
      "      \"grad_norm\": 0.0005749575793743134,\n",
      "      \"learning_rate\": 1.817296786389414e-05,\n",
      "      \"loss\": 0.0178,\n",
      "      \"step\": 101060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.369261791486815,\n",
      "      \"grad_norm\": 0.013464830815792084,\n",
      "      \"learning_rate\": 1.8166666666666667e-05,\n",
      "      \"loss\": 0.025,\n",
      "      \"step\": 101080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.370522070638646,\n",
      "      \"grad_norm\": 0.01630953885614872,\n",
      "      \"learning_rate\": 1.8160365469439196e-05,\n",
      "      \"loss\": 0.0194,\n",
      "      \"step\": 101100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.371782349790479,\n",
      "      \"grad_norm\": 0.0008974723168648779,\n",
      "      \"learning_rate\": 1.815406427221172e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 101120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.3730426289423106,\n",
      "      \"grad_norm\": 0.06450464576482773,\n",
      "      \"learning_rate\": 1.8147763074984247e-05,\n",
      "      \"loss\": 0.0534,\n",
      "      \"step\": 101140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.374302908094143,\n",
      "      \"grad_norm\": 0.017895130440592766,\n",
      "      \"learning_rate\": 1.8141461877756773e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 101160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.375563187245975,\n",
      "      \"grad_norm\": 0.04166996479034424,\n",
      "      \"learning_rate\": 1.81351606805293e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 101180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.376823466397807,\n",
      "      \"grad_norm\": 0.039448484778404236,\n",
      "      \"learning_rate\": 1.8128859483301827e-05,\n",
      "      \"loss\": 0.0228,\n",
      "      \"step\": 101200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.378083745549639,\n",
      "      \"grad_norm\": 0.00094661267939955,\n",
      "      \"learning_rate\": 1.8122558286074353e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 101220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.379344024701472,\n",
      "      \"grad_norm\": 4.918743133544922,\n",
      "      \"learning_rate\": 1.8116257088846882e-05,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 101240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.380604303853303,\n",
      "      \"grad_norm\": 0.0008035484934225678,\n",
      "      \"learning_rate\": 1.810995589161941e-05,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 101260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.381864583005136,\n",
      "      \"grad_norm\": 0.08889076113700867,\n",
      "      \"learning_rate\": 1.8103654694391937e-05,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 101280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.383124862156968,\n",
      "      \"grad_norm\": 0.0009560875478200614,\n",
      "      \"learning_rate\": 1.8097353497164462e-05,\n",
      "      \"loss\": 0.0039,\n",
      "      \"step\": 101300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.3843851413088,\n",
      "      \"grad_norm\": 0.0011570745846256614,\n",
      "      \"learning_rate\": 1.8091052299936988e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 101320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.385645420460632,\n",
      "      \"grad_norm\": 0.016708979383111,\n",
      "      \"learning_rate\": 1.8084751102709517e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 101340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.3869056996124645,\n",
      "      \"grad_norm\": 0.31248605251312256,\n",
      "      \"learning_rate\": 1.8078449905482043e-05,\n",
      "      \"loss\": 0.0298,\n",
      "      \"step\": 101360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.388165978764296,\n",
      "      \"grad_norm\": 0.0053339190781116486,\n",
      "      \"learning_rate\": 1.807214870825457e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 101380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.389426257916129,\n",
      "      \"grad_norm\": 0.017651071771979332,\n",
      "      \"learning_rate\": 1.8065847511027094e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 101400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.39068653706796,\n",
      "      \"grad_norm\": 0.03680659085512161,\n",
      "      \"learning_rate\": 1.8059546313799623e-05,\n",
      "      \"loss\": 0.0613,\n",
      "      \"step\": 101420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.391946816219793,\n",
      "      \"grad_norm\": 0.13030754029750824,\n",
      "      \"learning_rate\": 1.805324511657215e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 101440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.393207095371625,\n",
      "      \"grad_norm\": 0.0184840876609087,\n",
      "      \"learning_rate\": 1.8046943919344678e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 101460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.394467374523457,\n",
      "      \"grad_norm\": 0.010893265716731548,\n",
      "      \"learning_rate\": 1.8040642722117203e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 101480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.395727653675289,\n",
      "      \"grad_norm\": 0.0033618584275245667,\n",
      "      \"learning_rate\": 1.803434152488973e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 101500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.3969879328271215,\n",
      "      \"grad_norm\": 0.0038303339388221502,\n",
      "      \"learning_rate\": 1.8028040327662258e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 101520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.398248211978953,\n",
      "      \"grad_norm\": 0.23278461396694183,\n",
      "      \"learning_rate\": 1.8021739130434784e-05,\n",
      "      \"loss\": 0.0325,\n",
      "      \"step\": 101540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.399508491130786,\n",
      "      \"grad_norm\": 0.0017624256433919072,\n",
      "      \"learning_rate\": 1.801543793320731e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 101560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.4007687702826175,\n",
      "      \"grad_norm\": 0.004921425599604845,\n",
      "      \"learning_rate\": 1.800913673597984e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 101580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.40202904943445,\n",
      "      \"grad_norm\": 0.009282504208385944,\n",
      "      \"learning_rate\": 1.8002835538752364e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 101600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.403289328586282,\n",
      "      \"grad_norm\": 0.02625342644751072,\n",
      "      \"learning_rate\": 1.799653434152489e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 101620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.404549607738114,\n",
      "      \"grad_norm\": 0.0011262306943535805,\n",
      "      \"learning_rate\": 1.7990233144297415e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 101640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.405809886889946,\n",
      "      \"grad_norm\": 0.0022988326381891966,\n",
      "      \"learning_rate\": 1.7983931947069944e-05,\n",
      "      \"loss\": 0.0328,\n",
      "      \"step\": 101660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.407070166041779,\n",
      "      \"grad_norm\": 0.007749207783490419,\n",
      "      \"learning_rate\": 1.7977630749842473e-05,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 101680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.40833044519361,\n",
      "      \"grad_norm\": 0.012429668568074703,\n",
      "      \"learning_rate\": 1.7971329552615e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 101700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.409590724345443,\n",
      "      \"grad_norm\": 0.0005018443334847689,\n",
      "      \"learning_rate\": 1.7965028355387525e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 101720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.4108510034972745,\n",
      "      \"grad_norm\": 0.002804060000926256,\n",
      "      \"learning_rate\": 1.795872715816005e-05,\n",
      "      \"loss\": 0.0176,\n",
      "      \"step\": 101740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.412111282649107,\n",
      "      \"grad_norm\": 0.013904249295592308,\n",
      "      \"learning_rate\": 1.795242596093258e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 101760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.413371561800939,\n",
      "      \"grad_norm\": 0.0009531814721412957,\n",
      "      \"learning_rate\": 1.7946124763705105e-05,\n",
      "      \"loss\": 0.0142,\n",
      "      \"step\": 101780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.414631840952771,\n",
      "      \"grad_norm\": 0.0031563928350806236,\n",
      "      \"learning_rate\": 1.793982356647763e-05,\n",
      "      \"loss\": 0.0288,\n",
      "      \"step\": 101800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.415892120104603,\n",
      "      \"grad_norm\": 0.004009258933365345,\n",
      "      \"learning_rate\": 1.7933522369250156e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 101820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.417152399256436,\n",
      "      \"grad_norm\": 0.0013579358346760273,\n",
      "      \"learning_rate\": 1.7927221172022685e-05,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 101840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.418412678408267,\n",
      "      \"grad_norm\": 0.014471876434981823,\n",
      "      \"learning_rate\": 1.792091997479521e-05,\n",
      "      \"loss\": 0.016,\n",
      "      \"step\": 101860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.4196729575601,\n",
      "      \"grad_norm\": 0.03265970200300217,\n",
      "      \"learning_rate\": 1.7914618777567737e-05,\n",
      "      \"loss\": 0.0033,\n",
      "      \"step\": 101880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.420933236711932,\n",
      "      \"grad_norm\": 0.00031663168920204043,\n",
      "      \"learning_rate\": 1.7908317580340266e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 101900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.422193515863764,\n",
      "      \"grad_norm\": 0.0012905565090477467,\n",
      "      \"learning_rate\": 1.7902016383112795e-05,\n",
      "      \"loss\": 0.0319,\n",
      "      \"step\": 101920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.423453795015596,\n",
      "      \"grad_norm\": 0.0006301276152953506,\n",
      "      \"learning_rate\": 1.789571518588532e-05,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 101940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.4247140741674285,\n",
      "      \"grad_norm\": 0.058179281651973724,\n",
      "      \"learning_rate\": 1.7889413988657846e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 101960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.42597435331926,\n",
      "      \"grad_norm\": 0.0035565849393606186,\n",
      "      \"learning_rate\": 1.7883112791430372e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 101980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.427234632471093,\n",
      "      \"grad_norm\": 0.000403923710109666,\n",
      "      \"learning_rate\": 1.78768115942029e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 102000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.428494911622924,\n",
      "      \"grad_norm\": 0.0006018898566253483,\n",
      "      \"learning_rate\": 1.7870510396975426e-05,\n",
      "      \"loss\": 0.0165,\n",
      "      \"step\": 102020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.429755190774757,\n",
      "      \"grad_norm\": 0.002471313579007983,\n",
      "      \"learning_rate\": 1.7864209199747952e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 102040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.431015469926589,\n",
      "      \"grad_norm\": 0.009365169331431389,\n",
      "      \"learning_rate\": 1.7857908002520478e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 102060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.432275749078421,\n",
      "      \"grad_norm\": 0.013556285761296749,\n",
      "      \"learning_rate\": 1.7851606805293007e-05,\n",
      "      \"loss\": 0.0044,\n",
      "      \"step\": 102080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.433536028230253,\n",
      "      \"grad_norm\": 0.0007810837705619633,\n",
      "      \"learning_rate\": 1.7845305608065532e-05,\n",
      "      \"loss\": 0.019,\n",
      "      \"step\": 102100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.4347963073820855,\n",
      "      \"grad_norm\": 0.18477073311805725,\n",
      "      \"learning_rate\": 1.7839004410838058e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 102120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.436056586533917,\n",
      "      \"grad_norm\": 0.010669738985598087,\n",
      "      \"learning_rate\": 1.7832703213610587e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 102140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.43731686568575,\n",
      "      \"grad_norm\": 0.005148765631020069,\n",
      "      \"learning_rate\": 1.7826402016383116e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 102160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.4385771448375815,\n",
      "      \"grad_norm\": 0.00453925272449851,\n",
      "      \"learning_rate\": 1.7820100819155642e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 102180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.439837423989414,\n",
      "      \"grad_norm\": 0.15222927927970886,\n",
      "      \"learning_rate\": 1.7813799621928168e-05,\n",
      "      \"loss\": 0.0143,\n",
      "      \"step\": 102200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.441097703141246,\n",
      "      \"grad_norm\": 9.902254104614258,\n",
      "      \"learning_rate\": 1.7807498424700693e-05,\n",
      "      \"loss\": 0.0249,\n",
      "      \"step\": 102220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.442357982293078,\n",
      "      \"grad_norm\": 0.0022057397291064262,\n",
      "      \"learning_rate\": 1.7801197227473222e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 102240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.44361826144491,\n",
      "      \"grad_norm\": 0.0011484803399071097,\n",
      "      \"learning_rate\": 1.7794896030245748e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 102260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.444878540596743,\n",
      "      \"grad_norm\": 0.00021658578771166503,\n",
      "      \"learning_rate\": 1.7788594833018273e-05,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 102280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.446138819748574,\n",
      "      \"grad_norm\": 0.022975973784923553,\n",
      "      \"learning_rate\": 1.77822936357908e-05,\n",
      "      \"loss\": 0.0376,\n",
      "      \"step\": 102300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.447399098900407,\n",
      "      \"grad_norm\": 0.002366764936596155,\n",
      "      \"learning_rate\": 1.7775992438563328e-05,\n",
      "      \"loss\": 0.0048,\n",
      "      \"step\": 102320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.4486593780522385,\n",
      "      \"grad_norm\": 0.0007644788129255176,\n",
      "      \"learning_rate\": 1.7769691241335854e-05,\n",
      "      \"loss\": 0.0459,\n",
      "      \"step\": 102340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.44991965720407,\n",
      "      \"grad_norm\": 0.017668254673480988,\n",
      "      \"learning_rate\": 1.7763390044108383e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 102360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.451179936355903,\n",
      "      \"grad_norm\": 0.000364381616236642,\n",
      "      \"learning_rate\": 1.775708884688091e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 102380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.452440215507735,\n",
      "      \"grad_norm\": 0.024900516495108604,\n",
      "      \"learning_rate\": 1.7750787649653438e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 102400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.453700494659567,\n",
      "      \"grad_norm\": 0.003813581308349967,\n",
      "      \"learning_rate\": 1.7744486452425963e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 102420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.454960773811399,\n",
      "      \"grad_norm\": 0.00048431524191983044,\n",
      "      \"learning_rate\": 1.773818525519849e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 102440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.456221052963231,\n",
      "      \"grad_norm\": 0.02883129194378853,\n",
      "      \"learning_rate\": 1.7731884057971015e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 102460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.457481332115064,\n",
      "      \"grad_norm\": 0.02142641507089138,\n",
      "      \"learning_rate\": 1.7725582860743544e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 102480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.458741611266896,\n",
      "      \"grad_norm\": 0.0002560539869591594,\n",
      "      \"learning_rate\": 1.771928166351607e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 102500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.460001890418727,\n",
      "      \"grad_norm\": 0.001716138212941587,\n",
      "      \"learning_rate\": 1.7712980466288595e-05,\n",
      "      \"loss\": 0.0239,\n",
      "      \"step\": 102520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.46126216957056,\n",
      "      \"grad_norm\": 14.88001537322998,\n",
      "      \"learning_rate\": 1.770667926906112e-05,\n",
      "      \"loss\": 0.0387,\n",
      "      \"step\": 102540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.4625224487223925,\n",
      "      \"grad_norm\": 5.383803844451904,\n",
      "      \"learning_rate\": 1.770069313169502e-05,\n",
      "      \"loss\": 0.0018,\n",
      "      \"step\": 102560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.463782727874224,\n",
      "      \"grad_norm\": 0.004050132818520069,\n",
      "      \"learning_rate\": 1.7694706994328926e-05,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 102580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.465043007026056,\n",
      "      \"grad_norm\": 0.0007124511757865548,\n",
      "      \"learning_rate\": 1.768840579710145e-05,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 102600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.466303286177888,\n",
      "      \"grad_norm\": 0.00961103942245245,\n",
      "      \"learning_rate\": 1.7682104599873977e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 102620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.467563565329721,\n",
      "      \"grad_norm\": 0.190553218126297,\n",
      "      \"learning_rate\": 1.7675803402646503e-05,\n",
      "      \"loss\": 0.034,\n",
      "      \"step\": 102640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.468823844481553,\n",
      "      \"grad_norm\": 0.039578866213560104,\n",
      "      \"learning_rate\": 1.766950220541903e-05,\n",
      "      \"loss\": 0.0121,\n",
      "      \"step\": 102660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.470084123633384,\n",
      "      \"grad_norm\": 0.00029330106917768717,\n",
      "      \"learning_rate\": 1.7663201008191557e-05,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 102680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.471344402785217,\n",
      "      \"grad_norm\": 0.0034129521809518337,\n",
      "      \"learning_rate\": 1.7656899810964083e-05,\n",
      "      \"loss\": 0.0241,\n",
      "      \"step\": 102700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.4726046819370495,\n",
      "      \"grad_norm\": 0.02076614648103714,\n",
      "      \"learning_rate\": 1.765059861373661e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 102720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.473864961088881,\n",
      "      \"grad_norm\": 0.0018052480882033706,\n",
      "      \"learning_rate\": 1.7644297416509138e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 102740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.475125240240713,\n",
      "      \"grad_norm\": 0.02329915575683117,\n",
      "      \"learning_rate\": 1.7637996219281663e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 102760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.4763855193925455,\n",
      "      \"grad_norm\": 0.00044277659617364407,\n",
      "      \"learning_rate\": 1.763169502205419e-05,\n",
      "      \"loss\": 0.0121,\n",
      "      \"step\": 102780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.477645798544377,\n",
      "      \"grad_norm\": 0.00039372697938233614,\n",
      "      \"learning_rate\": 1.7625393824826718e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 102800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.47890607769621,\n",
      "      \"grad_norm\": 0.000613716256339103,\n",
      "      \"learning_rate\": 1.7619092627599247e-05,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 102820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.480166356848041,\n",
      "      \"grad_norm\": 11.74881362915039,\n",
      "      \"learning_rate\": 1.7612791430371773e-05,\n",
      "      \"loss\": 0.0487,\n",
      "      \"step\": 102840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.481426635999874,\n",
      "      \"grad_norm\": 0.14067040383815765,\n",
      "      \"learning_rate\": 1.76064902331443e-05,\n",
      "      \"loss\": 0.0057,\n",
      "      \"step\": 102860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.482686915151706,\n",
      "      \"grad_norm\": 0.022006236016750336,\n",
      "      \"learning_rate\": 1.7600189035916824e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 102880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.483947194303538,\n",
      "      \"grad_norm\": 0.0027320976369082928,\n",
      "      \"learning_rate\": 1.7593887838689353e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 102900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.48520747345537,\n",
      "      \"grad_norm\": 0.12177892029285431,\n",
      "      \"learning_rate\": 1.758758664146188e-05,\n",
      "      \"loss\": 0.0435,\n",
      "      \"step\": 102920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.4864677526072025,\n",
      "      \"grad_norm\": 0.2638971209526062,\n",
      "      \"learning_rate\": 1.7581285444234404e-05,\n",
      "      \"loss\": 0.0033,\n",
      "      \"step\": 102940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.487728031759034,\n",
      "      \"grad_norm\": 0.019416838884353638,\n",
      "      \"learning_rate\": 1.757498424700693e-05,\n",
      "      \"loss\": 0.0218,\n",
      "      \"step\": 102960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.488988310910867,\n",
      "      \"grad_norm\": 0.009712434373795986,\n",
      "      \"learning_rate\": 1.756868304977946e-05,\n",
      "      \"loss\": 0.0044,\n",
      "      \"step\": 102980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.4902485900626985,\n",
      "      \"grad_norm\": 0.0028524186927825212,\n",
      "      \"learning_rate\": 1.7562381852551985e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 103000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.491508869214531,\n",
      "      \"grad_norm\": 0.024756280705332756,\n",
      "      \"learning_rate\": 1.7556080655324514e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 103020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.492769148366363,\n",
      "      \"grad_norm\": 0.003157431725412607,\n",
      "      \"learning_rate\": 1.754977945809704e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 103040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.494029427518195,\n",
      "      \"grad_norm\": 0.005807386711239815,\n",
      "      \"learning_rate\": 1.754347826086957e-05,\n",
      "      \"loss\": 0.0027,\n",
      "      \"step\": 103060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.495289706670027,\n",
      "      \"grad_norm\": 0.05521770939230919,\n",
      "      \"learning_rate\": 1.7537177063642094e-05,\n",
      "      \"loss\": 0.0375,\n",
      "      \"step\": 103080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.49654998582186,\n",
      "      \"grad_norm\": 0.0010105030378326774,\n",
      "      \"learning_rate\": 1.753087586641462e-05,\n",
      "      \"loss\": 0.0235,\n",
      "      \"step\": 103100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.497810264973691,\n",
      "      \"grad_norm\": 0.0024893207009881735,\n",
      "      \"learning_rate\": 1.7524574669187145e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 103120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.499070544125524,\n",
      "      \"grad_norm\": 0.006457365117967129,\n",
      "      \"learning_rate\": 1.7518273471959674e-05,\n",
      "      \"loss\": 0.0209,\n",
      "      \"step\": 103140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.500330823277356,\n",
      "      \"grad_norm\": 0.013998370617628098,\n",
      "      \"learning_rate\": 1.75119722747322e-05,\n",
      "      \"loss\": 0.042,\n",
      "      \"step\": 103160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.501591102429188,\n",
      "      \"grad_norm\": 0.0206745732575655,\n",
      "      \"learning_rate\": 1.7505671077504726e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 103180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.50285138158102,\n",
      "      \"grad_norm\": 0.0017401615623384714,\n",
      "      \"learning_rate\": 1.749936988027725e-05,\n",
      "      \"loss\": 0.0255,\n",
      "      \"step\": 103200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.504111660732852,\n",
      "      \"grad_norm\": 0.005693976767361164,\n",
      "      \"learning_rate\": 1.749306868304978e-05,\n",
      "      \"loss\": 0.0237,\n",
      "      \"step\": 103220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.505371939884684,\n",
      "      \"grad_norm\": 0.10064733028411865,\n",
      "      \"learning_rate\": 1.7486767485822306e-05,\n",
      "      \"loss\": 0.0228,\n",
      "      \"step\": 103240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.506632219036517,\n",
      "      \"grad_norm\": 0.01543329656124115,\n",
      "      \"learning_rate\": 1.7480466288594835e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 103260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.507892498188348,\n",
      "      \"grad_norm\": 0.0290596392005682,\n",
      "      \"learning_rate\": 1.747416509136736e-05,\n",
      "      \"loss\": 0.0189,\n",
      "      \"step\": 103280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.509152777340181,\n",
      "      \"grad_norm\": 0.02092207968235016,\n",
      "      \"learning_rate\": 1.746786389413989e-05,\n",
      "      \"loss\": 0.0227,\n",
      "      \"step\": 103300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.510413056492013,\n",
      "      \"grad_norm\": 2.5730905532836914,\n",
      "      \"learning_rate\": 1.7461562696912415e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 103320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.511673335643845,\n",
      "      \"grad_norm\": 0.004143301863223314,\n",
      "      \"learning_rate\": 1.745526149968494e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 103340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.512933614795677,\n",
      "      \"grad_norm\": 0.023703020066022873,\n",
      "      \"learning_rate\": 1.7448960302457467e-05,\n",
      "      \"loss\": 0.023,\n",
      "      \"step\": 103360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.5141938939475095,\n",
      "      \"grad_norm\": 0.2333793193101883,\n",
      "      \"learning_rate\": 1.7442659105229996e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 103380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.515454173099341,\n",
      "      \"grad_norm\": 0.0033033087383955717,\n",
      "      \"learning_rate\": 1.743635790800252e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 103400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.516714452251174,\n",
      "      \"grad_norm\": 0.002325445180758834,\n",
      "      \"learning_rate\": 1.7430056710775047e-05,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 103420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.517974731403005,\n",
      "      \"grad_norm\": 0.11418270319700241,\n",
      "      \"learning_rate\": 1.7423755513547573e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 103440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.519235010554838,\n",
      "      \"grad_norm\": 0.0015584854409098625,\n",
      "      \"learning_rate\": 1.7417454316320102e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 103460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.52049528970667,\n",
      "      \"grad_norm\": 0.005266626365482807,\n",
      "      \"learning_rate\": 1.741115311909263e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 103480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.521755568858502,\n",
      "      \"grad_norm\": 0.001542927697300911,\n",
      "      \"learning_rate\": 1.7404851921865156e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 103500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.523015848010334,\n",
      "      \"grad_norm\": 0.062037982046604156,\n",
      "      \"learning_rate\": 1.7398550724637682e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 103520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.5242761271621665,\n",
      "      \"grad_norm\": 0.0010054621379822493,\n",
      "      \"learning_rate\": 1.7392249527410208e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 103540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.525536406313998,\n",
      "      \"grad_norm\": 0.014308031648397446,\n",
      "      \"learning_rate\": 1.7385948330182737e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 103560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.526796685465831,\n",
      "      \"grad_norm\": 0.003089840291067958,\n",
      "      \"learning_rate\": 1.7379647132955262e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 103580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.5280569646176625,\n",
      "      \"grad_norm\": 0.00024809790193103254,\n",
      "      \"learning_rate\": 1.7373345935727788e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 103600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.529317243769495,\n",
      "      \"grad_norm\": 0.010940727777779102,\n",
      "      \"learning_rate\": 1.7367044738500314e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 103620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.530577522921327,\n",
      "      \"grad_norm\": 0.008068239316344261,\n",
      "      \"learning_rate\": 1.7360743541272843e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 103640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.531837802073159,\n",
      "      \"grad_norm\": 0.056665003299713135,\n",
      "      \"learning_rate\": 1.735444234404537e-05,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 103660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.533098081224991,\n",
      "      \"grad_norm\": 3.9511961936950684,\n",
      "      \"learning_rate\": 1.7348141146817894e-05,\n",
      "      \"loss\": 0.0285,\n",
      "      \"step\": 103680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.534358360376824,\n",
      "      \"grad_norm\": 0.0040220520459115505,\n",
      "      \"learning_rate\": 1.7341839949590423e-05,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 103700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.535618639528655,\n",
      "      \"grad_norm\": 0.001206462038680911,\n",
      "      \"learning_rate\": 1.7335538752362952e-05,\n",
      "      \"loss\": 0.0284,\n",
      "      \"step\": 103720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.536878918680488,\n",
      "      \"grad_norm\": 0.0010764867765828967,\n",
      "      \"learning_rate\": 1.7329237555135478e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 103740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.53813919783232,\n",
      "      \"grad_norm\": 0.00226168823428452,\n",
      "      \"learning_rate\": 1.7322936357908003e-05,\n",
      "      \"loss\": 0.0434,\n",
      "      \"step\": 103760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.539399476984152,\n",
      "      \"grad_norm\": 0.00194121606182307,\n",
      "      \"learning_rate\": 1.731663516068053e-05,\n",
      "      \"loss\": 0.0057,\n",
      "      \"step\": 103780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.540659756135984,\n",
      "      \"grad_norm\": 0.005026898346841335,\n",
      "      \"learning_rate\": 1.7310333963453058e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 103800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.541920035287816,\n",
      "      \"grad_norm\": 0.025319842621684074,\n",
      "      \"learning_rate\": 1.7304032766225584e-05,\n",
      "      \"loss\": 0.0545,\n",
      "      \"step\": 103820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.543180314439648,\n",
      "      \"grad_norm\": 0.023689476773142815,\n",
      "      \"learning_rate\": 1.729773156899811e-05,\n",
      "      \"loss\": 0.0398,\n",
      "      \"step\": 103840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.544440593591481,\n",
      "      \"grad_norm\": 0.018140947446227074,\n",
      "      \"learning_rate\": 1.7291430371770635e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 103860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.545700872743312,\n",
      "      \"grad_norm\": 0.004620109684765339,\n",
      "      \"learning_rate\": 1.7285129174543164e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 103880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.546961151895145,\n",
      "      \"grad_norm\": 0.00046575063606724143,\n",
      "      \"learning_rate\": 1.727882797731569e-05,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 103900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.548221431046977,\n",
      "      \"grad_norm\": 15.468615531921387,\n",
      "      \"learning_rate\": 1.7272526780088215e-05,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 103920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.549481710198809,\n",
      "      \"grad_norm\": 0.0027344899717718363,\n",
      "      \"learning_rate\": 1.7266225582860745e-05,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 103940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.550741989350641,\n",
      "      \"grad_norm\": 0.0005953239160589874,\n",
      "      \"learning_rate\": 1.7259924385633274e-05,\n",
      "      \"loss\": 0.0255,\n",
      "      \"step\": 103960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.5520022685024735,\n",
      "      \"grad_norm\": 0.013784384354948997,\n",
      "      \"learning_rate\": 1.72536231884058e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 103980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.553262547654305,\n",
      "      \"grad_norm\": 0.0009312910842709243,\n",
      "      \"learning_rate\": 1.7247321991178325e-05,\n",
      "      \"loss\": 0.0145,\n",
      "      \"step\": 104000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.554522826806138,\n",
      "      \"grad_norm\": 0.005407911725342274,\n",
      "      \"learning_rate\": 1.724102079395085e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 104020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.555783105957969,\n",
      "      \"grad_norm\": 0.011000676080584526,\n",
      "      \"learning_rate\": 1.723471959672338e-05,\n",
      "      \"loss\": 0.0105,\n",
      "      \"step\": 104040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.557043385109802,\n",
      "      \"grad_norm\": 0.0034950734116137028,\n",
      "      \"learning_rate\": 1.7228418399495905e-05,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 104060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.558303664261634,\n",
      "      \"grad_norm\": 0.01858457550406456,\n",
      "      \"learning_rate\": 1.722211720226843e-05,\n",
      "      \"loss\": 0.0116,\n",
      "      \"step\": 104080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.559563943413466,\n",
      "      \"grad_norm\": 0.5405251979827881,\n",
      "      \"learning_rate\": 1.7215816005040956e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 104100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.560824222565298,\n",
      "      \"grad_norm\": 0.001104745315387845,\n",
      "      \"learning_rate\": 1.7209514807813486e-05,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 104120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.5620845017171305,\n",
      "      \"grad_norm\": 0.0025254974607378244,\n",
      "      \"learning_rate\": 1.720321361058601e-05,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 104140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.563344780868962,\n",
      "      \"grad_norm\": 0.0008462821133434772,\n",
      "      \"learning_rate\": 1.719691241335854e-05,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 104160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.564605060020795,\n",
      "      \"grad_norm\": 0.0003659627982415259,\n",
      "      \"learning_rate\": 1.7190611216131066e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 104180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.5658653391726265,\n",
      "      \"grad_norm\": 0.003479244187474251,\n",
      "      \"learning_rate\": 1.7184310018903595e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 104200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.567125618324459,\n",
      "      \"grad_norm\": 0.03157954663038254,\n",
      "      \"learning_rate\": 1.717800882167612e-05,\n",
      "      \"loss\": 0.021,\n",
      "      \"step\": 104220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.568385897476291,\n",
      "      \"grad_norm\": 0.25819799304008484,\n",
      "      \"learning_rate\": 1.7171707624448646e-05,\n",
      "      \"loss\": 0.0169,\n",
      "      \"step\": 104240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.569646176628123,\n",
      "      \"grad_norm\": 13.882959365844727,\n",
      "      \"learning_rate\": 1.7165406427221172e-05,\n",
      "      \"loss\": 0.0271,\n",
      "      \"step\": 104260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.570906455779955,\n",
      "      \"grad_norm\": 0.09900424629449844,\n",
      "      \"learning_rate\": 1.71591052299937e-05,\n",
      "      \"loss\": 0.0363,\n",
      "      \"step\": 104280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.572166734931788,\n",
      "      \"grad_norm\": 0.00871498603373766,\n",
      "      \"learning_rate\": 1.7152804032766227e-05,\n",
      "      \"loss\": 0.0274,\n",
      "      \"step\": 104300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.573427014083619,\n",
      "      \"grad_norm\": 0.0028314825613051653,\n",
      "      \"learning_rate\": 1.7146502835538752e-05,\n",
      "      \"loss\": 0.0558,\n",
      "      \"step\": 104320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.574687293235452,\n",
      "      \"grad_norm\": 0.0499248281121254,\n",
      "      \"learning_rate\": 1.7140201638311278e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 104340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.575947572387284,\n",
      "      \"grad_norm\": 0.0013351512607187033,\n",
      "      \"learning_rate\": 1.7133900441083807e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 104360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.577207851539116,\n",
      "      \"grad_norm\": 0.005565096624195576,\n",
      "      \"learning_rate\": 1.7127599243856336e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 104380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.578468130690948,\n",
      "      \"grad_norm\": 0.003769329749047756,\n",
      "      \"learning_rate\": 1.712129804662886e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 104400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.57972840984278,\n",
      "      \"grad_norm\": 8.981453895568848,\n",
      "      \"learning_rate\": 1.7114996849401387e-05,\n",
      "      \"loss\": 0.0246,\n",
      "      \"step\": 104420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.580988688994612,\n",
      "      \"grad_norm\": 0.07966204732656479,\n",
      "      \"learning_rate\": 1.7108695652173913e-05,\n",
      "      \"loss\": 0.0069,\n",
      "      \"step\": 104440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.582248968146445,\n",
      "      \"grad_norm\": 0.005801284220069647,\n",
      "      \"learning_rate\": 1.7102394454946442e-05,\n",
      "      \"loss\": 0.0068,\n",
      "      \"step\": 104460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.583509247298276,\n",
      "      \"grad_norm\": 0.0002476417284924537,\n",
      "      \"learning_rate\": 1.7096093257718968e-05,\n",
      "      \"loss\": 0.027,\n",
      "      \"step\": 104480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.584769526450109,\n",
      "      \"grad_norm\": 0.0003894978144671768,\n",
      "      \"learning_rate\": 1.7089792060491493e-05,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 104500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.586029805601941,\n",
      "      \"grad_norm\": 0.019223202019929886,\n",
      "      \"learning_rate\": 1.7083490863264022e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 104520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.587290084753773,\n",
      "      \"grad_norm\": 0.001486891065724194,\n",
      "      \"learning_rate\": 1.7077189666036548e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 104540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.588550363905605,\n",
      "      \"grad_norm\": 0.0006815321976318955,\n",
      "      \"learning_rate\": 1.7070888468809074e-05,\n",
      "      \"loss\": 0.0294,\n",
      "      \"step\": 104560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.5898106430574375,\n",
      "      \"grad_norm\": 0.0029453837778419256,\n",
      "      \"learning_rate\": 1.70645872715816e-05,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 104580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.591070922209269,\n",
      "      \"grad_norm\": 0.0016221507685258985,\n",
      "      \"learning_rate\": 1.70586011342155e-05,\n",
      "      \"loss\": 0.0191,\n",
      "      \"step\": 104600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.592331201361102,\n",
      "      \"grad_norm\": 0.006130504421889782,\n",
      "      \"learning_rate\": 1.705229993698803e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 104620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.593591480512933,\n",
      "      \"grad_norm\": 0.00040208137943409383,\n",
      "      \"learning_rate\": 1.7045998739760555e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 104640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.594851759664766,\n",
      "      \"grad_norm\": 0.0015735061606392264,\n",
      "      \"learning_rate\": 1.703969754253308e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 104660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.596112038816598,\n",
      "      \"grad_norm\": 0.11139006912708282,\n",
      "      \"learning_rate\": 1.7033396345305606e-05,\n",
      "      \"loss\": 0.0196,\n",
      "      \"step\": 104680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.59737231796843,\n",
      "      \"grad_norm\": 0.08953230828046799,\n",
      "      \"learning_rate\": 1.7027095148078135e-05,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 104700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.598632597120262,\n",
      "      \"grad_norm\": 7.868968486785889,\n",
      "      \"learning_rate\": 1.7020793950850664e-05,\n",
      "      \"loss\": 0.0315,\n",
      "      \"step\": 104720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.5998928762720945,\n",
      "      \"grad_norm\": 0.019433235749602318,\n",
      "      \"learning_rate\": 1.701449275362319e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 104740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.601153155423926,\n",
      "      \"grad_norm\": 0.0007471198914572597,\n",
      "      \"learning_rate\": 1.7008191556395716e-05,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 104760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.602413434575759,\n",
      "      \"grad_norm\": 0.008209183812141418,\n",
      "      \"learning_rate\": 1.7001890359168245e-05,\n",
      "      \"loss\": 0.0136,\n",
      "      \"step\": 104780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.6036737137275905,\n",
      "      \"grad_norm\": 0.0029262162279337645,\n",
      "      \"learning_rate\": 1.699558916194077e-05,\n",
      "      \"loss\": 0.0519,\n",
      "      \"step\": 104800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.604933992879423,\n",
      "      \"grad_norm\": 11.689729690551758,\n",
      "      \"learning_rate\": 1.6989287964713296e-05,\n",
      "      \"loss\": 0.0269,\n",
      "      \"step\": 104820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.606194272031255,\n",
      "      \"grad_norm\": 0.0019883664790540934,\n",
      "      \"learning_rate\": 1.698298676748582e-05,\n",
      "      \"loss\": 0.0027,\n",
      "      \"step\": 104840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.607454551183087,\n",
      "      \"grad_norm\": 0.005139951594173908,\n",
      "      \"learning_rate\": 1.697668557025835e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 104860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.608714830334919,\n",
      "      \"grad_norm\": 0.002603772096335888,\n",
      "      \"learning_rate\": 1.6970384373030876e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 104880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.609975109486752,\n",
      "      \"grad_norm\": 0.13543549180030823,\n",
      "      \"learning_rate\": 1.6964083175803402e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 104900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.611235388638583,\n",
      "      \"grad_norm\": 0.10698458552360535,\n",
      "      \"learning_rate\": 1.6957781978575927e-05,\n",
      "      \"loss\": 0.0285,\n",
      "      \"step\": 104920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.612495667790416,\n",
      "      \"grad_norm\": 0.03432459011673927,\n",
      "      \"learning_rate\": 1.695148078134846e-05,\n",
      "      \"loss\": 0.0356,\n",
      "      \"step\": 104940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.6137559469422476,\n",
      "      \"grad_norm\": 0.0038270854856818914,\n",
      "      \"learning_rate\": 1.6945179584120986e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 104960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.61501622609408,\n",
      "      \"grad_norm\": 0.0422358401119709,\n",
      "      \"learning_rate\": 1.693887838689351e-05,\n",
      "      \"loss\": 0.0045,\n",
      "      \"step\": 104980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.616276505245912,\n",
      "      \"grad_norm\": 0.13671284914016724,\n",
      "      \"learning_rate\": 1.6932577189666037e-05,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 105000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.617536784397744,\n",
      "      \"grad_norm\": 0.02472562901675701,\n",
      "      \"learning_rate\": 1.6926275992438566e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 105020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.618797063549576,\n",
      "      \"grad_norm\": 0.0002361806546105072,\n",
      "      \"learning_rate\": 1.691997479521109e-05,\n",
      "      \"loss\": 0.0062,\n",
      "      \"step\": 105040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.620057342701409,\n",
      "      \"grad_norm\": 0.00019508894183672965,\n",
      "      \"learning_rate\": 1.6913673597983617e-05,\n",
      "      \"loss\": 0.0046,\n",
      "      \"step\": 105060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.62131762185324,\n",
      "      \"grad_norm\": 0.0001551008754177019,\n",
      "      \"learning_rate\": 1.6907372400756143e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 105080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.622577901005073,\n",
      "      \"grad_norm\": 0.00033799087395891547,\n",
      "      \"learning_rate\": 1.6901071203528672e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 105100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.623838180156905,\n",
      "      \"grad_norm\": 0.3534078598022461,\n",
      "      \"learning_rate\": 1.6894770006301198e-05,\n",
      "      \"loss\": 0.0265,\n",
      "      \"step\": 105120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.625098459308737,\n",
      "      \"grad_norm\": 0.0011532001662999392,\n",
      "      \"learning_rate\": 1.6888468809073723e-05,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 105140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.626358738460569,\n",
      "      \"grad_norm\": 0.0054941680282354355,\n",
      "      \"learning_rate\": 1.6882167611846252e-05,\n",
      "      \"loss\": 0.0171,\n",
      "      \"step\": 105160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.6276190176124015,\n",
      "      \"grad_norm\": 0.31683793663978577,\n",
      "      \"learning_rate\": 1.6875866414618778e-05,\n",
      "      \"loss\": 0.0125,\n",
      "      \"step\": 105180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.628879296764233,\n",
      "      \"grad_norm\": 0.0002366891858400777,\n",
      "      \"learning_rate\": 1.6869565217391307e-05,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 105200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.630139575916066,\n",
      "      \"grad_norm\": 0.12343783676624298,\n",
      "      \"learning_rate\": 1.6863264020163833e-05,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 105220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.631399855067897,\n",
      "      \"grad_norm\": 13.061572074890137,\n",
      "      \"learning_rate\": 1.6856962822936358e-05,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 105240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.63266013421973,\n",
      "      \"grad_norm\": 2.635179042816162,\n",
      "      \"learning_rate\": 1.6850661625708887e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 105260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.633920413371562,\n",
      "      \"grad_norm\": 0.0005074107903055847,\n",
      "      \"learning_rate\": 1.6844360428481413e-05,\n",
      "      \"loss\": 0.0069,\n",
      "      \"step\": 105280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.635180692523394,\n",
      "      \"grad_norm\": 0.0005510636256076396,\n",
      "      \"learning_rate\": 1.683805923125394e-05,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 105300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.636440971675226,\n",
      "      \"grad_norm\": 0.0002865151618607342,\n",
      "      \"learning_rate\": 1.6831758034026464e-05,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 105320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.6377012508270585,\n",
      "      \"grad_norm\": 0.00015839865955058485,\n",
      "      \"learning_rate\": 1.6825456836798993e-05,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 105340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.63896152997889,\n",
      "      \"grad_norm\": 0.001280166208744049,\n",
      "      \"learning_rate\": 1.681915563957152e-05,\n",
      "      \"loss\": 0.0246,\n",
      "      \"step\": 105360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.640221809130723,\n",
      "      \"grad_norm\": 0.12168226391077042,\n",
      "      \"learning_rate\": 1.6812854442344045e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 105380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.6414820882825545,\n",
      "      \"grad_norm\": 0.0003321136755403131,\n",
      "      \"learning_rate\": 1.6806553245116574e-05,\n",
      "      \"loss\": 0.0625,\n",
      "      \"step\": 105400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.642742367434387,\n",
      "      \"grad_norm\": 0.03876860439777374,\n",
      "      \"learning_rate\": 1.68002520478891e-05,\n",
      "      \"loss\": 0.0367,\n",
      "      \"step\": 105420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.644002646586219,\n",
      "      \"grad_norm\": 0.035787712782621384,\n",
      "      \"learning_rate\": 1.6793950850661628e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 105440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.645262925738051,\n",
      "      \"grad_norm\": 0.46757540106773376,\n",
      "      \"learning_rate\": 1.6787649653434154e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 105460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.646523204889883,\n",
      "      \"grad_norm\": 0.002614045049995184,\n",
      "      \"learning_rate\": 1.678134845620668e-05,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 105480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.647783484041716,\n",
      "      \"grad_norm\": 0.0012109766248613596,\n",
      "      \"learning_rate\": 1.6775047258979205e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 105500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.649043763193547,\n",
      "      \"grad_norm\": 0.0003621673968154937,\n",
      "      \"learning_rate\": 1.6768746061751734e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 105520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.65030404234538,\n",
      "      \"grad_norm\": 0.0008349414565600455,\n",
      "      \"learning_rate\": 1.676244486452426e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 105540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.6515643214972116,\n",
      "      \"grad_norm\": 0.0033647301606833935,\n",
      "      \"learning_rate\": 1.6756143667296786e-05,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 105560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.652824600649044,\n",
      "      \"grad_norm\": 0.0048783281818032265,\n",
      "      \"learning_rate\": 1.6749842470069315e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 105580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.654084879800876,\n",
      "      \"grad_norm\": 0.0012520604068413377,\n",
      "      \"learning_rate\": 1.674354127284184e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 105600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.655345158952708,\n",
      "      \"grad_norm\": 0.010562346316874027,\n",
      "      \"learning_rate\": 1.673724007561437e-05,\n",
      "      \"loss\": 0.0049,\n",
      "      \"step\": 105620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.65660543810454,\n",
      "      \"grad_norm\": 0.0022268863394856453,\n",
      "      \"learning_rate\": 1.6730938878386895e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 105640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.657865717256373,\n",
      "      \"grad_norm\": 0.015109694562852383,\n",
      "      \"learning_rate\": 1.672463768115942e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 105660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.659125996408204,\n",
      "      \"grad_norm\": 0.0009419189300388098,\n",
      "      \"learning_rate\": 1.671833648393195e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 105680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.660386275560037,\n",
      "      \"grad_norm\": 0.0007500766078010201,\n",
      "      \"learning_rate\": 1.6712035286704475e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 105700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.661646554711869,\n",
      "      \"grad_norm\": 2.7976248264312744,\n",
      "      \"learning_rate\": 1.6705734089477e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 105720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.662906833863701,\n",
      "      \"grad_norm\": 0.0006319751846604049,\n",
      "      \"learning_rate\": 1.6699432892249527e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 105740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.664167113015533,\n",
      "      \"grad_norm\": 5.60234260559082,\n",
      "      \"learning_rate\": 1.6693131695022056e-05,\n",
      "      \"loss\": 0.0383,\n",
      "      \"step\": 105760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.665427392167365,\n",
      "      \"grad_norm\": 0.0028021943289786577,\n",
      "      \"learning_rate\": 1.668683049779458e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 105780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.666687671319197,\n",
      "      \"grad_norm\": 0.0007607264560647309,\n",
      "      \"learning_rate\": 1.6680529300567107e-05,\n",
      "      \"loss\": 0.0325,\n",
      "      \"step\": 105800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.66794795047103,\n",
      "      \"grad_norm\": 0.10106267780065536,\n",
      "      \"learning_rate\": 1.6674228103339633e-05,\n",
      "      \"loss\": 0.0142,\n",
      "      \"step\": 105820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.669208229622861,\n",
      "      \"grad_norm\": 0.005534300580620766,\n",
      "      \"learning_rate\": 1.6667926906112165e-05,\n",
      "      \"loss\": 0.0451,\n",
      "      \"step\": 105840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.670468508774693,\n",
      "      \"grad_norm\": 0.0032326076179742813,\n",
      "      \"learning_rate\": 1.666162570888469e-05,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 105860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.671728787926526,\n",
      "      \"grad_norm\": 0.06378413736820221,\n",
      "      \"learning_rate\": 1.6655324511657216e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 105880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.672989067078358,\n",
      "      \"grad_norm\": 0.0004917819751426578,\n",
      "      \"learning_rate\": 1.6649023314429742e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 105900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.67424934623019,\n",
      "      \"grad_norm\": 0.3837383985519409,\n",
      "      \"learning_rate\": 1.664272211720227e-05,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 105920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.675509625382022,\n",
      "      \"grad_norm\": 0.0002641940372996032,\n",
      "      \"learning_rate\": 1.6636420919974797e-05,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 105940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.676769904533854,\n",
      "      \"grad_norm\": 0.053333040326833725,\n",
      "      \"learning_rate\": 1.6630119722747322e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 105960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.678030183685687,\n",
      "      \"grad_norm\": 0.02810385636985302,\n",
      "      \"learning_rate\": 1.6623818525519848e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 105980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.6792904628375185,\n",
      "      \"grad_norm\": 0.0008830004371702671,\n",
      "      \"learning_rate\": 1.6617517328292377e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 106000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.68055074198935,\n",
      "      \"grad_norm\": 0.0070577142760157585,\n",
      "      \"learning_rate\": 1.6611216131064903e-05,\n",
      "      \"loss\": 0.0326,\n",
      "      \"step\": 106020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.681811021141183,\n",
      "      \"grad_norm\": 0.002008485607802868,\n",
      "      \"learning_rate\": 1.660491493383743e-05,\n",
      "      \"loss\": 0.0029,\n",
      "      \"step\": 106040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.683071300293015,\n",
      "      \"grad_norm\": 0.017477499321103096,\n",
      "      \"learning_rate\": 1.6598613736609954e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 106060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.684331579444847,\n",
      "      \"grad_norm\": 0.03788245841860771,\n",
      "      \"learning_rate\": 1.6592312539382483e-05,\n",
      "      \"loss\": 0.0162,\n",
      "      \"step\": 106080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.685591858596679,\n",
      "      \"grad_norm\": 0.0014501161640509963,\n",
      "      \"learning_rate\": 1.6586011342155012e-05,\n",
      "      \"loss\": 0.025,\n",
      "      \"step\": 106100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.686852137748511,\n",
      "      \"grad_norm\": 0.0011858720099553466,\n",
      "      \"learning_rate\": 1.6579710144927538e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 106120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.688112416900344,\n",
      "      \"grad_norm\": 0.003361194860190153,\n",
      "      \"learning_rate\": 1.6573408947700063e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 106140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.6893726960521755,\n",
      "      \"grad_norm\": 0.11998166888952255,\n",
      "      \"learning_rate\": 1.6567107750472592e-05,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 106160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.690632975204007,\n",
      "      \"grad_norm\": 0.0005441132234409451,\n",
      "      \"learning_rate\": 1.6560806553245118e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 106180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.69189325435584,\n",
      "      \"grad_norm\": 0.00015446289035025984,\n",
      "      \"learning_rate\": 1.6554505356017644e-05,\n",
      "      \"loss\": 0.0147,\n",
      "      \"step\": 106200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.693153533507672,\n",
      "      \"grad_norm\": 0.00018793610797729343,\n",
      "      \"learning_rate\": 1.654820415879017e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 106220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.694413812659504,\n",
      "      \"grad_norm\": 0.017129989340901375,\n",
      "      \"learning_rate\": 1.65419029615627e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 106240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.695674091811336,\n",
      "      \"grad_norm\": 0.0009238619823008776,\n",
      "      \"learning_rate\": 1.6535601764335224e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 106260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.696934370963168,\n",
      "      \"grad_norm\": 0.00021125191415194422,\n",
      "      \"learning_rate\": 1.652930056710775e-05,\n",
      "      \"loss\": 0.0291,\n",
      "      \"step\": 106280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.698194650115001,\n",
      "      \"grad_norm\": 0.0028766044415533543,\n",
      "      \"learning_rate\": 1.652299936988028e-05,\n",
      "      \"loss\": 0.0347,\n",
      "      \"step\": 106300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.699454929266833,\n",
      "      \"grad_norm\": 0.15572601556777954,\n",
      "      \"learning_rate\": 1.6516698172652804e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 106320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.700715208418664,\n",
      "      \"grad_norm\": 0.0043898681178689,\n",
      "      \"learning_rate\": 1.6510396975425333e-05,\n",
      "      \"loss\": 0.0362,\n",
      "      \"step\": 106340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.701975487570497,\n",
      "      \"grad_norm\": 0.0007255518576130271,\n",
      "      \"learning_rate\": 1.650409577819786e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 106360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.7032357667223295,\n",
      "      \"grad_norm\": 0.09442340582609177,\n",
      "      \"learning_rate\": 1.6497794580970385e-05,\n",
      "      \"loss\": 0.0283,\n",
      "      \"step\": 106380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.704496045874161,\n",
      "      \"grad_norm\": 0.06546695530414581,\n",
      "      \"learning_rate\": 1.649149338374291e-05,\n",
      "      \"loss\": 0.0024,\n",
      "      \"step\": 106400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.705756325025993,\n",
      "      \"grad_norm\": 0.007189302705228329,\n",
      "      \"learning_rate\": 1.648519218651544e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 106420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.707016604177825,\n",
      "      \"grad_norm\": 0.11166053265333176,\n",
      "      \"learning_rate\": 1.6478890989287965e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 106440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.708276883329658,\n",
      "      \"grad_norm\": 0.00039675948210060596,\n",
      "      \"learning_rate\": 1.647258979206049e-05,\n",
      "      \"loss\": 0.0311,\n",
      "      \"step\": 106460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.70953716248149,\n",
      "      \"grad_norm\": 0.0004433727008290589,\n",
      "      \"learning_rate\": 1.646628859483302e-05,\n",
      "      \"loss\": 0.0125,\n",
      "      \"step\": 106480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.710797441633321,\n",
      "      \"grad_norm\": 0.004070188384503126,\n",
      "      \"learning_rate\": 1.6459987397605545e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 106500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.712057720785154,\n",
      "      \"grad_norm\": 0.0037061034236103296,\n",
      "      \"learning_rate\": 1.6453686200378074e-05,\n",
      "      \"loss\": 0.0303,\n",
      "      \"step\": 106520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.7133179999369865,\n",
      "      \"grad_norm\": 0.03956811502575874,\n",
      "      \"learning_rate\": 1.64473850031506e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 106540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.714578279088818,\n",
      "      \"grad_norm\": 0.00033155520213767886,\n",
      "      \"learning_rate\": 1.6441083805923126e-05,\n",
      "      \"loss\": 0.0293,\n",
      "      \"step\": 106560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.71583855824065,\n",
      "      \"grad_norm\": 0.0016616330249235034,\n",
      "      \"learning_rate\": 1.6434782608695655e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 106580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.7170988373924825,\n",
      "      \"grad_norm\": 0.0003660277579911053,\n",
      "      \"learning_rate\": 1.642848141146818e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 106600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.718359116544315,\n",
      "      \"grad_norm\": 0.00037992894067429006,\n",
      "      \"learning_rate\": 1.6422180214240706e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 106620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.719619395696147,\n",
      "      \"grad_norm\": 0.000302514381473884,\n",
      "      \"learning_rate\": 1.6415879017013232e-05,\n",
      "      \"loss\": 0.0219,\n",
      "      \"step\": 106640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.720879674847978,\n",
      "      \"grad_norm\": 0.0017688244115561247,\n",
      "      \"learning_rate\": 1.640957781978576e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 106660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.722139953999811,\n",
      "      \"grad_norm\": 0.020665748044848442,\n",
      "      \"learning_rate\": 1.6403276622558286e-05,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 106680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.723400233151643,\n",
      "      \"grad_norm\": 0.003425518749281764,\n",
      "      \"learning_rate\": 1.6396975425330812e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 106700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.724660512303475,\n",
      "      \"grad_norm\": 0.007881436496973038,\n",
      "      \"learning_rate\": 1.6390674228103338e-05,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 106720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.725920791455307,\n",
      "      \"grad_norm\": 0.009489837102591991,\n",
      "      \"learning_rate\": 1.6384373030875867e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 106740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.7271810706071395,\n",
      "      \"grad_norm\": 0.0004084970278199762,\n",
      "      \"learning_rate\": 1.6378071833648396e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 106760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.728441349758971,\n",
      "      \"grad_norm\": 0.07263673841953278,\n",
      "      \"learning_rate\": 1.637177063642092e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 106780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.729701628910804,\n",
      "      \"grad_norm\": 0.00670256232842803,\n",
      "      \"learning_rate\": 1.6365469439193447e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 106800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.7309619080626355,\n",
      "      \"grad_norm\": 0.00015937109128572047,\n",
      "      \"learning_rate\": 1.6359168241965976e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 106820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.732222187214468,\n",
      "      \"grad_norm\": 0.06241126358509064,\n",
      "      \"learning_rate\": 1.6352867044738502e-05,\n",
      "      \"loss\": 0.0237,\n",
      "      \"step\": 106840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.7334824663663,\n",
      "      \"grad_norm\": 0.0014601118164137006,\n",
      "      \"learning_rate\": 1.6346565847511027e-05,\n",
      "      \"loss\": 0.0067,\n",
      "      \"step\": 106860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.734742745518132,\n",
      "      \"grad_norm\": 0.013499253429472446,\n",
      "      \"learning_rate\": 1.6340264650283553e-05,\n",
      "      \"loss\": 0.0056,\n",
      "      \"step\": 106880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.736003024669964,\n",
      "      \"grad_norm\": 0.005021548364311457,\n",
      "      \"learning_rate\": 1.6333963453056082e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 106900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.737263303821797,\n",
      "      \"grad_norm\": 0.26712968945503235,\n",
      "      \"learning_rate\": 1.6327662255828608e-05,\n",
      "      \"loss\": 0.0283,\n",
      "      \"step\": 106920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.738523582973628,\n",
      "      \"grad_norm\": 0.136556014418602,\n",
      "      \"learning_rate\": 1.6321361058601133e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 106940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.739783862125461,\n",
      "      \"grad_norm\": 0.02369793877005577,\n",
      "      \"learning_rate\": 1.631505986137366e-05,\n",
      "      \"loss\": 0.0261,\n",
      "      \"step\": 106960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.741044141277293,\n",
      "      \"grad_norm\": 0.04528401046991348,\n",
      "      \"learning_rate\": 1.6308758664146188e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 106980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.742304420429125,\n",
      "      \"grad_norm\": 5.353135585784912,\n",
      "      \"learning_rate\": 1.6302457466918717e-05,\n",
      "      \"loss\": 0.0516,\n",
      "      \"step\": 107000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.743564699580957,\n",
      "      \"grad_norm\": 0.0002701618359424174,\n",
      "      \"learning_rate\": 1.6296471329552615e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 107020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.744824978732789,\n",
      "      \"grad_norm\": 0.0013939031632617116,\n",
      "      \"learning_rate\": 1.629017013232514e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 107040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.746085257884621,\n",
      "      \"grad_norm\": 0.0018138388404622674,\n",
      "      \"learning_rate\": 1.628386893509767e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 107060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.747345537036454,\n",
      "      \"grad_norm\": 0.01354619488120079,\n",
      "      \"learning_rate\": 1.62775677378702e-05,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 107080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.748605816188285,\n",
      "      \"grad_norm\": 0.0014584105229005218,\n",
      "      \"learning_rate\": 1.6271266540642724e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 107100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.749866095340118,\n",
      "      \"grad_norm\": 0.0030829133465886116,\n",
      "      \"learning_rate\": 1.626496534341525e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 107120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.75112637449195,\n",
      "      \"grad_norm\": 0.025221887975931168,\n",
      "      \"learning_rate\": 1.6258664146187775e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 107140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.752386653643782,\n",
      "      \"grad_norm\": 0.4073100686073303,\n",
      "      \"learning_rate\": 1.6252362948960304e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 107160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.753646932795614,\n",
      "      \"grad_norm\": 0.008419300429522991,\n",
      "      \"learning_rate\": 1.624606175173283e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 107180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.7549072119474465,\n",
      "      \"grad_norm\": 0.009501692838966846,\n",
      "      \"learning_rate\": 1.6239760554505356e-05,\n",
      "      \"loss\": 0.0193,\n",
      "      \"step\": 107200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.756167491099278,\n",
      "      \"grad_norm\": 0.012505311518907547,\n",
      "      \"learning_rate\": 1.6233459357277885e-05,\n",
      "      \"loss\": 0.029,\n",
      "      \"step\": 107220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.757427770251111,\n",
      "      \"grad_norm\": 0.0004189228930044919,\n",
      "      \"learning_rate\": 1.622715816005041e-05,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 107240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.758688049402942,\n",
      "      \"grad_norm\": 0.004012293182313442,\n",
      "      \"learning_rate\": 1.6220856962822936e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 107260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.759948328554775,\n",
      "      \"grad_norm\": 0.07422848045825958,\n",
      "      \"learning_rate\": 1.6214555765595462e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 107280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.761208607706607,\n",
      "      \"grad_norm\": 0.0002700738550629467,\n",
      "      \"learning_rate\": 1.620825456836799e-05,\n",
      "      \"loss\": 0.0044,\n",
      "      \"step\": 107300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.762468886858439,\n",
      "      \"grad_norm\": 0.0022122585214674473,\n",
      "      \"learning_rate\": 1.620195337114052e-05,\n",
      "      \"loss\": 0.038,\n",
      "      \"step\": 107320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.763729166010271,\n",
      "      \"grad_norm\": 5.735742092132568,\n",
      "      \"learning_rate\": 1.6195652173913045e-05,\n",
      "      \"loss\": 0.0202,\n",
      "      \"step\": 107340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.7649894451621035,\n",
      "      \"grad_norm\": 0.003001282224431634,\n",
      "      \"learning_rate\": 1.618935097668557e-05,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 107360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.766249724313935,\n",
      "      \"grad_norm\": 0.00021444179583340883,\n",
      "      \"learning_rate\": 1.6183049779458097e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 107380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.767510003465768,\n",
      "      \"grad_norm\": 0.016043469309806824,\n",
      "      \"learning_rate\": 1.6176748582230626e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 107400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.7687702826175995,\n",
      "      \"grad_norm\": 0.00021147496590856463,\n",
      "      \"learning_rate\": 1.617044738500315e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 107420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.770030561769432,\n",
      "      \"grad_norm\": 26.631967544555664,\n",
      "      \"learning_rate\": 1.6164146187775677e-05,\n",
      "      \"loss\": 0.0049,\n",
      "      \"step\": 107440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.771290840921264,\n",
      "      \"grad_norm\": 1.4325040578842163,\n",
      "      \"learning_rate\": 1.6157844990548203e-05,\n",
      "      \"loss\": 0.0129,\n",
      "      \"step\": 107460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.772551120073096,\n",
      "      \"grad_norm\": 0.2735553979873657,\n",
      "      \"learning_rate\": 1.6151543793320732e-05,\n",
      "      \"loss\": 0.0567,\n",
      "      \"step\": 107480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.773811399224928,\n",
      "      \"grad_norm\": 0.10703229159116745,\n",
      "      \"learning_rate\": 1.6145242596093257e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 107500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.775071678376761,\n",
      "      \"grad_norm\": 0.0026033103931695223,\n",
      "      \"learning_rate\": 1.6138941398865783e-05,\n",
      "      \"loss\": 0.0204,\n",
      "      \"step\": 107520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.776331957528592,\n",
      "      \"grad_norm\": 0.00039854817441664636,\n",
      "      \"learning_rate\": 1.6132640201638312e-05,\n",
      "      \"loss\": 0.0018,\n",
      "      \"step\": 107540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.777592236680425,\n",
      "      \"grad_norm\": 0.00023585191229358315,\n",
      "      \"learning_rate\": 1.612633900441084e-05,\n",
      "      \"loss\": 0.0042,\n",
      "      \"step\": 107560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.778852515832257,\n",
      "      \"grad_norm\": 0.01168738678097725,\n",
      "      \"learning_rate\": 1.6120037807183367e-05,\n",
      "      \"loss\": 0.0371,\n",
      "      \"step\": 107580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.780112794984089,\n",
      "      \"grad_norm\": 0.0025767795741558075,\n",
      "      \"learning_rate\": 1.6113736609955892e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 107600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.781373074135921,\n",
      "      \"grad_norm\": 0.0002462442498654127,\n",
      "      \"learning_rate\": 1.6107435412728418e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 107620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.782633353287753,\n",
      "      \"grad_norm\": 0.002813527826219797,\n",
      "      \"learning_rate\": 1.6101134215500947e-05,\n",
      "      \"loss\": 0.0196,\n",
      "      \"step\": 107640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.783893632439585,\n",
      "      \"grad_norm\": 0.007872737012803555,\n",
      "      \"learning_rate\": 1.6094833018273473e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 107660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.785153911591418,\n",
      "      \"grad_norm\": 0.0017814356833696365,\n",
      "      \"learning_rate\": 1.6088531821046e-05,\n",
      "      \"loss\": 0.0372,\n",
      "      \"step\": 107680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.786414190743249,\n",
      "      \"grad_norm\": 0.00024424263392575085,\n",
      "      \"learning_rate\": 1.6082230623818524e-05,\n",
      "      \"loss\": 0.0528,\n",
      "      \"step\": 107700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.787674469895082,\n",
      "      \"grad_norm\": 0.006884775590151548,\n",
      "      \"learning_rate\": 1.6075929426591053e-05,\n",
      "      \"loss\": 0.0282,\n",
      "      \"step\": 107720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.788934749046914,\n",
      "      \"grad_norm\": 0.19139531254768372,\n",
      "      \"learning_rate\": 1.606962822936358e-05,\n",
      "      \"loss\": 0.0232,\n",
      "      \"step\": 107740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.790195028198746,\n",
      "      \"grad_norm\": 0.03709007427096367,\n",
      "      \"learning_rate\": 1.6063327032136108e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 107760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.791455307350578,\n",
      "      \"grad_norm\": 0.009332071989774704,\n",
      "      \"learning_rate\": 1.6057025834908633e-05,\n",
      "      \"loss\": 0.0219,\n",
      "      \"step\": 107780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.7927155865024105,\n",
      "      \"grad_norm\": 0.00042563682654872537,\n",
      "      \"learning_rate\": 1.6050724637681163e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 107800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.793975865654242,\n",
      "      \"grad_norm\": 0.007660808507353067,\n",
      "      \"learning_rate\": 1.6044423440453688e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 107820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.795236144806075,\n",
      "      \"grad_norm\": 0.0003604423254728317,\n",
      "      \"learning_rate\": 1.6038122243226214e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 107840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.796496423957906,\n",
      "      \"grad_norm\": 0.12885601818561554,\n",
      "      \"learning_rate\": 1.603182104599874e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 107860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.797756703109739,\n",
      "      \"grad_norm\": 0.0002655617718119174,\n",
      "      \"learning_rate\": 1.602551984877127e-05,\n",
      "      \"loss\": 0.0019,\n",
      "      \"step\": 107880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.799016982261571,\n",
      "      \"grad_norm\": 0.0002342418592888862,\n",
      "      \"learning_rate\": 1.6019218651543794e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 107900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.800277261413403,\n",
      "      \"grad_norm\": 0.00024081517767626792,\n",
      "      \"learning_rate\": 1.601291745431632e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 107920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.801537540565235,\n",
      "      \"grad_norm\": 0.0005012645269744098,\n",
      "      \"learning_rate\": 1.6006616257088845e-05,\n",
      "      \"loss\": 0.013,\n",
      "      \"step\": 107940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.8027978197170675,\n",
      "      \"grad_norm\": 0.0002687173255253583,\n",
      "      \"learning_rate\": 1.6000315059861375e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 107960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.804058098868899,\n",
      "      \"grad_norm\": 0.0019646736327558756,\n",
      "      \"learning_rate\": 1.5994013862633904e-05,\n",
      "      \"loss\": 0.0242,\n",
      "      \"step\": 107980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.805318378020732,\n",
      "      \"grad_norm\": 0.00032653118250891566,\n",
      "      \"learning_rate\": 1.598771266540643e-05,\n",
      "      \"loss\": 0.0376,\n",
      "      \"step\": 108000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.8065786571725635,\n",
      "      \"grad_norm\": 0.040634121745824814,\n",
      "      \"learning_rate\": 1.5981411468178955e-05,\n",
      "      \"loss\": 0.0262,\n",
      "      \"step\": 108020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.807838936324396,\n",
      "      \"grad_norm\": 0.0047883568331599236,\n",
      "      \"learning_rate\": 1.597511027095148e-05,\n",
      "      \"loss\": 0.0059,\n",
      "      \"step\": 108040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.809099215476228,\n",
      "      \"grad_norm\": 0.0026828425470739603,\n",
      "      \"learning_rate\": 1.596880907372401e-05,\n",
      "      \"loss\": 0.0233,\n",
      "      \"step\": 108060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.81035949462806,\n",
      "      \"grad_norm\": 0.018421029672026634,\n",
      "      \"learning_rate\": 1.5962507876496535e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 108080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.811619773779892,\n",
      "      \"grad_norm\": 0.09529475122690201,\n",
      "      \"learning_rate\": 1.595620667926906e-05,\n",
      "      \"loss\": 0.0146,\n",
      "      \"step\": 108100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.812880052931725,\n",
      "      \"grad_norm\": 0.002063763327896595,\n",
      "      \"learning_rate\": 1.594990548204159e-05,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 108120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.814140332083556,\n",
      "      \"grad_norm\": 0.0045663947239518166,\n",
      "      \"learning_rate\": 1.5943604284814116e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 108140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.815400611235389,\n",
      "      \"grad_norm\": 0.011536065489053726,\n",
      "      \"learning_rate\": 1.593730308758664e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 108160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.816660890387221,\n",
      "      \"grad_norm\": 8.40312385559082,\n",
      "      \"learning_rate\": 1.5931001890359167e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 108180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.817921169539053,\n",
      "      \"grad_norm\": 0.000276818813290447,\n",
      "      \"learning_rate\": 1.5924700693131696e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 108200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.819181448690885,\n",
      "      \"grad_norm\": 0.000716770242433995,\n",
      "      \"learning_rate\": 1.5918399495904225e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 108220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.820441727842717,\n",
      "      \"grad_norm\": 0.0002780468203127384,\n",
      "      \"learning_rate\": 1.591209829867675e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 108240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.821702006994549,\n",
      "      \"grad_norm\": 0.00019248411990702152,\n",
      "      \"learning_rate\": 1.5905797101449276e-05,\n",
      "      \"loss\": 0.0297,\n",
      "      \"step\": 108260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.822962286146382,\n",
      "      \"grad_norm\": 0.0007500271894969046,\n",
      "      \"learning_rate\": 1.5899495904221802e-05,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 108280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.824222565298213,\n",
      "      \"grad_norm\": 0.029368024319410324,\n",
      "      \"learning_rate\": 1.589319470699433e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 108300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.825482844450046,\n",
      "      \"grad_norm\": 0.9027369618415833,\n",
      "      \"learning_rate\": 1.5886893509766857e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 108320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.826743123601878,\n",
      "      \"grad_norm\": 0.0022380517330020666,\n",
      "      \"learning_rate\": 1.5880592312539382e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 108340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.82800340275371,\n",
      "      \"grad_norm\": 0.0001330771337961778,\n",
      "      \"learning_rate\": 1.5874291115311908e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 108360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.829263681905542,\n",
      "      \"grad_norm\": 0.9284080266952515,\n",
      "      \"learning_rate\": 1.5867989918084437e-05,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 108380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.8305239610573745,\n",
      "      \"grad_norm\": 0.0014646088238805532,\n",
      "      \"learning_rate\": 1.5861688720856963e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 108400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.831784240209206,\n",
      "      \"grad_norm\": 0.00018979636661242694,\n",
      "      \"learning_rate\": 1.5855387523629488e-05,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 108420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.833044519361039,\n",
      "      \"grad_norm\": 0.006301101762801409,\n",
      "      \"learning_rate\": 1.5849086326402017e-05,\n",
      "      \"loss\": 0.043,\n",
      "      \"step\": 108440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.83430479851287,\n",
      "      \"grad_norm\": 0.00018725795962382108,\n",
      "      \"learning_rate\": 1.5842785129174546e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 108460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.835565077664703,\n",
      "      \"grad_norm\": 0.0020735154394060373,\n",
      "      \"learning_rate\": 1.5836483931947072e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 108480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.836825356816535,\n",
      "      \"grad_norm\": 0.0014966011513024569,\n",
      "      \"learning_rate\": 1.5830182734719598e-05,\n",
      "      \"loss\": 0.0043,\n",
      "      \"step\": 108500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.838085635968367,\n",
      "      \"grad_norm\": 0.0011743904324248433,\n",
      "      \"learning_rate\": 1.5823881537492123e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 108520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.839345915120199,\n",
      "      \"grad_norm\": 0.00097112130606547,\n",
      "      \"learning_rate\": 1.5817580340264652e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 108540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.8406061942720315,\n",
      "      \"grad_norm\": 0.0036962239537388086,\n",
      "      \"learning_rate\": 1.5811279143037178e-05,\n",
      "      \"loss\": 0.0212,\n",
      "      \"step\": 108560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.841866473423863,\n",
      "      \"grad_norm\": 0.0014521806733682752,\n",
      "      \"learning_rate\": 1.5804977945809704e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 108580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.843126752575696,\n",
      "      \"grad_norm\": 0.01866931840777397,\n",
      "      \"learning_rate\": 1.579867674858223e-05,\n",
      "      \"loss\": 0.0196,\n",
      "      \"step\": 108600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.8443870317275275,\n",
      "      \"grad_norm\": 0.14343570172786713,\n",
      "      \"learning_rate\": 1.5792375551354758e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 108620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.84564731087936,\n",
      "      \"grad_norm\": 0.010055549442768097,\n",
      "      \"learning_rate\": 1.5786074354127284e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 108640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.846907590031192,\n",
      "      \"grad_norm\": 0.0007497018086723983,\n",
      "      \"learning_rate\": 1.5779773156899813e-05,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 108660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.848167869183024,\n",
      "      \"grad_norm\": 0.0013260237174108624,\n",
      "      \"learning_rate\": 1.577347195967234e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 108680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.849428148334856,\n",
      "      \"grad_norm\": 0.0010564592666924,\n",
      "      \"learning_rate\": 1.5767170762444868e-05,\n",
      "      \"loss\": 0.0069,\n",
      "      \"step\": 108700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.850688427486689,\n",
      "      \"grad_norm\": 0.003704966977238655,\n",
      "      \"learning_rate\": 1.5760869565217393e-05,\n",
      "      \"loss\": 0.025,\n",
      "      \"step\": 108720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.85194870663852,\n",
      "      \"grad_norm\": 0.025232961401343346,\n",
      "      \"learning_rate\": 1.575456836798992e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 108740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.853208985790353,\n",
      "      \"grad_norm\": 0.002009197138249874,\n",
      "      \"learning_rate\": 1.5748267170762445e-05,\n",
      "      \"loss\": 0.0329,\n",
      "      \"step\": 108760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.854469264942185,\n",
      "      \"grad_norm\": 0.0009895736584439874,\n",
      "      \"learning_rate\": 1.5741965973534974e-05,\n",
      "      \"loss\": 0.0356,\n",
      "      \"step\": 108780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.855729544094017,\n",
      "      \"grad_norm\": 0.004939867649227381,\n",
      "      \"learning_rate\": 1.57356647763075e-05,\n",
      "      \"loss\": 0.0294,\n",
      "      \"step\": 108800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.856989823245849,\n",
      "      \"grad_norm\": 0.037947069853544235,\n",
      "      \"learning_rate\": 1.5729363579080025e-05,\n",
      "      \"loss\": 0.0125,\n",
      "      \"step\": 108820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.858250102397681,\n",
      "      \"grad_norm\": 0.001387227326631546,\n",
      "      \"learning_rate\": 1.572306238185255e-05,\n",
      "      \"loss\": 0.0124,\n",
      "      \"step\": 108840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.859510381549513,\n",
      "      \"grad_norm\": 0.7783551216125488,\n",
      "      \"learning_rate\": 1.571676118462508e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 108860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.860770660701346,\n",
      "      \"grad_norm\": 0.00035303630284033716,\n",
      "      \"learning_rate\": 1.5710459987397605e-05,\n",
      "      \"loss\": 0.0067,\n",
      "      \"step\": 108880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.862030939853177,\n",
      "      \"grad_norm\": 0.0006003589951433241,\n",
      "      \"learning_rate\": 1.5704158790170134e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 108900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.86329121900501,\n",
      "      \"grad_norm\": 0.0011965305311605334,\n",
      "      \"learning_rate\": 1.569785759294266e-05,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 108920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.864551498156842,\n",
      "      \"grad_norm\": 0.00021603095228783786,\n",
      "      \"learning_rate\": 1.5691556395715186e-05,\n",
      "      \"loss\": 0.0069,\n",
      "      \"step\": 108940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.865811777308674,\n",
      "      \"grad_norm\": 0.002150836633518338,\n",
      "      \"learning_rate\": 1.5685255198487715e-05,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 108960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.867072056460506,\n",
      "      \"grad_norm\": 0.21292056143283844,\n",
      "      \"learning_rate\": 1.567895400126024e-05,\n",
      "      \"loss\": 0.0347,\n",
      "      \"step\": 108980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.8683323356123385,\n",
      "      \"grad_norm\": 0.004118210636079311,\n",
      "      \"learning_rate\": 1.5672652804032766e-05,\n",
      "      \"loss\": 0.0059,\n",
      "      \"step\": 109000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.86959261476417,\n",
      "      \"grad_norm\": 0.010049271397292614,\n",
      "      \"learning_rate\": 1.5666351606805295e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 109020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.870852893916003,\n",
      "      \"grad_norm\": 3.068244695663452,\n",
      "      \"learning_rate\": 1.566005040957782e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 109040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.872113173067834,\n",
      "      \"grad_norm\": 0.051650308072566986,\n",
      "      \"learning_rate\": 1.5653749212350346e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 109060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.873373452219667,\n",
      "      \"grad_norm\": 0.0008030452881939709,\n",
      "      \"learning_rate\": 1.5647448015122872e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 109080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.874633731371499,\n",
      "      \"grad_norm\": 0.007102629169821739,\n",
      "      \"learning_rate\": 1.56411468178954e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 109100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.875894010523331,\n",
      "      \"grad_norm\": 0.03816420957446098,\n",
      "      \"learning_rate\": 1.563484562066793e-05,\n",
      "      \"loss\": 0.0526,\n",
      "      \"step\": 109120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.877154289675163,\n",
      "      \"grad_norm\": 0.0083634527400136,\n",
      "      \"learning_rate\": 1.5628544423440456e-05,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 109140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.8784145688269955,\n",
      "      \"grad_norm\": 0.025524621829390526,\n",
      "      \"learning_rate\": 1.562224322621298e-05,\n",
      "      \"loss\": 0.0093,\n",
      "      \"step\": 109160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.879674847978827,\n",
      "      \"grad_norm\": 0.0010769701329991221,\n",
      "      \"learning_rate\": 1.5615942028985507e-05,\n",
      "      \"loss\": 0.0313,\n",
      "      \"step\": 109180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.88093512713066,\n",
      "      \"grad_norm\": 0.0017105781007558107,\n",
      "      \"learning_rate\": 1.5609640831758036e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 109200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.8821954062824915,\n",
      "      \"grad_norm\": 0.6254060864448547,\n",
      "      \"learning_rate\": 1.5603339634530562e-05,\n",
      "      \"loss\": 0.0359,\n",
      "      \"step\": 109220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.883455685434324,\n",
      "      \"grad_norm\": 0.04943128302693367,\n",
      "      \"learning_rate\": 1.5597038437303087e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 109240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.884715964586156,\n",
      "      \"grad_norm\": 0.004652506206184626,\n",
      "      \"learning_rate\": 1.5590737240075613e-05,\n",
      "      \"loss\": 0.0046,\n",
      "      \"step\": 109260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.885976243737988,\n",
      "      \"grad_norm\": 0.0008007162250578403,\n",
      "      \"learning_rate\": 1.5584436042848142e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 109280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.88723652288982,\n",
      "      \"grad_norm\": 0.019187036901712418,\n",
      "      \"learning_rate\": 1.5578134845620668e-05,\n",
      "      \"loss\": 0.02,\n",
      "      \"step\": 109300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.888496802041653,\n",
      "      \"grad_norm\": 0.005380817223340273,\n",
      "      \"learning_rate\": 1.5571833648393193e-05,\n",
      "      \"loss\": 0.0093,\n",
      "      \"step\": 109320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.889757081193484,\n",
      "      \"grad_norm\": 0.00461457297205925,\n",
      "      \"learning_rate\": 1.5565532451165722e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 109340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.891017360345317,\n",
      "      \"grad_norm\": 0.03015115298330784,\n",
      "      \"learning_rate\": 1.555923125393825e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 109360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.8922776394971486,\n",
      "      \"grad_norm\": 0.0004156299401074648,\n",
      "      \"learning_rate\": 1.5552930056710777e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 109380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.893537918648981,\n",
      "      \"grad_norm\": 0.00044617478852160275,\n",
      "      \"learning_rate\": 1.5546628859483303e-05,\n",
      "      \"loss\": 0.0202,\n",
      "      \"step\": 109400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.894798197800813,\n",
      "      \"grad_norm\": 0.03483996540307999,\n",
      "      \"learning_rate\": 1.554032766225583e-05,\n",
      "      \"loss\": 0.016,\n",
      "      \"step\": 109420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.8960584769526445,\n",
      "      \"grad_norm\": 0.0002807669807225466,\n",
      "      \"learning_rate\": 1.5534026465028357e-05,\n",
      "      \"loss\": 0.0227,\n",
      "      \"step\": 109440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.897318756104477,\n",
      "      \"grad_norm\": 0.0027252137660980225,\n",
      "      \"learning_rate\": 1.5527725267800883e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 109460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.89857903525631,\n",
      "      \"grad_norm\": 0.07146478444337845,\n",
      "      \"learning_rate\": 1.552142407057341e-05,\n",
      "      \"loss\": 0.023,\n",
      "      \"step\": 109480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.899839314408141,\n",
      "      \"grad_norm\": 0.008615667931735516,\n",
      "      \"learning_rate\": 1.5515122873345934e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 109500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.901099593559973,\n",
      "      \"grad_norm\": 0.0038403328508138657,\n",
      "      \"learning_rate\": 1.5508821676118463e-05,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 109520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.902359872711806,\n",
      "      \"grad_norm\": 0.001126140938140452,\n",
      "      \"learning_rate\": 1.550252047889099e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 109540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.903620151863638,\n",
      "      \"grad_norm\": 0.0040579368360340595,\n",
      "      \"learning_rate\": 1.5496219281663515e-05,\n",
      "      \"loss\": 0.0043,\n",
      "      \"step\": 109560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.90488043101547,\n",
      "      \"grad_norm\": 0.037240877747535706,\n",
      "      \"learning_rate\": 1.5489918084436044e-05,\n",
      "      \"loss\": 0.0239,\n",
      "      \"step\": 109580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.906140710167302,\n",
      "      \"grad_norm\": 0.000588348601013422,\n",
      "      \"learning_rate\": 1.5483616887208573e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 109600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.907400989319134,\n",
      "      \"grad_norm\": 0.017968175932765007,\n",
      "      \"learning_rate\": 1.54773156899811e-05,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 109620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.908661268470967,\n",
      "      \"grad_norm\": 0.0015192287974059582,\n",
      "      \"learning_rate\": 1.5471014492753624e-05,\n",
      "      \"loss\": 0.0062,\n",
      "      \"step\": 109640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.909921547622798,\n",
      "      \"grad_norm\": 0.0024884017184376717,\n",
      "      \"learning_rate\": 1.546471329552615e-05,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 109660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.91118182677463,\n",
      "      \"grad_norm\": 0.006483623757958412,\n",
      "      \"learning_rate\": 1.545841209829868e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 109680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.912442105926463,\n",
      "      \"grad_norm\": 0.002767290221527219,\n",
      "      \"learning_rate\": 1.5452110901071204e-05,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 109700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.913702385078295,\n",
      "      \"grad_norm\": 0.0010292667429894209,\n",
      "      \"learning_rate\": 1.544580970384373e-05,\n",
      "      \"loss\": 0.0035,\n",
      "      \"step\": 109720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.914962664230127,\n",
      "      \"grad_norm\": 0.0006578910979442298,\n",
      "      \"learning_rate\": 1.5439508506616256e-05,\n",
      "      \"loss\": 0.0296,\n",
      "      \"step\": 109740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.916222943381959,\n",
      "      \"grad_norm\": 0.0013393920380622149,\n",
      "      \"learning_rate\": 1.5433207309388785e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 109760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.917483222533791,\n",
      "      \"grad_norm\": 0.004253422375768423,\n",
      "      \"learning_rate\": 1.542690611216131e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 109780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.918743501685624,\n",
      "      \"grad_norm\": 0.0025442049372941256,\n",
      "      \"learning_rate\": 1.542060491493384e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 109800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.9200037808374555,\n",
      "      \"grad_norm\": 0.014454183168709278,\n",
      "      \"learning_rate\": 1.5414303717706365e-05,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 109820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.921264059989287,\n",
      "      \"grad_norm\": 19.21925926208496,\n",
      "      \"learning_rate\": 1.5408002520478894e-05,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 109840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.92252433914112,\n",
      "      \"grad_norm\": 0.0001513231109129265,\n",
      "      \"learning_rate\": 1.540170132325142e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 109860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.923784618292952,\n",
      "      \"grad_norm\": 0.0011811336735263467,\n",
      "      \"learning_rate\": 1.5395400126023945e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 109880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.925044897444784,\n",
      "      \"grad_norm\": 0.028399845585227013,\n",
      "      \"learning_rate\": 1.538909892879647e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 109900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.926305176596616,\n",
      "      \"grad_norm\": 0.010570559650659561,\n",
      "      \"learning_rate\": 1.5382797731569e-05,\n",
      "      \"loss\": 0.0231,\n",
      "      \"step\": 109920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.927565455748448,\n",
      "      \"grad_norm\": 0.0015846241731196642,\n",
      "      \"learning_rate\": 1.5376496534341526e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 109940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.928825734900281,\n",
      "      \"grad_norm\": 0.004746708087623119,\n",
      "      \"learning_rate\": 1.537019533711405e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 109960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.9300860140521126,\n",
      "      \"grad_norm\": 0.0007754777325317264,\n",
      "      \"learning_rate\": 1.5363894139886577e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 109980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.931346293203944,\n",
      "      \"grad_norm\": 0.14724399149417877,\n",
      "      \"learning_rate\": 1.5357592942659106e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 110000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.932606572355777,\n",
      "      \"grad_norm\": 0.013955091126263142,\n",
      "      \"learning_rate\": 1.5351291745431635e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 110020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.933866851507609,\n",
      "      \"grad_norm\": 0.0014300462789833546,\n",
      "      \"learning_rate\": 1.534499054820416e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 110040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.935127130659441,\n",
      "      \"grad_norm\": 0.04945112392306328,\n",
      "      \"learning_rate\": 1.5338689350976686e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 110060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.936387409811273,\n",
      "      \"grad_norm\": 0.0008143085869960487,\n",
      "      \"learning_rate\": 1.5332388153749212e-05,\n",
      "      \"loss\": 0.0282,\n",
      "      \"step\": 110080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.937647688963105,\n",
      "      \"grad_norm\": 0.0010850117541849613,\n",
      "      \"learning_rate\": 1.532608695652174e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 110100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.938907968114938,\n",
      "      \"grad_norm\": 0.00326952850446105,\n",
      "      \"learning_rate\": 1.5319785759294267e-05,\n",
      "      \"loss\": 0.0247,\n",
      "      \"step\": 110120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.94016824726677,\n",
      "      \"grad_norm\": 0.005488715134561062,\n",
      "      \"learning_rate\": 1.5313484562066792e-05,\n",
      "      \"loss\": 0.0018,\n",
      "      \"step\": 110140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.941428526418601,\n",
      "      \"grad_norm\": 0.00030483369482681155,\n",
      "      \"learning_rate\": 1.5307183364839318e-05,\n",
      "      \"loss\": 0.0331,\n",
      "      \"step\": 110160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.942688805570434,\n",
      "      \"grad_norm\": 0.014497676864266396,\n",
      "      \"learning_rate\": 1.5300882167611847e-05,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 110180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.9439490847222665,\n",
      "      \"grad_norm\": 0.0004473730514291674,\n",
      "      \"learning_rate\": 1.5294580970384373e-05,\n",
      "      \"loss\": 0.017,\n",
      "      \"step\": 110200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.945209363874098,\n",
      "      \"grad_norm\": 10.078654289245605,\n",
      "      \"learning_rate\": 1.52882797731569e-05,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 110220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.94646964302593,\n",
      "      \"grad_norm\": 0.0001655233500059694,\n",
      "      \"learning_rate\": 1.5281978575929428e-05,\n",
      "      \"loss\": 0.0349,\n",
      "      \"step\": 110240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.947729922177762,\n",
      "      \"grad_norm\": 0.026087094098329544,\n",
      "      \"learning_rate\": 1.5275677378701957e-05,\n",
      "      \"loss\": 0.0105,\n",
      "      \"step\": 110260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.948990201329595,\n",
      "      \"grad_norm\": 0.004934697411954403,\n",
      "      \"learning_rate\": 1.5269376181474482e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 110280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.950250480481427,\n",
      "      \"grad_norm\": 0.06895933300256729,\n",
      "      \"learning_rate\": 1.5263074984247008e-05,\n",
      "      \"loss\": 0.0546,\n",
      "      \"step\": 110300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.951510759633258,\n",
      "      \"grad_norm\": 0.04863622412085533,\n",
      "      \"learning_rate\": 1.5256773787019535e-05,\n",
      "      \"loss\": 0.0399,\n",
      "      \"step\": 110320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.952771038785091,\n",
      "      \"grad_norm\": 4.977039813995361,\n",
      "      \"learning_rate\": 1.5250472589792061e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 110340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.954031317936923,\n",
      "      \"grad_norm\": 0.007442661095410585,\n",
      "      \"learning_rate\": 1.5244171392564588e-05,\n",
      "      \"loss\": 0.0116,\n",
      "      \"step\": 110360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.955291597088755,\n",
      "      \"grad_norm\": 0.6118318438529968,\n",
      "      \"learning_rate\": 1.5237870195337114e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 110380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.956551876240587,\n",
      "      \"grad_norm\": 0.0028202200774103403,\n",
      "      \"learning_rate\": 1.5231568998109641e-05,\n",
      "      \"loss\": 0.0303,\n",
      "      \"step\": 110400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.9578121553924195,\n",
      "      \"grad_norm\": 0.006159901153296232,\n",
      "      \"learning_rate\": 1.5225267800882167e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 110420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.959072434544251,\n",
      "      \"grad_norm\": 0.0010831956751644611,\n",
      "      \"learning_rate\": 1.5218966603654694e-05,\n",
      "      \"loss\": 0.0029,\n",
      "      \"step\": 110440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.960332713696084,\n",
      "      \"grad_norm\": 0.0019601474050432444,\n",
      "      \"learning_rate\": 1.521266540642722e-05,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 110460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.961592992847915,\n",
      "      \"grad_norm\": 0.0008460350800305605,\n",
      "      \"learning_rate\": 1.520636420919975e-05,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 110480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.962853271999748,\n",
      "      \"grad_norm\": 0.004511416424065828,\n",
      "      \"learning_rate\": 1.5200063011972276e-05,\n",
      "      \"loss\": 0.0255,\n",
      "      \"step\": 110500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.96411355115158,\n",
      "      \"grad_norm\": 0.09314680099487305,\n",
      "      \"learning_rate\": 1.5193761814744804e-05,\n",
      "      \"loss\": 0.0222,\n",
      "      \"step\": 110520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.965373830303412,\n",
      "      \"grad_norm\": 0.006219709757715464,\n",
      "      \"learning_rate\": 1.518746061751733e-05,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 110540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.966634109455244,\n",
      "      \"grad_norm\": 55.48247146606445,\n",
      "      \"learning_rate\": 1.5181159420289857e-05,\n",
      "      \"loss\": 0.0035,\n",
      "      \"step\": 110560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.9678943886070766,\n",
      "      \"grad_norm\": 0.0006913793040439487,\n",
      "      \"learning_rate\": 1.5174858223062382e-05,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 110580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.969154667758908,\n",
      "      \"grad_norm\": 0.005929479841142893,\n",
      "      \"learning_rate\": 1.516855702583491e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 110600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.970414946910741,\n",
      "      \"grad_norm\": 0.001998740015551448,\n",
      "      \"learning_rate\": 1.5162255828607435e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 110620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.9716752260625725,\n",
      "      \"grad_norm\": 0.04036388918757439,\n",
      "      \"learning_rate\": 1.5155954631379963e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 110640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.972935505214405,\n",
      "      \"grad_norm\": 0.0005481003317981958,\n",
      "      \"learning_rate\": 1.5149653434152488e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 110660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.974195784366237,\n",
      "      \"grad_norm\": 0.0004673657240346074,\n",
      "      \"learning_rate\": 1.5143352236925016e-05,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 110680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.975456063518069,\n",
      "      \"grad_norm\": 0.009188386611640453,\n",
      "      \"learning_rate\": 1.5137051039697545e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 110700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.976716342669901,\n",
      "      \"grad_norm\": 0.00023694658011663705,\n",
      "      \"learning_rate\": 1.5130749842470072e-05,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 110720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.977976621821734,\n",
      "      \"grad_norm\": 0.001073898863978684,\n",
      "      \"learning_rate\": 1.5124448645242598e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 110740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.979236900973565,\n",
      "      \"grad_norm\": 0.0017211752710863948,\n",
      "      \"learning_rate\": 1.5118147448015125e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 110760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.980497180125398,\n",
      "      \"grad_norm\": 0.0038234007079154253,\n",
      "      \"learning_rate\": 1.511184625078765e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 110780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.98175745927723,\n",
      "      \"grad_norm\": 0.00026248188805766404,\n",
      "      \"learning_rate\": 1.5105545053560178e-05,\n",
      "      \"loss\": 0.0172,\n",
      "      \"step\": 110800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.983017738429062,\n",
      "      \"grad_norm\": 0.0013854635180905461,\n",
      "      \"learning_rate\": 1.5099243856332704e-05,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 110820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.984278017580894,\n",
      "      \"grad_norm\": 0.0003712873440235853,\n",
      "      \"learning_rate\": 1.5092942659105231e-05,\n",
      "      \"loss\": 0.0312,\n",
      "      \"step\": 110840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.985538296732726,\n",
      "      \"grad_norm\": 0.0009032660746015608,\n",
      "      \"learning_rate\": 1.5086641461877757e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 110860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.986798575884558,\n",
      "      \"grad_norm\": 0.008687109686434269,\n",
      "      \"learning_rate\": 1.5080340264650284e-05,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 110880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.988058855036391,\n",
      "      \"grad_norm\": 0.0002491203194949776,\n",
      "      \"learning_rate\": 1.507403906742281e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 110900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.989319134188222,\n",
      "      \"grad_norm\": 0.018569760024547577,\n",
      "      \"learning_rate\": 1.5067737870195339e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 110920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.990579413340055,\n",
      "      \"grad_norm\": 0.000265832815784961,\n",
      "      \"learning_rate\": 1.5061436672967866e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 110940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.991839692491887,\n",
      "      \"grad_norm\": 0.0011243846965953708,\n",
      "      \"learning_rate\": 1.5055135475740392e-05,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 110960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.993099971643719,\n",
      "      \"grad_norm\": 0.0008214768022298813,\n",
      "      \"learning_rate\": 1.5048834278512919e-05,\n",
      "      \"loss\": 0.017,\n",
      "      \"step\": 110980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.994360250795551,\n",
      "      \"grad_norm\": 0.010075805708765984,\n",
      "      \"learning_rate\": 1.5042533081285445e-05,\n",
      "      \"loss\": 0.0115,\n",
      "      \"step\": 111000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.9956205299473835,\n",
      "      \"grad_norm\": 0.003959765657782555,\n",
      "      \"learning_rate\": 1.5036231884057972e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 111020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.996880809099215,\n",
      "      \"grad_norm\": 0.0008540104026906192,\n",
      "      \"learning_rate\": 1.5029930686830498e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 111040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.998141088251048,\n",
      "      \"grad_norm\": 0.35466861724853516,\n",
      "      \"learning_rate\": 1.5023629489603025e-05,\n",
      "      \"loss\": 0.0201,\n",
      "      \"step\": 111060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.999401367402879,\n",
      "      \"grad_norm\": 0.000882082968018949,\n",
      "      \"learning_rate\": 1.5017328292375552e-05,\n",
      "      \"loss\": 0.006,\n",
      "      \"step\": 111080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.0,\n",
      "      \"eval_accuracy\": 0.9997794719929431,\n",
      "      \"eval_f1\": 0.9997794789402388,\n",
      "      \"eval_loss\": 0.001036217319779098,\n",
      "      \"eval_precision\": 0.9997479838709677,\n",
      "      \"eval_recall\": 0.9998109759939512,\n",
      "      \"eval_runtime\": 585.41,\n",
      "      \"eval_samples_per_second\": 54.222,\n",
      "      \"eval_steps_per_second\": 6.778,\n",
      "      \"step\": 111090\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.000630139575916,\n",
      "      \"grad_norm\": 0.0012657576007768512,\n",
      "      \"learning_rate\": 1.5011027095148078e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 111100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.001890418727748,\n",
      "      \"grad_norm\": 0.0026836893521249294,\n",
      "      \"learning_rate\": 1.5004725897920605e-05,\n",
      "      \"loss\": 0.0294,\n",
      "      \"step\": 111120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.0031506978795806,\n",
      "      \"grad_norm\": 0.007930541411042213,\n",
      "      \"learning_rate\": 1.4998424700693131e-05,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 111140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.004410977031412,\n",
      "      \"grad_norm\": 0.015964217483997345,\n",
      "      \"learning_rate\": 1.499212350346566e-05,\n",
      "      \"loss\": 0.0227,\n",
      "      \"step\": 111160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.005671256183245,\n",
      "      \"grad_norm\": 0.004682738799601793,\n",
      "      \"learning_rate\": 1.498613736609956e-05,\n",
      "      \"loss\": 0.0028,\n",
      "      \"step\": 111180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.0069315353350765,\n",
      "      \"grad_norm\": 0.0019683856517076492,\n",
      "      \"learning_rate\": 1.4979836168872085e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 111200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.008191814486909,\n",
      "      \"grad_norm\": 0.0009165548253804445,\n",
      "      \"learning_rate\": 1.4973534971644612e-05,\n",
      "      \"loss\": 0.0124,\n",
      "      \"step\": 111220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.009452093638741,\n",
      "      \"grad_norm\": 0.00043644188554026186,\n",
      "      \"learning_rate\": 1.496723377441714e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 111240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.010712372790573,\n",
      "      \"grad_norm\": 0.007678766269236803,\n",
      "      \"learning_rate\": 1.4960932577189669e-05,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 111260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.011972651942405,\n",
      "      \"grad_norm\": 0.0008280428592115641,\n",
      "      \"learning_rate\": 1.4954631379962194e-05,\n",
      "      \"loss\": 0.0325,\n",
      "      \"step\": 111280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.013232931094238,\n",
      "      \"grad_norm\": 1.6550780534744263,\n",
      "      \"learning_rate\": 1.4948330182734722e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 111300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.014493210246069,\n",
      "      \"grad_norm\": 0.009832622483372688,\n",
      "      \"learning_rate\": 1.4942028985507247e-05,\n",
      "      \"loss\": 0.0235,\n",
      "      \"step\": 111320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.015753489397902,\n",
      "      \"grad_norm\": 0.23248399794101715,\n",
      "      \"learning_rate\": 1.4935727788279775e-05,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 111340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.017013768549734,\n",
      "      \"grad_norm\": 0.0010059457272291183,\n",
      "      \"learning_rate\": 1.49294265910523e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 111360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.018274047701566,\n",
      "      \"grad_norm\": 0.08474384248256683,\n",
      "      \"learning_rate\": 1.4923125393824828e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 111380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.019534326853398,\n",
      "      \"grad_norm\": 0.016896985471248627,\n",
      "      \"learning_rate\": 1.4916824196597353e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 111400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.02079460600523,\n",
      "      \"grad_norm\": 0.005686615128070116,\n",
      "      \"learning_rate\": 1.491052299936988e-05,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 111420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.022054885157062,\n",
      "      \"grad_norm\": 0.00806407630443573,\n",
      "      \"learning_rate\": 1.4904221802142406e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 111440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.023315164308895,\n",
      "      \"grad_norm\": 0.010211918503046036,\n",
      "      \"learning_rate\": 1.4897920604914934e-05,\n",
      "      \"loss\": 0.023,\n",
      "      \"step\": 111460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.024575443460726,\n",
      "      \"grad_norm\": 0.020232588052749634,\n",
      "      \"learning_rate\": 1.4891619407687463e-05,\n",
      "      \"loss\": 0.0211,\n",
      "      \"step\": 111480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.025835722612559,\n",
      "      \"grad_norm\": 0.00026513871853239834,\n",
      "      \"learning_rate\": 1.488531821045999e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 111500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.027096001764391,\n",
      "      \"grad_norm\": 0.0002622136380523443,\n",
      "      \"learning_rate\": 1.4879017013232516e-05,\n",
      "      \"loss\": 0.0064,\n",
      "      \"step\": 111520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.028356280916223,\n",
      "      \"grad_norm\": 0.0003820976708084345,\n",
      "      \"learning_rate\": 1.4872715816005043e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 111540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.029616560068055,\n",
      "      \"grad_norm\": 0.00025120965437963605,\n",
      "      \"learning_rate\": 1.4866414618777569e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 111560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.0308768392198875,\n",
      "      \"grad_norm\": 0.00016225507715716958,\n",
      "      \"learning_rate\": 1.4860113421550096e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 111580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.032137118371719,\n",
      "      \"grad_norm\": 0.06965028494596481,\n",
      "      \"learning_rate\": 1.4853812224322622e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 111600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.033397397523552,\n",
      "      \"grad_norm\": 0.0018611011328175664,\n",
      "      \"learning_rate\": 1.4847511027095149e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 111620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.034657676675383,\n",
      "      \"grad_norm\": 0.00021870792261324823,\n",
      "      \"learning_rate\": 1.4841209829867675e-05,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 111640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.035917955827216,\n",
      "      \"grad_norm\": 0.02188381552696228,\n",
      "      \"learning_rate\": 1.4834908632640202e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 111660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.037178234979048,\n",
      "      \"grad_norm\": 0.0001732004020595923,\n",
      "      \"learning_rate\": 1.4828607435412728e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 111680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.03843851413088,\n",
      "      \"grad_norm\": 0.07546132802963257,\n",
      "      \"learning_rate\": 1.4822306238185255e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 111700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.039698793282712,\n",
      "      \"grad_norm\": 0.000174848479218781,\n",
      "      \"learning_rate\": 1.4816005040957784e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 111720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.0409590724345446,\n",
      "      \"grad_norm\": 0.02933935448527336,\n",
      "      \"learning_rate\": 1.480970384373031e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 111740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.042219351586376,\n",
      "      \"grad_norm\": 0.014536773785948753,\n",
      "      \"learning_rate\": 1.4803402646502837e-05,\n",
      "      \"loss\": 0.0212,\n",
      "      \"step\": 111760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.043479630738209,\n",
      "      \"grad_norm\": 0.0005674221902154386,\n",
      "      \"learning_rate\": 1.4797101449275364e-05,\n",
      "      \"loss\": 0.0132,\n",
      "      \"step\": 111780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.0447399098900405,\n",
      "      \"grad_norm\": 0.004313579294830561,\n",
      "      \"learning_rate\": 1.479080025204789e-05,\n",
      "      \"loss\": 0.0272,\n",
      "      \"step\": 111800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.046000189041873,\n",
      "      \"grad_norm\": 0.0005884470883756876,\n",
      "      \"learning_rate\": 1.4784499054820417e-05,\n",
      "      \"loss\": 0.0105,\n",
      "      \"step\": 111820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.047260468193705,\n",
      "      \"grad_norm\": 0.00017963987193070352,\n",
      "      \"learning_rate\": 1.4778197857592943e-05,\n",
      "      \"loss\": 0.0221,\n",
      "      \"step\": 111840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.048520747345537,\n",
      "      \"grad_norm\": 0.05822773277759552,\n",
      "      \"learning_rate\": 1.477189666036547e-05,\n",
      "      \"loss\": 0.062,\n",
      "      \"step\": 111860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.049781026497369,\n",
      "      \"grad_norm\": 0.0008525626617483795,\n",
      "      \"learning_rate\": 1.4765595463137996e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 111880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.051041305649202,\n",
      "      \"grad_norm\": 0.002625421155244112,\n",
      "      \"learning_rate\": 1.4759294265910523e-05,\n",
      "      \"loss\": 0.0279,\n",
      "      \"step\": 111900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.052301584801033,\n",
      "      \"grad_norm\": 0.0006649586139246821,\n",
      "      \"learning_rate\": 1.4752993068683049e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 111920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.053561863952866,\n",
      "      \"grad_norm\": 0.0004739620489999652,\n",
      "      \"learning_rate\": 1.4746691871455578e-05,\n",
      "      \"loss\": 0.0056,\n",
      "      \"step\": 111940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.054822143104698,\n",
      "      \"grad_norm\": 0.12469391524791718,\n",
      "      \"learning_rate\": 1.4740390674228105e-05,\n",
      "      \"loss\": 0.013,\n",
      "      \"step\": 111960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.05608242225653,\n",
      "      \"grad_norm\": 0.0007217824459075928,\n",
      "      \"learning_rate\": 1.4734089477000631e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 111980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.057342701408362,\n",
      "      \"grad_norm\": 0.017922354862093925,\n",
      "      \"learning_rate\": 1.4727788279773158e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 112000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.058602980560194,\n",
      "      \"grad_norm\": 0.004097905475646257,\n",
      "      \"learning_rate\": 1.4721487082545684e-05,\n",
      "      \"loss\": 0.0252,\n",
      "      \"step\": 112020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.059863259712026,\n",
      "      \"grad_norm\": 14.58777141571045,\n",
      "      \"learning_rate\": 1.4715500945179587e-05,\n",
      "      \"loss\": 0.0324,\n",
      "      \"step\": 112040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.061123538863859,\n",
      "      \"grad_norm\": 0.00020569964544847608,\n",
      "      \"learning_rate\": 1.4709199747952112e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 112060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.06238381801569,\n",
      "      \"grad_norm\": 2.6314697265625,\n",
      "      \"learning_rate\": 1.470289855072464e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 112080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.063644097167523,\n",
      "      \"grad_norm\": 0.0017311325063928962,\n",
      "      \"learning_rate\": 1.4696597353497165e-05,\n",
      "      \"loss\": 0.0533,\n",
      "      \"step\": 112100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.064904376319355,\n",
      "      \"grad_norm\": 0.009238183498382568,\n",
      "      \"learning_rate\": 1.4690296156269693e-05,\n",
      "      \"loss\": 0.0065,\n",
      "      \"step\": 112120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.066164655471187,\n",
      "      \"grad_norm\": 0.00047862710198387504,\n",
      "      \"learning_rate\": 1.4683994959042218e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 112140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.067424934623019,\n",
      "      \"grad_norm\": 5.10548734664917,\n",
      "      \"learning_rate\": 1.4677693761814746e-05,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 112160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.0686852137748515,\n",
      "      \"grad_norm\": 1.024289608001709,\n",
      "      \"learning_rate\": 1.4671392564587271e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 112180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.069945492926683,\n",
      "      \"grad_norm\": 0.0004881648637820035,\n",
      "      \"learning_rate\": 1.4665091367359799e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 112200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.071205772078516,\n",
      "      \"grad_norm\": 0.0031339689157903194,\n",
      "      \"learning_rate\": 1.4658790170132324e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 112220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.072466051230347,\n",
      "      \"grad_norm\": 0.001351954648271203,\n",
      "      \"learning_rate\": 1.4652488972904852e-05,\n",
      "      \"loss\": 0.0281,\n",
      "      \"step\": 112240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.07372633038218,\n",
      "      \"grad_norm\": 0.00039964664028957486,\n",
      "      \"learning_rate\": 1.464618777567738e-05,\n",
      "      \"loss\": 0.0061,\n",
      "      \"step\": 112260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.074986609534012,\n",
      "      \"grad_norm\": 0.15431435406208038,\n",
      "      \"learning_rate\": 1.4639886578449908e-05,\n",
      "      \"loss\": 0.0283,\n",
      "      \"step\": 112280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.076246888685844,\n",
      "      \"grad_norm\": 0.5296014547348022,\n",
      "      \"learning_rate\": 1.4633585381222434e-05,\n",
      "      \"loss\": 0.0238,\n",
      "      \"step\": 112300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.077507167837676,\n",
      "      \"grad_norm\": 0.0015285394620150328,\n",
      "      \"learning_rate\": 1.4627284183994961e-05,\n",
      "      \"loss\": 0.0305,\n",
      "      \"step\": 112320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.0787674469895085,\n",
      "      \"grad_norm\": 0.01383171696215868,\n",
      "      \"learning_rate\": 1.4620982986767487e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 112340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.08002772614134,\n",
      "      \"grad_norm\": 0.004405132960528135,\n",
      "      \"learning_rate\": 1.4614681789540014e-05,\n",
      "      \"loss\": 0.021,\n",
      "      \"step\": 112360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.081288005293173,\n",
      "      \"grad_norm\": 0.3051109313964844,\n",
      "      \"learning_rate\": 1.460838059231254e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 112380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.0825482844450045,\n",
      "      \"grad_norm\": 0.0442645363509655,\n",
      "      \"learning_rate\": 1.4602079395085067e-05,\n",
      "      \"loss\": 0.0266,\n",
      "      \"step\": 112400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.083808563596837,\n",
      "      \"grad_norm\": 0.0003625748213380575,\n",
      "      \"learning_rate\": 1.4595778197857593e-05,\n",
      "      \"loss\": 0.0228,\n",
      "      \"step\": 112420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.085068842748669,\n",
      "      \"grad_norm\": 0.03926757350564003,\n",
      "      \"learning_rate\": 1.458947700063012e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 112440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.086329121900501,\n",
      "      \"grad_norm\": 0.00032083579571917653,\n",
      "      \"learning_rate\": 1.4583175803402646e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 112460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.087589401052333,\n",
      "      \"grad_norm\": 0.00022407557116821408,\n",
      "      \"learning_rate\": 1.4576874606175173e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 112480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.088849680204166,\n",
      "      \"grad_norm\": 0.0011087484890595078,\n",
      "      \"learning_rate\": 1.4570573408947702e-05,\n",
      "      \"loss\": 0.0356,\n",
      "      \"step\": 112500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.090109959355997,\n",
      "      \"grad_norm\": 0.00019594274635892361,\n",
      "      \"learning_rate\": 1.456427221172023e-05,\n",
      "      \"loss\": 0.0073,\n",
      "      \"step\": 112520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.09137023850783,\n",
      "      \"grad_norm\": 0.0025684030260890722,\n",
      "      \"learning_rate\": 1.4557971014492755e-05,\n",
      "      \"loss\": 0.037,\n",
      "      \"step\": 112540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.092630517659662,\n",
      "      \"grad_norm\": 0.03751349449157715,\n",
      "      \"learning_rate\": 1.4551984877126654e-05,\n",
      "      \"loss\": 0.0276,\n",
      "      \"step\": 112560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.093890796811494,\n",
      "      \"grad_norm\": 0.3057771921157837,\n",
      "      \"learning_rate\": 1.454568367989918e-05,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 112580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.095151075963326,\n",
      "      \"grad_norm\": 0.3857063353061676,\n",
      "      \"learning_rate\": 1.4539382482671709e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 112600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.096411355115158,\n",
      "      \"grad_norm\": 0.027708750218153,\n",
      "      \"learning_rate\": 1.4533081285444236e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 112620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.09767163426699,\n",
      "      \"grad_norm\": 0.2752816677093506,\n",
      "      \"learning_rate\": 1.4526780088216762e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 112640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.098931913418823,\n",
      "      \"grad_norm\": 0.0012136590667068958,\n",
      "      \"learning_rate\": 1.452047889098929e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 112660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.100192192570654,\n",
      "      \"grad_norm\": 0.0015420380514115095,\n",
      "      \"learning_rate\": 1.4514177693761817e-05,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 112680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.101452471722487,\n",
      "      \"grad_norm\": 0.000548043695744127,\n",
      "      \"learning_rate\": 1.4507876496534342e-05,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 112700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.102712750874319,\n",
      "      \"grad_norm\": 0.004925621673464775,\n",
      "      \"learning_rate\": 1.450157529930687e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 112720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.103973030026151,\n",
      "      \"grad_norm\": 0.000323886692058295,\n",
      "      \"learning_rate\": 1.4495274102079395e-05,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 112740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.105233309177983,\n",
      "      \"grad_norm\": 0.11839644610881805,\n",
      "      \"learning_rate\": 1.4488972904851923e-05,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 112760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.1064935883298155,\n",
      "      \"grad_norm\": 0.0002350910217501223,\n",
      "      \"learning_rate\": 1.4482671707624448e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 112780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.107753867481647,\n",
      "      \"grad_norm\": 0.00031727825989946723,\n",
      "      \"learning_rate\": 1.4476370510396976e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 112800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.10901414663348,\n",
      "      \"grad_norm\": 0.057123199105262756,\n",
      "      \"learning_rate\": 1.4470069313169505e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 112820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.110274425785311,\n",
      "      \"grad_norm\": 0.00020940296235494316,\n",
      "      \"learning_rate\": 1.446376811594203e-05,\n",
      "      \"loss\": 0.0246,\n",
      "      \"step\": 112840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.111534704937144,\n",
      "      \"grad_norm\": 0.0002104634331772104,\n",
      "      \"learning_rate\": 1.4457466918714558e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 112860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.112794984088976,\n",
      "      \"grad_norm\": 0.020412322133779526,\n",
      "      \"learning_rate\": 1.4451165721487083e-05,\n",
      "      \"loss\": 0.0041,\n",
      "      \"step\": 112880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.114055263240807,\n",
      "      \"grad_norm\": 0.01963650807738304,\n",
      "      \"learning_rate\": 1.444486452425961e-05,\n",
      "      \"loss\": 0.0065,\n",
      "      \"step\": 112900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.11531554239264,\n",
      "      \"grad_norm\": 0.0004745492187794298,\n",
      "      \"learning_rate\": 1.4438563327032136e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 112920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.1165758215444725,\n",
      "      \"grad_norm\": 0.000238617358263582,\n",
      "      \"learning_rate\": 1.4432262129804664e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 112940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.117836100696304,\n",
      "      \"grad_norm\": 0.0009821183048188686,\n",
      "      \"learning_rate\": 1.442596093257719e-05,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 112960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.119096379848136,\n",
      "      \"grad_norm\": 0.0020462318789213896,\n",
      "      \"learning_rate\": 1.4419659735349717e-05,\n",
      "      \"loss\": 0.029,\n",
      "      \"step\": 112980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.1203566589999685,\n",
      "      \"grad_norm\": 0.0012692204909399152,\n",
      "      \"learning_rate\": 1.4413358538122242e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 113000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.1216169381518,\n",
      "      \"grad_norm\": 0.0002539987035561353,\n",
      "      \"learning_rate\": 1.440705734089477e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 113020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.122877217303633,\n",
      "      \"grad_norm\": 0.00033077356056310236,\n",
      "      \"learning_rate\": 1.4400756143667299e-05,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 113040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.1241374964554645,\n",
      "      \"grad_norm\": 0.0037054852582514286,\n",
      "      \"learning_rate\": 1.4394454946439826e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 113060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.125397775607297,\n",
      "      \"grad_norm\": 0.006993443239480257,\n",
      "      \"learning_rate\": 1.4388153749212352e-05,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 113080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.126658054759129,\n",
      "      \"grad_norm\": 0.000244398252107203,\n",
      "      \"learning_rate\": 1.4381852551984879e-05,\n",
      "      \"loss\": 0.0188,\n",
      "      \"step\": 113100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.127918333910961,\n",
      "      \"grad_norm\": 0.0886397734284401,\n",
      "      \"learning_rate\": 1.4375551354757405e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 113120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.129178613062793,\n",
      "      \"grad_norm\": 0.00020845269318670034,\n",
      "      \"learning_rate\": 1.4369250157529932e-05,\n",
      "      \"loss\": 0.0041,\n",
      "      \"step\": 113140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.130438892214626,\n",
      "      \"grad_norm\": 0.00013773524551652372,\n",
      "      \"learning_rate\": 1.4362948960302458e-05,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 113160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.131699171366457,\n",
      "      \"grad_norm\": 0.0029994139913469553,\n",
      "      \"learning_rate\": 1.4356647763074985e-05,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 113180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.13295945051829,\n",
      "      \"grad_norm\": 0.00016832922119647264,\n",
      "      \"learning_rate\": 1.435034656584751e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 113200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.1342197296701215,\n",
      "      \"grad_norm\": 0.00021143272169865668,\n",
      "      \"learning_rate\": 1.4344045368620038e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 113220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.135480008821954,\n",
      "      \"grad_norm\": 0.15894731879234314,\n",
      "      \"learning_rate\": 1.4337744171392564e-05,\n",
      "      \"loss\": 0.0482,\n",
      "      \"step\": 113240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.136740287973786,\n",
      "      \"grad_norm\": 0.00017993261280935258,\n",
      "      \"learning_rate\": 1.4331442974165091e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 113260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.138000567125618,\n",
      "      \"grad_norm\": 0.0002129430795321241,\n",
      "      \"learning_rate\": 1.432514177693762e-05,\n",
      "      \"loss\": 0.0056,\n",
      "      \"step\": 113280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.13926084627745,\n",
      "      \"grad_norm\": 0.000248421827564016,\n",
      "      \"learning_rate\": 1.4318840579710147e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 113300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.140521125429283,\n",
      "      \"grad_norm\": 0.00023845446412451565,\n",
      "      \"learning_rate\": 1.4312539382482673e-05,\n",
      "      \"loss\": 0.0298,\n",
      "      \"step\": 113320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.141781404581114,\n",
      "      \"grad_norm\": 0.2291291058063507,\n",
      "      \"learning_rate\": 1.43062381852552e-05,\n",
      "      \"loss\": 0.0197,\n",
      "      \"step\": 113340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.143041683732947,\n",
      "      \"grad_norm\": 14.995170593261719,\n",
      "      \"learning_rate\": 1.4299936988027726e-05,\n",
      "      \"loss\": 0.0077,\n",
      "      \"step\": 113360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.144301962884779,\n",
      "      \"grad_norm\": 0.1051713228225708,\n",
      "      \"learning_rate\": 1.4293635790800253e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 113380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.145562242036611,\n",
      "      \"grad_norm\": 0.02583727054297924,\n",
      "      \"learning_rate\": 1.4287334593572779e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 113400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.146822521188443,\n",
      "      \"grad_norm\": 0.010230179876089096,\n",
      "      \"learning_rate\": 1.4281033396345306e-05,\n",
      "      \"loss\": 0.0182,\n",
      "      \"step\": 113420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.148082800340275,\n",
      "      \"grad_norm\": 0.00273062358610332,\n",
      "      \"learning_rate\": 1.4274732199117832e-05,\n",
      "      \"loss\": 0.0329,\n",
      "      \"step\": 113440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.149343079492107,\n",
      "      \"grad_norm\": 0.0013673322973772883,\n",
      "      \"learning_rate\": 1.426843100189036e-05,\n",
      "      \"loss\": 0.0311,\n",
      "      \"step\": 113460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.15060335864394,\n",
      "      \"grad_norm\": 0.002673404524102807,\n",
      "      \"learning_rate\": 1.4262129804662885e-05,\n",
      "      \"loss\": 0.02,\n",
      "      \"step\": 113480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.151863637795771,\n",
      "      \"grad_norm\": 0.0017198872519657016,\n",
      "      \"learning_rate\": 1.4255828607435414e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 113500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.153123916947604,\n",
      "      \"grad_norm\": 0.00048794076428748667,\n",
      "      \"learning_rate\": 1.4249527410207941e-05,\n",
      "      \"loss\": 0.0271,\n",
      "      \"step\": 113520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.154384196099436,\n",
      "      \"grad_norm\": 0.11538393050432205,\n",
      "      \"learning_rate\": 1.4243226212980467e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 113540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.155644475251268,\n",
      "      \"grad_norm\": 0.03285098820924759,\n",
      "      \"learning_rate\": 1.4236925015752994e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 113560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.1569047544031,\n",
      "      \"grad_norm\": 0.04923637583851814,\n",
      "      \"learning_rate\": 1.4230623818525522e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 113580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.1581650335549325,\n",
      "      \"grad_norm\": 0.0036235915031284094,\n",
      "      \"learning_rate\": 1.4224322621298047e-05,\n",
      "      \"loss\": 0.0379,\n",
      "      \"step\": 113600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.159425312706764,\n",
      "      \"grad_norm\": 0.01487757358700037,\n",
      "      \"learning_rate\": 1.4218021424070575e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 113620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.160685591858597,\n",
      "      \"grad_norm\": 0.008735265582799911,\n",
      "      \"learning_rate\": 1.42117202268431e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 113640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.1619458710104285,\n",
      "      \"grad_norm\": 0.0047021666541695595,\n",
      "      \"learning_rate\": 1.4205419029615628e-05,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 113660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.163206150162261,\n",
      "      \"grad_norm\": 0.0008418092038482428,\n",
      "      \"learning_rate\": 1.4199117832388153e-05,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 113680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.164466429314093,\n",
      "      \"grad_norm\": 0.000497056869789958,\n",
      "      \"learning_rate\": 1.419281663516068e-05,\n",
      "      \"loss\": 0.021,\n",
      "      \"step\": 113700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.165726708465925,\n",
      "      \"grad_norm\": 0.0017335773445665836,\n",
      "      \"learning_rate\": 1.418651543793321e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 113720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.166986987617757,\n",
      "      \"grad_norm\": 0.17127954959869385,\n",
      "      \"learning_rate\": 1.4180214240705735e-05,\n",
      "      \"loss\": 0.0312,\n",
      "      \"step\": 113740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.16824726676959,\n",
      "      \"grad_norm\": 0.002056380733847618,\n",
      "      \"learning_rate\": 1.4173913043478263e-05,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 113760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.169507545921421,\n",
      "      \"grad_norm\": 0.016521120443940163,\n",
      "      \"learning_rate\": 1.4167611846250788e-05,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 113780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.170767825073254,\n",
      "      \"grad_norm\": 0.0025477278977632523,\n",
      "      \"learning_rate\": 1.4161310649023316e-05,\n",
      "      \"loss\": 0.033,\n",
      "      \"step\": 113800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.1720281042250855,\n",
      "      \"grad_norm\": 0.0196840763092041,\n",
      "      \"learning_rate\": 1.4155009451795841e-05,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 113820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.173288383376918,\n",
      "      \"grad_norm\": 0.0359262079000473,\n",
      "      \"learning_rate\": 1.4148708254568369e-05,\n",
      "      \"loss\": 0.0184,\n",
      "      \"step\": 113840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.17454866252875,\n",
      "      \"grad_norm\": 0.0026369441766291857,\n",
      "      \"learning_rate\": 1.4142407057340894e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 113860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.175808941680582,\n",
      "      \"grad_norm\": 0.041517969220876694,\n",
      "      \"learning_rate\": 1.4136105860113422e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 113880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.177069220832414,\n",
      "      \"grad_norm\": 0.002376285847276449,\n",
      "      \"learning_rate\": 1.4129804662885949e-05,\n",
      "      \"loss\": 0.04,\n",
      "      \"step\": 113900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.178329499984247,\n",
      "      \"grad_norm\": 0.0008490423788316548,\n",
      "      \"learning_rate\": 1.4123503465658475e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 113920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.179589779136078,\n",
      "      \"grad_norm\": 0.047495197504758835,\n",
      "      \"learning_rate\": 1.4117202268431002e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 113940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.180850058287911,\n",
      "      \"grad_norm\": 0.5398576259613037,\n",
      "      \"learning_rate\": 1.4110901071203531e-05,\n",
      "      \"loss\": 0.0633,\n",
      "      \"step\": 113960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.182110337439743,\n",
      "      \"grad_norm\": 0.060952041298151016,\n",
      "      \"learning_rate\": 1.4104599873976057e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 113980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.183370616591575,\n",
      "      \"grad_norm\": 0.0046154106967151165,\n",
      "      \"learning_rate\": 1.4098298676748584e-05,\n",
      "      \"loss\": 0.006,\n",
      "      \"step\": 114000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.184630895743407,\n",
      "      \"grad_norm\": 0.00397549057379365,\n",
      "      \"learning_rate\": 1.409199747952111e-05,\n",
      "      \"loss\": 0.0052,\n",
      "      \"step\": 114020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.185891174895239,\n",
      "      \"grad_norm\": 0.0028118379414081573,\n",
      "      \"learning_rate\": 1.4085696282293637e-05,\n",
      "      \"loss\": 0.0145,\n",
      "      \"step\": 114040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.187151454047071,\n",
      "      \"grad_norm\": 0.04732392355799675,\n",
      "      \"learning_rate\": 1.4079395085066163e-05,\n",
      "      \"loss\": 0.0198,\n",
      "      \"step\": 114060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.188411733198904,\n",
      "      \"grad_norm\": 4.674064636230469,\n",
      "      \"learning_rate\": 1.407309388783869e-05,\n",
      "      \"loss\": 0.0193,\n",
      "      \"step\": 114080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.189672012350735,\n",
      "      \"grad_norm\": 0.0012179523473605514,\n",
      "      \"learning_rate\": 1.4066792690611216e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 114100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.190932291502568,\n",
      "      \"grad_norm\": 6.473933696746826,\n",
      "      \"learning_rate\": 1.4060491493383743e-05,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 114120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.1921925706544,\n",
      "      \"grad_norm\": 0.058216262608766556,\n",
      "      \"learning_rate\": 1.4054190296156269e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 114140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.193452849806232,\n",
      "      \"grad_norm\": 0.00993555411696434,\n",
      "      \"learning_rate\": 1.4047889098928796e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 114160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.194713128958064,\n",
      "      \"grad_norm\": 10.167006492614746,\n",
      "      \"learning_rate\": 1.4041587901701325e-05,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 114180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.1959734081098965,\n",
      "      \"grad_norm\": 0.05450037121772766,\n",
      "      \"learning_rate\": 1.4035286704473852e-05,\n",
      "      \"loss\": 0.0143,\n",
      "      \"step\": 114200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.197233687261728,\n",
      "      \"grad_norm\": 0.012023387476801872,\n",
      "      \"learning_rate\": 1.4028985507246378e-05,\n",
      "      \"loss\": 0.0205,\n",
      "      \"step\": 114220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.198493966413561,\n",
      "      \"grad_norm\": 3.5921249389648438,\n",
      "      \"learning_rate\": 1.4022684310018905e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 114240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.1997542455653925,\n",
      "      \"grad_norm\": 0.004243590869009495,\n",
      "      \"learning_rate\": 1.4016383112791431e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 114260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.201014524717225,\n",
      "      \"grad_norm\": 0.0021667282562702894,\n",
      "      \"learning_rate\": 1.4010081915563958e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 114280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.202274803869057,\n",
      "      \"grad_norm\": 0.009285449050366879,\n",
      "      \"learning_rate\": 1.4003780718336484e-05,\n",
      "      \"loss\": 0.0024,\n",
      "      \"step\": 114300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.203535083020889,\n",
      "      \"grad_norm\": 0.2720876634120941,\n",
      "      \"learning_rate\": 1.3997479521109011e-05,\n",
      "      \"loss\": 0.0458,\n",
      "      \"step\": 114320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.204795362172721,\n",
      "      \"grad_norm\": 0.000726042315363884,\n",
      "      \"learning_rate\": 1.3991178323881537e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 114340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.206055641324554,\n",
      "      \"grad_norm\": 0.0019735777750611305,\n",
      "      \"learning_rate\": 1.3984877126654064e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 114360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.207315920476385,\n",
      "      \"grad_norm\": 0.0049825082533061504,\n",
      "      \"learning_rate\": 1.397857592942659e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 114380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.208576199628218,\n",
      "      \"grad_norm\": 0.02603810280561447,\n",
      "      \"learning_rate\": 1.3972274732199119e-05,\n",
      "      \"loss\": 0.0233,\n",
      "      \"step\": 114400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.2098364787800495,\n",
      "      \"grad_norm\": 9.49677562713623,\n",
      "      \"learning_rate\": 1.3965973534971646e-05,\n",
      "      \"loss\": 0.0244,\n",
      "      \"step\": 114420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.211096757931882,\n",
      "      \"grad_norm\": 0.0006877564592286944,\n",
      "      \"learning_rate\": 1.3959672337744172e-05,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 114440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.212357037083714,\n",
      "      \"grad_norm\": 0.000421922275563702,\n",
      "      \"learning_rate\": 1.39533711405167e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 114460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.213617316235546,\n",
      "      \"grad_norm\": 0.004828219767659903,\n",
      "      \"learning_rate\": 1.3947069943289227e-05,\n",
      "      \"loss\": 0.0083,\n",
      "      \"step\": 114480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.214877595387378,\n",
      "      \"grad_norm\": 0.0005505259614437819,\n",
      "      \"learning_rate\": 1.3940768746061752e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 114500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.216137874539211,\n",
      "      \"grad_norm\": 0.001278611714951694,\n",
      "      \"learning_rate\": 1.393446754883428e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 114520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.217398153691042,\n",
      "      \"grad_norm\": 0.2772202789783478,\n",
      "      \"learning_rate\": 1.3928166351606805e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 114540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.218658432842875,\n",
      "      \"grad_norm\": 0.00855436734855175,\n",
      "      \"learning_rate\": 1.3921865154379333e-05,\n",
      "      \"loss\": 0.0243,\n",
      "      \"step\": 114560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.219918711994707,\n",
      "      \"grad_norm\": 0.09018059819936752,\n",
      "      \"learning_rate\": 1.3915563957151858e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 114580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.221178991146539,\n",
      "      \"grad_norm\": 0.03686016425490379,\n",
      "      \"learning_rate\": 1.3909262759924386e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 114600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.222439270298371,\n",
      "      \"grad_norm\": 0.015191703103482723,\n",
      "      \"learning_rate\": 1.3902961562696911e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 114620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.223699549450203,\n",
      "      \"grad_norm\": 10.063544273376465,\n",
      "      \"learning_rate\": 1.389666036546944e-05,\n",
      "      \"loss\": 0.0315,\n",
      "      \"step\": 114640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.224959828602035,\n",
      "      \"grad_norm\": 0.0005270588444545865,\n",
      "      \"learning_rate\": 1.3890359168241968e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 114660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.226220107753868,\n",
      "      \"grad_norm\": 0.0024651973508298397,\n",
      "      \"learning_rate\": 1.3884057971014493e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 114680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.227480386905699,\n",
      "      \"grad_norm\": 0.0005786618567071855,\n",
      "      \"learning_rate\": 1.387775677378702e-05,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 114700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.228740666057532,\n",
      "      \"grad_norm\": 0.0009823342552408576,\n",
      "      \"learning_rate\": 1.3871455576559546e-05,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 114720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.230000945209364,\n",
      "      \"grad_norm\": 0.0014015575870871544,\n",
      "      \"learning_rate\": 1.3865154379332074e-05,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 114740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.231261224361196,\n",
      "      \"grad_norm\": 0.0007991743623279035,\n",
      "      \"learning_rate\": 1.38588531821046e-05,\n",
      "      \"loss\": 0.0135,\n",
      "      \"step\": 114760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.232521503513028,\n",
      "      \"grad_norm\": 0.030576171353459358,\n",
      "      \"learning_rate\": 1.3852551984877127e-05,\n",
      "      \"loss\": 0.0073,\n",
      "      \"step\": 114780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.2337817826648605,\n",
      "      \"grad_norm\": 0.000778499583248049,\n",
      "      \"learning_rate\": 1.3846250787649654e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 114800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.235042061816692,\n",
      "      \"grad_norm\": 0.0005492019117809832,\n",
      "      \"learning_rate\": 1.383994959042218e-05,\n",
      "      \"loss\": 0.0076,\n",
      "      \"step\": 114820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.236302340968525,\n",
      "      \"grad_norm\": 0.46042707562446594,\n",
      "      \"learning_rate\": 1.3833648393194707e-05,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 114840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.2375626201203564,\n",
      "      \"grad_norm\": 0.002168976003304124,\n",
      "      \"learning_rate\": 1.3827347195967236e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 114860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.238822899272189,\n",
      "      \"grad_norm\": 0.023060565814375877,\n",
      "      \"learning_rate\": 1.3821045998739762e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 114880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.240083178424021,\n",
      "      \"grad_norm\": 0.0006996972369961441,\n",
      "      \"learning_rate\": 1.381474480151229e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 114900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.241343457575853,\n",
      "      \"grad_norm\": 0.00030043587321415544,\n",
      "      \"learning_rate\": 1.3808443604284815e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 114920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.242603736727685,\n",
      "      \"grad_norm\": 0.0003928103542421013,\n",
      "      \"learning_rate\": 1.3802142407057342e-05,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 114940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.243864015879518,\n",
      "      \"grad_norm\": 0.0023636494297534227,\n",
      "      \"learning_rate\": 1.3795841209829868e-05,\n",
      "      \"loss\": 0.0219,\n",
      "      \"step\": 114960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.245124295031349,\n",
      "      \"grad_norm\": 0.0011204826878383756,\n",
      "      \"learning_rate\": 1.3789540012602395e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 114980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.246384574183182,\n",
      "      \"grad_norm\": 0.002918022917583585,\n",
      "      \"learning_rate\": 1.378323881537492e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 115000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.2476448533350135,\n",
      "      \"grad_norm\": 0.0016922251088544726,\n",
      "      \"learning_rate\": 1.3776937618147448e-05,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 115020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.248905132486846,\n",
      "      \"grad_norm\": 7.21608829498291,\n",
      "      \"learning_rate\": 1.3770636420919974e-05,\n",
      "      \"loss\": 0.0166,\n",
      "      \"step\": 115040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.250165411638678,\n",
      "      \"grad_norm\": 0.00032321910839527845,\n",
      "      \"learning_rate\": 1.3764335223692501e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 115060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.25142569079051,\n",
      "      \"grad_norm\": 0.0005678602028638124,\n",
      "      \"learning_rate\": 1.375803402646503e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 115080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.252685969942342,\n",
      "      \"grad_norm\": 0.41400110721588135,\n",
      "      \"learning_rate\": 1.3751732829237558e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 115100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.253946249094175,\n",
      "      \"grad_norm\": 0.051557306200265884,\n",
      "      \"learning_rate\": 1.3745431632010083e-05,\n",
      "      \"loss\": 0.0278,\n",
      "      \"step\": 115120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.255206528246006,\n",
      "      \"grad_norm\": 0.0002564695314504206,\n",
      "      \"learning_rate\": 1.373913043478261e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 115140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.256466807397839,\n",
      "      \"grad_norm\": 0.00026137224631384015,\n",
      "      \"learning_rate\": 1.3732829237555136e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 115160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.257727086549671,\n",
      "      \"grad_norm\": 0.0002151589433196932,\n",
      "      \"learning_rate\": 1.3726528040327664e-05,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 115180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.258987365701503,\n",
      "      \"grad_norm\": 0.0013053033035248518,\n",
      "      \"learning_rate\": 1.372022684310019e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 115200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.260247644853335,\n",
      "      \"grad_norm\": 0.0047729830257594585,\n",
      "      \"learning_rate\": 1.3713925645872717e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 115220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.261507924005167,\n",
      "      \"grad_norm\": 0.0014506573788821697,\n",
      "      \"learning_rate\": 1.3707624448645242e-05,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 115240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.262768203156999,\n",
      "      \"grad_norm\": 0.012662197463214397,\n",
      "      \"learning_rate\": 1.370132325141777e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 115260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.264028482308832,\n",
      "      \"grad_norm\": 0.035460080951452255,\n",
      "      \"learning_rate\": 1.3695022054190295e-05,\n",
      "      \"loss\": 0.0517,\n",
      "      \"step\": 115280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.265288761460663,\n",
      "      \"grad_norm\": 0.0010154704796150327,\n",
      "      \"learning_rate\": 1.3688720856962823e-05,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 115300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.266549040612496,\n",
      "      \"grad_norm\": 0.0017751604318618774,\n",
      "      \"learning_rate\": 1.3682419659735352e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 115320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.267809319764328,\n",
      "      \"grad_norm\": 0.0033421998377889395,\n",
      "      \"learning_rate\": 1.3676118462507879e-05,\n",
      "      \"loss\": 0.0043,\n",
      "      \"step\": 115340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.26906959891616,\n",
      "      \"grad_norm\": 10.065231323242188,\n",
      "      \"learning_rate\": 1.3669817265280405e-05,\n",
      "      \"loss\": 0.0062,\n",
      "      \"step\": 115360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.270329878067992,\n",
      "      \"grad_norm\": 0.0011864672414958477,\n",
      "      \"learning_rate\": 1.3663516068052932e-05,\n",
      "      \"loss\": 0.0178,\n",
      "      \"step\": 115380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.2715901572198245,\n",
      "      \"grad_norm\": 5.313183784484863,\n",
      "      \"learning_rate\": 1.3657214870825458e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 115400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.272850436371656,\n",
      "      \"grad_norm\": 3.2469427585601807,\n",
      "      \"learning_rate\": 1.3650913673597985e-05,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 115420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.274110715523489,\n",
      "      \"grad_norm\": 0.00392597122117877,\n",
      "      \"learning_rate\": 1.364461247637051e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 115440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.2753709946753204,\n",
      "      \"grad_norm\": 6.456264019012451,\n",
      "      \"learning_rate\": 1.3638311279143038e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 115460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.276631273827153,\n",
      "      \"grad_norm\": 0.001082180766388774,\n",
      "      \"learning_rate\": 1.3632010081915564e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 115480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.277891552978985,\n",
      "      \"grad_norm\": 0.0007528387359343469,\n",
      "      \"learning_rate\": 1.3625708884688091e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 115500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.279151832130817,\n",
      "      \"grad_norm\": 0.003751940792426467,\n",
      "      \"learning_rate\": 1.3619407687460617e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 115520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.280412111282649,\n",
      "      \"grad_norm\": 0.0017378749325871468,\n",
      "      \"learning_rate\": 1.3613106490233146e-05,\n",
      "      \"loss\": 0.0228,\n",
      "      \"step\": 115540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.281672390434482,\n",
      "      \"grad_norm\": 0.007386401295661926,\n",
      "      \"learning_rate\": 1.3606805293005673e-05,\n",
      "      \"loss\": 0.0361,\n",
      "      \"step\": 115560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.282932669586313,\n",
      "      \"grad_norm\": 0.05246616154909134,\n",
      "      \"learning_rate\": 1.3600504095778199e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 115580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.284192948738146,\n",
      "      \"grad_norm\": 0.053747039288282394,\n",
      "      \"learning_rate\": 1.3594202898550726e-05,\n",
      "      \"loss\": 0.0166,\n",
      "      \"step\": 115600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.2854532278899775,\n",
      "      \"grad_norm\": 0.002753088716417551,\n",
      "      \"learning_rate\": 1.3587901701323252e-05,\n",
      "      \"loss\": 0.035,\n",
      "      \"step\": 115620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.28671350704181,\n",
      "      \"grad_norm\": 0.004181596450507641,\n",
      "      \"learning_rate\": 1.3581600504095779e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 115640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.287973786193642,\n",
      "      \"grad_norm\": 0.0018995985155925155,\n",
      "      \"learning_rate\": 1.3575299306868305e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 115660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.289234065345474,\n",
      "      \"grad_norm\": 0.00770299369469285,\n",
      "      \"learning_rate\": 1.3568998109640832e-05,\n",
      "      \"loss\": 0.0334,\n",
      "      \"step\": 115680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.290494344497306,\n",
      "      \"grad_norm\": 0.012508951127529144,\n",
      "      \"learning_rate\": 1.356269691241336e-05,\n",
      "      \"loss\": 0.0242,\n",
      "      \"step\": 115700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.291754623649139,\n",
      "      \"grad_norm\": 0.0456513836979866,\n",
      "      \"learning_rate\": 1.3556395715185885e-05,\n",
      "      \"loss\": 0.0121,\n",
      "      \"step\": 115720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.29301490280097,\n",
      "      \"grad_norm\": 0.9024998545646667,\n",
      "      \"learning_rate\": 1.3550094517958412e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 115740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.294275181952803,\n",
      "      \"grad_norm\": 0.004663773346692324,\n",
      "      \"learning_rate\": 1.3543793320730941e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 115760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.295535461104635,\n",
      "      \"grad_norm\": 0.0035547080915421247,\n",
      "      \"learning_rate\": 1.3537492123503467e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 115780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.296795740256467,\n",
      "      \"grad_norm\": 0.004707281477749348,\n",
      "      \"learning_rate\": 1.3531190926275994e-05,\n",
      "      \"loss\": 0.0073,\n",
      "      \"step\": 115800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.298056019408299,\n",
      "      \"grad_norm\": 0.005624313838779926,\n",
      "      \"learning_rate\": 1.352488972904852e-05,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 115820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.299316298560131,\n",
      "      \"grad_norm\": 0.008681269362568855,\n",
      "      \"learning_rate\": 1.3518588531821047e-05,\n",
      "      \"loss\": 0.0249,\n",
      "      \"step\": 115840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.300576577711963,\n",
      "      \"grad_norm\": 0.0018844541627913713,\n",
      "      \"learning_rate\": 1.3512287334593573e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 115860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.301836856863796,\n",
      "      \"grad_norm\": 0.012026331387460232,\n",
      "      \"learning_rate\": 1.35059861373661e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 115880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.303097136015627,\n",
      "      \"grad_norm\": 2.38692307472229,\n",
      "      \"learning_rate\": 1.3499684940138626e-05,\n",
      "      \"loss\": 0.0037,\n",
      "      \"step\": 115900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.30435741516746,\n",
      "      \"grad_norm\": 0.000352833274519071,\n",
      "      \"learning_rate\": 1.3493383742911153e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 115920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.305617694319292,\n",
      "      \"grad_norm\": 0.000753004162106663,\n",
      "      \"learning_rate\": 1.3487082545683679e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 115940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.306877973471124,\n",
      "      \"grad_norm\": 0.0005953144864179194,\n",
      "      \"learning_rate\": 1.3480781348456206e-05,\n",
      "      \"loss\": 0.0205,\n",
      "      \"step\": 115960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.308138252622956,\n",
      "      \"grad_norm\": 0.011337502859532833,\n",
      "      \"learning_rate\": 1.3474480151228732e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 115980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.3093985317747885,\n",
      "      \"grad_norm\": 0.0010929560521617532,\n",
      "      \"learning_rate\": 1.3468178954001263e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 116000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.31065881092662,\n",
      "      \"grad_norm\": 0.0014399326173588634,\n",
      "      \"learning_rate\": 1.3461877756773788e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 116020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.311919090078453,\n",
      "      \"grad_norm\": 0.0003302237018942833,\n",
      "      \"learning_rate\": 1.3455576559546316e-05,\n",
      "      \"loss\": 0.0077,\n",
      "      \"step\": 116040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.313179369230284,\n",
      "      \"grad_norm\": 0.36000579595565796,\n",
      "      \"learning_rate\": 1.3449275362318841e-05,\n",
      "      \"loss\": 0.0296,\n",
      "      \"step\": 116060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.314439648382117,\n",
      "      \"grad_norm\": 0.0024509215727448463,\n",
      "      \"learning_rate\": 1.3442974165091369e-05,\n",
      "      \"loss\": 0.0029,\n",
      "      \"step\": 116080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.315699927533949,\n",
      "      \"grad_norm\": 0.01920166052877903,\n",
      "      \"learning_rate\": 1.3436672967863894e-05,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 116100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.316960206685781,\n",
      "      \"grad_norm\": 0.01300735305994749,\n",
      "      \"learning_rate\": 1.3430371770636422e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 116120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.318220485837613,\n",
      "      \"grad_norm\": 1.2398492097854614,\n",
      "      \"learning_rate\": 1.3424070573408947e-05,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 116140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.3194807649894456,\n",
      "      \"grad_norm\": 0.01268233172595501,\n",
      "      \"learning_rate\": 1.3417769376181475e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 116160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.320741044141277,\n",
      "      \"grad_norm\": 0.40367457270622253,\n",
      "      \"learning_rate\": 1.3411468178954e-05,\n",
      "      \"loss\": 0.0361,\n",
      "      \"step\": 116180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.32200132329311,\n",
      "      \"grad_norm\": 6.0572004318237305,\n",
      "      \"learning_rate\": 1.3405166981726528e-05,\n",
      "      \"loss\": 0.0197,\n",
      "      \"step\": 116200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.3232616024449415,\n",
      "      \"grad_norm\": 0.02976946160197258,\n",
      "      \"learning_rate\": 1.3398865784499057e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 116220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.324521881596774,\n",
      "      \"grad_norm\": 0.001188577269203961,\n",
      "      \"learning_rate\": 1.3392564587271584e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 116240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.325782160748606,\n",
      "      \"grad_norm\": 9.361519813537598,\n",
      "      \"learning_rate\": 1.338626339004411e-05,\n",
      "      \"loss\": 0.0699,\n",
      "      \"step\": 116260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.327042439900438,\n",
      "      \"grad_norm\": 0.006925121881067753,\n",
      "      \"learning_rate\": 1.3379962192816637e-05,\n",
      "      \"loss\": 0.0425,\n",
      "      \"step\": 116280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.32830271905227,\n",
      "      \"grad_norm\": 0.0038318370934575796,\n",
      "      \"learning_rate\": 1.3373660995589163e-05,\n",
      "      \"loss\": 0.0274,\n",
      "      \"step\": 116300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.329562998204103,\n",
      "      \"grad_norm\": 0.16465312242507935,\n",
      "      \"learning_rate\": 1.336735979836169e-05,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 116320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.330823277355934,\n",
      "      \"grad_norm\": 0.09467293322086334,\n",
      "      \"learning_rate\": 1.3361058601134216e-05,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 116340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.332083556507767,\n",
      "      \"grad_norm\": 0.021366804838180542,\n",
      "      \"learning_rate\": 1.3354757403906743e-05,\n",
      "      \"loss\": 0.0201,\n",
      "      \"step\": 116360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.333343835659599,\n",
      "      \"grad_norm\": 0.08014735579490662,\n",
      "      \"learning_rate\": 1.3348456206679269e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 116380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.33460411481143,\n",
      "      \"grad_norm\": 0.007121267728507519,\n",
      "      \"learning_rate\": 1.3342155009451796e-05,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 116400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.335864393963263,\n",
      "      \"grad_norm\": 0.09882161766290665,\n",
      "      \"learning_rate\": 1.3335853812224322e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 116420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.337124673115095,\n",
      "      \"grad_norm\": 0.0691923275589943,\n",
      "      \"learning_rate\": 1.332955261499685e-05,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 116440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.338384952266927,\n",
      "      \"grad_norm\": 4.307723045349121,\n",
      "      \"learning_rate\": 1.3323251417769378e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 116460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.339645231418759,\n",
      "      \"grad_norm\": 0.0010900124907493591,\n",
      "      \"learning_rate\": 1.3316950220541904e-05,\n",
      "      \"loss\": 0.0172,\n",
      "      \"step\": 116480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.340905510570591,\n",
      "      \"grad_norm\": 0.014610052108764648,\n",
      "      \"learning_rate\": 1.3310649023314431e-05,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 116500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.342165789722424,\n",
      "      \"grad_norm\": 0.013571327552199364,\n",
      "      \"learning_rate\": 1.3304347826086957e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 116520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.343426068874256,\n",
      "      \"grad_norm\": 0.00042050969204865396,\n",
      "      \"learning_rate\": 1.3298046628859484e-05,\n",
      "      \"loss\": 0.0221,\n",
      "      \"step\": 116540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.344686348026087,\n",
      "      \"grad_norm\": 0.0002930218761321157,\n",
      "      \"learning_rate\": 1.3291745431632011e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 116560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.34594662717792,\n",
      "      \"grad_norm\": 0.0007942056399770081,\n",
      "      \"learning_rate\": 1.3285444234404537e-05,\n",
      "      \"loss\": 0.0369,\n",
      "      \"step\": 116580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.3472069063297525,\n",
      "      \"grad_norm\": 0.0596182681620121,\n",
      "      \"learning_rate\": 1.3279143037177064e-05,\n",
      "      \"loss\": 0.0093,\n",
      "      \"step\": 116600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.348467185481584,\n",
      "      \"grad_norm\": 0.0019544477108865976,\n",
      "      \"learning_rate\": 1.327284183994959e-05,\n",
      "      \"loss\": 0.0246,\n",
      "      \"step\": 116620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.349727464633416,\n",
      "      \"grad_norm\": 0.012654435820877552,\n",
      "      \"learning_rate\": 1.3266540642722117e-05,\n",
      "      \"loss\": 0.0238,\n",
      "      \"step\": 116640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.350987743785248,\n",
      "      \"grad_norm\": 0.003045075573027134,\n",
      "      \"learning_rate\": 1.3260239445494643e-05,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 116660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.352248022937081,\n",
      "      \"grad_norm\": 0.01179954782128334,\n",
      "      \"learning_rate\": 1.3253938248267172e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 116680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.353508302088913,\n",
      "      \"grad_norm\": 0.0013513155281543732,\n",
      "      \"learning_rate\": 1.32476370510397e-05,\n",
      "      \"loss\": 0.0116,\n",
      "      \"step\": 116700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.354768581240744,\n",
      "      \"grad_norm\": 0.03230908513069153,\n",
      "      \"learning_rate\": 1.3241335853812225e-05,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 116720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.356028860392577,\n",
      "      \"grad_norm\": 0.2999938130378723,\n",
      "      \"learning_rate\": 1.3235034656584752e-05,\n",
      "      \"loss\": 0.0287,\n",
      "      \"step\": 116740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.3572891395444096,\n",
      "      \"grad_norm\": 0.001901401556096971,\n",
      "      \"learning_rate\": 1.3228733459357278e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 116760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.358549418696241,\n",
      "      \"grad_norm\": 0.013494845479726791,\n",
      "      \"learning_rate\": 1.3222432262129805e-05,\n",
      "      \"loss\": 0.0394,\n",
      "      \"step\": 116780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.359809697848073,\n",
      "      \"grad_norm\": 0.002921921666711569,\n",
      "      \"learning_rate\": 1.3216131064902331e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 116800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.3610699769999055,\n",
      "      \"grad_norm\": 0.03622999042272568,\n",
      "      \"learning_rate\": 1.3209829867674858e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 116820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.362330256151737,\n",
      "      \"grad_norm\": 0.0011759839253500104,\n",
      "      \"learning_rate\": 1.3203528670447384e-05,\n",
      "      \"loss\": 0.0256,\n",
      "      \"step\": 116840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.36359053530357,\n",
      "      \"grad_norm\": 0.014745187014341354,\n",
      "      \"learning_rate\": 1.3197227473219911e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 116860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.3648508144554015,\n",
      "      \"grad_norm\": 0.005495032295584679,\n",
      "      \"learning_rate\": 1.3190926275992437e-05,\n",
      "      \"loss\": 0.0156,\n",
      "      \"step\": 116880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.366111093607234,\n",
      "      \"grad_norm\": 0.03460552915930748,\n",
      "      \"learning_rate\": 1.3184625078764968e-05,\n",
      "      \"loss\": 0.0076,\n",
      "      \"step\": 116900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.367371372759066,\n",
      "      \"grad_norm\": 0.3615204691886902,\n",
      "      \"learning_rate\": 1.3178323881537493e-05,\n",
      "      \"loss\": 0.0146,\n",
      "      \"step\": 116920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.368631651910898,\n",
      "      \"grad_norm\": 0.07531636208295822,\n",
      "      \"learning_rate\": 1.317202268431002e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 116940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.36989193106273,\n",
      "      \"grad_norm\": 0.002436671871691942,\n",
      "      \"learning_rate\": 1.3165721487082546e-05,\n",
      "      \"loss\": 0.0054,\n",
      "      \"step\": 116960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.371152210214563,\n",
      "      \"grad_norm\": 0.002787306671962142,\n",
      "      \"learning_rate\": 1.3159420289855074e-05,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 116980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.372412489366394,\n",
      "      \"grad_norm\": 0.012205415405333042,\n",
      "      \"learning_rate\": 1.31531190926276e-05,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 117000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.373672768518227,\n",
      "      \"grad_norm\": 0.00020965654402971268,\n",
      "      \"learning_rate\": 1.3146817895400127e-05,\n",
      "      \"loss\": 0.0026,\n",
      "      \"step\": 117020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.3749330476700585,\n",
      "      \"grad_norm\": 0.20294712483882904,\n",
      "      \"learning_rate\": 1.3140516698172652e-05,\n",
      "      \"loss\": 0.0622,\n",
      "      \"step\": 117040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.376193326821891,\n",
      "      \"grad_norm\": 0.0014202585443854332,\n",
      "      \"learning_rate\": 1.313421550094518e-05,\n",
      "      \"loss\": 0.0026,\n",
      "      \"step\": 117060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.377453605973723,\n",
      "      \"grad_norm\": 0.03744423761963844,\n",
      "      \"learning_rate\": 1.3127914303717705e-05,\n",
      "      \"loss\": 0.0073,\n",
      "      \"step\": 117080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.378713885125555,\n",
      "      \"grad_norm\": 0.0004353483091108501,\n",
      "      \"learning_rate\": 1.3121613106490233e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 117100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.379974164277387,\n",
      "      \"grad_norm\": 0.0026059106457978487,\n",
      "      \"learning_rate\": 1.3115311909262762e-05,\n",
      "      \"loss\": 0.0056,\n",
      "      \"step\": 117120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.38123444342922,\n",
      "      \"grad_norm\": 0.0013378338189795613,\n",
      "      \"learning_rate\": 1.310901071203529e-05,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 117140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.382494722581051,\n",
      "      \"grad_norm\": 0.008478393778204918,\n",
      "      \"learning_rate\": 1.3102709514807815e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 117160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.383755001732884,\n",
      "      \"grad_norm\": 5.836461544036865,\n",
      "      \"learning_rate\": 1.3096408317580342e-05,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 117180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.385015280884716,\n",
      "      \"grad_norm\": 0.0030008582398295403,\n",
      "      \"learning_rate\": 1.3090107120352868e-05,\n",
      "      \"loss\": 0.0194,\n",
      "      \"step\": 117200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.386275560036548,\n",
      "      \"grad_norm\": 0.002197715686634183,\n",
      "      \"learning_rate\": 1.3083805923125395e-05,\n",
      "      \"loss\": 0.022,\n",
      "      \"step\": 117220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.38753583918838,\n",
      "      \"grad_norm\": 0.0017514873761683702,\n",
      "      \"learning_rate\": 1.307750472589792e-05,\n",
      "      \"loss\": 0.0317,\n",
      "      \"step\": 117240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.388796118340212,\n",
      "      \"grad_norm\": 0.05890806391835213,\n",
      "      \"learning_rate\": 1.3071203528670448e-05,\n",
      "      \"loss\": 0.0272,\n",
      "      \"step\": 117260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.390056397492044,\n",
      "      \"grad_norm\": 0.007156189996749163,\n",
      "      \"learning_rate\": 1.3064902331442974e-05,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 117280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.391316676643877,\n",
      "      \"grad_norm\": 1.0011850595474243,\n",
      "      \"learning_rate\": 1.3058601134215501e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 117300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.392576955795708,\n",
      "      \"grad_norm\": 0.003183006541803479,\n",
      "      \"learning_rate\": 1.3052299936988027e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 117320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.393837234947541,\n",
      "      \"grad_norm\": 0.005155435297638178,\n",
      "      \"learning_rate\": 1.3045998739760554e-05,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 117340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.395097514099373,\n",
      "      \"grad_norm\": 0.04726250842213631,\n",
      "      \"learning_rate\": 1.3039697542533083e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 117360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.396357793251205,\n",
      "      \"grad_norm\": 0.10990689694881439,\n",
      "      \"learning_rate\": 1.3033396345305609e-05,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 117380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.397618072403037,\n",
      "      \"grad_norm\": 0.0017761742928996682,\n",
      "      \"learning_rate\": 1.3027095148078136e-05,\n",
      "      \"loss\": 0.002,\n",
      "      \"step\": 117400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.3988783515548695,\n",
      "      \"grad_norm\": 0.005496037658303976,\n",
      "      \"learning_rate\": 1.3020793950850662e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 117420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.400138630706701,\n",
      "      \"grad_norm\": 0.0004450202395673841,\n",
      "      \"learning_rate\": 1.301449275362319e-05,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 117440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.401398909858534,\n",
      "      \"grad_norm\": 0.1935167759656906,\n",
      "      \"learning_rate\": 1.3008191556395717e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 117460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.4026591890103655,\n",
      "      \"grad_norm\": 0.0013028417015448213,\n",
      "      \"learning_rate\": 1.3001890359168242e-05,\n",
      "      \"loss\": 0.0034,\n",
      "      \"step\": 117480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.403919468162198,\n",
      "      \"grad_norm\": 0.001223270664922893,\n",
      "      \"learning_rate\": 1.299558916194077e-05,\n",
      "      \"loss\": 0.0256,\n",
      "      \"step\": 117500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.40517974731403,\n",
      "      \"grad_norm\": 0.009546741843223572,\n",
      "      \"learning_rate\": 1.2989287964713295e-05,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 117520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.406440026465862,\n",
      "      \"grad_norm\": 0.046673886477947235,\n",
      "      \"learning_rate\": 1.2982986767485823e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 117540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.407700305617694,\n",
      "      \"grad_norm\": 0.0024846354499459267,\n",
      "      \"learning_rate\": 1.2976685570258348e-05,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 117560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.408960584769527,\n",
      "      \"grad_norm\": 0.003385863034054637,\n",
      "      \"learning_rate\": 1.2970384373030877e-05,\n",
      "      \"loss\": 0.0033,\n",
      "      \"step\": 117580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.410220863921358,\n",
      "      \"grad_norm\": 0.04146428406238556,\n",
      "      \"learning_rate\": 1.2964083175803405e-05,\n",
      "      \"loss\": 0.0417,\n",
      "      \"step\": 117600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.411481143073191,\n",
      "      \"grad_norm\": 0.002456675749272108,\n",
      "      \"learning_rate\": 1.295778197857593e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 117620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.4127414222250225,\n",
      "      \"grad_norm\": 0.0013428436359390616,\n",
      "      \"learning_rate\": 1.2951480781348458e-05,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 117640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.414001701376855,\n",
      "      \"grad_norm\": 0.0007707533077336848,\n",
      "      \"learning_rate\": 1.2945179584120983e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 117660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.415261980528687,\n",
      "      \"grad_norm\": 0.018542401492595673,\n",
      "      \"learning_rate\": 1.293887838689351e-05,\n",
      "      \"loss\": 0.0239,\n",
      "      \"step\": 117680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.416522259680519,\n",
      "      \"grad_norm\": 0.0004674614465329796,\n",
      "      \"learning_rate\": 1.2932577189666036e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 117700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.417782538832351,\n",
      "      \"grad_norm\": 0.0005057717789895833,\n",
      "      \"learning_rate\": 1.2926275992438564e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 117720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.419042817984184,\n",
      "      \"grad_norm\": 0.04764317721128464,\n",
      "      \"learning_rate\": 1.291997479521109e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 117740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.420303097136015,\n",
      "      \"grad_norm\": 0.0008136985707096756,\n",
      "      \"learning_rate\": 1.2913673597983617e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 117760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.421563376287848,\n",
      "      \"grad_norm\": 0.04552401602268219,\n",
      "      \"learning_rate\": 1.2907372400756144e-05,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 117780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.42282365543968,\n",
      "      \"grad_norm\": 0.0020824167877435684,\n",
      "      \"learning_rate\": 1.2901071203528673e-05,\n",
      "      \"loss\": 0.0188,\n",
      "      \"step\": 117800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.424083934591512,\n",
      "      \"grad_norm\": 0.057063449174165726,\n",
      "      \"learning_rate\": 1.2894770006301199e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 117820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.425344213743344,\n",
      "      \"grad_norm\": 0.00039364915573969483,\n",
      "      \"learning_rate\": 1.2888468809073726e-05,\n",
      "      \"loss\": 0.0122,\n",
      "      \"step\": 117840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.426604492895176,\n",
      "      \"grad_norm\": 0.006506551988422871,\n",
      "      \"learning_rate\": 1.2882167611846252e-05,\n",
      "      \"loss\": 0.018,\n",
      "      \"step\": 117860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.427864772047008,\n",
      "      \"grad_norm\": 0.021851759403944016,\n",
      "      \"learning_rate\": 1.2875866414618779e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 117880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.429125051198841,\n",
      "      \"grad_norm\": 0.0002909678150899708,\n",
      "      \"learning_rate\": 1.2869565217391305e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 117900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.430385330350672,\n",
      "      \"grad_norm\": 0.028673697263002396,\n",
      "      \"learning_rate\": 1.2863264020163832e-05,\n",
      "      \"loss\": 0.0277,\n",
      "      \"step\": 117920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.431645609502505,\n",
      "      \"grad_norm\": 0.0015862772706896067,\n",
      "      \"learning_rate\": 1.2856962822936358e-05,\n",
      "      \"loss\": 0.0233,\n",
      "      \"step\": 117940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.432905888654337,\n",
      "      \"grad_norm\": 0.03941342607140541,\n",
      "      \"learning_rate\": 1.2850661625708885e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 117960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.434166167806169,\n",
      "      \"grad_norm\": 0.0002862018591258675,\n",
      "      \"learning_rate\": 1.284436042848141e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 117980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.435426446958001,\n",
      "      \"grad_norm\": 0.003478539874777198,\n",
      "      \"learning_rate\": 1.2838059231253938e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 118000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.4366867261098335,\n",
      "      \"grad_norm\": 0.00966833345592022,\n",
      "      \"learning_rate\": 1.2831758034026464e-05,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 118020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.437947005261665,\n",
      "      \"grad_norm\": 0.01105912309139967,\n",
      "      \"learning_rate\": 1.2825456836798994e-05,\n",
      "      \"loss\": 0.0196,\n",
      "      \"step\": 118040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.439207284413498,\n",
      "      \"grad_norm\": 0.0030026861932128668,\n",
      "      \"learning_rate\": 1.281915563957152e-05,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 118060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.4404675635653295,\n",
      "      \"grad_norm\": 0.005853915121406317,\n",
      "      \"learning_rate\": 1.2812854442344047e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 118080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.441727842717162,\n",
      "      \"grad_norm\": 0.0011535362573340535,\n",
      "      \"learning_rate\": 1.2806553245116573e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 118100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.442988121868994,\n",
      "      \"grad_norm\": 0.011538390070199966,\n",
      "      \"learning_rate\": 1.28002520478891e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 118120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.444248401020826,\n",
      "      \"grad_norm\": 0.005532060284167528,\n",
      "      \"learning_rate\": 1.2793950850661626e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 118140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.445508680172658,\n",
      "      \"grad_norm\": 4.193800926208496,\n",
      "      \"learning_rate\": 1.2787649653434153e-05,\n",
      "      \"loss\": 0.0131,\n",
      "      \"step\": 118160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.446768959324491,\n",
      "      \"grad_norm\": 0.00044971323222853243,\n",
      "      \"learning_rate\": 1.2781348456206679e-05,\n",
      "      \"loss\": 0.0184,\n",
      "      \"step\": 118180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.448029238476322,\n",
      "      \"grad_norm\": 0.00040315240039490163,\n",
      "      \"learning_rate\": 1.2775047258979206e-05,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 118200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.449289517628155,\n",
      "      \"grad_norm\": 0.0190092995762825,\n",
      "      \"learning_rate\": 1.2768746061751732e-05,\n",
      "      \"loss\": 0.0532,\n",
      "      \"step\": 118220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.4505497967799865,\n",
      "      \"grad_norm\": 35.662052154541016,\n",
      "      \"learning_rate\": 1.276244486452426e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 118240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.451810075931819,\n",
      "      \"grad_norm\": 0.0004638593236450106,\n",
      "      \"learning_rate\": 1.2756143667296788e-05,\n",
      "      \"loss\": 0.0217,\n",
      "      \"step\": 118260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.453070355083651,\n",
      "      \"grad_norm\": 0.00031840064912103117,\n",
      "      \"learning_rate\": 1.2749842470069314e-05,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 118280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.454330634235483,\n",
      "      \"grad_norm\": 0.0007821081671863794,\n",
      "      \"learning_rate\": 1.2743541272841841e-05,\n",
      "      \"loss\": 0.0166,\n",
      "      \"step\": 118300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.455590913387315,\n",
      "      \"grad_norm\": 0.0010928093688562512,\n",
      "      \"learning_rate\": 1.273755513547574e-05,\n",
      "      \"loss\": 0.0257,\n",
      "      \"step\": 118320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.456851192539148,\n",
      "      \"grad_norm\": 0.0018824109574779868,\n",
      "      \"learning_rate\": 1.2731253938248266e-05,\n",
      "      \"loss\": 0.0043,\n",
      "      \"step\": 118340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.458111471690979,\n",
      "      \"grad_norm\": 0.00245984666980803,\n",
      "      \"learning_rate\": 1.2724952741020795e-05,\n",
      "      \"loss\": 0.0197,\n",
      "      \"step\": 118360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.459371750842812,\n",
      "      \"grad_norm\": 0.08098667860031128,\n",
      "      \"learning_rate\": 1.2718651543793323e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 118380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.460632029994644,\n",
      "      \"grad_norm\": 0.0004931919975206256,\n",
      "      \"learning_rate\": 1.2712350346565848e-05,\n",
      "      \"loss\": 0.0298,\n",
      "      \"step\": 118400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.461892309146476,\n",
      "      \"grad_norm\": 0.0016857441514730453,\n",
      "      \"learning_rate\": 1.2706049149338376e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 118420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.463152588298308,\n",
      "      \"grad_norm\": 0.012548723258078098,\n",
      "      \"learning_rate\": 1.2699747952110901e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 118440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.46441286745014,\n",
      "      \"grad_norm\": 0.0019546085968613625,\n",
      "      \"learning_rate\": 1.2693446754883429e-05,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 118460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.465673146601972,\n",
      "      \"grad_norm\": 0.007755594328045845,\n",
      "      \"learning_rate\": 1.2687145557655954e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 118480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.466933425753805,\n",
      "      \"grad_norm\": 0.0009571858099661767,\n",
      "      \"learning_rate\": 1.2680844360428482e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 118500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.468193704905636,\n",
      "      \"grad_norm\": 0.0049577695317566395,\n",
      "      \"learning_rate\": 1.2674543163201009e-05,\n",
      "      \"loss\": 0.0302,\n",
      "      \"step\": 118520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.469453984057469,\n",
      "      \"grad_norm\": 0.01724172756075859,\n",
      "      \"learning_rate\": 1.2668241965973535e-05,\n",
      "      \"loss\": 0.0159,\n",
      "      \"step\": 118540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.470714263209301,\n",
      "      \"grad_norm\": 0.0008047897717915475,\n",
      "      \"learning_rate\": 1.2661940768746062e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 118560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.471974542361133,\n",
      "      \"grad_norm\": 0.08653459697961807,\n",
      "      \"learning_rate\": 1.2655639571518591e-05,\n",
      "      \"loss\": 0.0124,\n",
      "      \"step\": 118580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.473234821512965,\n",
      "      \"grad_norm\": 0.4372335374355316,\n",
      "      \"learning_rate\": 1.2649338374291117e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 118600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.4744951006647975,\n",
      "      \"grad_norm\": 0.01020901184529066,\n",
      "      \"learning_rate\": 1.2643037177063644e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 118620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.475755379816629,\n",
      "      \"grad_norm\": 0.0031555064488202333,\n",
      "      \"learning_rate\": 1.263673597983617e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 118640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.477015658968462,\n",
      "      \"grad_norm\": 0.10599327087402344,\n",
      "      \"learning_rate\": 1.2630434782608697e-05,\n",
      "      \"loss\": 0.0263,\n",
      "      \"step\": 118660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.4782759381202935,\n",
      "      \"grad_norm\": 0.2208806872367859,\n",
      "      \"learning_rate\": 1.2624133585381223e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 118680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.479536217272126,\n",
      "      \"grad_norm\": 0.007935994304716587,\n",
      "      \"learning_rate\": 1.261783238815375e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 118700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.480796496423958,\n",
      "      \"grad_norm\": 0.0004713066737167537,\n",
      "      \"learning_rate\": 1.2611531190926276e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 118720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.48205677557579,\n",
      "      \"grad_norm\": 0.0008357292390428483,\n",
      "      \"learning_rate\": 1.2605229993698803e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 118740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.483317054727622,\n",
      "      \"grad_norm\": 0.15333262085914612,\n",
      "      \"learning_rate\": 1.2598928796471329e-05,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 118760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.484577333879455,\n",
      "      \"grad_norm\": 0.0069938222877681255,\n",
      "      \"learning_rate\": 1.2592627599243856e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 118780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.485837613031286,\n",
      "      \"grad_norm\": 0.0076973759569227695,\n",
      "      \"learning_rate\": 1.2586326402016382e-05,\n",
      "      \"loss\": 0.0145,\n",
      "      \"step\": 118800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.487097892183119,\n",
      "      \"grad_norm\": 0.0014027656288817525,\n",
      "      \"learning_rate\": 1.2580025204788912e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 118820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.4883581713349505,\n",
      "      \"grad_norm\": 0.004303243476897478,\n",
      "      \"learning_rate\": 1.2573724007561438e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 118840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.489618450486783,\n",
      "      \"grad_norm\": 0.008986429311335087,\n",
      "      \"learning_rate\": 1.2567422810333965e-05,\n",
      "      \"loss\": 0.0202,\n",
      "      \"step\": 118860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.490878729638615,\n",
      "      \"grad_norm\": 0.00576997734606266,\n",
      "      \"learning_rate\": 1.2561121613106491e-05,\n",
      "      \"loss\": 0.0076,\n",
      "      \"step\": 118880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.492139008790447,\n",
      "      \"grad_norm\": 0.0017098801909014583,\n",
      "      \"learning_rate\": 1.2554820415879018e-05,\n",
      "      \"loss\": 0.0172,\n",
      "      \"step\": 118900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.493399287942279,\n",
      "      \"grad_norm\": 0.028879396617412567,\n",
      "      \"learning_rate\": 1.2548519218651544e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 118920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.494659567094112,\n",
      "      \"grad_norm\": 0.0002579561551101506,\n",
      "      \"learning_rate\": 1.2542218021424071e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 118940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.495919846245943,\n",
      "      \"grad_norm\": 0.007344708777964115,\n",
      "      \"learning_rate\": 1.2535916824196597e-05,\n",
      "      \"loss\": 0.0125,\n",
      "      \"step\": 118960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.497180125397776,\n",
      "      \"grad_norm\": 0.010258140973746777,\n",
      "      \"learning_rate\": 1.2529615626969124e-05,\n",
      "      \"loss\": 0.0328,\n",
      "      \"step\": 118980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.498440404549608,\n",
      "      \"grad_norm\": 0.1693432331085205,\n",
      "      \"learning_rate\": 1.252331442974165e-05,\n",
      "      \"loss\": 0.0018,\n",
      "      \"step\": 119000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.49970068370144,\n",
      "      \"grad_norm\": 17.25600242614746,\n",
      "      \"learning_rate\": 1.2517013232514177e-05,\n",
      "      \"loss\": 0.0125,\n",
      "      \"step\": 119020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.500960962853272,\n",
      "      \"grad_norm\": 0.00039848143933340907,\n",
      "      \"learning_rate\": 1.2510712035286706e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 119040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.502221242005104,\n",
      "      \"grad_norm\": 0.0003510845999699086,\n",
      "      \"learning_rate\": 1.2504410838059234e-05,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 119060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.503481521156936,\n",
      "      \"grad_norm\": 0.0005813061725348234,\n",
      "      \"learning_rate\": 1.2498109640831758e-05,\n",
      "      \"loss\": 0.0073,\n",
      "      \"step\": 119080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.504741800308769,\n",
      "      \"grad_norm\": 0.14516790211200714,\n",
      "      \"learning_rate\": 1.2491808443604287e-05,\n",
      "      \"loss\": 0.018,\n",
      "      \"step\": 119100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.5060020794606,\n",
      "      \"grad_norm\": 0.05740983039140701,\n",
      "      \"learning_rate\": 1.2485507246376812e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 119120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.507262358612433,\n",
      "      \"grad_norm\": 0.0006804196164011955,\n",
      "      \"learning_rate\": 1.247920604914934e-05,\n",
      "      \"loss\": 0.0523,\n",
      "      \"step\": 119140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.508522637764265,\n",
      "      \"grad_norm\": 0.0067227850668132305,\n",
      "      \"learning_rate\": 1.2472904851921865e-05,\n",
      "      \"loss\": 0.0093,\n",
      "      \"step\": 119160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.509782916916097,\n",
      "      \"grad_norm\": 0.005802999250590801,\n",
      "      \"learning_rate\": 1.2466603654694393e-05,\n",
      "      \"loss\": 0.0034,\n",
      "      \"step\": 119180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.511043196067929,\n",
      "      \"grad_norm\": 0.0013961050426587462,\n",
      "      \"learning_rate\": 1.246030245746692e-05,\n",
      "      \"loss\": 0.0241,\n",
      "      \"step\": 119200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.5123034752197615,\n",
      "      \"grad_norm\": 0.0002494376094546169,\n",
      "      \"learning_rate\": 1.2454001260239447e-05,\n",
      "      \"loss\": 0.013,\n",
      "      \"step\": 119220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.513563754371593,\n",
      "      \"grad_norm\": 0.001513199182227254,\n",
      "      \"learning_rate\": 1.2447700063011973e-05,\n",
      "      \"loss\": 0.0064,\n",
      "      \"step\": 119240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.514824033523426,\n",
      "      \"grad_norm\": 0.0007309332140721381,\n",
      "      \"learning_rate\": 1.24413988657845e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 119260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.5160843126752575,\n",
      "      \"grad_norm\": 0.0009472830570302904,\n",
      "      \"learning_rate\": 1.2435097668557026e-05,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 119280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.51734459182709,\n",
      "      \"grad_norm\": 0.0002962603175546974,\n",
      "      \"learning_rate\": 1.2428796471329553e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 119300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.518604870978922,\n",
      "      \"grad_norm\": 0.0031267174053937197,\n",
      "      \"learning_rate\": 1.242249527410208e-05,\n",
      "      \"loss\": 0.0067,\n",
      "      \"step\": 119320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.519865150130754,\n",
      "      \"grad_norm\": 0.00022361541050486267,\n",
      "      \"learning_rate\": 1.2416194076874606e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 119340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.521125429282586,\n",
      "      \"grad_norm\": 0.00032384286168962717,\n",
      "      \"learning_rate\": 1.2409892879647134e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 119360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.522385708434419,\n",
      "      \"grad_norm\": 0.0002919167745858431,\n",
      "      \"learning_rate\": 1.2403591682419661e-05,\n",
      "      \"loss\": 0.0268,\n",
      "      \"step\": 119380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.52364598758625,\n",
      "      \"grad_norm\": 0.0004225531592965126,\n",
      "      \"learning_rate\": 1.2397290485192187e-05,\n",
      "      \"loss\": 0.0366,\n",
      "      \"step\": 119400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.524906266738083,\n",
      "      \"grad_norm\": 0.08147653937339783,\n",
      "      \"learning_rate\": 1.2390989287964714e-05,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 119420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.5261665458899145,\n",
      "      \"grad_norm\": 0.0534718818962574,\n",
      "      \"learning_rate\": 1.2384688090737241e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 119440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.527426825041747,\n",
      "      \"grad_norm\": 0.0023993004579097033,\n",
      "      \"learning_rate\": 1.2378386893509767e-05,\n",
      "      \"loss\": 0.0353,\n",
      "      \"step\": 119460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.528687104193579,\n",
      "      \"grad_norm\": 0.062452469021081924,\n",
      "      \"learning_rate\": 1.2372085696282294e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 119480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.529947383345411,\n",
      "      \"grad_norm\": 0.055693067610263824,\n",
      "      \"learning_rate\": 1.236578449905482e-05,\n",
      "      \"loss\": 0.0165,\n",
      "      \"step\": 119500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.531207662497243,\n",
      "      \"grad_norm\": 0.002331273863092065,\n",
      "      \"learning_rate\": 1.2359483301827347e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 119520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.532467941649076,\n",
      "      \"grad_norm\": 0.10417307168245316,\n",
      "      \"learning_rate\": 1.2353182104599875e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 119540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.533728220800907,\n",
      "      \"grad_norm\": 0.0061228289268910885,\n",
      "      \"learning_rate\": 1.2346880907372402e-05,\n",
      "      \"loss\": 0.0296,\n",
      "      \"step\": 119560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.53498849995274,\n",
      "      \"grad_norm\": 0.006556641310453415,\n",
      "      \"learning_rate\": 1.2340579710144928e-05,\n",
      "      \"loss\": 0.0171,\n",
      "      \"step\": 119580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.536248779104572,\n",
      "      \"grad_norm\": 0.004268113523721695,\n",
      "      \"learning_rate\": 1.2334278512917455e-05,\n",
      "      \"loss\": 0.0263,\n",
      "      \"step\": 119600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.537509058256404,\n",
      "      \"grad_norm\": 0.04815881326794624,\n",
      "      \"learning_rate\": 1.232797731568998e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 119620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.538769337408236,\n",
      "      \"grad_norm\": 0.006480524316430092,\n",
      "      \"learning_rate\": 1.2321676118462508e-05,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 119640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.540029616560068,\n",
      "      \"grad_norm\": 0.013410696759819984,\n",
      "      \"learning_rate\": 1.2315374921235035e-05,\n",
      "      \"loss\": 0.0239,\n",
      "      \"step\": 119660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.5412898957119,\n",
      "      \"grad_norm\": 0.022119468078017235,\n",
      "      \"learning_rate\": 1.2309073724007563e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 119680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.542550174863733,\n",
      "      \"grad_norm\": 0.014298087917268276,\n",
      "      \"learning_rate\": 1.2302772526780088e-05,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 119700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.543810454015564,\n",
      "      \"grad_norm\": 0.0015629598638042808,\n",
      "      \"learning_rate\": 1.2296471329552616e-05,\n",
      "      \"loss\": 0.018,\n",
      "      \"step\": 119720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.545070733167397,\n",
      "      \"grad_norm\": 0.000936555617954582,\n",
      "      \"learning_rate\": 1.2290170132325141e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 119740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.546331012319229,\n",
      "      \"grad_norm\": 0.008173744194209576,\n",
      "      \"learning_rate\": 1.2283868935097669e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 119760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.547591291471061,\n",
      "      \"grad_norm\": 0.0005802882369607687,\n",
      "      \"learning_rate\": 1.2277567737870196e-05,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 119780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.548851570622893,\n",
      "      \"grad_norm\": 0.0007642056443728507,\n",
      "      \"learning_rate\": 1.2271266540642723e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 119800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.550111849774725,\n",
      "      \"grad_norm\": 0.00884279701858759,\n",
      "      \"learning_rate\": 1.2264965343415249e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 119820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.551372128926557,\n",
      "      \"grad_norm\": 0.0004678538825828582,\n",
      "      \"learning_rate\": 1.2258664146187776e-05,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 119840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.55263240807839,\n",
      "      \"grad_norm\": 0.015233599580824375,\n",
      "      \"learning_rate\": 1.2252362948960302e-05,\n",
      "      \"loss\": 0.0182,\n",
      "      \"step\": 119860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.5538926872302214,\n",
      "      \"grad_norm\": 0.0028922136407345533,\n",
      "      \"learning_rate\": 1.2246061751732831e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 119880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.555152966382053,\n",
      "      \"grad_norm\": 0.0032527740113437176,\n",
      "      \"learning_rate\": 1.2239760554505357e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 119900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.556413245533886,\n",
      "      \"grad_norm\": 0.18625590205192566,\n",
      "      \"learning_rate\": 1.2233459357277884e-05,\n",
      "      \"loss\": 0.0216,\n",
      "      \"step\": 119920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.557673524685718,\n",
      "      \"grad_norm\": 0.0035687373019754887,\n",
      "      \"learning_rate\": 1.222715816005041e-05,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 119940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.55893380383755,\n",
      "      \"grad_norm\": 0.007546201813966036,\n",
      "      \"learning_rate\": 1.2220856962822937e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 119960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.560194082989382,\n",
      "      \"grad_norm\": 0.0067407432943582535,\n",
      "      \"learning_rate\": 1.2214555765595463e-05,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 119980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.561454362141214,\n",
      "      \"grad_norm\": 0.0008248984813690186,\n",
      "      \"learning_rate\": 1.2208254568367992e-05,\n",
      "      \"loss\": 0.0218,\n",
      "      \"step\": 120000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.562714641293047,\n",
      "      \"grad_norm\": 0.0004081177758052945,\n",
      "      \"learning_rate\": 1.2201953371140517e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 120020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.5639749204448785,\n",
      "      \"grad_norm\": 0.0008194748079404235,\n",
      "      \"learning_rate\": 1.2195652173913045e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 120040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.56523519959671,\n",
      "      \"grad_norm\": 0.0008780789794400334,\n",
      "      \"learning_rate\": 1.218935097668557e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 120060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.566495478748543,\n",
      "      \"grad_norm\": 0.0028510631527751684,\n",
      "      \"learning_rate\": 1.2183049779458098e-05,\n",
      "      \"loss\": 0.013,\n",
      "      \"step\": 120080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.567755757900375,\n",
      "      \"grad_norm\": 0.00019887629605364054,\n",
      "      \"learning_rate\": 1.2176748582230623e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 120100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.569016037052207,\n",
      "      \"grad_norm\": 0.02167625166475773,\n",
      "      \"learning_rate\": 1.2170447385003152e-05,\n",
      "      \"loss\": 0.0076,\n",
      "      \"step\": 120120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.570276316204039,\n",
      "      \"grad_norm\": 0.00019640471145976335,\n",
      "      \"learning_rate\": 1.2164146187775678e-05,\n",
      "      \"loss\": 0.0211,\n",
      "      \"step\": 120140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.571536595355871,\n",
      "      \"grad_norm\": 0.022999156266450882,\n",
      "      \"learning_rate\": 1.2157844990548205e-05,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 120160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.572796874507704,\n",
      "      \"grad_norm\": 0.0004137078649364412,\n",
      "      \"learning_rate\": 1.2151543793320731e-05,\n",
      "      \"loss\": 0.0343,\n",
      "      \"step\": 120180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.574057153659536,\n",
      "      \"grad_norm\": 0.0006913826218806207,\n",
      "      \"learning_rate\": 1.2145242596093258e-05,\n",
      "      \"loss\": 0.0076,\n",
      "      \"step\": 120200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.575317432811367,\n",
      "      \"grad_norm\": 0.01830201968550682,\n",
      "      \"learning_rate\": 1.2138941398865786e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 120220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.5765777119632,\n",
      "      \"grad_norm\": 0.0012407545000314713,\n",
      "      \"learning_rate\": 1.2132640201638311e-05,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 120240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.577837991115032,\n",
      "      \"grad_norm\": 0.0024521234445273876,\n",
      "      \"learning_rate\": 1.2126339004410839e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 120260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.579098270266864,\n",
      "      \"grad_norm\": 0.45980730652809143,\n",
      "      \"learning_rate\": 1.2120037807183366e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 120280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.580358549418696,\n",
      "      \"grad_norm\": 0.00029318517772480845,\n",
      "      \"learning_rate\": 1.2113736609955892e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 120300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.581618828570528,\n",
      "      \"grad_norm\": 0.0004491743165999651,\n",
      "      \"learning_rate\": 1.2107435412728419e-05,\n",
      "      \"loss\": 0.0323,\n",
      "      \"step\": 120320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.582879107722361,\n",
      "      \"grad_norm\": 0.0002582662273198366,\n",
      "      \"learning_rate\": 1.2101134215500946e-05,\n",
      "      \"loss\": 0.0075,\n",
      "      \"step\": 120340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.584139386874193,\n",
      "      \"grad_norm\": 0.00297733792103827,\n",
      "      \"learning_rate\": 1.2094833018273472e-05,\n",
      "      \"loss\": 0.0258,\n",
      "      \"step\": 120360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.585399666026024,\n",
      "      \"grad_norm\": 0.0014129611663520336,\n",
      "      \"learning_rate\": 1.2088531821046e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 120380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.586659945177857,\n",
      "      \"grad_norm\": 0.023482803255319595,\n",
      "      \"learning_rate\": 1.2082230623818525e-05,\n",
      "      \"loss\": 0.003,\n",
      "      \"step\": 120400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.5879202243296895,\n",
      "      \"grad_norm\": 0.0007037204341031611,\n",
      "      \"learning_rate\": 1.2075929426591052e-05,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 120420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.589180503481521,\n",
      "      \"grad_norm\": 0.0009076485293917358,\n",
      "      \"learning_rate\": 1.206962822936358e-05,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 120440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.590440782633353,\n",
      "      \"grad_norm\": 0.0020356394816190004,\n",
      "      \"learning_rate\": 1.2063327032136107e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 120460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.5917010617851854,\n",
      "      \"grad_norm\": 0.0020784258376806974,\n",
      "      \"learning_rate\": 1.2057025834908633e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 120480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.592961340937018,\n",
      "      \"grad_norm\": 0.009955675341188908,\n",
      "      \"learning_rate\": 1.205072463768116e-05,\n",
      "      \"loss\": 0.0021,\n",
      "      \"step\": 120500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.59422162008885,\n",
      "      \"grad_norm\": 0.023508872836828232,\n",
      "      \"learning_rate\": 1.2044423440453686e-05,\n",
      "      \"loss\": 0.0297,\n",
      "      \"step\": 120520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.595481899240681,\n",
      "      \"grad_norm\": 0.01697690598666668,\n",
      "      \"learning_rate\": 1.2038122243226213e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 120540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.596742178392514,\n",
      "      \"grad_norm\": 0.00679831113666296,\n",
      "      \"learning_rate\": 1.203182104599874e-05,\n",
      "      \"loss\": 0.0233,\n",
      "      \"step\": 120560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.5980024575443466,\n",
      "      \"grad_norm\": 0.0038773519918322563,\n",
      "      \"learning_rate\": 1.2025519848771268e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 120580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.599262736696178,\n",
      "      \"grad_norm\": 0.00021192590065766126,\n",
      "      \"learning_rate\": 1.2019218651543794e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 120600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.60052301584801,\n",
      "      \"grad_norm\": 0.003893007058650255,\n",
      "      \"learning_rate\": 1.2012917454316321e-05,\n",
      "      \"loss\": 0.0042,\n",
      "      \"step\": 120620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.6017832949998425,\n",
      "      \"grad_norm\": 0.0005039997049607337,\n",
      "      \"learning_rate\": 1.2006616257088847e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 120640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.603043574151675,\n",
      "      \"grad_norm\": 0.2198227345943451,\n",
      "      \"learning_rate\": 1.2000315059861374e-05,\n",
      "      \"loss\": 0.0246,\n",
      "      \"step\": 120660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.604303853303507,\n",
      "      \"grad_norm\": 0.007628342602401972,\n",
      "      \"learning_rate\": 1.1994013862633901e-05,\n",
      "      \"loss\": 0.0057,\n",
      "      \"step\": 120680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.6055641324553385,\n",
      "      \"grad_norm\": 0.0006796290399506688,\n",
      "      \"learning_rate\": 1.1987712665406429e-05,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 120700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.606824411607171,\n",
      "      \"grad_norm\": 0.20257185399532318,\n",
      "      \"learning_rate\": 1.1981411468178954e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 120720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.608084690759003,\n",
      "      \"grad_norm\": 0.0006531430408358574,\n",
      "      \"learning_rate\": 1.1975110270951482e-05,\n",
      "      \"loss\": 0.021,\n",
      "      \"step\": 120740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.609344969910835,\n",
      "      \"grad_norm\": 0.0032712898682802916,\n",
      "      \"learning_rate\": 1.196912413358538e-05,\n",
      "      \"loss\": 0.0239,\n",
      "      \"step\": 120760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.610605249062667,\n",
      "      \"grad_norm\": 0.005732722580432892,\n",
      "      \"learning_rate\": 1.196282293635791e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 120780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.6118655282145,\n",
      "      \"grad_norm\": 0.002270590281113982,\n",
      "      \"learning_rate\": 1.1956521739130435e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 120800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.613125807366331,\n",
      "      \"grad_norm\": 0.0023699661251157522,\n",
      "      \"learning_rate\": 1.1950220541902963e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 120820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.614386086518164,\n",
      "      \"grad_norm\": 0.0007649965700693429,\n",
      "      \"learning_rate\": 1.1943919344675488e-05,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 120840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.6156463656699955,\n",
      "      \"grad_norm\": 0.0003981472982559353,\n",
      "      \"learning_rate\": 1.1937618147448016e-05,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 120860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.616906644821828,\n",
      "      \"grad_norm\": 0.002342908876016736,\n",
      "      \"learning_rate\": 1.1931316950220541e-05,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 120880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.61816692397366,\n",
      "      \"grad_norm\": 0.005009409040212631,\n",
      "      \"learning_rate\": 1.192501575299307e-05,\n",
      "      \"loss\": 0.0359,\n",
      "      \"step\": 120900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.619427203125492,\n",
      "      \"grad_norm\": 0.09677492827177048,\n",
      "      \"learning_rate\": 1.1918714555765596e-05,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 120920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.620687482277324,\n",
      "      \"grad_norm\": 0.003372771665453911,\n",
      "      \"learning_rate\": 1.1912413358538123e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 120940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.621947761429157,\n",
      "      \"grad_norm\": 0.0017395694740116596,\n",
      "      \"learning_rate\": 1.1906112161310649e-05,\n",
      "      \"loss\": 0.0352,\n",
      "      \"step\": 120960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.623208040580988,\n",
      "      \"grad_norm\": 0.007086005061864853,\n",
      "      \"learning_rate\": 1.1899810964083176e-05,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 120980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.624468319732821,\n",
      "      \"grad_norm\": 0.04698668047785759,\n",
      "      \"learning_rate\": 1.1893509766855704e-05,\n",
      "      \"loss\": 0.0255,\n",
      "      \"step\": 121000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.625728598884653,\n",
      "      \"grad_norm\": 0.005722848232835531,\n",
      "      \"learning_rate\": 1.1887208569628231e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 121020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.626988878036485,\n",
      "      \"grad_norm\": 0.00034140661591663957,\n",
      "      \"learning_rate\": 1.1880907372400757e-05,\n",
      "      \"loss\": 0.0043,\n",
      "      \"step\": 121040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.628249157188317,\n",
      "      \"grad_norm\": 0.004982343874871731,\n",
      "      \"learning_rate\": 1.1874606175173284e-05,\n",
      "      \"loss\": 0.0231,\n",
      "      \"step\": 121060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.629509436340149,\n",
      "      \"grad_norm\": 0.002387558575719595,\n",
      "      \"learning_rate\": 1.186830497794581e-05,\n",
      "      \"loss\": 0.0197,\n",
      "      \"step\": 121080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.630769715491981,\n",
      "      \"grad_norm\": 0.09766718000173569,\n",
      "      \"learning_rate\": 1.1862003780718337e-05,\n",
      "      \"loss\": 0.0206,\n",
      "      \"step\": 121100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.632029994643814,\n",
      "      \"grad_norm\": 0.0014827803242951632,\n",
      "      \"learning_rate\": 1.1855702583490864e-05,\n",
      "      \"loss\": 0.0383,\n",
      "      \"step\": 121120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.633290273795645,\n",
      "      \"grad_norm\": 0.17956110835075378,\n",
      "      \"learning_rate\": 1.184940138626339e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 121140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.634550552947478,\n",
      "      \"grad_norm\": 0.034645866602659225,\n",
      "      \"learning_rate\": 1.1843100189035917e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 121160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.63581083209931,\n",
      "      \"grad_norm\": 0.015323965810239315,\n",
      "      \"learning_rate\": 1.1836798991808445e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 121180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.637071111251142,\n",
      "      \"grad_norm\": 0.00505315326154232,\n",
      "      \"learning_rate\": 1.183049779458097e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 121200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.638331390402974,\n",
      "      \"grad_norm\": 0.005355458240956068,\n",
      "      \"learning_rate\": 1.1824196597353498e-05,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 121220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.6395916695548065,\n",
      "      \"grad_norm\": 0.2001417577266693,\n",
      "      \"learning_rate\": 1.1817895400126025e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 121240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.640851948706638,\n",
      "      \"grad_norm\": 0.0013050328707322478,\n",
      "      \"learning_rate\": 1.181159420289855e-05,\n",
      "      \"loss\": 0.0075,\n",
      "      \"step\": 121260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.642112227858471,\n",
      "      \"grad_norm\": 0.0011230931850150228,\n",
      "      \"learning_rate\": 1.1805293005671078e-05,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 121280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.6433725070103025,\n",
      "      \"grad_norm\": 0.0006782920099794865,\n",
      "      \"learning_rate\": 1.1798991808443604e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 121300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.644632786162135,\n",
      "      \"grad_norm\": 0.0039102681912481785,\n",
      "      \"learning_rate\": 1.1792690611216131e-05,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 121320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.645893065313967,\n",
      "      \"grad_norm\": 0.0005729232216253877,\n",
      "      \"learning_rate\": 1.1786389413988659e-05,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 121340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.647153344465799,\n",
      "      \"grad_norm\": 0.03551805764436722,\n",
      "      \"learning_rate\": 1.1780088216761186e-05,\n",
      "      \"loss\": 0.0093,\n",
      "      \"step\": 121360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.648413623617631,\n",
      "      \"grad_norm\": 0.07929953187704086,\n",
      "      \"learning_rate\": 1.1773787019533712e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 121380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.649673902769464,\n",
      "      \"grad_norm\": 0.0011649830266833305,\n",
      "      \"learning_rate\": 1.1767485822306239e-05,\n",
      "      \"loss\": 0.0256,\n",
      "      \"step\": 121400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.650934181921295,\n",
      "      \"grad_norm\": 0.0006314334459602833,\n",
      "      \"learning_rate\": 1.1761184625078764e-05,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 121420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.652194461073128,\n",
      "      \"grad_norm\": 0.003361557377502322,\n",
      "      \"learning_rate\": 1.1754883427851292e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 121440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.6534547402249595,\n",
      "      \"grad_norm\": 0.0010352057870477438,\n",
      "      \"learning_rate\": 1.174858223062382e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 121460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.654715019376792,\n",
      "      \"grad_norm\": 0.0005436607170850039,\n",
      "      \"learning_rate\": 1.1742281033396347e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 121480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.655975298528624,\n",
      "      \"grad_norm\": 0.0010519050993025303,\n",
      "      \"learning_rate\": 1.1735979836168872e-05,\n",
      "      \"loss\": 0.0147,\n",
      "      \"step\": 121500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.657235577680456,\n",
      "      \"grad_norm\": 0.0009142283815890551,\n",
      "      \"learning_rate\": 1.17296786389414e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 121520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.658495856832288,\n",
      "      \"grad_norm\": 0.000863234861753881,\n",
      "      \"learning_rate\": 1.1723377441713925e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 121540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.659756135984121,\n",
      "      \"grad_norm\": 29.302526473999023,\n",
      "      \"learning_rate\": 1.1717076244486453e-05,\n",
      "      \"loss\": 0.0034,\n",
      "      \"step\": 121560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.661016415135952,\n",
      "      \"grad_norm\": 0.0016012067208066583,\n",
      "      \"learning_rate\": 1.171077504725898e-05,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 121580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.662276694287785,\n",
      "      \"grad_norm\": 0.0019318412523716688,\n",
      "      \"learning_rate\": 1.1704473850031507e-05,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 121600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.663536973439617,\n",
      "      \"grad_norm\": 0.0053177885711193085,\n",
      "      \"learning_rate\": 1.1698172652804033e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 121620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.664797252591449,\n",
      "      \"grad_norm\": 0.0002756134490482509,\n",
      "      \"learning_rate\": 1.169187145557656e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 121640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.666057531743281,\n",
      "      \"grad_norm\": 0.0007432882557623088,\n",
      "      \"learning_rate\": 1.1685570258349086e-05,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 121660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.667317810895113,\n",
      "      \"grad_norm\": 0.0012610278790816665,\n",
      "      \"learning_rate\": 1.1679269061121615e-05,\n",
      "      \"loss\": 0.0188,\n",
      "      \"step\": 121680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.668578090046945,\n",
      "      \"grad_norm\": 0.008253459818661213,\n",
      "      \"learning_rate\": 1.167296786389414e-05,\n",
      "      \"loss\": 0.0297,\n",
      "      \"step\": 121700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.669838369198778,\n",
      "      \"grad_norm\": 0.0028130793944001198,\n",
      "      \"learning_rate\": 1.1666666666666668e-05,\n",
      "      \"loss\": 0.0227,\n",
      "      \"step\": 121720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.671098648350609,\n",
      "      \"grad_norm\": 37.80892562866211,\n",
      "      \"learning_rate\": 1.1660365469439194e-05,\n",
      "      \"loss\": 0.0034,\n",
      "      \"step\": 121740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.672358927502442,\n",
      "      \"grad_norm\": 0.001728240866214037,\n",
      "      \"learning_rate\": 1.1654064272211721e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 121760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.673619206654274,\n",
      "      \"grad_norm\": 0.0026749682147055864,\n",
      "      \"learning_rate\": 1.1647763074984247e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 121780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.674879485806106,\n",
      "      \"grad_norm\": 0.01960216835141182,\n",
      "      \"learning_rate\": 1.1641461877756776e-05,\n",
      "      \"loss\": 0.0069,\n",
      "      \"step\": 121800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.676139764957938,\n",
      "      \"grad_norm\": 0.0006469625514000654,\n",
      "      \"learning_rate\": 1.1635160680529301e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 121820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.6774000441097705,\n",
      "      \"grad_norm\": 0.00696556456387043,\n",
      "      \"learning_rate\": 1.1628859483301829e-05,\n",
      "      \"loss\": 0.0047,\n",
      "      \"step\": 121840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.678660323261602,\n",
      "      \"grad_norm\": 0.004311937373131514,\n",
      "      \"learning_rate\": 1.1622558286074354e-05,\n",
      "      \"loss\": 0.017,\n",
      "      \"step\": 121860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.679920602413435,\n",
      "      \"grad_norm\": 0.00127814756706357,\n",
      "      \"learning_rate\": 1.1616257088846882e-05,\n",
      "      \"loss\": 0.0161,\n",
      "      \"step\": 121880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.6811808815652665,\n",
      "      \"grad_norm\": 0.0004334760596975684,\n",
      "      \"learning_rate\": 1.1609955891619407e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 121900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.682441160717099,\n",
      "      \"grad_norm\": 0.0002948651381302625,\n",
      "      \"learning_rate\": 1.1603654694391936e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 121920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.683701439868931,\n",
      "      \"grad_norm\": 0.004256247542798519,\n",
      "      \"learning_rate\": 1.1597353497164462e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 121940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.684961719020763,\n",
      "      \"grad_norm\": 0.029165325686335564,\n",
      "      \"learning_rate\": 1.159105229993699e-05,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 121960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.686221998172595,\n",
      "      \"grad_norm\": 0.020091639831662178,\n",
      "      \"learning_rate\": 1.1584751102709515e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 121980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.687482277324428,\n",
      "      \"grad_norm\": 0.35731762647628784,\n",
      "      \"learning_rate\": 1.1578449905482042e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 122000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.688742556476259,\n",
      "      \"grad_norm\": 0.002902493579313159,\n",
      "      \"learning_rate\": 1.157214870825457e-05,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 122020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.690002835628092,\n",
      "      \"grad_norm\": 0.0007957850466482341,\n",
      "      \"learning_rate\": 1.1565847511027097e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 122040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.6912631147799235,\n",
      "      \"grad_norm\": 0.0008900929242372513,\n",
      "      \"learning_rate\": 1.1559546313799623e-05,\n",
      "      \"loss\": 0.0271,\n",
      "      \"step\": 122060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.692523393931756,\n",
      "      \"grad_norm\": 53.06438446044922,\n",
      "      \"learning_rate\": 1.155324511657215e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 122080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.693783673083588,\n",
      "      \"grad_norm\": 0.0002994168607983738,\n",
      "      \"learning_rate\": 1.1546943919344676e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 122100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.69504395223542,\n",
      "      \"grad_norm\": 0.024119848385453224,\n",
      "      \"learning_rate\": 1.1540642722117203e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 122120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.696304231387252,\n",
      "      \"grad_norm\": 0.00038379969191737473,\n",
      "      \"learning_rate\": 1.153434152488973e-05,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 122140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.697564510539085,\n",
      "      \"grad_norm\": 0.00036741813528351486,\n",
      "      \"learning_rate\": 1.1528040327662256e-05,\n",
      "      \"loss\": 0.0057,\n",
      "      \"step\": 122160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.698824789690916,\n",
      "      \"grad_norm\": 0.012623128481209278,\n",
      "      \"learning_rate\": 1.1521739130434783e-05,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 122180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.700085068842749,\n",
      "      \"grad_norm\": 0.004424954764544964,\n",
      "      \"learning_rate\": 1.151543793320731e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 122200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.701345347994581,\n",
      "      \"grad_norm\": 0.0009633747395128012,\n",
      "      \"learning_rate\": 1.1509136735979836e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 122220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.702605627146413,\n",
      "      \"grad_norm\": 0.0002791194710880518,\n",
      "      \"learning_rate\": 1.1502835538752364e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 122240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.703865906298245,\n",
      "      \"grad_norm\": 0.13028739392757416,\n",
      "      \"learning_rate\": 1.1496534341524891e-05,\n",
      "      \"loss\": 0.0215,\n",
      "      \"step\": 122260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.705126185450077,\n",
      "      \"grad_norm\": 0.0008473580237478018,\n",
      "      \"learning_rate\": 1.1490233144297417e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 122280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.706386464601909,\n",
      "      \"grad_norm\": 0.0017781880451366305,\n",
      "      \"learning_rate\": 1.1483931947069944e-05,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 122300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.707646743753742,\n",
      "      \"grad_norm\": 0.001355186221189797,\n",
      "      \"learning_rate\": 1.147763074984247e-05,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 122320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.708907022905573,\n",
      "      \"grad_norm\": 0.4623054265975952,\n",
      "      \"learning_rate\": 1.1471329552614997e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 122340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.710167302057406,\n",
      "      \"grad_norm\": 0.001529407105408609,\n",
      "      \"learning_rate\": 1.1465028355387524e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 122360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.711427581209238,\n",
      "      \"grad_norm\": 0.0016027926467359066,\n",
      "      \"learning_rate\": 1.1458727158160052e-05,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 122380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.71268786036107,\n",
      "      \"grad_norm\": 0.0004989914014004171,\n",
      "      \"learning_rate\": 1.1452425960932577e-05,\n",
      "      \"loss\": 0.0429,\n",
      "      \"step\": 122400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.713948139512902,\n",
      "      \"grad_norm\": 0.0016779943834990263,\n",
      "      \"learning_rate\": 1.1446124763705105e-05,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 122420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.7152084186647345,\n",
      "      \"grad_norm\": 0.007888083346188068,\n",
      "      \"learning_rate\": 1.143982356647763e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 122440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.716468697816566,\n",
      "      \"grad_norm\": 0.004585557617247105,\n",
      "      \"learning_rate\": 1.1433522369250158e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 122460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.717728976968399,\n",
      "      \"grad_norm\": 0.004449544940143824,\n",
      "      \"learning_rate\": 1.1427221172022685e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 122480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.7189892561202305,\n",
      "      \"grad_norm\": 0.0032786333467811346,\n",
      "      \"learning_rate\": 1.1420919974795212e-05,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 122500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.720249535272063,\n",
      "      \"grad_norm\": 0.116331085562706,\n",
      "      \"learning_rate\": 1.1414618777567738e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 122520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.721509814423895,\n",
      "      \"grad_norm\": 0.001514087663963437,\n",
      "      \"learning_rate\": 1.1408317580340265e-05,\n",
      "      \"loss\": 0.0201,\n",
      "      \"step\": 122540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.722770093575727,\n",
      "      \"grad_norm\": 0.0009309032466262579,\n",
      "      \"learning_rate\": 1.1402016383112791e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 122560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.724030372727559,\n",
      "      \"grad_norm\": 0.00036541742156259716,\n",
      "      \"learning_rate\": 1.1395715185885318e-05,\n",
      "      \"loss\": 0.0197,\n",
      "      \"step\": 122580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.725290651879392,\n",
      "      \"grad_norm\": 0.005621828138828278,\n",
      "      \"learning_rate\": 1.1389413988657846e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 122600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.726550931031223,\n",
      "      \"grad_norm\": 0.03984002023935318,\n",
      "      \"learning_rate\": 1.1383112791430373e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 122620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.727811210183056,\n",
      "      \"grad_norm\": 15.218769073486328,\n",
      "      \"learning_rate\": 1.1376811594202899e-05,\n",
      "      \"loss\": 0.0067,\n",
      "      \"step\": 122640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.7290714893348875,\n",
      "      \"grad_norm\": 0.0013402996119111776,\n",
      "      \"learning_rate\": 1.1370510396975426e-05,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 122660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.73033176848672,\n",
      "      \"grad_norm\": 0.0002927418099716306,\n",
      "      \"learning_rate\": 1.1364209199747952e-05,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 122680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.731592047638552,\n",
      "      \"grad_norm\": 0.0008754981681704521,\n",
      "      \"learning_rate\": 1.135790800252048e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 122700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.732852326790384,\n",
      "      \"grad_norm\": 0.000244099588599056,\n",
      "      \"learning_rate\": 1.1351606805293006e-05,\n",
      "      \"loss\": 0.0159,\n",
      "      \"step\": 122720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.734112605942216,\n",
      "      \"grad_norm\": 0.0255496297031641,\n",
      "      \"learning_rate\": 1.1345305608065534e-05,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 122740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.735372885094049,\n",
      "      \"grad_norm\": 0.0018255519680678844,\n",
      "      \"learning_rate\": 1.133900441083806e-05,\n",
      "      \"loss\": 0.0221,\n",
      "      \"step\": 122760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.73663316424588,\n",
      "      \"grad_norm\": 0.0004833216080442071,\n",
      "      \"learning_rate\": 1.1332703213610587e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 122780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.737893443397713,\n",
      "      \"grad_norm\": 0.003945418167859316,\n",
      "      \"learning_rate\": 1.1326402016383112e-05,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 122800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.739153722549545,\n",
      "      \"grad_norm\": 22.266094207763672,\n",
      "      \"learning_rate\": 1.1320100819155641e-05,\n",
      "      \"loss\": 0.0208,\n",
      "      \"step\": 122820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.740414001701377,\n",
      "      \"grad_norm\": 0.022612309083342552,\n",
      "      \"learning_rate\": 1.1313799621928167e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 122840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.741674280853209,\n",
      "      \"grad_norm\": 0.0005078167887404561,\n",
      "      \"learning_rate\": 1.1307498424700694e-05,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 122860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.742934560005041,\n",
      "      \"grad_norm\": 0.07746904343366623,\n",
      "      \"learning_rate\": 1.130119722747322e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 122880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.744194839156873,\n",
      "      \"grad_norm\": 0.0013660662807524204,\n",
      "      \"learning_rate\": 1.1294896030245747e-05,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 122900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.745455118308706,\n",
      "      \"grad_norm\": 0.0043814838863909245,\n",
      "      \"learning_rate\": 1.1288594833018273e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 122920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.746715397460537,\n",
      "      \"grad_norm\": 0.0002635751152411103,\n",
      "      \"learning_rate\": 1.1282293635790802e-05,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 122940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.74797567661237,\n",
      "      \"grad_norm\": 0.03982513025403023,\n",
      "      \"learning_rate\": 1.1275992438563328e-05,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 122960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.749235955764202,\n",
      "      \"grad_norm\": 0.0011825929395854473,\n",
      "      \"learning_rate\": 1.1269691241335855e-05,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 122980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.750496234916034,\n",
      "      \"grad_norm\": 0.007321624085307121,\n",
      "      \"learning_rate\": 1.126339004410838e-05,\n",
      "      \"loss\": 0.0265,\n",
      "      \"step\": 123000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.751756514067866,\n",
      "      \"grad_norm\": 0.004256932996213436,\n",
      "      \"learning_rate\": 1.1257088846880908e-05,\n",
      "      \"loss\": 0.0116,\n",
      "      \"step\": 123020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.7530167932196985,\n",
      "      \"grad_norm\": 5.4663920402526855,\n",
      "      \"learning_rate\": 1.1250787649653435e-05,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 123040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.75427707237153,\n",
      "      \"grad_norm\": 0.0011501119006425142,\n",
      "      \"learning_rate\": 1.1244486452425961e-05,\n",
      "      \"loss\": 0.0076,\n",
      "      \"step\": 123060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.755537351523363,\n",
      "      \"grad_norm\": 0.0017440373776480556,\n",
      "      \"learning_rate\": 1.1238185255198488e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 123080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.7567976306751945,\n",
      "      \"grad_norm\": 0.01751803420484066,\n",
      "      \"learning_rate\": 1.1231884057971016e-05,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 123100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.758057909827027,\n",
      "      \"grad_norm\": 0.0007690682541579008,\n",
      "      \"learning_rate\": 1.1225582860743541e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 123120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.759318188978859,\n",
      "      \"grad_norm\": 0.020846357569098473,\n",
      "      \"learning_rate\": 1.1219281663516069e-05,\n",
      "      \"loss\": 0.018,\n",
      "      \"step\": 123140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.760578468130691,\n",
      "      \"grad_norm\": 0.3883315920829773,\n",
      "      \"learning_rate\": 1.1212980466288596e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 123160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.761838747282523,\n",
      "      \"grad_norm\": 0.000560867425519973,\n",
      "      \"learning_rate\": 1.1206679269061122e-05,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 123180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.763099026434356,\n",
      "      \"grad_norm\": 0.009873617440462112,\n",
      "      \"learning_rate\": 1.1200378071833649e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 123200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.764359305586187,\n",
      "      \"grad_norm\": 0.0026131749618798494,\n",
      "      \"learning_rate\": 1.1194076874606175e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 123220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.76561958473802,\n",
      "      \"grad_norm\": 0.30722394585609436,\n",
      "      \"learning_rate\": 1.1187775677378702e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 123240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.7668798638898515,\n",
      "      \"grad_norm\": 0.002625528257340193,\n",
      "      \"learning_rate\": 1.118147448015123e-05,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 123260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.768140143041684,\n",
      "      \"grad_norm\": 0.00042696372838690877,\n",
      "      \"learning_rate\": 1.1175173282923757e-05,\n",
      "      \"loss\": 0.0256,\n",
      "      \"step\": 123280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.769400422193516,\n",
      "      \"grad_norm\": 0.0013101933291181922,\n",
      "      \"learning_rate\": 1.1168872085696282e-05,\n",
      "      \"loss\": 0.0603,\n",
      "      \"step\": 123300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.770660701345348,\n",
      "      \"grad_norm\": 0.032284047454595566,\n",
      "      \"learning_rate\": 1.116257088846881e-05,\n",
      "      \"loss\": 0.0123,\n",
      "      \"step\": 123320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.77192098049718,\n",
      "      \"grad_norm\": 0.006890129763633013,\n",
      "      \"learning_rate\": 1.1156269691241335e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 123340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.773181259649013,\n",
      "      \"grad_norm\": 0.025427088141441345,\n",
      "      \"learning_rate\": 1.1149968494013863e-05,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 123360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.774441538800844,\n",
      "      \"grad_norm\": 0.002340087201446295,\n",
      "      \"learning_rate\": 1.114366729678639e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 123380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.775701817952677,\n",
      "      \"grad_norm\": 0.00048495468217879534,\n",
      "      \"learning_rate\": 1.1137366099558917e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 123400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.776962097104509,\n",
      "      \"grad_norm\": 0.0010762475430965424,\n",
      "      \"learning_rate\": 1.1131064902331443e-05,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 123420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.778222376256341,\n",
      "      \"grad_norm\": 0.000417747680330649,\n",
      "      \"learning_rate\": 1.112476370510397e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 123440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.779482655408173,\n",
      "      \"grad_norm\": 4.617095947265625,\n",
      "      \"learning_rate\": 1.1118462507876496e-05,\n",
      "      \"loss\": 0.0424,\n",
      "      \"step\": 123460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.7807429345600045,\n",
      "      \"grad_norm\": 9.275386810302734,\n",
      "      \"learning_rate\": 1.1112161310649023e-05,\n",
      "      \"loss\": 0.043,\n",
      "      \"step\": 123480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.782003213711837,\n",
      "      \"grad_norm\": 0.006213606335222721,\n",
      "      \"learning_rate\": 1.1106175173282924e-05,\n",
      "      \"loss\": 0.0031,\n",
      "      \"step\": 123500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.78326349286367,\n",
      "      \"grad_norm\": 0.0003793565556406975,\n",
      "      \"learning_rate\": 1.1099873976055452e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 123520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.784523772015501,\n",
      "      \"grad_norm\": 0.0006481921300292015,\n",
      "      \"learning_rate\": 1.1093572778827977e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 123540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.785784051167333,\n",
      "      \"grad_norm\": 0.010298061184585094,\n",
      "      \"learning_rate\": 1.1087271581600505e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 123560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.787044330319166,\n",
      "      \"grad_norm\": 0.0014408050337806344,\n",
      "      \"learning_rate\": 1.108097038437303e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 123580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.788304609470998,\n",
      "      \"grad_norm\": 14.489429473876953,\n",
      "      \"learning_rate\": 1.107466918714556e-05,\n",
      "      \"loss\": 0.0064,\n",
      "      \"step\": 123600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.78956488862283,\n",
      "      \"grad_norm\": 0.01721227914094925,\n",
      "      \"learning_rate\": 1.1068367989918085e-05,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 123620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.790825167774662,\n",
      "      \"grad_norm\": 0.0009285298874601722,\n",
      "      \"learning_rate\": 1.1062066792690612e-05,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 123640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.792085446926494,\n",
      "      \"grad_norm\": 0.005545068997889757,\n",
      "      \"learning_rate\": 1.1055765595463138e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 123660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.793345726078327,\n",
      "      \"grad_norm\": 0.011690926738083363,\n",
      "      \"learning_rate\": 1.1049464398235665e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 123680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.7946060052301585,\n",
      "      \"grad_norm\": 10.973498344421387,\n",
      "      \"learning_rate\": 1.1043163201008191e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 123700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.79586628438199,\n",
      "      \"grad_norm\": 0.00034443233744241297,\n",
      "      \"learning_rate\": 1.103686200378072e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 123720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.797126563533823,\n",
      "      \"grad_norm\": 0.0006753619527444243,\n",
      "      \"learning_rate\": 1.1030560806553246e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 123740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.798386842685655,\n",
      "      \"grad_norm\": 0.010343743488192558,\n",
      "      \"learning_rate\": 1.1024259609325773e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 123760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.799647121837487,\n",
      "      \"grad_norm\": 0.00043793441727757454,\n",
      "      \"learning_rate\": 1.1017958412098299e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 123780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.800907400989319,\n",
      "      \"grad_norm\": 0.00018088193610310555,\n",
      "      \"learning_rate\": 1.1011657214870826e-05,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 123800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.802167680141151,\n",
      "      \"grad_norm\": 0.0002640306483954191,\n",
      "      \"learning_rate\": 1.1005356017643353e-05,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 123820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.803427959292984,\n",
      "      \"grad_norm\": 0.0011139455018565059,\n",
      "      \"learning_rate\": 1.099905482041588e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 123840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.8046882384448155,\n",
      "      \"grad_norm\": 10.423893928527832,\n",
      "      \"learning_rate\": 1.0992753623188406e-05,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 123860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.805948517596647,\n",
      "      \"grad_norm\": 0.0003832829825114459,\n",
      "      \"learning_rate\": 1.0986452425960934e-05,\n",
      "      \"loss\": 0.0517,\n",
      "      \"step\": 123880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.80720879674848,\n",
      "      \"grad_norm\": 0.0019857045263051987,\n",
      "      \"learning_rate\": 1.098015122873346e-05,\n",
      "      \"loss\": 0.0099,\n",
      "      \"step\": 123900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.808469075900312,\n",
      "      \"grad_norm\": 0.0074261887930333614,\n",
      "      \"learning_rate\": 1.0973850031505987e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 123920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.809729355052144,\n",
      "      \"grad_norm\": 0.0933842808008194,\n",
      "      \"learning_rate\": 1.0967548834278514e-05,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 123940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.810989634203976,\n",
      "      \"grad_norm\": 0.01593383029103279,\n",
      "      \"learning_rate\": 1.096124763705104e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 123960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.812249913355808,\n",
      "      \"grad_norm\": 0.0002283391950186342,\n",
      "      \"learning_rate\": 1.0954946439823567e-05,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 123980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.813510192507641,\n",
      "      \"grad_norm\": 0.12187399715185165,\n",
      "      \"learning_rate\": 1.0948645242596094e-05,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 124000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.814770471659473,\n",
      "      \"grad_norm\": 0.025102868676185608,\n",
      "      \"learning_rate\": 1.094234404536862e-05,\n",
      "      \"loss\": 0.0245,\n",
      "      \"step\": 124020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.816030750811304,\n",
      "      \"grad_norm\": 0.0002625955967232585,\n",
      "      \"learning_rate\": 1.0936042848141147e-05,\n",
      "      \"loss\": 0.0048,\n",
      "      \"step\": 124040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.817291029963137,\n",
      "      \"grad_norm\": 0.00026581736165098846,\n",
      "      \"learning_rate\": 1.0929741650913675e-05,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 124060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.818551309114969,\n",
      "      \"grad_norm\": 0.0001960020454134792,\n",
      "      \"learning_rate\": 1.09234404536862e-05,\n",
      "      \"loss\": 0.007,\n",
      "      \"step\": 124080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.819811588266801,\n",
      "      \"grad_norm\": 0.00022540014469996095,\n",
      "      \"learning_rate\": 1.0917139256458728e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 124100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.821071867418633,\n",
      "      \"grad_norm\": 0.0002282554196426645,\n",
      "      \"learning_rate\": 1.0910838059231253e-05,\n",
      "      \"loss\": 0.0201,\n",
      "      \"step\": 124120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.822332146570465,\n",
      "      \"grad_norm\": 0.002649637870490551,\n",
      "      \"learning_rate\": 1.090453686200378e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 124140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.823592425722298,\n",
      "      \"grad_norm\": 0.0004901366773992777,\n",
      "      \"learning_rate\": 1.0898235664776308e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 124160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.82485270487413,\n",
      "      \"grad_norm\": 0.0011365010868757963,\n",
      "      \"learning_rate\": 1.0891934467548835e-05,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 124180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.826112984025961,\n",
      "      \"grad_norm\": 0.00042424394632689655,\n",
      "      \"learning_rate\": 1.0885633270321361e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 124200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.827373263177794,\n",
      "      \"grad_norm\": 0.08368854969739914,\n",
      "      \"learning_rate\": 1.0879332073093888e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 124220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.8286335423296265,\n",
      "      \"grad_norm\": 0.007091875188052654,\n",
      "      \"learning_rate\": 1.0873030875866414e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 124240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.829893821481458,\n",
      "      \"grad_norm\": 0.0006253921310417354,\n",
      "      \"learning_rate\": 1.0866729678638941e-05,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 124260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.83115410063329,\n",
      "      \"grad_norm\": 0.00021534856932703406,\n",
      "      \"learning_rate\": 1.0860428481411469e-05,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 124280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.8324143797851224,\n",
      "      \"grad_norm\": 0.00015966790670063347,\n",
      "      \"learning_rate\": 1.0854127284183996e-05,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 124300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.833674658936955,\n",
      "      \"grad_norm\": 0.00045785229303874075,\n",
      "      \"learning_rate\": 1.0847826086956522e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 124320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.834934938088787,\n",
      "      \"grad_norm\": 0.001573823275975883,\n",
      "      \"learning_rate\": 1.0841524889729049e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 124340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.836195217240618,\n",
      "      \"grad_norm\": 0.00016129219147842377,\n",
      "      \"learning_rate\": 1.0835223692501575e-05,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 124360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.837455496392451,\n",
      "      \"grad_norm\": 0.9893419146537781,\n",
      "      \"learning_rate\": 1.0828922495274102e-05,\n",
      "      \"loss\": 0.0033,\n",
      "      \"step\": 124380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.838715775544283,\n",
      "      \"grad_norm\": 0.0002638553560245782,\n",
      "      \"learning_rate\": 1.082262129804663e-05,\n",
      "      \"loss\": 0.033,\n",
      "      \"step\": 124400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.839976054696115,\n",
      "      \"grad_norm\": 0.024259645491838455,\n",
      "      \"learning_rate\": 1.0816320100819157e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 124420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.841236333847947,\n",
      "      \"grad_norm\": 0.3319488763809204,\n",
      "      \"learning_rate\": 1.0810018903591682e-05,\n",
      "      \"loss\": 0.0333,\n",
      "      \"step\": 124440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.8424966129997795,\n",
      "      \"grad_norm\": 0.0037441772874444723,\n",
      "      \"learning_rate\": 1.080371770636421e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 124460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.843756892151611,\n",
      "      \"grad_norm\": 0.00035391567507758737,\n",
      "      \"learning_rate\": 1.0797416509136735e-05,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 124480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.845017171303444,\n",
      "      \"grad_norm\": 0.001052888110280037,\n",
      "      \"learning_rate\": 1.0791115311909265e-05,\n",
      "      \"loss\": 0.0192,\n",
      "      \"step\": 124500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.8462774504552755,\n",
      "      \"grad_norm\": 0.4980679154396057,\n",
      "      \"learning_rate\": 1.078481411468179e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 124520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.847537729607108,\n",
      "      \"grad_norm\": 0.0006503256154246628,\n",
      "      \"learning_rate\": 1.0778512917454318e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 124540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.84879800875894,\n",
      "      \"grad_norm\": 0.03084505721926689,\n",
      "      \"learning_rate\": 1.0772211720226843e-05,\n",
      "      \"loss\": 0.0206,\n",
      "      \"step\": 124560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.850058287910772,\n",
      "      \"grad_norm\": 0.007408510893583298,\n",
      "      \"learning_rate\": 1.076591052299937e-05,\n",
      "      \"loss\": 0.0629,\n",
      "      \"step\": 124580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.851318567062604,\n",
      "      \"grad_norm\": 0.0022798492573201656,\n",
      "      \"learning_rate\": 1.0759609325771896e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 124600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.852578846214437,\n",
      "      \"grad_norm\": 0.0010439979378134012,\n",
      "      \"learning_rate\": 1.0753308128544425e-05,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 124620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.853839125366268,\n",
      "      \"grad_norm\": 0.00540164764970541,\n",
      "      \"learning_rate\": 1.0747006931316951e-05,\n",
      "      \"loss\": 0.0293,\n",
      "      \"step\": 124640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.855099404518101,\n",
      "      \"grad_norm\": 0.02096172794699669,\n",
      "      \"learning_rate\": 1.0740705734089478e-05,\n",
      "      \"loss\": 0.0073,\n",
      "      \"step\": 124660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.8563596836699325,\n",
      "      \"grad_norm\": 0.0007485761307179928,\n",
      "      \"learning_rate\": 1.0734404536862004e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 124680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.857619962821765,\n",
      "      \"grad_norm\": 0.0007675742381252348,\n",
      "      \"learning_rate\": 1.0728103339634531e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 124700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.858880241973597,\n",
      "      \"grad_norm\": 0.000257110339589417,\n",
      "      \"learning_rate\": 1.0721802142407057e-05,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 124720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.860140521125429,\n",
      "      \"grad_norm\": 0.003831474808976054,\n",
      "      \"learning_rate\": 1.0715500945179586e-05,\n",
      "      \"loss\": 0.0049,\n",
      "      \"step\": 124740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.861400800277261,\n",
      "      \"grad_norm\": 0.0003896659763995558,\n",
      "      \"learning_rate\": 1.0709199747952112e-05,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 124760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.862661079429094,\n",
      "      \"grad_norm\": 0.09382949024438858,\n",
      "      \"learning_rate\": 1.0702898550724639e-05,\n",
      "      \"loss\": 0.0309,\n",
      "      \"step\": 124780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.863921358580925,\n",
      "      \"grad_norm\": 0.002133628586307168,\n",
      "      \"learning_rate\": 1.0696597353497165e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 124800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.865181637732758,\n",
      "      \"grad_norm\": 0.01117304340004921,\n",
      "      \"learning_rate\": 1.0690296156269692e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 124820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.86644191688459,\n",
      "      \"grad_norm\": 0.0012653983430936933,\n",
      "      \"learning_rate\": 1.068399495904222e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 124840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.867702196036422,\n",
      "      \"grad_norm\": 0.002854253863915801,\n",
      "      \"learning_rate\": 1.0677693761814747e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 124860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.868962475188254,\n",
      "      \"grad_norm\": 0.0025861160829663277,\n",
      "      \"learning_rate\": 1.0671392564587272e-05,\n",
      "      \"loss\": 0.0238,\n",
      "      \"step\": 124880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.8702227543400864,\n",
      "      \"grad_norm\": 0.009255819953978062,\n",
      "      \"learning_rate\": 1.06650913673598e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 124900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.871483033491918,\n",
      "      \"grad_norm\": 0.007312634028494358,\n",
      "      \"learning_rate\": 1.0658790170132325e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 124920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.872743312643751,\n",
      "      \"grad_norm\": 0.00018003713921643794,\n",
      "      \"learning_rate\": 1.0652488972904853e-05,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 124940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.874003591795582,\n",
      "      \"grad_norm\": 0.0002586798509582877,\n",
      "      \"learning_rate\": 1.064618777567738e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 124960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.875263870947415,\n",
      "      \"grad_norm\": 0.000698390940669924,\n",
      "      \"learning_rate\": 1.0639886578449906e-05,\n",
      "      \"loss\": 0.0372,\n",
      "      \"step\": 124980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.876524150099247,\n",
      "      \"grad_norm\": 0.006160728167742491,\n",
      "      \"learning_rate\": 1.0633585381222433e-05,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 125000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.877784429251079,\n",
      "      \"grad_norm\": 0.00031161803053691983,\n",
      "      \"learning_rate\": 1.0627284183994959e-05,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 125020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.879044708402911,\n",
      "      \"grad_norm\": 0.0013582250103354454,\n",
      "      \"learning_rate\": 1.0620982986767486e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 125040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.8803049875547435,\n",
      "      \"grad_norm\": 0.004489963408559561,\n",
      "      \"learning_rate\": 1.0614681789540013e-05,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 125060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.881565266706575,\n",
      "      \"grad_norm\": 0.00032471149461343884,\n",
      "      \"learning_rate\": 1.060838059231254e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 125080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.882825545858408,\n",
      "      \"grad_norm\": 0.0003395304665900767,\n",
      "      \"learning_rate\": 1.0602079395085066e-05,\n",
      "      \"loss\": 0.0065,\n",
      "      \"step\": 125100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.8840858250102395,\n",
      "      \"grad_norm\": 2.078958034515381,\n",
      "      \"learning_rate\": 1.0595778197857594e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 125120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.885346104162072,\n",
      "      \"grad_norm\": 0.0015897291013970971,\n",
      "      \"learning_rate\": 1.058947700063012e-05,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 125140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.886606383313904,\n",
      "      \"grad_norm\": 0.21558067202568054,\n",
      "      \"learning_rate\": 1.0583175803402647e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 125160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.887866662465736,\n",
      "      \"grad_norm\": 0.0007657670648768544,\n",
      "      \"learning_rate\": 1.0576874606175174e-05,\n",
      "      \"loss\": 0.0212,\n",
      "      \"step\": 125180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.889126941617568,\n",
      "      \"grad_norm\": 0.01943327486515045,\n",
      "      \"learning_rate\": 1.0570573408947701e-05,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 125200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.890387220769401,\n",
      "      \"grad_norm\": 0.0009476662962697446,\n",
      "      \"learning_rate\": 1.0564272211720227e-05,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 125220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.891647499921232,\n",
      "      \"grad_norm\": 0.002007211558520794,\n",
      "      \"learning_rate\": 1.0557971014492754e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 125240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.892907779073065,\n",
      "      \"grad_norm\": 0.0008892440819181502,\n",
      "      \"learning_rate\": 1.055166981726528e-05,\n",
      "      \"loss\": 0.0036,\n",
      "      \"step\": 125260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.8941680582248965,\n",
      "      \"grad_norm\": 0.0019327125046402216,\n",
      "      \"learning_rate\": 1.0545368620037807e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 125280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.895428337376729,\n",
      "      \"grad_norm\": 0.40238842368125916,\n",
      "      \"learning_rate\": 1.0539067422810335e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 125300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.896688616528561,\n",
      "      \"grad_norm\": 0.0012163359206169844,\n",
      "      \"learning_rate\": 1.0532766225582862e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 125320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.897948895680393,\n",
      "      \"grad_norm\": 0.013416984118521214,\n",
      "      \"learning_rate\": 1.0526465028355388e-05,\n",
      "      \"loss\": 0.0183,\n",
      "      \"step\": 125340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.899209174832225,\n",
      "      \"grad_norm\": 0.010740861296653748,\n",
      "      \"learning_rate\": 1.0520163831127915e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 125360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.900469453984058,\n",
      "      \"grad_norm\": 23.867271423339844,\n",
      "      \"learning_rate\": 1.051386263390044e-05,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 125380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.901729733135889,\n",
      "      \"grad_norm\": 0.0006990902475081384,\n",
      "      \"learning_rate\": 1.0507561436672968e-05,\n",
      "      \"loss\": 0.0325,\n",
      "      \"step\": 125400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.902990012287722,\n",
      "      \"grad_norm\": 0.011692473664879799,\n",
      "      \"learning_rate\": 1.0501260239445495e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 125420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.904250291439554,\n",
      "      \"grad_norm\": 0.23939009010791779,\n",
      "      \"learning_rate\": 1.0494959042218023e-05,\n",
      "      \"loss\": 0.0055,\n",
      "      \"step\": 125440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.905510570591386,\n",
      "      \"grad_norm\": 0.0014019689988344908,\n",
      "      \"learning_rate\": 1.0488657844990548e-05,\n",
      "      \"loss\": 0.0168,\n",
      "      \"step\": 125460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.906770849743218,\n",
      "      \"grad_norm\": 0.0002356399199925363,\n",
      "      \"learning_rate\": 1.0482356647763076e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 125480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.90803112889505,\n",
      "      \"grad_norm\": 0.0019347440684214234,\n",
      "      \"learning_rate\": 1.0476055450535601e-05,\n",
      "      \"loss\": 0.0143,\n",
      "      \"step\": 125500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.909291408046882,\n",
      "      \"grad_norm\": 0.0008968940237537026,\n",
      "      \"learning_rate\": 1.0470069313169504e-05,\n",
      "      \"loss\": 0.004,\n",
      "      \"step\": 125520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.910551687198715,\n",
      "      \"grad_norm\": Infinity,\n",
      "      \"learning_rate\": 1.046376811594203e-05,\n",
      "      \"loss\": 0.0305,\n",
      "      \"step\": 125540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.911811966350546,\n",
      "      \"grad_norm\": 0.002007176633924246,\n",
      "      \"learning_rate\": 1.045778197857593e-05,\n",
      "      \"loss\": 0.0248,\n",
      "      \"step\": 125560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.913072245502379,\n",
      "      \"grad_norm\": 0.00043366500176489353,\n",
      "      \"learning_rate\": 1.0451480781348456e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 125580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.914332524654211,\n",
      "      \"grad_norm\": 0.008265726268291473,\n",
      "      \"learning_rate\": 1.0445179584120983e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 125600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.915592803806043,\n",
      "      \"grad_norm\": 0.002994392067193985,\n",
      "      \"learning_rate\": 1.043887838689351e-05,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 125620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.916853082957875,\n",
      "      \"grad_norm\": 0.027759840711951256,\n",
      "      \"learning_rate\": 1.0432577189666038e-05,\n",
      "      \"loss\": 0.0031,\n",
      "      \"step\": 125640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.9181133621097075,\n",
      "      \"grad_norm\": 0.0012488554930314422,\n",
      "      \"learning_rate\": 1.0426275992438564e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 125660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.919373641261539,\n",
      "      \"grad_norm\": 0.251274436712265,\n",
      "      \"learning_rate\": 1.0419974795211091e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 125680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.920633920413372,\n",
      "      \"grad_norm\": 0.006829714868217707,\n",
      "      \"learning_rate\": 1.0413673597983617e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 125700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.9218941995652035,\n",
      "      \"grad_norm\": 0.004626258742064238,\n",
      "      \"learning_rate\": 1.0407372400756144e-05,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 125720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.923154478717036,\n",
      "      \"grad_norm\": 0.011991666629910469,\n",
      "      \"learning_rate\": 1.0401071203528671e-05,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 125740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.924414757868868,\n",
      "      \"grad_norm\": 0.006392361596226692,\n",
      "      \"learning_rate\": 1.0394770006301199e-05,\n",
      "      \"loss\": 0.023,\n",
      "      \"step\": 125760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.9256750370207,\n",
      "      \"grad_norm\": 3.73667311668396,\n",
      "      \"learning_rate\": 1.0388468809073724e-05,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 125780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.926935316172532,\n",
      "      \"grad_norm\": 0.0023840104695409536,\n",
      "      \"learning_rate\": 1.0382167611846252e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 125800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.928195595324365,\n",
      "      \"grad_norm\": 0.0034717924427241087,\n",
      "      \"learning_rate\": 1.0375866414618777e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 125820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.929455874476196,\n",
      "      \"grad_norm\": 0.00043478465522639453,\n",
      "      \"learning_rate\": 1.0369565217391305e-05,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 125840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.930716153628029,\n",
      "      \"grad_norm\": 0.011217106133699417,\n",
      "      \"learning_rate\": 1.0363264020163832e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 125860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.9319764327798605,\n",
      "      \"grad_norm\": 0.0005187938222661614,\n",
      "      \"learning_rate\": 1.0356962822936358e-05,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 125880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.933236711931693,\n",
      "      \"grad_norm\": 0.008025549352169037,\n",
      "      \"learning_rate\": 1.0350661625708885e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 125900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.934496991083525,\n",
      "      \"grad_norm\": 0.0019404558697715402,\n",
      "      \"learning_rate\": 1.034436042848141e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 125920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.935757270235357,\n",
      "      \"grad_norm\": 0.001067968551069498,\n",
      "      \"learning_rate\": 1.0338059231253938e-05,\n",
      "      \"loss\": 0.016,\n",
      "      \"step\": 125940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.937017549387189,\n",
      "      \"grad_norm\": 0.0317855104804039,\n",
      "      \"learning_rate\": 1.0331758034026465e-05,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 125960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.938277828539022,\n",
      "      \"grad_norm\": 0.002724225865676999,\n",
      "      \"learning_rate\": 1.0325456836798993e-05,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 125980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.939538107690853,\n",
      "      \"grad_norm\": 0.00439343461766839,\n",
      "      \"learning_rate\": 1.0319155639571518e-05,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 126000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.940798386842686,\n",
      "      \"grad_norm\": 0.002926316112279892,\n",
      "      \"learning_rate\": 1.0312854442344046e-05,\n",
      "      \"loss\": 0.052,\n",
      "      \"step\": 126020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.942058665994518,\n",
      "      \"grad_norm\": 0.007874641567468643,\n",
      "      \"learning_rate\": 1.0306553245116571e-05,\n",
      "      \"loss\": 0.007,\n",
      "      \"step\": 126040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.94331894514635,\n",
      "      \"grad_norm\": 0.0035168880131095648,\n",
      "      \"learning_rate\": 1.0300252047889099e-05,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 126060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.944579224298182,\n",
      "      \"grad_norm\": 0.0005661760806106031,\n",
      "      \"learning_rate\": 1.0293950850661626e-05,\n",
      "      \"loss\": 0.0093,\n",
      "      \"step\": 126080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.945839503450014,\n",
      "      \"grad_norm\": 0.0009354579960927367,\n",
      "      \"learning_rate\": 1.0287649653434154e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 126100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.947099782601846,\n",
      "      \"grad_norm\": 0.001648909063078463,\n",
      "      \"learning_rate\": 1.028134845620668e-05,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 126120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.948360061753679,\n",
      "      \"grad_norm\": 0.007041812874376774,\n",
      "      \"learning_rate\": 1.0275047258979207e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 126140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.94962034090551,\n",
      "      \"grad_norm\": 0.07771102339029312,\n",
      "      \"learning_rate\": 1.0268746061751732e-05,\n",
      "      \"loss\": 0.0363,\n",
      "      \"step\": 126160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.950880620057343,\n",
      "      \"grad_norm\": 0.0026350480038672686,\n",
      "      \"learning_rate\": 1.0262444864524261e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 126180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.952140899209175,\n",
      "      \"grad_norm\": 0.012285676784813404,\n",
      "      \"learning_rate\": 1.0256143667296787e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 126200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.953401178361007,\n",
      "      \"grad_norm\": 0.003076213877648115,\n",
      "      \"learning_rate\": 1.0249842470069314e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 126220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.954661457512839,\n",
      "      \"grad_norm\": 0.0006167920073494315,\n",
      "      \"learning_rate\": 1.024354127284184e-05,\n",
      "      \"loss\": 0.0147,\n",
      "      \"step\": 126240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.9559217366646715,\n",
      "      \"grad_norm\": 0.0017262876499444246,\n",
      "      \"learning_rate\": 1.0237240075614367e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 126260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.957182015816503,\n",
      "      \"grad_norm\": 0.24827326834201813,\n",
      "      \"learning_rate\": 1.0230938878386893e-05,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 126280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.958442294968336,\n",
      "      \"grad_norm\": 0.02681068144738674,\n",
      "      \"learning_rate\": 1.0224637681159422e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 126300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.9597025741201675,\n",
      "      \"grad_norm\": 0.005218984559178352,\n",
      "      \"learning_rate\": 1.0218336483931948e-05,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 126320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.960962853272,\n",
      "      \"grad_norm\": 0.5021748542785645,\n",
      "      \"learning_rate\": 1.0212035286704475e-05,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 126340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.962223132423832,\n",
      "      \"grad_norm\": 0.01350592914968729,\n",
      "      \"learning_rate\": 1.0205734089477e-05,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 126360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.963483411575664,\n",
      "      \"grad_norm\": 0.0008443482802249491,\n",
      "      \"learning_rate\": 1.0199432892249528e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 126380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.964743690727496,\n",
      "      \"grad_norm\": 0.007365004625171423,\n",
      "      \"learning_rate\": 1.0193131695022054e-05,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 126400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.966003969879329,\n",
      "      \"grad_norm\": 0.2565501034259796,\n",
      "      \"learning_rate\": 1.0186830497794583e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 126420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.96726424903116,\n",
      "      \"grad_norm\": 0.0010663265129551291,\n",
      "      \"learning_rate\": 1.0180529300567108e-05,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 126440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.968524528182993,\n",
      "      \"grad_norm\": 0.0009784763678908348,\n",
      "      \"learning_rate\": 1.0174228103339636e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 126460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.9697848073348245,\n",
      "      \"grad_norm\": 0.0006504747434519231,\n",
      "      \"learning_rate\": 1.0167926906112161e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 126480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.971045086486657,\n",
      "      \"grad_norm\": 0.009310496039688587,\n",
      "      \"learning_rate\": 1.0161625708884689e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 126500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.972305365638489,\n",
      "      \"grad_norm\": 0.0813043937087059,\n",
      "      \"learning_rate\": 1.0155324511657216e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 126520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.973565644790321,\n",
      "      \"grad_norm\": 0.00013699298142455518,\n",
      "      \"learning_rate\": 1.0149023314429743e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 126540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.974825923942153,\n",
      "      \"grad_norm\": 0.017598843201994896,\n",
      "      \"learning_rate\": 1.0142722117202269e-05,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 126560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.976086203093986,\n",
      "      \"grad_norm\": 0.002292613498866558,\n",
      "      \"learning_rate\": 1.0136420919974796e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 126580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.977346482245817,\n",
      "      \"grad_norm\": 3.3189797401428223,\n",
      "      \"learning_rate\": 1.0130119722747322e-05,\n",
      "      \"loss\": 0.0266,\n",
      "      \"step\": 126600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.97860676139765,\n",
      "      \"grad_norm\": 0.0017172168008983135,\n",
      "      \"learning_rate\": 1.012381852551985e-05,\n",
      "      \"loss\": 0.03,\n",
      "      \"step\": 126620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.979867040549482,\n",
      "      \"grad_norm\": 0.020851003006100655,\n",
      "      \"learning_rate\": 1.0117517328292377e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 126640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.981127319701314,\n",
      "      \"grad_norm\": 0.0010891290148720145,\n",
      "      \"learning_rate\": 1.0111216131064904e-05,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 126660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.982387598853146,\n",
      "      \"grad_norm\": 0.004918340127915144,\n",
      "      \"learning_rate\": 1.010491493383743e-05,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 126680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.983647878004978,\n",
      "      \"grad_norm\": 0.01062848325818777,\n",
      "      \"learning_rate\": 1.0098613736609957e-05,\n",
      "      \"loss\": 0.0209,\n",
      "      \"step\": 126700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.98490815715681,\n",
      "      \"grad_norm\": 0.037595849484205246,\n",
      "      \"learning_rate\": 1.0092312539382483e-05,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 126720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.986168436308643,\n",
      "      \"grad_norm\": 0.0037599881179630756,\n",
      "      \"learning_rate\": 1.008601134215501e-05,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 126740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.987428715460474,\n",
      "      \"grad_norm\": 0.024595770984888077,\n",
      "      \"learning_rate\": 1.0079710144927537e-05,\n",
      "      \"loss\": 0.0229,\n",
      "      \"step\": 126760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.988688994612307,\n",
      "      \"grad_norm\": 0.010902360081672668,\n",
      "      \"learning_rate\": 1.0073408947700063e-05,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 126780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.989949273764139,\n",
      "      \"grad_norm\": 0.0033614616841077805,\n",
      "      \"learning_rate\": 1.006710775047259e-05,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 126800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.991209552915971,\n",
      "      \"grad_norm\": 9.630043983459473,\n",
      "      \"learning_rate\": 1.0060806553245118e-05,\n",
      "      \"loss\": 0.0021,\n",
      "      \"step\": 126820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.992469832067803,\n",
      "      \"grad_norm\": 0.0004768209473695606,\n",
      "      \"learning_rate\": 1.0054505356017643e-05,\n",
      "      \"loss\": 0.0024,\n",
      "      \"step\": 126840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.9937301112196355,\n",
      "      \"grad_norm\": 0.00616412190720439,\n",
      "      \"learning_rate\": 1.004820415879017e-05,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 126860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.994990390371467,\n",
      "      \"grad_norm\": 0.00045894007780589163,\n",
      "      \"learning_rate\": 1.0041902961562698e-05,\n",
      "      \"loss\": 0.0053,\n",
      "      \"step\": 126880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.9962506695233,\n",
      "      \"grad_norm\": 0.0020030359737575054,\n",
      "      \"learning_rate\": 1.0035601764335224e-05,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 126900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.9975109486751315,\n",
      "      \"grad_norm\": 0.010367918759584427,\n",
      "      \"learning_rate\": 1.0029300567107751e-05,\n",
      "      \"loss\": 0.0157,\n",
      "      \"step\": 126920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.998771227826964,\n",
      "      \"grad_norm\": 0.004557915963232517,\n",
      "      \"learning_rate\": 1.0022999369880277e-05,\n",
      "      \"loss\": 0.0264,\n",
      "      \"step\": 126940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.0,\n",
      "      \"grad_norm\": 8.775237802183256e-05,\n",
      "      \"learning_rate\": 1.0016698172652804e-05,\n",
      "      \"loss\": 0.0067,\n",
      "      \"step\": 126960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.0,\n",
      "      \"eval_accuracy\": 0.9993699199798375,\n",
      "      \"eval_f1\": 0.9993697611394718,\n",
      "      \"eval_loss\": 0.002226889133453369,\n",
      "      \"eval_precision\": 0.9996217613314001,\n",
      "      \"eval_recall\": 0.9991178879717724,\n",
      "      \"eval_runtime\": 701.2996,\n",
      "      \"eval_samples_per_second\": 45.262,\n",
      "      \"eval_steps_per_second\": 5.658,\n",
      "      \"step\": 126960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.001260279151833,\n",
      "      \"grad_norm\": 0.001118907704949379,\n",
      "      \"learning_rate\": 1.0010396975425331e-05,\n",
      "      \"loss\": 0.0287,\n",
      "      \"step\": 126980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.002520558303663,\n",
      "      \"grad_norm\": 0.0031238622032105923,\n",
      "      \"learning_rate\": 1.0004095778197859e-05,\n",
      "      \"loss\": 0.033,\n",
      "      \"step\": 127000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.003780837455496,\n",
      "      \"grad_norm\": 0.002475812565535307,\n",
      "      \"learning_rate\": 9.997794580970384e-06,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 127020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.005041116607329,\n",
      "      \"grad_norm\": 0.0004916329053230584,\n",
      "      \"learning_rate\": 9.991493383742912e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 127040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.006301395759161,\n",
      "      \"grad_norm\": 0.0988631471991539,\n",
      "      \"learning_rate\": 9.985192186515437e-06,\n",
      "      \"loss\": 0.0027,\n",
      "      \"step\": 127060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.007561674910992,\n",
      "      \"grad_norm\": 0.026667701080441475,\n",
      "      \"learning_rate\": 9.978890989287965e-06,\n",
      "      \"loss\": 0.0188,\n",
      "      \"step\": 127080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.008821954062824,\n",
      "      \"grad_norm\": 0.0003357070963829756,\n",
      "      \"learning_rate\": 9.972589792060492e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 127100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.010082233214657,\n",
      "      \"grad_norm\": 0.009524660184979439,\n",
      "      \"learning_rate\": 9.96628859483302e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 127120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.01134251236649,\n",
      "      \"grad_norm\": 0.007138988934457302,\n",
      "      \"learning_rate\": 9.959987397605545e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 127140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.01260279151832,\n",
      "      \"grad_norm\": 0.0006452269153669477,\n",
      "      \"learning_rate\": 9.953686200378072e-06,\n",
      "      \"loss\": 0.0236,\n",
      "      \"step\": 127160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.013863070670153,\n",
      "      \"grad_norm\": 0.006296651437878609,\n",
      "      \"learning_rate\": 9.947385003150598e-06,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 127180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.015123349821986,\n",
      "      \"grad_norm\": 0.004047042224556208,\n",
      "      \"learning_rate\": 9.941083805923127e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 127200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.016383628973818,\n",
      "      \"grad_norm\": 0.002689429558813572,\n",
      "      \"learning_rate\": 9.934782608695653e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 127220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.017643908125649,\n",
      "      \"grad_norm\": 0.250213623046875,\n",
      "      \"learning_rate\": 9.92848141146818e-06,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 127240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.018904187277482,\n",
      "      \"grad_norm\": 0.001406086259521544,\n",
      "      \"learning_rate\": 9.922180214240706e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 127260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.020164466429314,\n",
      "      \"grad_norm\": 0.01674129068851471,\n",
      "      \"learning_rate\": 9.915879017013233e-06,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 127280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.021424745581147,\n",
      "      \"grad_norm\": 0.039298225194215775,\n",
      "      \"learning_rate\": 9.909577819785759e-06,\n",
      "      \"loss\": 0.0392,\n",
      "      \"step\": 127300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.022685024732978,\n",
      "      \"grad_norm\": 0.16589443385601044,\n",
      "      \"learning_rate\": 9.903276622558288e-06,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 127320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.02394530388481,\n",
      "      \"grad_norm\": 0.32982972264289856,\n",
      "      \"learning_rate\": 9.896975425330813e-06,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 127340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.025205583036643,\n",
      "      \"grad_norm\": 0.02969363145530224,\n",
      "      \"learning_rate\": 9.89067422810334e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 127360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.026465862188475,\n",
      "      \"grad_norm\": 0.03448863700032234,\n",
      "      \"learning_rate\": 9.884373030875866e-06,\n",
      "      \"loss\": 0.0065,\n",
      "      \"step\": 127380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.027726141340306,\n",
      "      \"grad_norm\": 0.0008775339229032397,\n",
      "      \"learning_rate\": 9.878071833648394e-06,\n",
      "      \"loss\": 0.0519,\n",
      "      \"step\": 127400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.028986420492139,\n",
      "      \"grad_norm\": 0.037844572216272354,\n",
      "      \"learning_rate\": 9.87177063642092e-06,\n",
      "      \"loss\": 0.0125,\n",
      "      \"step\": 127420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.030246699643971,\n",
      "      \"grad_norm\": 8.335280418395996,\n",
      "      \"learning_rate\": 9.865469439193448e-06,\n",
      "      \"loss\": 0.0265,\n",
      "      \"step\": 127440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.031506978795804,\n",
      "      \"grad_norm\": 0.0011646851198747754,\n",
      "      \"learning_rate\": 9.859168241965974e-06,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 127460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.032767257947635,\n",
      "      \"grad_norm\": 0.0030605755746364594,\n",
      "      \"learning_rate\": 9.852867044738501e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 127480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.034027537099467,\n",
      "      \"grad_norm\": 0.00023263154434971511,\n",
      "      \"learning_rate\": 9.846565847511027e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 127500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.0352878162513,\n",
      "      \"grad_norm\": 0.0017594583332538605,\n",
      "      \"learning_rate\": 9.840264650283554e-06,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 127520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.036548095403132,\n",
      "      \"grad_norm\": 0.002048883819952607,\n",
      "      \"learning_rate\": 9.833963453056082e-06,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 127540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.037808374554963,\n",
      "      \"grad_norm\": 0.016664305701851845,\n",
      "      \"learning_rate\": 9.827662255828609e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 127560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.039068653706796,\n",
      "      \"grad_norm\": 0.010857585817575455,\n",
      "      \"learning_rate\": 9.821361058601135e-06,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 127580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.040328932858628,\n",
      "      \"grad_norm\": 0.0011143661104142666,\n",
      "      \"learning_rate\": 9.815059861373662e-06,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 127600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.04158921201046,\n",
      "      \"grad_norm\": 0.003493440803140402,\n",
      "      \"learning_rate\": 9.808758664146188e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 127620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.042849491162292,\n",
      "      \"grad_norm\": 0.08331611752510071,\n",
      "      \"learning_rate\": 9.802457466918715e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 127640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.044109770314124,\n",
      "      \"grad_norm\": 0.02558109536767006,\n",
      "      \"learning_rate\": 9.796156269691242e-06,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 127660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.045370049465957,\n",
      "      \"grad_norm\": 0.006967690773308277,\n",
      "      \"learning_rate\": 9.789855072463768e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 127680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.04663032861779,\n",
      "      \"grad_norm\": 0.0004127165302634239,\n",
      "      \"learning_rate\": 9.783553875236295e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 127700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.04789060776962,\n",
      "      \"grad_norm\": 0.0003368593752384186,\n",
      "      \"learning_rate\": 9.777252678008823e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 127720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.049150886921453,\n",
      "      \"grad_norm\": 0.00018503059982322156,\n",
      "      \"learning_rate\": 9.770951480781348e-06,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 127740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.050411166073285,\n",
      "      \"grad_norm\": 0.0001511359732830897,\n",
      "      \"learning_rate\": 9.764650283553876e-06,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 127760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.051671445225118,\n",
      "      \"grad_norm\": 0.000507079646922648,\n",
      "      \"learning_rate\": 9.758349086326403e-06,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 127780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.052931724376949,\n",
      "      \"grad_norm\": 0.37100669741630554,\n",
      "      \"learning_rate\": 9.752047889098929e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 127800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.054192003528781,\n",
      "      \"grad_norm\": 0.0004293117090128362,\n",
      "      \"learning_rate\": 9.745746691871456e-06,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 127820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.055452282680614,\n",
      "      \"grad_norm\": 0.09147856384515762,\n",
      "      \"learning_rate\": 9.739445494643982e-06,\n",
      "      \"loss\": 0.0076,\n",
      "      \"step\": 127840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.056712561832446,\n",
      "      \"grad_norm\": 0.03950168564915657,\n",
      "      \"learning_rate\": 9.733144297416509e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 127860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.057972840984277,\n",
      "      \"grad_norm\": 0.0031865721102803946,\n",
      "      \"learning_rate\": 9.726843100189036e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 127880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.05923312013611,\n",
      "      \"grad_norm\": 0.0006479143048636615,\n",
      "      \"learning_rate\": 9.720541902961564e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 127900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.060493399287942,\n",
      "      \"grad_norm\": 0.045610543340444565,\n",
      "      \"learning_rate\": 9.71424070573409e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 127920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.061753678439775,\n",
      "      \"grad_norm\": 0.13557951152324677,\n",
      "      \"learning_rate\": 9.707939508506617e-06,\n",
      "      \"loss\": 0.0219,\n",
      "      \"step\": 127940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.063013957591606,\n",
      "      \"grad_norm\": 0.0033366356510668993,\n",
      "      \"learning_rate\": 9.701638311279142e-06,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 127960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.064274236743438,\n",
      "      \"grad_norm\": 0.008922352455556393,\n",
      "      \"learning_rate\": 9.69533711405167e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 127980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.065534515895271,\n",
      "      \"grad_norm\": 0.00013918620243202895,\n",
      "      \"learning_rate\": 9.689035916824197e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 128000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.066794795047104,\n",
      "      \"grad_norm\": 0.0001423655921826139,\n",
      "      \"learning_rate\": 9.682734719596724e-06,\n",
      "      \"loss\": 0.0115,\n",
      "      \"step\": 128020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.068055074198934,\n",
      "      \"grad_norm\": 8.220004081726074,\n",
      "      \"learning_rate\": 9.67643352236925e-06,\n",
      "      \"loss\": 0.045,\n",
      "      \"step\": 128040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.069315353350767,\n",
      "      \"grad_norm\": 0.00028145615942776203,\n",
      "      \"learning_rate\": 9.670132325141777e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 128060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.0705756325026,\n",
      "      \"grad_norm\": 0.0009967992082238197,\n",
      "      \"learning_rate\": 9.663831127914303e-06,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 128080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.071835911654432,\n",
      "      \"grad_norm\": 0.0015005669556558132,\n",
      "      \"learning_rate\": 9.657529930686832e-06,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 128100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.073096190806263,\n",
      "      \"grad_norm\": 0.00023775527370162308,\n",
      "      \"learning_rate\": 9.651228733459358e-06,\n",
      "      \"loss\": 0.0043,\n",
      "      \"step\": 128120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.074356469958095,\n",
      "      \"grad_norm\": 0.00019850391254294664,\n",
      "      \"learning_rate\": 9.644927536231885e-06,\n",
      "      \"loss\": 0.0046,\n",
      "      \"step\": 128140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.075616749109928,\n",
      "      \"grad_norm\": 0.0002575138059910387,\n",
      "      \"learning_rate\": 9.63862633900441e-06,\n",
      "      \"loss\": 0.0343,\n",
      "      \"step\": 128160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.07687702826176,\n",
      "      \"grad_norm\": 0.013763445429503918,\n",
      "      \"learning_rate\": 9.632325141776938e-06,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 128180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.078137307413591,\n",
      "      \"grad_norm\": 0.029317021369934082,\n",
      "      \"learning_rate\": 9.626023944549464e-06,\n",
      "      \"loss\": 0.0172,\n",
      "      \"step\": 128200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.079397586565424,\n",
      "      \"grad_norm\": 8.686432838439941,\n",
      "      \"learning_rate\": 9.619722747321993e-06,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 128220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.080657865717257,\n",
      "      \"grad_norm\": 0.00024246142129413784,\n",
      "      \"learning_rate\": 9.613421550094518e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 128240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.081918144869089,\n",
      "      \"grad_norm\": 0.015069340355694294,\n",
      "      \"learning_rate\": 9.607120352867046e-06,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 128260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.08317842402092,\n",
      "      \"grad_norm\": 35.661590576171875,\n",
      "      \"learning_rate\": 9.600819155639571e-06,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 128280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.084438703172752,\n",
      "      \"grad_norm\": 0.00027294366736896336,\n",
      "      \"learning_rate\": 9.594517958412099e-06,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 128300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.085698982324585,\n",
      "      \"grad_norm\": 0.0006254806066863239,\n",
      "      \"learning_rate\": 9.588216761184624e-06,\n",
      "      \"loss\": 0.0116,\n",
      "      \"step\": 128320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.086959261476418,\n",
      "      \"grad_norm\": 0.012257840484380722,\n",
      "      \"learning_rate\": 9.581915563957154e-06,\n",
      "      \"loss\": 0.0028,\n",
      "      \"step\": 128340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.088219540628248,\n",
      "      \"grad_norm\": 0.002203742042183876,\n",
      "      \"learning_rate\": 9.57561436672968e-06,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 128360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.089479819780081,\n",
      "      \"grad_norm\": 0.00016882350610103458,\n",
      "      \"learning_rate\": 9.569313169502207e-06,\n",
      "      \"loss\": 0.0165,\n",
      "      \"step\": 128380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.090740098931914,\n",
      "      \"grad_norm\": 0.0003149068506900221,\n",
      "      \"learning_rate\": 9.563011972274732e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 128400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.092000378083746,\n",
      "      \"grad_norm\": 0.0005151392542757094,\n",
      "      \"learning_rate\": 9.55671077504726e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 128420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.093260657235577,\n",
      "      \"grad_norm\": 0.01208331435918808,\n",
      "      \"learning_rate\": 9.550409577819787e-06,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 128440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.09452093638741,\n",
      "      \"grad_norm\": 0.000433919281931594,\n",
      "      \"learning_rate\": 9.544108380592314e-06,\n",
      "      \"loss\": 0.0023,\n",
      "      \"step\": 128460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.095781215539242,\n",
      "      \"grad_norm\": 0.00031503825448453426,\n",
      "      \"learning_rate\": 9.53780718336484e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 128480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.097041494691075,\n",
      "      \"grad_norm\": 0.00012122825864935294,\n",
      "      \"learning_rate\": 9.531505986137367e-06,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 128500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.098301773842906,\n",
      "      \"grad_norm\": 0.011056342162191868,\n",
      "      \"learning_rate\": 9.525204788909893e-06,\n",
      "      \"loss\": 0.0067,\n",
      "      \"step\": 128520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.099562052994738,\n",
      "      \"grad_norm\": 0.00011764273949665949,\n",
      "      \"learning_rate\": 9.51890359168242e-06,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 128540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.10082233214657,\n",
      "      \"grad_norm\": 0.0029156177770346403,\n",
      "      \"learning_rate\": 9.512602394454948e-06,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 128560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.102082611298403,\n",
      "      \"grad_norm\": 0.0028959899209439754,\n",
      "      \"learning_rate\": 9.506301197227475e-06,\n",
      "      \"loss\": 0.0129,\n",
      "      \"step\": 128580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.103342890450234,\n",
      "      \"grad_norm\": 0.00033466884633526206,\n",
      "      \"learning_rate\": 9.5e-06,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 128600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.104603169602067,\n",
      "      \"grad_norm\": 0.00011561645806068555,\n",
      "      \"learning_rate\": 9.493698802772528e-06,\n",
      "      \"loss\": 0.0019,\n",
      "      \"step\": 128620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.1058634487539,\n",
      "      \"grad_norm\": 0.0013257405953481793,\n",
      "      \"learning_rate\": 9.487397605545054e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 128640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.107123727905732,\n",
      "      \"grad_norm\": 0.00015319630620069802,\n",
      "      \"learning_rate\": 9.481096408317581e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 128660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.108384007057563,\n",
      "      \"grad_norm\": 0.012004565447568893,\n",
      "      \"learning_rate\": 9.474795211090108e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 128680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.109644286209395,\n",
      "      \"grad_norm\": 0.005872335750609636,\n",
      "      \"learning_rate\": 9.468494013862634e-06,\n",
      "      \"loss\": 0.0215,\n",
      "      \"step\": 128700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.110904565361228,\n",
      "      \"grad_norm\": 6.398638725280762,\n",
      "      \"learning_rate\": 9.462192816635161e-06,\n",
      "      \"loss\": 0.0456,\n",
      "      \"step\": 128720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.11216484451306,\n",
      "      \"grad_norm\": 0.00023652761592529714,\n",
      "      \"learning_rate\": 9.455891619407687e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 128740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.113425123664891,\n",
      "      \"grad_norm\": 0.08058943599462509,\n",
      "      \"learning_rate\": 9.449590422180214e-06,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 128760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.114685402816724,\n",
      "      \"grad_norm\": 0.007866153493523598,\n",
      "      \"learning_rate\": 9.443289224952742e-06,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 128780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.115945681968556,\n",
      "      \"grad_norm\": 0.0011886690044775605,\n",
      "      \"learning_rate\": 9.436988027725269e-06,\n",
      "      \"loss\": 0.0058,\n",
      "      \"step\": 128800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.117205961120389,\n",
      "      \"grad_norm\": 0.0010241154814139009,\n",
      "      \"learning_rate\": 9.430686830497795e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 128820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.11846624027222,\n",
      "      \"grad_norm\": 0.04119183123111725,\n",
      "      \"learning_rate\": 9.424385633270322e-06,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 128840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.119726519424052,\n",
      "      \"grad_norm\": 0.006927611771970987,\n",
      "      \"learning_rate\": 9.418084436042848e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 128860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.120986798575885,\n",
      "      \"grad_norm\": 0.00018751804600469768,\n",
      "      \"learning_rate\": 9.411783238815375e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 128880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.122247077727717,\n",
      "      \"grad_norm\": 0.01676298677921295,\n",
      "      \"learning_rate\": 9.405482041587902e-06,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 128900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.123507356879548,\n",
      "      \"grad_norm\": 0.00013209400640334934,\n",
      "      \"learning_rate\": 9.39918084436043e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 128920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.12476763603138,\n",
      "      \"grad_norm\": 0.022955752909183502,\n",
      "      \"learning_rate\": 9.392879647132955e-06,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 128940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.126027915183213,\n",
      "      \"grad_norm\": 0.00030962523305788636,\n",
      "      \"learning_rate\": 9.386578449905483e-06,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 128960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.127288194335046,\n",
      "      \"grad_norm\": 0.0004133876063860953,\n",
      "      \"learning_rate\": 9.380277252678008e-06,\n",
      "      \"loss\": 0.0284,\n",
      "      \"step\": 128980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.128548473486877,\n",
      "      \"grad_norm\": 0.0004985088598914444,\n",
      "      \"learning_rate\": 9.373976055450536e-06,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 129000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.12980875263871,\n",
      "      \"grad_norm\": 0.010190253145992756,\n",
      "      \"learning_rate\": 9.367674858223063e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 129020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.131069031790542,\n",
      "      \"grad_norm\": 0.014909696765244007,\n",
      "      \"learning_rate\": 9.36137366099559e-06,\n",
      "      \"loss\": 0.0047,\n",
      "      \"step\": 129040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.132329310942374,\n",
      "      \"grad_norm\": 0.0002978002594318241,\n",
      "      \"learning_rate\": 9.355072463768116e-06,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 129060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.133589590094205,\n",
      "      \"grad_norm\": 0.0009680129587650299,\n",
      "      \"learning_rate\": 9.348771266540643e-06,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 129080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.134849869246038,\n",
      "      \"grad_norm\": 0.0019135993206873536,\n",
      "      \"learning_rate\": 9.342470069313169e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 129100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.13611014839787,\n",
      "      \"grad_norm\": 0.0013891835696995258,\n",
      "      \"learning_rate\": 9.336168872085698e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 129120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.137370427549703,\n",
      "      \"grad_norm\": 0.015390363521873951,\n",
      "      \"learning_rate\": 9.329867674858224e-06,\n",
      "      \"loss\": 0.0023,\n",
      "      \"step\": 129140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.138630706701534,\n",
      "      \"grad_norm\": 0.00023320777108892798,\n",
      "      \"learning_rate\": 9.323566477630751e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 129160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.139890985853366,\n",
      "      \"grad_norm\": 0.000743294192943722,\n",
      "      \"learning_rate\": 9.317265280403277e-06,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 129180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.141151265005199,\n",
      "      \"grad_norm\": 0.018478643149137497,\n",
      "      \"learning_rate\": 9.310964083175804e-06,\n",
      "      \"loss\": 0.0445,\n",
      "      \"step\": 129200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.142411544157031,\n",
      "      \"grad_norm\": 0.00781252235174179,\n",
      "      \"learning_rate\": 9.30466288594833e-06,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 129220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.143671823308862,\n",
      "      \"grad_norm\": 0.0007523305830545723,\n",
      "      \"learning_rate\": 9.298361688720859e-06,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 129240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.144932102460695,\n",
      "      \"grad_norm\": 0.0011026663705706596,\n",
      "      \"learning_rate\": 9.292060491493384e-06,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 129260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.146192381612527,\n",
      "      \"grad_norm\": 0.00026972623891197145,\n",
      "      \"learning_rate\": 9.285759294265912e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 129280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.14745266076436,\n",
      "      \"grad_norm\": 0.0001423214125679806,\n",
      "      \"learning_rate\": 9.279458097038437e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 129300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.14871293991619,\n",
      "      \"grad_norm\": 0.00030820901156403124,\n",
      "      \"learning_rate\": 9.273156899810965e-06,\n",
      "      \"loss\": 0.0215,\n",
      "      \"step\": 129320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.149973219068023,\n",
      "      \"grad_norm\": 0.00074117595795542,\n",
      "      \"learning_rate\": 9.26685570258349e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 129340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.151233498219856,\n",
      "      \"grad_norm\": 0.006331837736070156,\n",
      "      \"learning_rate\": 9.26055450535602e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 129360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.152493777371689,\n",
      "      \"grad_norm\": 0.001203198335133493,\n",
      "      \"learning_rate\": 9.254253308128545e-06,\n",
      "      \"loss\": 0.0021,\n",
      "      \"step\": 129380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.15375405652352,\n",
      "      \"grad_norm\": 0.20253483951091766,\n",
      "      \"learning_rate\": 9.247952110901072e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 129400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.155014335675352,\n",
      "      \"grad_norm\": 0.0005478989914990962,\n",
      "      \"learning_rate\": 9.241650913673598e-06,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 129420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.156274614827185,\n",
      "      \"grad_norm\": 0.00013566605048254132,\n",
      "      \"learning_rate\": 9.235349716446125e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 129440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.157534893979017,\n",
      "      \"grad_norm\": 0.019885031506419182,\n",
      "      \"learning_rate\": 9.229048519218653e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 129460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.158795173130848,\n",
      "      \"grad_norm\": 0.5439887642860413,\n",
      "      \"learning_rate\": 9.22274732199118e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 129480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.16005545228268,\n",
      "      \"grad_norm\": 0.07918044179677963,\n",
      "      \"learning_rate\": 9.216446124763706e-06,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 129500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.161315731434513,\n",
      "      \"grad_norm\": 0.0005228636437095702,\n",
      "      \"learning_rate\": 9.210144927536233e-06,\n",
      "      \"loss\": 0.0263,\n",
      "      \"step\": 129520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.162576010586346,\n",
      "      \"grad_norm\": 0.0009864737512543797,\n",
      "      \"learning_rate\": 9.203843730308759e-06,\n",
      "      \"loss\": 0.0274,\n",
      "      \"step\": 129540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.163836289738176,\n",
      "      \"grad_norm\": 0.0007229426992125809,\n",
      "      \"learning_rate\": 9.197542533081286e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 129560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.165096568890009,\n",
      "      \"grad_norm\": 0.0013746228069067001,\n",
      "      \"learning_rate\": 9.191241335853813e-06,\n",
      "      \"loss\": 0.0461,\n",
      "      \"step\": 129580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.166356848041842,\n",
      "      \"grad_norm\": 0.15744492411613464,\n",
      "      \"learning_rate\": 9.184940138626339e-06,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 129600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.167617127193674,\n",
      "      \"grad_norm\": 0.006886073388159275,\n",
      "      \"learning_rate\": 9.178638941398866e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 129620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.168877406345505,\n",
      "      \"grad_norm\": 6.674540042877197,\n",
      "      \"learning_rate\": 9.172337744171394e-06,\n",
      "      \"loss\": 0.0188,\n",
      "      \"step\": 129640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.170137685497338,\n",
      "      \"grad_norm\": 0.003787324298173189,\n",
      "      \"learning_rate\": 9.16603654694392e-06,\n",
      "      \"loss\": 0.0286,\n",
      "      \"step\": 129660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.17139796464917,\n",
      "      \"grad_norm\": 0.0003944695636164397,\n",
      "      \"learning_rate\": 9.159735349716447e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 129680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.172658243801003,\n",
      "      \"grad_norm\": 0.006610071286559105,\n",
      "      \"learning_rate\": 9.153434152488974e-06,\n",
      "      \"loss\": 0.0135,\n",
      "      \"step\": 129700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.173918522952833,\n",
      "      \"grad_norm\": 0.001716345432214439,\n",
      "      \"learning_rate\": 9.1471329552615e-06,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 129720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.175178802104666,\n",
      "      \"grad_norm\": 0.002673524199053645,\n",
      "      \"learning_rate\": 9.140831758034027e-06,\n",
      "      \"loss\": 0.0217,\n",
      "      \"step\": 129740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.176439081256499,\n",
      "      \"grad_norm\": 0.0003801368875429034,\n",
      "      \"learning_rate\": 9.134530560806553e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 129760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.177699360408331,\n",
      "      \"grad_norm\": 0.0006467937491834164,\n",
      "      \"learning_rate\": 9.12822936357908e-06,\n",
      "      \"loss\": 0.0041,\n",
      "      \"step\": 129780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.178959639560162,\n",
      "      \"grad_norm\": 0.0010232031345367432,\n",
      "      \"learning_rate\": 9.121928166351607e-06,\n",
      "      \"loss\": 0.0397,\n",
      "      \"step\": 129800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.180219918711995,\n",
      "      \"grad_norm\": 13.20952033996582,\n",
      "      \"learning_rate\": 9.115626969124135e-06,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 129820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.181480197863827,\n",
      "      \"grad_norm\": 0.0005856941570527852,\n",
      "      \"learning_rate\": 9.10932577189666e-06,\n",
      "      \"loss\": 0.0336,\n",
      "      \"step\": 129840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.18274047701566,\n",
      "      \"grad_norm\": 0.003641652874648571,\n",
      "      \"learning_rate\": 9.103024574669188e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 129860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.18400075616749,\n",
      "      \"grad_norm\": 0.018047640100121498,\n",
      "      \"learning_rate\": 9.096723377441713e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 129880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.185261035319323,\n",
      "      \"grad_norm\": 0.11731016635894775,\n",
      "      \"learning_rate\": 9.09042218021424e-06,\n",
      "      \"loss\": 0.0122,\n",
      "      \"step\": 129900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.186521314471156,\n",
      "      \"grad_norm\": 0.08375347405672073,\n",
      "      \"learning_rate\": 9.084120982986768e-06,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 129920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.187781593622988,\n",
      "      \"grad_norm\": 0.009399285539984703,\n",
      "      \"learning_rate\": 9.077819785759295e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 129940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.189041872774819,\n",
      "      \"grad_norm\": 0.008920036256313324,\n",
      "      \"learning_rate\": 9.071518588531821e-06,\n",
      "      \"loss\": 0.0061,\n",
      "      \"step\": 129960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.190302151926652,\n",
      "      \"grad_norm\": 0.0002268063835799694,\n",
      "      \"learning_rate\": 9.065217391304348e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 129980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.191562431078484,\n",
      "      \"grad_norm\": 0.00022423392510972917,\n",
      "      \"learning_rate\": 9.058916194076874e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 130000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.192822710230317,\n",
      "      \"grad_norm\": 0.002871032804250717,\n",
      "      \"learning_rate\": 9.052614996849401e-06,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 130020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.194082989382148,\n",
      "      \"grad_norm\": 0.0013352441601455212,\n",
      "      \"learning_rate\": 9.046313799621929e-06,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 130040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.19534326853398,\n",
      "      \"grad_norm\": 0.0034197289496660233,\n",
      "      \"learning_rate\": 9.040012602394456e-06,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 130060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.196603547685813,\n",
      "      \"grad_norm\": 0.0017703591147437692,\n",
      "      \"learning_rate\": 9.033711405166982e-06,\n",
      "      \"loss\": 0.0243,\n",
      "      \"step\": 130080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.197863826837645,\n",
      "      \"grad_norm\": 0.00036662755883298814,\n",
      "      \"learning_rate\": 9.027410207939509e-06,\n",
      "      \"loss\": 0.0023,\n",
      "      \"step\": 130100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.199124105989476,\n",
      "      \"grad_norm\": 0.00340255000628531,\n",
      "      \"learning_rate\": 9.021109010712035e-06,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 130120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.200384385141309,\n",
      "      \"grad_norm\": 0.00013662029232364148,\n",
      "      \"learning_rate\": 9.014807813484564e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 130140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.201644664293141,\n",
      "      \"grad_norm\": 0.00014515582006424665,\n",
      "      \"learning_rate\": 9.00850661625709e-06,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 130160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.202904943444974,\n",
      "      \"grad_norm\": 0.00011623731552390382,\n",
      "      \"learning_rate\": 9.002205419029617e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 130180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.204165222596805,\n",
      "      \"grad_norm\": 0.013916851952672005,\n",
      "      \"learning_rate\": 8.995904221802142e-06,\n",
      "      \"loss\": 0.0122,\n",
      "      \"step\": 130200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.205425501748637,\n",
      "      \"grad_norm\": 0.0002980266872327775,\n",
      "      \"learning_rate\": 8.98960302457467e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 130220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.20668578090047,\n",
      "      \"grad_norm\": 8.004499435424805,\n",
      "      \"learning_rate\": 8.983301827347195e-06,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 130240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.207946060052302,\n",
      "      \"grad_norm\": 0.0011467195581644773,\n",
      "      \"learning_rate\": 8.977000630119724e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 130260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.209206339204133,\n",
      "      \"grad_norm\": 0.004037331789731979,\n",
      "      \"learning_rate\": 8.97069943289225e-06,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 130280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.210466618355966,\n",
      "      \"grad_norm\": 0.0001411707344232127,\n",
      "      \"learning_rate\": 8.964398235664777e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 130300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.211726897507798,\n",
      "      \"grad_norm\": 0.00013813450641464442,\n",
      "      \"learning_rate\": 8.958097038437303e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 130320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.212987176659631,\n",
      "      \"grad_norm\": 0.26392731070518494,\n",
      "      \"learning_rate\": 8.95179584120983e-06,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 130340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.214247455811462,\n",
      "      \"grad_norm\": 0.0001308751670876518,\n",
      "      \"learning_rate\": 8.945494643982356e-06,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 130360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.215507734963294,\n",
      "      \"grad_norm\": 0.007598413620144129,\n",
      "      \"learning_rate\": 8.939193446754885e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 130380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.216768014115127,\n",
      "      \"grad_norm\": 0.00010863132774829865,\n",
      "      \"learning_rate\": 8.93289224952741e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 130400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.21802829326696,\n",
      "      \"grad_norm\": 0.0003063041949644685,\n",
      "      \"learning_rate\": 8.926591052299938e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 130420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.21928857241879,\n",
      "      \"grad_norm\": 0.00013385622878558934,\n",
      "      \"learning_rate\": 8.920289855072464e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 130440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.220548851570623,\n",
      "      \"grad_norm\": 0.00023625665926374495,\n",
      "      \"learning_rate\": 8.913988657844991e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 130460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.221809130722455,\n",
      "      \"grad_norm\": 0.00010239780385745689,\n",
      "      \"learning_rate\": 8.907687460617518e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 130480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.223069409874288,\n",
      "      \"grad_norm\": 0.29264068603515625,\n",
      "      \"learning_rate\": 8.901386263390044e-06,\n",
      "      \"loss\": 0.0242,\n",
      "      \"step\": 130500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.224329689026119,\n",
      "      \"grad_norm\": 0.012536752037703991,\n",
      "      \"learning_rate\": 8.895085066162571e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 130520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.225589968177951,\n",
      "      \"grad_norm\": 0.0012065930059179664,\n",
      "      \"learning_rate\": 8.888783868935099e-06,\n",
      "      \"loss\": 0.0173,\n",
      "      \"step\": 130540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.226850247329784,\n",
      "      \"grad_norm\": 0.00014832675515208393,\n",
      "      \"learning_rate\": 8.882482671707624e-06,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 130560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.228110526481615,\n",
      "      \"grad_norm\": 0.000285218411590904,\n",
      "      \"learning_rate\": 8.876181474480152e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 130580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.229370805633447,\n",
      "      \"grad_norm\": 0.012170635163784027,\n",
      "      \"learning_rate\": 8.869880277252679e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 130600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.23063108478528,\n",
      "      \"grad_norm\": 0.00012848516053054482,\n",
      "      \"learning_rate\": 8.863579080025205e-06,\n",
      "      \"loss\": 0.0062,\n",
      "      \"step\": 130620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.231891363937113,\n",
      "      \"grad_norm\": 0.0071332501247525215,\n",
      "      \"learning_rate\": 8.857277882797732e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 130640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.233151643088945,\n",
      "      \"grad_norm\": 0.0013954514870420098,\n",
      "      \"learning_rate\": 8.850976685570258e-06,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 130660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.234411922240776,\n",
      "      \"grad_norm\": 0.0008469817112199962,\n",
      "      \"learning_rate\": 8.844675488342785e-06,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 130680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.235672201392608,\n",
      "      \"grad_norm\": 6.182370662689209,\n",
      "      \"learning_rate\": 8.838374291115312e-06,\n",
      "      \"loss\": 0.0377,\n",
      "      \"step\": 130700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.236932480544441,\n",
      "      \"grad_norm\": 0.002292471705004573,\n",
      "      \"learning_rate\": 8.83207309388784e-06,\n",
      "      \"loss\": 0.025,\n",
      "      \"step\": 130720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.238192759696272,\n",
      "      \"grad_norm\": 0.00018476526020094752,\n",
      "      \"learning_rate\": 8.825771896660365e-06,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 130740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.239453038848104,\n",
      "      \"grad_norm\": 0.00017592844960745424,\n",
      "      \"learning_rate\": 8.819470699432893e-06,\n",
      "      \"loss\": 0.0191,\n",
      "      \"step\": 130760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.240713317999937,\n",
      "      \"grad_norm\": 0.0030651690904051065,\n",
      "      \"learning_rate\": 8.813169502205418e-06,\n",
      "      \"loss\": 0.0218,\n",
      "      \"step\": 130780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.24197359715177,\n",
      "      \"grad_norm\": 0.00016620542737655342,\n",
      "      \"learning_rate\": 8.806868304977946e-06,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 130800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.2432338763036,\n",
      "      \"grad_norm\": 0.6304194927215576,\n",
      "      \"learning_rate\": 8.800567107750473e-06,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 130820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.244494155455433,\n",
      "      \"grad_norm\": 0.007549475412815809,\n",
      "      \"learning_rate\": 8.794265910523e-06,\n",
      "      \"loss\": 0.0168,\n",
      "      \"step\": 130840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.245754434607266,\n",
      "      \"grad_norm\": 0.015514175407588482,\n",
      "      \"learning_rate\": 8.787964713295526e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 130860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.247014713759098,\n",
      "      \"grad_norm\": 0.021774377673864365,\n",
      "      \"learning_rate\": 8.781663516068054e-06,\n",
      "      \"loss\": 0.0306,\n",
      "      \"step\": 130880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.248274992910929,\n",
      "      \"grad_norm\": 0.0016872085398063064,\n",
      "      \"learning_rate\": 8.77536231884058e-06,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 130900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.249535272062761,\n",
      "      \"grad_norm\": 0.026942621916532516,\n",
      "      \"learning_rate\": 8.769061121613107e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 130920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.250795551214594,\n",
      "      \"grad_norm\": 0.00550943985581398,\n",
      "      \"learning_rate\": 8.762759924385634e-06,\n",
      "      \"loss\": 0.0105,\n",
      "      \"step\": 130940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.252055830366427,\n",
      "      \"grad_norm\": 0.003149712458252907,\n",
      "      \"learning_rate\": 8.756458727158161e-06,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 130960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.253316109518257,\n",
      "      \"grad_norm\": 0.0003380440466571599,\n",
      "      \"learning_rate\": 8.750157529930687e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 130980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.25457638867009,\n",
      "      \"grad_norm\": 0.00020810222486034036,\n",
      "      \"learning_rate\": 8.743856332703214e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 131000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.255836667821923,\n",
      "      \"grad_norm\": 0.005595638882368803,\n",
      "      \"learning_rate\": 8.73755513547574e-06,\n",
      "      \"loss\": 0.005,\n",
      "      \"step\": 131020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.257096946973755,\n",
      "      \"grad_norm\": 0.0007770949741825461,\n",
      "      \"learning_rate\": 8.731253938248267e-06,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 131040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.258357226125586,\n",
      "      \"grad_norm\": 0.00010145487613044679,\n",
      "      \"learning_rate\": 8.724952741020795e-06,\n",
      "      \"loss\": 0.0166,\n",
      "      \"step\": 131060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.259617505277419,\n",
      "      \"grad_norm\": 0.011414219625294209,\n",
      "      \"learning_rate\": 8.718651543793322e-06,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 131080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.260877784429251,\n",
      "      \"grad_norm\": 0.0019391144160181284,\n",
      "      \"learning_rate\": 8.712350346565848e-06,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 131100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.262138063581084,\n",
      "      \"grad_norm\": 0.003068476216867566,\n",
      "      \"learning_rate\": 8.706049149338375e-06,\n",
      "      \"loss\": 0.0142,\n",
      "      \"step\": 131120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.263398342732915,\n",
      "      \"grad_norm\": 0.0006867682677693665,\n",
      "      \"learning_rate\": 8.6997479521109e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 131140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.264658621884747,\n",
      "      \"grad_norm\": 0.0006827611941844225,\n",
      "      \"learning_rate\": 8.69344675488343e-06,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 131160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.26591890103658,\n",
      "      \"grad_norm\": 0.00016068069089669734,\n",
      "      \"learning_rate\": 8.687145557655955e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 131180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.267179180188412,\n",
      "      \"grad_norm\": 0.003441368928179145,\n",
      "      \"learning_rate\": 8.680844360428483e-06,\n",
      "      \"loss\": 0.0077,\n",
      "      \"step\": 131200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.268439459340243,\n",
      "      \"grad_norm\": 0.005009790882468224,\n",
      "      \"learning_rate\": 8.674543163201008e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 131220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.269699738492076,\n",
      "      \"grad_norm\": 0.00016350497025996447,\n",
      "      \"learning_rate\": 8.668241965973536e-06,\n",
      "      \"loss\": 0.0121,\n",
      "      \"step\": 131240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.270960017643908,\n",
      "      \"grad_norm\": 0.008478697389364243,\n",
      "      \"learning_rate\": 8.661940768746061e-06,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 131260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.27222029679574,\n",
      "      \"grad_norm\": 0.5229585766792297,\n",
      "      \"learning_rate\": 8.65563957151859e-06,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 131280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.273480575947572,\n",
      "      \"grad_norm\": 0.0010546408593654633,\n",
      "      \"learning_rate\": 8.649338374291116e-06,\n",
      "      \"loss\": 0.0121,\n",
      "      \"step\": 131300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.274740855099404,\n",
      "      \"grad_norm\": 0.002955140545964241,\n",
      "      \"learning_rate\": 8.643037177063643e-06,\n",
      "      \"loss\": 0.0325,\n",
      "      \"step\": 131320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.276001134251237,\n",
      "      \"grad_norm\": 0.025497881695628166,\n",
      "      \"learning_rate\": 8.636735979836169e-06,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 131340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.27726141340307,\n",
      "      \"grad_norm\": 0.014247342012822628,\n",
      "      \"learning_rate\": 8.630434782608696e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 131360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.2785216925549,\n",
      "      \"grad_norm\": 0.16000744700431824,\n",
      "      \"learning_rate\": 8.624133585381222e-06,\n",
      "      \"loss\": 0.0105,\n",
      "      \"step\": 131380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.279781971706733,\n",
      "      \"grad_norm\": 0.00022025674115866423,\n",
      "      \"learning_rate\": 8.617832388153751e-06,\n",
      "      \"loss\": 0.0166,\n",
      "      \"step\": 131400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.281042250858565,\n",
      "      \"grad_norm\": 0.0005559085984714329,\n",
      "      \"learning_rate\": 8.611531190926277e-06,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 131420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.282302530010398,\n",
      "      \"grad_norm\": 0.00012789886386599392,\n",
      "      \"learning_rate\": 8.605229993698804e-06,\n",
      "      \"loss\": 0.006,\n",
      "      \"step\": 131440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.283562809162229,\n",
      "      \"grad_norm\": 1.1582554578781128,\n",
      "      \"learning_rate\": 8.59892879647133e-06,\n",
      "      \"loss\": 0.0057,\n",
      "      \"step\": 131460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.284823088314061,\n",
      "      \"grad_norm\": 0.0036377047654241323,\n",
      "      \"learning_rate\": 8.592627599243857e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 131480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.286083367465894,\n",
      "      \"grad_norm\": 0.0015580598264932632,\n",
      "      \"learning_rate\": 8.586326402016384e-06,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 131500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.287343646617726,\n",
      "      \"grad_norm\": 0.0003183419175911695,\n",
      "      \"learning_rate\": 8.58002520478891e-06,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 131520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.288603925769557,\n",
      "      \"grad_norm\": 0.001125091454014182,\n",
      "      \"learning_rate\": 8.573724007561437e-06,\n",
      "      \"loss\": 0.0171,\n",
      "      \"step\": 131540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.28986420492139,\n",
      "      \"grad_norm\": 0.0003716256469488144,\n",
      "      \"learning_rate\": 8.567422810333963e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 131560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.291124484073222,\n",
      "      \"grad_norm\": 0.00862039253115654,\n",
      "      \"learning_rate\": 8.56112161310649e-06,\n",
      "      \"loss\": 0.006,\n",
      "      \"step\": 131580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.292384763225055,\n",
      "      \"grad_norm\": 0.002606837311759591,\n",
      "      \"learning_rate\": 8.554820415879018e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 131600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.293645042376886,\n",
      "      \"grad_norm\": 0.0024887260515242815,\n",
      "      \"learning_rate\": 8.548519218651545e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 131620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.294905321528718,\n",
      "      \"grad_norm\": 0.003722428809851408,\n",
      "      \"learning_rate\": 8.54221802142407e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 131640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.29616560068055,\n",
      "      \"grad_norm\": 0.0005634759436361492,\n",
      "      \"learning_rate\": 8.535916824196598e-06,\n",
      "      \"loss\": 0.0023,\n",
      "      \"step\": 131660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.297425879832383,\n",
      "      \"grad_norm\": 0.00010016570740845054,\n",
      "      \"learning_rate\": 8.529615626969124e-06,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 131680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.298686158984214,\n",
      "      \"grad_norm\": 9.430942736798897e-05,\n",
      "      \"learning_rate\": 8.523314429741651e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 131700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.299946438136047,\n",
      "      \"grad_norm\": 9.18911027838476e-05,\n",
      "      \"learning_rate\": 8.517013232514177e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 131720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.30120671728788,\n",
      "      \"grad_norm\": 0.035757943987846375,\n",
      "      \"learning_rate\": 8.510712035286706e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 131740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.302466996439712,\n",
      "      \"grad_norm\": 0.0030381835531443357,\n",
      "      \"learning_rate\": 8.504410838059231e-06,\n",
      "      \"loss\": 0.0049,\n",
      "      \"step\": 131760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.303727275591543,\n",
      "      \"grad_norm\": 0.03097405843436718,\n",
      "      \"learning_rate\": 8.498109640831759e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 131780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.304987554743375,\n",
      "      \"grad_norm\": 0.0009061443852260709,\n",
      "      \"learning_rate\": 8.491808443604284e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 131800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.306247833895208,\n",
      "      \"grad_norm\": 0.00035284622572362423,\n",
      "      \"learning_rate\": 8.485507246376812e-06,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 131820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.30750811304704,\n",
      "      \"grad_norm\": 0.00014650011144112796,\n",
      "      \"learning_rate\": 8.479206049149339e-06,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 131840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.308768392198871,\n",
      "      \"grad_norm\": 0.007351969368755817,\n",
      "      \"learning_rate\": 8.472904851921866e-06,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 131860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.310028671350704,\n",
      "      \"grad_norm\": 0.004503405187278986,\n",
      "      \"learning_rate\": 8.466603654694392e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 131880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.311288950502536,\n",
      "      \"grad_norm\": 0.00012728391448035836,\n",
      "      \"learning_rate\": 8.46030245746692e-06,\n",
      "      \"loss\": 0.0274,\n",
      "      \"step\": 131900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.312549229654369,\n",
      "      \"grad_norm\": 0.0002077406970784068,\n",
      "      \"learning_rate\": 8.454001260239445e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 131920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.3138095088062,\n",
      "      \"grad_norm\": 0.004084331449121237,\n",
      "      \"learning_rate\": 8.448015122873348e-06,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 131940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.315069787958032,\n",
      "      \"grad_norm\": 0.00010995537741109729,\n",
      "      \"learning_rate\": 8.441713925645873e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 131960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.316330067109865,\n",
      "      \"grad_norm\": 0.05540168285369873,\n",
      "      \"learning_rate\": 8.4354127284184e-06,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 131980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.317590346261698,\n",
      "      \"grad_norm\": 0.0020117152016609907,\n",
      "      \"learning_rate\": 8.429111531190926e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 132000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.318850625413528,\n",
      "      \"grad_norm\": 0.00010793682304210961,\n",
      "      \"learning_rate\": 8.423125393824827e-06,\n",
      "      \"loss\": 0.0182,\n",
      "      \"step\": 132020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.320110904565361,\n",
      "      \"grad_norm\": 0.007952429354190826,\n",
      "      \"learning_rate\": 8.416824196597353e-06,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 132040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.321371183717194,\n",
      "      \"grad_norm\": 0.0022883955389261246,\n",
      "      \"learning_rate\": 8.410522999369882e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 132060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.322631462869026,\n",
      "      \"grad_norm\": 0.024940816685557365,\n",
      "      \"learning_rate\": 8.404221802142407e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 132080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.323891742020857,\n",
      "      \"grad_norm\": 0.0005565240862779319,\n",
      "      \"learning_rate\": 8.397920604914935e-06,\n",
      "      \"loss\": 0.0142,\n",
      "      \"step\": 132100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.32515202117269,\n",
      "      \"grad_norm\": 0.00013015889271628112,\n",
      "      \"learning_rate\": 8.39161940768746e-06,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 132120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.326412300324522,\n",
      "      \"grad_norm\": 0.00010708011541282758,\n",
      "      \"learning_rate\": 8.385318210459988e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 132140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.327672579476355,\n",
      "      \"grad_norm\": 0.0003832976508419961,\n",
      "      \"learning_rate\": 8.379017013232515e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 132160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.328932858628185,\n",
      "      \"grad_norm\": 0.00013216622755862772,\n",
      "      \"learning_rate\": 8.372715816005042e-06,\n",
      "      \"loss\": 0.0204,\n",
      "      \"step\": 132180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.330193137780018,\n",
      "      \"grad_norm\": 0.010505151003599167,\n",
      "      \"learning_rate\": 8.366414618777568e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 132200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.33145341693185,\n",
      "      \"grad_norm\": 0.0005349224084056914,\n",
      "      \"learning_rate\": 8.360113421550095e-06,\n",
      "      \"loss\": 0.0069,\n",
      "      \"step\": 132220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.332713696083683,\n",
      "      \"grad_norm\": 9.809862240217626e-05,\n",
      "      \"learning_rate\": 8.353812224322621e-06,\n",
      "      \"loss\": 0.0217,\n",
      "      \"step\": 132240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.333973975235514,\n",
      "      \"grad_norm\": 0.0040389662608504295,\n",
      "      \"learning_rate\": 8.347511027095148e-06,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 132260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.335234254387347,\n",
      "      \"grad_norm\": 0.00331940408796072,\n",
      "      \"learning_rate\": 8.341209829867676e-06,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 132280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.33649453353918,\n",
      "      \"grad_norm\": 0.00019606012210715562,\n",
      "      \"learning_rate\": 8.334908632640203e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 132300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.337754812691012,\n",
      "      \"grad_norm\": 0.009373890236020088,\n",
      "      \"learning_rate\": 8.328607435412729e-06,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 132320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.339015091842843,\n",
      "      \"grad_norm\": 0.3245407044887543,\n",
      "      \"learning_rate\": 8.322306238185256e-06,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 132340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.340275370994675,\n",
      "      \"grad_norm\": 0.00034286780282855034,\n",
      "      \"learning_rate\": 8.316005040957782e-06,\n",
      "      \"loss\": 0.0297,\n",
      "      \"step\": 132360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.341535650146508,\n",
      "      \"grad_norm\": 0.00044475181493908167,\n",
      "      \"learning_rate\": 8.30970384373031e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 132380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.34279592929834,\n",
      "      \"grad_norm\": 0.00059048569528386,\n",
      "      \"learning_rate\": 8.303402646502837e-06,\n",
      "      \"loss\": 0.0346,\n",
      "      \"step\": 132400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.344056208450171,\n",
      "      \"grad_norm\": 0.020731357857584953,\n",
      "      \"learning_rate\": 8.297101449275362e-06,\n",
      "      \"loss\": 0.0169,\n",
      "      \"step\": 132420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.345316487602004,\n",
      "      \"grad_norm\": 0.0017424781108275056,\n",
      "      \"learning_rate\": 8.29080025204789e-06,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 132440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.346576766753836,\n",
      "      \"grad_norm\": 0.030049355700612068,\n",
      "      \"learning_rate\": 8.284499054820415e-06,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 132460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.347837045905669,\n",
      "      \"grad_norm\": 0.0022398463916033506,\n",
      "      \"learning_rate\": 8.278197857592942e-06,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 132480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.3490973250575,\n",
      "      \"grad_norm\": 0.0001027515681926161,\n",
      "      \"learning_rate\": 8.27189666036547e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 132500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.350357604209332,\n",
      "      \"grad_norm\": 0.0670759379863739,\n",
      "      \"learning_rate\": 8.265595463137997e-06,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 132520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.351617883361165,\n",
      "      \"grad_norm\": 0.00013590100570581853,\n",
      "      \"learning_rate\": 8.259294265910523e-06,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 132540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.352878162512997,\n",
      "      \"grad_norm\": 0.0008145617321133614,\n",
      "      \"learning_rate\": 8.25299306868305e-06,\n",
      "      \"loss\": 0.0075,\n",
      "      \"step\": 132560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.354138441664828,\n",
      "      \"grad_norm\": 0.02391478605568409,\n",
      "      \"learning_rate\": 8.246691871455576e-06,\n",
      "      \"loss\": 0.0384,\n",
      "      \"step\": 132580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.35539872081666,\n",
      "      \"grad_norm\": 0.001272651948966086,\n",
      "      \"learning_rate\": 8.240390674228103e-06,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 132600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.356658999968493,\n",
      "      \"grad_norm\": 0.0017882490064948797,\n",
      "      \"learning_rate\": 8.23408947700063e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 132620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.357919279120326,\n",
      "      \"grad_norm\": 0.00047192751662805676,\n",
      "      \"learning_rate\": 8.227788279773158e-06,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 132640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.359179558272157,\n",
      "      \"grad_norm\": 0.000840004242490977,\n",
      "      \"learning_rate\": 8.221487082545684e-06,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 132660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.36043983742399,\n",
      "      \"grad_norm\": 0.001756441779434681,\n",
      "      \"learning_rate\": 8.215185885318211e-06,\n",
      "      \"loss\": 0.0033,\n",
      "      \"step\": 132680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.361700116575822,\n",
      "      \"grad_norm\": 0.00041653827065601945,\n",
      "      \"learning_rate\": 8.208884688090737e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 132700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.362960395727654,\n",
      "      \"grad_norm\": 0.11814401298761368,\n",
      "      \"learning_rate\": 8.202583490863264e-06,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 132720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.364220674879485,\n",
      "      \"grad_norm\": 0.3153782784938812,\n",
      "      \"learning_rate\": 8.196282293635791e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 132740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.365480954031318,\n",
      "      \"grad_norm\": 0.0034033264964818954,\n",
      "      \"learning_rate\": 8.189981096408319e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 132760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.36674123318315,\n",
      "      \"grad_norm\": 0.0028559849597513676,\n",
      "      \"learning_rate\": 8.183679899180844e-06,\n",
      "      \"loss\": 0.0146,\n",
      "      \"step\": 132780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.368001512334983,\n",
      "      \"grad_norm\": 0.00011874768097186461,\n",
      "      \"learning_rate\": 8.177378701953372e-06,\n",
      "      \"loss\": 0.0306,\n",
      "      \"step\": 132800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.369261791486814,\n",
      "      \"grad_norm\": 0.002739471383392811,\n",
      "      \"learning_rate\": 8.171077504725897e-06,\n",
      "      \"loss\": 0.0313,\n",
      "      \"step\": 132820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.370522070638646,\n",
      "      \"grad_norm\": 9.825800952967256e-05,\n",
      "      \"learning_rate\": 8.164776307498426e-06,\n",
      "      \"loss\": 0.01,\n",
      "      \"step\": 132840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.371782349790479,\n",
      "      \"grad_norm\": 0.0001347368088318035,\n",
      "      \"learning_rate\": 8.158475110270952e-06,\n",
      "      \"loss\": 0.0278,\n",
      "      \"step\": 132860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.373042628942311,\n",
      "      \"grad_norm\": 0.02588067576289177,\n",
      "      \"learning_rate\": 8.15217391304348e-06,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 132880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.374302908094142,\n",
      "      \"grad_norm\": 0.01577811874449253,\n",
      "      \"learning_rate\": 8.145872715816005e-06,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 132900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.375563187245975,\n",
      "      \"grad_norm\": 0.027289891615509987,\n",
      "      \"learning_rate\": 8.139571518588532e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 132920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.376823466397807,\n",
      "      \"grad_norm\": 0.014882334508001804,\n",
      "      \"learning_rate\": 8.133270321361058e-06,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 132940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.37808374554964,\n",
      "      \"grad_norm\": 0.0004025286762043834,\n",
      "      \"learning_rate\": 8.126969124133587e-06,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 132960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.37934402470147,\n",
      "      \"grad_norm\": 0.07332516461610794,\n",
      "      \"learning_rate\": 8.120667926906113e-06,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 132980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.380604303853303,\n",
      "      \"grad_norm\": 0.0008484560530632734,\n",
      "      \"learning_rate\": 8.11436672967864e-06,\n",
      "      \"loss\": 0.0445,\n",
      "      \"step\": 133000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.381864583005136,\n",
      "      \"grad_norm\": 0.0019876703154295683,\n",
      "      \"learning_rate\": 8.108065532451166e-06,\n",
      "      \"loss\": 0.0138,\n",
      "      \"step\": 133020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.383124862156969,\n",
      "      \"grad_norm\": 0.0002531423233449459,\n",
      "      \"learning_rate\": 8.101764335223693e-06,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 133040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.3843851413088,\n",
      "      \"grad_norm\": 0.0018769684247672558,\n",
      "      \"learning_rate\": 8.09546313799622e-06,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 133060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.385645420460632,\n",
      "      \"grad_norm\": 0.006893844809383154,\n",
      "      \"learning_rate\": 8.089161940768748e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 133080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.386905699612464,\n",
      "      \"grad_norm\": 0.020251348614692688,\n",
      "      \"learning_rate\": 8.082860743541273e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 133100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.388165978764297,\n",
      "      \"grad_norm\": 0.0002329863782506436,\n",
      "      \"learning_rate\": 8.0765595463138e-06,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 133120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.389426257916128,\n",
      "      \"grad_norm\": 0.001070609549060464,\n",
      "      \"learning_rate\": 8.070258349086326e-06,\n",
      "      \"loss\": 0.0121,\n",
      "      \"step\": 133140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.39068653706796,\n",
      "      \"grad_norm\": 0.006221068557351828,\n",
      "      \"learning_rate\": 8.063957151858854e-06,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 133160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.391946816219793,\n",
      "      \"grad_norm\": 0.004087844397872686,\n",
      "      \"learning_rate\": 8.057655954631381e-06,\n",
      "      \"loss\": 0.055,\n",
      "      \"step\": 133180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.393207095371626,\n",
      "      \"grad_norm\": 0.013963080011308193,\n",
      "      \"learning_rate\": 8.051354757403908e-06,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 133200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.394467374523456,\n",
      "      \"grad_norm\": 0.002553327940404415,\n",
      "      \"learning_rate\": 8.045053560176434e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 133220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.395727653675289,\n",
      "      \"grad_norm\": 0.00010981906234519556,\n",
      "      \"learning_rate\": 8.038752362948961e-06,\n",
      "      \"loss\": 0.0057,\n",
      "      \"step\": 133240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.396987932827122,\n",
      "      \"grad_norm\": 0.0008950263145379722,\n",
      "      \"learning_rate\": 8.032451165721487e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 133260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.398248211978954,\n",
      "      \"grad_norm\": 0.004715878516435623,\n",
      "      \"learning_rate\": 8.026149968494014e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 133280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.399508491130785,\n",
      "      \"grad_norm\": 0.0029950938187539577,\n",
      "      \"learning_rate\": 8.019848771266542e-06,\n",
      "      \"loss\": 0.0056,\n",
      "      \"step\": 133300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.400768770282617,\n",
      "      \"grad_norm\": 0.0006106389337219298,\n",
      "      \"learning_rate\": 8.013547574039067e-06,\n",
      "      \"loss\": 0.0043,\n",
      "      \"step\": 133320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.40202904943445,\n",
      "      \"grad_norm\": 0.005490676034241915,\n",
      "      \"learning_rate\": 8.007246376811595e-06,\n",
      "      \"loss\": 0.0083,\n",
      "      \"step\": 133340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.403289328586283,\n",
      "      \"grad_norm\": 0.0004789214872289449,\n",
      "      \"learning_rate\": 8.000945179584122e-06,\n",
      "      \"loss\": 0.0398,\n",
      "      \"step\": 133360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.404549607738113,\n",
      "      \"grad_norm\": 0.0001522029924672097,\n",
      "      \"learning_rate\": 7.994643982356648e-06,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 133380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.405809886889946,\n",
      "      \"grad_norm\": 0.0008773941663093865,\n",
      "      \"learning_rate\": 7.988342785129175e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 133400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.407070166041779,\n",
      "      \"grad_norm\": 0.0014476119540631771,\n",
      "      \"learning_rate\": 7.982041587901702e-06,\n",
      "      \"loss\": 0.0129,\n",
      "      \"step\": 133420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.408330445193611,\n",
      "      \"grad_norm\": 0.00012671045260503888,\n",
      "      \"learning_rate\": 7.975740390674228e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 133440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.409590724345442,\n",
      "      \"grad_norm\": 0.6599092483520508,\n",
      "      \"learning_rate\": 7.969439193446755e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 133460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.410851003497275,\n",
      "      \"grad_norm\": 0.024182720109820366,\n",
      "      \"learning_rate\": 7.963137996219281e-06,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 133480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.412111282649107,\n",
      "      \"grad_norm\": 0.000293637189315632,\n",
      "      \"learning_rate\": 7.956836798991808e-06,\n",
      "      \"loss\": 0.0173,\n",
      "      \"step\": 133500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.41337156180094,\n",
      "      \"grad_norm\": 0.0008898461237549782,\n",
      "      \"learning_rate\": 7.950535601764336e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 133520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.41463184095277,\n",
      "      \"grad_norm\": 0.0002598132996354252,\n",
      "      \"learning_rate\": 7.944234404536863e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 133540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.415892120104603,\n",
      "      \"grad_norm\": 0.00020138865511398762,\n",
      "      \"learning_rate\": 7.937933207309389e-06,\n",
      "      \"loss\": 0.0308,\n",
      "      \"step\": 133560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.417152399256436,\n",
      "      \"grad_norm\": 0.0012608234537765384,\n",
      "      \"learning_rate\": 7.931632010081916e-06,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 133580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.418412678408268,\n",
      "      \"grad_norm\": 0.0019570179283618927,\n",
      "      \"learning_rate\": 7.925330812854442e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 133600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.419672957560099,\n",
      "      \"grad_norm\": 0.02692805416882038,\n",
      "      \"learning_rate\": 7.919029615626969e-06,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 133620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.420933236711932,\n",
      "      \"grad_norm\": 0.00010303931776434183,\n",
      "      \"learning_rate\": 7.912728418399496e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 133640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.422193515863764,\n",
      "      \"grad_norm\": 0.08419369906187057,\n",
      "      \"learning_rate\": 7.906427221172024e-06,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 133660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.423453795015597,\n",
      "      \"grad_norm\": 0.0008764831000007689,\n",
      "      \"learning_rate\": 7.90012602394455e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 133680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.424714074167428,\n",
      "      \"grad_norm\": 0.0006451099179685116,\n",
      "      \"learning_rate\": 7.893824826717077e-06,\n",
      "      \"loss\": 0.0257,\n",
      "      \"step\": 133700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.42597435331926,\n",
      "      \"grad_norm\": 0.00012947767390869558,\n",
      "      \"learning_rate\": 7.887523629489602e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 133720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.427234632471093,\n",
      "      \"grad_norm\": 0.0010281564900651574,\n",
      "      \"learning_rate\": 7.881222432262131e-06,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 133740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.428494911622925,\n",
      "      \"grad_norm\": 0.0005213657859712839,\n",
      "      \"learning_rate\": 7.874921235034657e-06,\n",
      "      \"loss\": 0.0267,\n",
      "      \"step\": 133760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.429755190774756,\n",
      "      \"grad_norm\": 0.012214023619890213,\n",
      "      \"learning_rate\": 7.868620037807184e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 133780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.431015469926589,\n",
      "      \"grad_norm\": 0.005895507521927357,\n",
      "      \"learning_rate\": 7.86231884057971e-06,\n",
      "      \"loss\": 0.0171,\n",
      "      \"step\": 133800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.432275749078421,\n",
      "      \"grad_norm\": 0.015290195122361183,\n",
      "      \"learning_rate\": 7.856017643352237e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 133820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.433536028230254,\n",
      "      \"grad_norm\": 0.0075549171306192875,\n",
      "      \"learning_rate\": 7.849716446124763e-06,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 133840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.434796307382085,\n",
      "      \"grad_norm\": 0.0013085175305604935,\n",
      "      \"learning_rate\": 7.843415248897292e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 133860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.436056586533917,\n",
      "      \"grad_norm\": 0.024044279009103775,\n",
      "      \"learning_rate\": 7.837114051669818e-06,\n",
      "      \"loss\": 0.0171,\n",
      "      \"step\": 133880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.43731686568575,\n",
      "      \"grad_norm\": 0.02206774242222309,\n",
      "      \"learning_rate\": 7.830812854442345e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 133900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.438577144837582,\n",
      "      \"grad_norm\": 0.01955932378768921,\n",
      "      \"learning_rate\": 7.82451165721487e-06,\n",
      "      \"loss\": 0.0146,\n",
      "      \"step\": 133920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.439837423989413,\n",
      "      \"grad_norm\": 0.6846614480018616,\n",
      "      \"learning_rate\": 7.818210459987398e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 133940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.441097703141246,\n",
      "      \"grad_norm\": 0.000426584534579888,\n",
      "      \"learning_rate\": 7.811909262759924e-06,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 133960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.442357982293078,\n",
      "      \"grad_norm\": 0.05834796652197838,\n",
      "      \"learning_rate\": 7.805608065532453e-06,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 133980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.443618261444911,\n",
      "      \"grad_norm\": 0.008759578689932823,\n",
      "      \"learning_rate\": 7.799306868304978e-06,\n",
      "      \"loss\": 0.0018,\n",
      "      \"step\": 134000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.444878540596742,\n",
      "      \"grad_norm\": 0.0017704222118481994,\n",
      "      \"learning_rate\": 7.793005671077506e-06,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 134020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.446138819748574,\n",
      "      \"grad_norm\": 0.00016876240260899067,\n",
      "      \"learning_rate\": 7.786704473850031e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 134040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.447399098900407,\n",
      "      \"grad_norm\": 0.010172251611948013,\n",
      "      \"learning_rate\": 7.780403276622559e-06,\n",
      "      \"loss\": 0.0213,\n",
      "      \"step\": 134060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.448659378052238,\n",
      "      \"grad_norm\": 0.0001715739635983482,\n",
      "      \"learning_rate\": 7.774102079395086e-06,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 134080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.44991965720407,\n",
      "      \"grad_norm\": 0.0002284507208969444,\n",
      "      \"learning_rate\": 7.767800882167613e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 134100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.451179936355903,\n",
      "      \"grad_norm\": 0.07226414978504181,\n",
      "      \"learning_rate\": 7.761499684940139e-06,\n",
      "      \"loss\": 0.0068,\n",
      "      \"step\": 134120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.452440215507735,\n",
      "      \"grad_norm\": 0.4939504563808441,\n",
      "      \"learning_rate\": 7.755198487712666e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 134140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.453700494659568,\n",
      "      \"grad_norm\": 0.0009682413074187934,\n",
      "      \"learning_rate\": 7.748897290485192e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 134160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.454960773811399,\n",
      "      \"grad_norm\": 0.000767894322052598,\n",
      "      \"learning_rate\": 7.74259609325772e-06,\n",
      "      \"loss\": 0.0222,\n",
      "      \"step\": 134180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.456221052963231,\n",
      "      \"grad_norm\": 0.0009025505860336125,\n",
      "      \"learning_rate\": 7.736294896030247e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 134200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.457481332115064,\n",
      "      \"grad_norm\": 0.0001561097742523998,\n",
      "      \"learning_rate\": 7.729993698802772e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 134220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.458741611266895,\n",
      "      \"grad_norm\": 0.0005809118738397956,\n",
      "      \"learning_rate\": 7.7236925015753e-06,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 134240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.460001890418727,\n",
      "      \"grad_norm\": 0.00017426065460313112,\n",
      "      \"learning_rate\": 7.717391304347827e-06,\n",
      "      \"loss\": 0.0373,\n",
      "      \"step\": 134260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.46126216957056,\n",
      "      \"grad_norm\": 0.008461224846541882,\n",
      "      \"learning_rate\": 7.711090107120353e-06,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 134280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.462522448722392,\n",
      "      \"grad_norm\": 0.00015301712846849114,\n",
      "      \"learning_rate\": 7.70478890989288e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 134300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.463782727874225,\n",
      "      \"grad_norm\": 0.0007776034763082862,\n",
      "      \"learning_rate\": 7.698487712665407e-06,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 134320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.465043007026056,\n",
      "      \"grad_norm\": 0.00013148669677320868,\n",
      "      \"learning_rate\": 7.692186515437933e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 134340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.466303286177888,\n",
      "      \"grad_norm\": 0.0023318377789109945,\n",
      "      \"learning_rate\": 7.68588531821046e-06,\n",
      "      \"loss\": 0.0065,\n",
      "      \"step\": 134360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.467563565329721,\n",
      "      \"grad_norm\": 0.0007798756123520434,\n",
      "      \"learning_rate\": 7.679584120982986e-06,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 134380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.468823844481552,\n",
      "      \"grad_norm\": 0.00016653123020660132,\n",
      "      \"learning_rate\": 7.673282923755513e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 134400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.470084123633384,\n",
      "      \"grad_norm\": 0.00027451213099993765,\n",
      "      \"learning_rate\": 7.66698172652804e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 134420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.471344402785217,\n",
      "      \"grad_norm\": 0.00021556504361797124,\n",
      "      \"learning_rate\": 7.660680529300568e-06,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 134440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.47260468193705,\n",
      "      \"grad_norm\": 0.0002621205640025437,\n",
      "      \"learning_rate\": 7.654379332073094e-06,\n",
      "      \"loss\": 0.0378,\n",
      "      \"step\": 134460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.473864961088882,\n",
      "      \"grad_norm\": 0.033807285130023956,\n",
      "      \"learning_rate\": 7.648078134845621e-06,\n",
      "      \"loss\": 0.0028,\n",
      "      \"step\": 134480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.475125240240713,\n",
      "      \"grad_norm\": 0.010151942260563374,\n",
      "      \"learning_rate\": 7.641776937618147e-06,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 134500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.476385519392545,\n",
      "      \"grad_norm\": 0.012959492392838001,\n",
      "      \"learning_rate\": 7.635475740390674e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 134520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.477645798544378,\n",
      "      \"grad_norm\": 0.00028478557942435145,\n",
      "      \"learning_rate\": 7.629174543163202e-06,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 134540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.478906077696209,\n",
      "      \"grad_norm\": 0.00018829124746844172,\n",
      "      \"learning_rate\": 7.622873345935729e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 134560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.480166356848041,\n",
      "      \"grad_norm\": 0.00010983559332089499,\n",
      "      \"learning_rate\": 7.616572148708255e-06,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 134580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.481426635999874,\n",
      "      \"grad_norm\": 0.0021357499063014984,\n",
      "      \"learning_rate\": 7.610270951480782e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 134600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.482686915151707,\n",
      "      \"grad_norm\": 0.0018634448060765862,\n",
      "      \"learning_rate\": 7.603969754253308e-06,\n",
      "      \"loss\": 0.0031,\n",
      "      \"step\": 134620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.483947194303537,\n",
      "      \"grad_norm\": 0.006260426715016365,\n",
      "      \"learning_rate\": 7.597668557025835e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 134640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.48520747345537,\n",
      "      \"grad_norm\": 0.0020019719377160072,\n",
      "      \"learning_rate\": 7.591367359798362e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 134660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.486467752607203,\n",
      "      \"grad_norm\": 0.5088613033294678,\n",
      "      \"learning_rate\": 7.5850661625708895e-06,\n",
      "      \"loss\": 0.0246,\n",
      "      \"step\": 134680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.487728031759035,\n",
      "      \"grad_norm\": 0.00032056577038019896,\n",
      "      \"learning_rate\": 7.578764965343416e-06,\n",
      "      \"loss\": 0.0313,\n",
      "      \"step\": 134700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.488988310910866,\n",
      "      \"grad_norm\": 0.0006273136241361499,\n",
      "      \"learning_rate\": 7.5724637681159425e-06,\n",
      "      \"loss\": 0.0057,\n",
      "      \"step\": 134720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.490248590062698,\n",
      "      \"grad_norm\": 0.0009047561907209456,\n",
      "      \"learning_rate\": 7.566162570888469e-06,\n",
      "      \"loss\": 0.007,\n",
      "      \"step\": 134740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.491508869214531,\n",
      "      \"grad_norm\": 0.00018061458831653,\n",
      "      \"learning_rate\": 7.559861373660996e-06,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 134760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.492769148366364,\n",
      "      \"grad_norm\": 0.00018625950906425714,\n",
      "      \"learning_rate\": 7.553560176433523e-06,\n",
      "      \"loss\": 0.0216,\n",
      "      \"step\": 134780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.494029427518194,\n",
      "      \"grad_norm\": 0.018140757456421852,\n",
      "      \"learning_rate\": 7.547258979206049e-06,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 134800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.495289706670027,\n",
      "      \"grad_norm\": 6.1804585456848145,\n",
      "      \"learning_rate\": 7.540957781978576e-06,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 134820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.49654998582186,\n",
      "      \"grad_norm\": 0.00449550012126565,\n",
      "      \"learning_rate\": 7.534656584751102e-06,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 134840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.497810264973692,\n",
      "      \"grad_norm\": 0.03936542570590973,\n",
      "      \"learning_rate\": 7.52835538752363e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 134860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.499070544125523,\n",
      "      \"grad_norm\": 0.0028866382781416178,\n",
      "      \"learning_rate\": 7.522054190296157e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 134880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.500330823277356,\n",
      "      \"grad_norm\": 0.0002628221991471946,\n",
      "      \"learning_rate\": 7.5157529930686835e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 134900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.501591102429188,\n",
      "      \"grad_norm\": 0.00019365530170034617,\n",
      "      \"learning_rate\": 7.50945179584121e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 134920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.50285138158102,\n",
      "      \"grad_norm\": 0.04585576429963112,\n",
      "      \"learning_rate\": 7.5031505986137365e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 134940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.504111660732852,\n",
      "      \"grad_norm\": 0.0011882183607667685,\n",
      "      \"learning_rate\": 7.496849401386263e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 134960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.505371939884684,\n",
      "      \"grad_norm\": 0.02415909431874752,\n",
      "      \"learning_rate\": 7.4905482041587895e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 134980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.506632219036517,\n",
      "      \"grad_norm\": 0.011335032992064953,\n",
      "      \"learning_rate\": 7.484247006931318e-06,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 135000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.50789249818835,\n",
      "      \"grad_norm\": 0.00010692001524148509,\n",
      "      \"learning_rate\": 7.477945809703844e-06,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 135020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.50915277734018,\n",
      "      \"grad_norm\": 0.00010943468805635348,\n",
      "      \"learning_rate\": 7.471644612476371e-06,\n",
      "      \"loss\": 0.002,\n",
      "      \"step\": 135040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.510413056492013,\n",
      "      \"grad_norm\": 0.0002622315369080752,\n",
      "      \"learning_rate\": 7.465343415248897e-06,\n",
      "      \"loss\": 0.0299,\n",
      "      \"step\": 135060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.511673335643845,\n",
      "      \"grad_norm\": 0.0012838152470067143,\n",
      "      \"learning_rate\": 7.459042218021424e-06,\n",
      "      \"loss\": 0.0083,\n",
      "      \"step\": 135080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.512933614795678,\n",
      "      \"grad_norm\": 0.000567717244848609,\n",
      "      \"learning_rate\": 7.452741020793952e-06,\n",
      "      \"loss\": 0.0035,\n",
      "      \"step\": 135100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.514193893947509,\n",
      "      \"grad_norm\": 0.0003567603707779199,\n",
      "      \"learning_rate\": 7.446439823566478e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 135120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.515454173099341,\n",
      "      \"grad_norm\": 0.00017547378956805915,\n",
      "      \"learning_rate\": 7.440138626339005e-06,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 135140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.516714452251174,\n",
      "      \"grad_norm\": 0.006303766276687384,\n",
      "      \"learning_rate\": 7.433837429111531e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 135160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.517974731403006,\n",
      "      \"grad_norm\": 0.08817987889051437,\n",
      "      \"learning_rate\": 7.427536231884058e-06,\n",
      "      \"loss\": 0.0061,\n",
      "      \"step\": 135180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.519235010554837,\n",
      "      \"grad_norm\": 0.0027797860093414783,\n",
      "      \"learning_rate\": 7.421235034656584e-06,\n",
      "      \"loss\": 0.0053,\n",
      "      \"step\": 135200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.52049528970667,\n",
      "      \"grad_norm\": 0.00011267123045399785,\n",
      "      \"learning_rate\": 7.4149338374291126e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 135220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.521755568858502,\n",
      "      \"grad_norm\": 0.00010998510697390884,\n",
      "      \"learning_rate\": 7.408632640201639e-06,\n",
      "      \"loss\": 0.0322,\n",
      "      \"step\": 135240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.523015848010335,\n",
      "      \"grad_norm\": 0.0004002646019216627,\n",
      "      \"learning_rate\": 7.4023314429741656e-06,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 135260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.524276127162166,\n",
      "      \"grad_norm\": 8.999481201171875,\n",
      "      \"learning_rate\": 7.396030245746692e-06,\n",
      "      \"loss\": 0.0124,\n",
      "      \"step\": 135280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.525536406313998,\n",
      "      \"grad_norm\": 0.07713868468999863,\n",
      "      \"learning_rate\": 7.3897290485192186e-06,\n",
      "      \"loss\": 0.0197,\n",
      "      \"step\": 135300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.52679668546583,\n",
      "      \"grad_norm\": 0.004495766945183277,\n",
      "      \"learning_rate\": 7.383427851291745e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 135320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.528056964617663,\n",
      "      \"grad_norm\": 0.0011577934492379427,\n",
      "      \"learning_rate\": 7.377126654064273e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 135340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.529317243769494,\n",
      "      \"grad_norm\": 0.0008660642197355628,\n",
      "      \"learning_rate\": 7.3708254568368e-06,\n",
      "      \"loss\": 0.0131,\n",
      "      \"step\": 135360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.530577522921327,\n",
      "      \"grad_norm\": 0.2681417167186737,\n",
      "      \"learning_rate\": 7.364524259609326e-06,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 135380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.53183780207316,\n",
      "      \"grad_norm\": 0.011558551341295242,\n",
      "      \"learning_rate\": 7.358538122243226e-06,\n",
      "      \"loss\": 0.0044,\n",
      "      \"step\": 135400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.533098081224992,\n",
      "      \"grad_norm\": 0.0010334508260712028,\n",
      "      \"learning_rate\": 7.352236925015753e-06,\n",
      "      \"loss\": 0.029,\n",
      "      \"step\": 135420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.534358360376823,\n",
      "      \"grad_norm\": 0.08683495968580246,\n",
      "      \"learning_rate\": 7.345935727788281e-06,\n",
      "      \"loss\": 0.0253,\n",
      "      \"step\": 135440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.535618639528655,\n",
      "      \"grad_norm\": 0.01211067009717226,\n",
      "      \"learning_rate\": 7.3396345305608075e-06,\n",
      "      \"loss\": 0.0323,\n",
      "      \"step\": 135460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.536878918680488,\n",
      "      \"grad_norm\": 0.005547256674617529,\n",
      "      \"learning_rate\": 7.333333333333334e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 135480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.53813919783232,\n",
      "      \"grad_norm\": 0.0001763670879881829,\n",
      "      \"learning_rate\": 7.3270321361058605e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 135500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.539399476984151,\n",
      "      \"grad_norm\": 0.0001184361899504438,\n",
      "      \"learning_rate\": 7.320730938878387e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 135520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.540659756135984,\n",
      "      \"grad_norm\": 0.0010886774398386478,\n",
      "      \"learning_rate\": 7.314429741650914e-06,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 135540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.541920035287816,\n",
      "      \"grad_norm\": 0.0020635982509702444,\n",
      "      \"learning_rate\": 7.308128544423441e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 135560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.543180314439649,\n",
      "      \"grad_norm\": 0.7031936049461365,\n",
      "      \"learning_rate\": 7.301827347195968e-06,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 135580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.54444059359148,\n",
      "      \"grad_norm\": 0.00021424547594506294,\n",
      "      \"learning_rate\": 7.295526149968495e-06,\n",
      "      \"loss\": 0.0122,\n",
      "      \"step\": 135600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.545700872743312,\n",
      "      \"grad_norm\": 0.0002694365684874356,\n",
      "      \"learning_rate\": 7.289224952741021e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 135620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.546961151895145,\n",
      "      \"grad_norm\": 0.0025533924344927073,\n",
      "      \"learning_rate\": 7.282923755513548e-06,\n",
      "      \"loss\": 0.0046,\n",
      "      \"step\": 135640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.548221431046978,\n",
      "      \"grad_norm\": 0.017758315429091454,\n",
      "      \"learning_rate\": 7.276622558286075e-06,\n",
      "      \"loss\": 0.0032,\n",
      "      \"step\": 135660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.549481710198808,\n",
      "      \"grad_norm\": 0.00011182034359080717,\n",
      "      \"learning_rate\": 7.2703213610586015e-06,\n",
      "      \"loss\": 0.0271,\n",
      "      \"step\": 135680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.550741989350641,\n",
      "      \"grad_norm\": 0.00011848742724396288,\n",
      "      \"learning_rate\": 7.264020163831128e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 135700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.552002268502473,\n",
      "      \"grad_norm\": 0.010849480517208576,\n",
      "      \"learning_rate\": 7.2577189666036545e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 135720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.553262547654306,\n",
      "      \"grad_norm\": 0.061032988131046295,\n",
      "      \"learning_rate\": 7.251417769376182e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 135740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.554522826806137,\n",
      "      \"grad_norm\": 0.0017817456973716617,\n",
      "      \"learning_rate\": 7.245116572148708e-06,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 135760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.55578310595797,\n",
      "      \"grad_norm\": 0.5034969449043274,\n",
      "      \"learning_rate\": 7.238815374921236e-06,\n",
      "      \"loss\": 0.0304,\n",
      "      \"step\": 135780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.557043385109802,\n",
      "      \"grad_norm\": 0.005768963601440191,\n",
      "      \"learning_rate\": 7.232514177693762e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 135800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.558303664261635,\n",
      "      \"grad_norm\": 0.00018109953089151531,\n",
      "      \"learning_rate\": 7.226212980466289e-06,\n",
      "      \"loss\": 0.0061,\n",
      "      \"step\": 135820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.559563943413465,\n",
      "      \"grad_norm\": 0.6420472860336304,\n",
      "      \"learning_rate\": 7.219911783238815e-06,\n",
      "      \"loss\": 0.0212,\n",
      "      \"step\": 135840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.560824222565298,\n",
      "      \"grad_norm\": 0.040036361664533615,\n",
      "      \"learning_rate\": 7.213610586011342e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 135860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.56208450171713,\n",
      "      \"grad_norm\": 0.000968651904258877,\n",
      "      \"learning_rate\": 7.20730938878387e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 135880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.563344780868963,\n",
      "      \"grad_norm\": 0.0028168701101094484,\n",
      "      \"learning_rate\": 7.201008191556396e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 135900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.564605060020794,\n",
      "      \"grad_norm\": 0.0006055306294001639,\n",
      "      \"learning_rate\": 7.194706994328923e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 135920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.565865339172626,\n",
      "      \"grad_norm\": 0.0005626953789032996,\n",
      "      \"learning_rate\": 7.188405797101449e-06,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 135940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.567125618324459,\n",
      "      \"grad_norm\": 0.002086857333779335,\n",
      "      \"learning_rate\": 7.182104599873976e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 135960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.568385897476292,\n",
      "      \"grad_norm\": 0.009971785359084606,\n",
      "      \"learning_rate\": 7.175803402646502e-06,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 135980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.569646176628122,\n",
      "      \"grad_norm\": 0.0020625879988074303,\n",
      "      \"learning_rate\": 7.1695022054190306e-06,\n",
      "      \"loss\": 0.0157,\n",
      "      \"step\": 136000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.570906455779955,\n",
      "      \"grad_norm\": 0.03090379573404789,\n",
      "      \"learning_rate\": 7.163201008191557e-06,\n",
      "      \"loss\": 0.0287,\n",
      "      \"step\": 136020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.572166734931788,\n",
      "      \"grad_norm\": 0.0018269082065671682,\n",
      "      \"learning_rate\": 7.1568998109640836e-06,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 136040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.57342701408362,\n",
      "      \"grad_norm\": 0.0013022094499319792,\n",
      "      \"learning_rate\": 7.15059861373661e-06,\n",
      "      \"loss\": 0.0286,\n",
      "      \"step\": 136060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.574687293235451,\n",
      "      \"grad_norm\": 0.0010357038117945194,\n",
      "      \"learning_rate\": 7.1442974165091366e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 136080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.575947572387284,\n",
      "      \"grad_norm\": 0.004242544528096914,\n",
      "      \"learning_rate\": 7.137996219281663e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 136100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.577207851539116,\n",
      "      \"grad_norm\": 1.5397270917892456,\n",
      "      \"learning_rate\": 7.131695022054191e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 136120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.578468130690949,\n",
      "      \"grad_norm\": 0.0012820655247196555,\n",
      "      \"learning_rate\": 7.125393824826718e-06,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 136140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.57972840984278,\n",
      "      \"grad_norm\": 0.0004151896864641458,\n",
      "      \"learning_rate\": 7.119092627599244e-06,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 136160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.580988688994612,\n",
      "      \"grad_norm\": 0.08146847039461136,\n",
      "      \"learning_rate\": 7.112791430371771e-06,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 136180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.582248968146445,\n",
      "      \"grad_norm\": 0.0003220044309273362,\n",
      "      \"learning_rate\": 7.106490233144297e-06,\n",
      "      \"loss\": 0.0057,\n",
      "      \"step\": 136200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.583509247298277,\n",
      "      \"grad_norm\": 0.06704185158014297,\n",
      "      \"learning_rate\": 7.1001890359168254e-06,\n",
      "      \"loss\": 0.0338,\n",
      "      \"step\": 136220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.584769526450108,\n",
      "      \"grad_norm\": 0.0012420954881235957,\n",
      "      \"learning_rate\": 7.093887838689352e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 136240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.58602980560194,\n",
      "      \"grad_norm\": 0.01389087550342083,\n",
      "      \"learning_rate\": 7.0875866414618784e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 136260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.587290084753773,\n",
      "      \"grad_norm\": 0.0006565432995557785,\n",
      "      \"learning_rate\": 7.081285444234405e-06,\n",
      "      \"loss\": 0.0194,\n",
      "      \"step\": 136280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.588550363905606,\n",
      "      \"grad_norm\": 0.27486178278923035,\n",
      "      \"learning_rate\": 7.0749842470069314e-06,\n",
      "      \"loss\": 0.0031,\n",
      "      \"step\": 136300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.589810643057437,\n",
      "      \"grad_norm\": 0.00044154017814435065,\n",
      "      \"learning_rate\": 7.068683049779458e-06,\n",
      "      \"loss\": 0.0031,\n",
      "      \"step\": 136320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.59107092220927,\n",
      "      \"grad_norm\": 7.9233078956604,\n",
      "      \"learning_rate\": 7.062381852551986e-06,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 136340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.592331201361102,\n",
      "      \"grad_norm\": 0.0074655842036008835,\n",
      "      \"learning_rate\": 7.056080655324513e-06,\n",
      "      \"loss\": 0.0536,\n",
      "      \"step\": 136360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.593591480512934,\n",
      "      \"grad_norm\": 0.0041787708178162575,\n",
      "      \"learning_rate\": 7.049779458097039e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 136380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.594851759664765,\n",
      "      \"grad_norm\": 0.0003875411639455706,\n",
      "      \"learning_rate\": 7.043478260869566e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 136400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.596112038816598,\n",
      "      \"grad_norm\": 0.011625971645116806,\n",
      "      \"learning_rate\": 7.037177063642092e-06,\n",
      "      \"loss\": 0.0065,\n",
      "      \"step\": 136420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.59737231796843,\n",
      "      \"grad_norm\": 0.00023346276429947466,\n",
      "      \"learning_rate\": 7.030875866414619e-06,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 136440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.598632597120263,\n",
      "      \"grad_norm\": 0.00018720717343967408,\n",
      "      \"learning_rate\": 7.024574669187147e-06,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 136460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.599892876272094,\n",
      "      \"grad_norm\": 0.036486539989709854,\n",
      "      \"learning_rate\": 7.018273471959673e-06,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 136480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.601153155423926,\n",
      "      \"grad_norm\": 0.009992234408855438,\n",
      "      \"learning_rate\": 7.0119722747322e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 136500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.602413434575759,\n",
      "      \"grad_norm\": 0.00225091353058815,\n",
      "      \"learning_rate\": 7.005671077504726e-06,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 136520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.603673713727591,\n",
      "      \"grad_norm\": 0.4517008364200592,\n",
      "      \"learning_rate\": 6.999369880277253e-06,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 136540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.604933992879422,\n",
      "      \"grad_norm\": 0.00048055194201879203,\n",
      "      \"learning_rate\": 6.99306868304978e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 136560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.606194272031255,\n",
      "      \"grad_norm\": 0.01475328579545021,\n",
      "      \"learning_rate\": 6.986767485822307e-06,\n",
      "      \"loss\": 0.0183,\n",
      "      \"step\": 136580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.607454551183087,\n",
      "      \"grad_norm\": 0.0010269287740811706,\n",
      "      \"learning_rate\": 6.980466288594833e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 136600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.60871483033492,\n",
      "      \"grad_norm\": 0.02548684924840927,\n",
      "      \"learning_rate\": 6.9741650913673605e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 136620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.60997510948675,\n",
      "      \"grad_norm\": 0.00017471116734668612,\n",
      "      \"learning_rate\": 6.967863894139887e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 136640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.611235388638583,\n",
      "      \"grad_norm\": 0.0011463292175903916,\n",
      "      \"learning_rate\": 6.9615626969124135e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 136660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.612495667790416,\n",
      "      \"grad_norm\": 0.0007013143040239811,\n",
      "      \"learning_rate\": 6.955261499684941e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 136680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.613755946942248,\n",
      "      \"grad_norm\": 0.08458921313285828,\n",
      "      \"learning_rate\": 6.948960302457467e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 136700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.61501622609408,\n",
      "      \"grad_norm\": 0.00014348917466122657,\n",
      "      \"learning_rate\": 6.942659105229994e-06,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 136720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.616276505245912,\n",
      "      \"grad_norm\": 0.0005037599476054311,\n",
      "      \"learning_rate\": 6.93635790800252e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 136740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.617536784397744,\n",
      "      \"grad_norm\": 5.991250514984131,\n",
      "      \"learning_rate\": 6.930056710775047e-06,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 136760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.618797063549577,\n",
      "      \"grad_norm\": 0.07773474603891373,\n",
      "      \"learning_rate\": 6.923755513547573e-06,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 136780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.620057342701408,\n",
      "      \"grad_norm\": 0.0007665210869163275,\n",
      "      \"learning_rate\": 6.9174543163201015e-06,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 136800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.62131762185324,\n",
      "      \"grad_norm\": 0.00017901012324728072,\n",
      "      \"learning_rate\": 6.911153119092628e-06,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 136820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.622577901005073,\n",
      "      \"grad_norm\": 0.3050171136856079,\n",
      "      \"learning_rate\": 6.9048519218651545e-06,\n",
      "      \"loss\": 0.0051,\n",
      "      \"step\": 136840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.623838180156906,\n",
      "      \"grad_norm\": 0.0001942907110787928,\n",
      "      \"learning_rate\": 6.898550724637681e-06,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 136860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.625098459308736,\n",
      "      \"grad_norm\": 0.181131049990654,\n",
      "      \"learning_rate\": 6.8922495274102075e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 136880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.626358738460569,\n",
      "      \"grad_norm\": 0.00043055342393927276,\n",
      "      \"learning_rate\": 6.885948330182736e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 136900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.627619017612401,\n",
      "      \"grad_norm\": 0.010732666589319706,\n",
      "      \"learning_rate\": 6.879647132955262e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 136920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.628879296764234,\n",
      "      \"grad_norm\": 0.0038347956724464893,\n",
      "      \"learning_rate\": 6.873345935727789e-06,\n",
      "      \"loss\": 0.005,\n",
      "      \"step\": 136940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.630139575916065,\n",
      "      \"grad_norm\": 0.0005135172978043556,\n",
      "      \"learning_rate\": 6.867044738500315e-06,\n",
      "      \"loss\": 0.0171,\n",
      "      \"step\": 136960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.631399855067897,\n",
      "      \"grad_norm\": 0.0001138716033892706,\n",
      "      \"learning_rate\": 6.860743541272842e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 136980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.63266013421973,\n",
      "      \"grad_norm\": 0.00019577342027332634,\n",
      "      \"learning_rate\": 6.854442344045368e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 137000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.633920413371563,\n",
      "      \"grad_norm\": 0.0012970223324373364,\n",
      "      \"learning_rate\": 6.848141146817896e-06,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 137020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.635180692523393,\n",
      "      \"grad_norm\": 0.0001923254458233714,\n",
      "      \"learning_rate\": 6.841839949590423e-06,\n",
      "      \"loss\": 0.0162,\n",
      "      \"step\": 137040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.636440971675226,\n",
      "      \"grad_norm\": 0.025479597970843315,\n",
      "      \"learning_rate\": 6.835538752362949e-06,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 137060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.637701250827059,\n",
      "      \"grad_norm\": 0.0008252555853687227,\n",
      "      \"learning_rate\": 6.829237555135476e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 137080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.638961529978891,\n",
      "      \"grad_norm\": 0.004133940674364567,\n",
      "      \"learning_rate\": 6.822936357908002e-06,\n",
      "      \"loss\": 0.0416,\n",
      "      \"step\": 137100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.640221809130722,\n",
      "      \"grad_norm\": 0.001710026292130351,\n",
      "      \"learning_rate\": 6.816635160680529e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 137120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.641482088282554,\n",
      "      \"grad_norm\": 0.13318514823913574,\n",
      "      \"learning_rate\": 6.810333963453057e-06,\n",
      "      \"loss\": 0.0284,\n",
      "      \"step\": 137140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.642742367434387,\n",
      "      \"grad_norm\": 0.0001301246229559183,\n",
      "      \"learning_rate\": 6.8040327662255836e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 137160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.64400264658622,\n",
      "      \"grad_norm\": 0.00010165549610974267,\n",
      "      \"learning_rate\": 6.79773156899811e-06,\n",
      "      \"loss\": 0.0168,\n",
      "      \"step\": 137180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.64526292573805,\n",
      "      \"grad_norm\": 0.003762961830943823,\n",
      "      \"learning_rate\": 6.7914303717706366e-06,\n",
      "      \"loss\": 0.0083,\n",
      "      \"step\": 137200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.646523204889883,\n",
      "      \"grad_norm\": 0.011957546696066856,\n",
      "      \"learning_rate\": 6.785129174543163e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 137220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.647783484041716,\n",
      "      \"grad_norm\": 0.012066108174622059,\n",
      "      \"learning_rate\": 6.778827977315691e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 137240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.649043763193548,\n",
      "      \"grad_norm\": 0.02553712949156761,\n",
      "      \"learning_rate\": 6.772526780088218e-06,\n",
      "      \"loss\": 0.021,\n",
      "      \"step\": 137260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.650304042345379,\n",
      "      \"grad_norm\": 0.0037194297183305025,\n",
      "      \"learning_rate\": 6.766225582860744e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 137280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.651564321497212,\n",
      "      \"grad_norm\": 0.0004200258699711412,\n",
      "      \"learning_rate\": 6.759924385633271e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 137300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.652824600649044,\n",
      "      \"grad_norm\": 0.00021931080846115947,\n",
      "      \"learning_rate\": 6.753623188405797e-06,\n",
      "      \"loss\": 0.0059,\n",
      "      \"step\": 137320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.654084879800877,\n",
      "      \"grad_norm\": 0.001553331152535975,\n",
      "      \"learning_rate\": 6.747321991178324e-06,\n",
      "      \"loss\": 0.0291,\n",
      "      \"step\": 137340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.655345158952708,\n",
      "      \"grad_norm\": 0.0007106673438102007,\n",
      "      \"learning_rate\": 6.741020793950852e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 137360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.65660543810454,\n",
      "      \"grad_norm\": 0.009632816538214684,\n",
      "      \"learning_rate\": 6.7347195967233784e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 137380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.657865717256373,\n",
      "      \"grad_norm\": 0.005987036973237991,\n",
      "      \"learning_rate\": 6.728418399495905e-06,\n",
      "      \"loss\": 0.0156,\n",
      "      \"step\": 137400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.659125996408205,\n",
      "      \"grad_norm\": 0.037071023136377335,\n",
      "      \"learning_rate\": 6.7221172022684314e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 137420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.660386275560036,\n",
      "      \"grad_norm\": 0.00038013889570720494,\n",
      "      \"learning_rate\": 6.715816005040958e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 137440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.661646554711869,\n",
      "      \"grad_norm\": 0.09552371501922607,\n",
      "      \"learning_rate\": 6.7095148078134844e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 137460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.662906833863701,\n",
      "      \"grad_norm\": 0.0011073428904637694,\n",
      "      \"learning_rate\": 6.703213610586012e-06,\n",
      "      \"loss\": 0.0175,\n",
      "      \"step\": 137480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.664167113015534,\n",
      "      \"grad_norm\": 0.016265496611595154,\n",
      "      \"learning_rate\": 6.696912413358539e-06,\n",
      "      \"loss\": 0.0057,\n",
      "      \"step\": 137500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.665427392167365,\n",
      "      \"grad_norm\": 0.00014286294754128903,\n",
      "      \"learning_rate\": 6.690611216131066e-06,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 137520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.666687671319197,\n",
      "      \"grad_norm\": 0.00010246864258078858,\n",
      "      \"learning_rate\": 6.684310018903592e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 137540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.66794795047103,\n",
      "      \"grad_norm\": 8.099483489990234,\n",
      "      \"learning_rate\": 6.678008821676119e-06,\n",
      "      \"loss\": 0.0349,\n",
      "      \"step\": 137560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.66920822962286,\n",
      "      \"grad_norm\": 0.021775895729660988,\n",
      "      \"learning_rate\": 6.671707624448646e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 137580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.670468508774693,\n",
      "      \"grad_norm\": 0.00013436962035484612,\n",
      "      \"learning_rate\": 6.6654064272211725e-06,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 137600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.671728787926526,\n",
      "      \"grad_norm\": 0.0004954367177560925,\n",
      "      \"learning_rate\": 6.659105229993699e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 137620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.672989067078358,\n",
      "      \"grad_norm\": 0.020081577822566032,\n",
      "      \"learning_rate\": 6.6528040327662255e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 137640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.67424934623019,\n",
      "      \"grad_norm\": 0.002635079203173518,\n",
      "      \"learning_rate\": 6.646502835538752e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 137660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.675509625382022,\n",
      "      \"grad_norm\": 0.0003237223136238754,\n",
      "      \"learning_rate\": 6.640201638311279e-06,\n",
      "      \"loss\": 0.0149,\n",
      "      \"step\": 137680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.676769904533854,\n",
      "      \"grad_norm\": 0.0020767180249094963,\n",
      "      \"learning_rate\": 6.633900441083807e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 137700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.678030183685687,\n",
      "      \"grad_norm\": 0.03997866064310074,\n",
      "      \"learning_rate\": 6.627599243856333e-06,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 137720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.679290462837518,\n",
      "      \"grad_norm\": 0.00015051277296151966,\n",
      "      \"learning_rate\": 6.62129804662886e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 137740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.68055074198935,\n",
      "      \"grad_norm\": 0.0020895900670439005,\n",
      "      \"learning_rate\": 6.614996849401386e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 137760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.681811021141183,\n",
      "      \"grad_norm\": 0.006999033968895674,\n",
      "      \"learning_rate\": 6.608695652173913e-06,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 137780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.683071300293015,\n",
      "      \"grad_norm\": 0.0002020858955802396,\n",
      "      \"learning_rate\": 6.602394454946439e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 137800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.684331579444848,\n",
      "      \"grad_norm\": 0.00010854764695977792,\n",
      "      \"learning_rate\": 6.596093257718967e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 137820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.685591858596679,\n",
      "      \"grad_norm\": 0.016063500195741653,\n",
      "      \"learning_rate\": 6.589792060491494e-06,\n",
      "      \"loss\": 0.0296,\n",
      "      \"step\": 137840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.686852137748511,\n",
      "      \"grad_norm\": 0.002333821728825569,\n",
      "      \"learning_rate\": 6.58349086326402e-06,\n",
      "      \"loss\": 0.0372,\n",
      "      \"step\": 137860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.688112416900344,\n",
      "      \"grad_norm\": 0.032869815826416016,\n",
      "      \"learning_rate\": 6.577189666036547e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 137880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.689372696052175,\n",
      "      \"grad_norm\": 0.00013375471462495625,\n",
      "      \"learning_rate\": 6.570888468809073e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 137900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.690632975204007,\n",
      "      \"grad_norm\": 0.010671300813555717,\n",
      "      \"learning_rate\": 6.5645872715816015e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 137920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.69189325435584,\n",
      "      \"grad_norm\": 0.00013369510998018086,\n",
      "      \"learning_rate\": 6.558286074354128e-06,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 137940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.693153533507672,\n",
      "      \"grad_norm\": 0.0051332623697817326,\n",
      "      \"learning_rate\": 6.5519848771266545e-06,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 137960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.694413812659505,\n",
      "      \"grad_norm\": 0.020800428465008736,\n",
      "      \"learning_rate\": 6.545683679899181e-06,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 137980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.695674091811336,\n",
      "      \"grad_norm\": 0.038852088153362274,\n",
      "      \"learning_rate\": 6.5393824826717075e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 138000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.696934370963168,\n",
      "      \"grad_norm\": 13.6749267578125,\n",
      "      \"learning_rate\": 6.533081285444234e-06,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 138020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.698194650115001,\n",
      "      \"grad_norm\": 0.00010925883543677628,\n",
      "      \"learning_rate\": 6.526780088216762e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 138040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.699454929266832,\n",
      "      \"grad_norm\": 0.00014374263992067426,\n",
      "      \"learning_rate\": 6.520478890989289e-06,\n",
      "      \"loss\": 0.022,\n",
      "      \"step\": 138060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.700715208418664,\n",
      "      \"grad_norm\": 0.003616684814915061,\n",
      "      \"learning_rate\": 6.514177693761815e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 138080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.701975487570497,\n",
      "      \"grad_norm\": 0.13915890455245972,\n",
      "      \"learning_rate\": 6.507876496534342e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 138100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.70323576672233,\n",
      "      \"grad_norm\": 0.01031077653169632,\n",
      "      \"learning_rate\": 6.501575299306868e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 138120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.704496045874162,\n",
      "      \"grad_norm\": 0.015863001346588135,\n",
      "      \"learning_rate\": 6.495274102079395e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 138140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.705756325025993,\n",
      "      \"grad_norm\": 0.016209641471505165,\n",
      "      \"learning_rate\": 6.488972904851923e-06,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 138160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.707016604177825,\n",
      "      \"grad_norm\": 0.1974872499704361,\n",
      "      \"learning_rate\": 6.482671707624449e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 138180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.708276883329658,\n",
      "      \"grad_norm\": 0.00012201628851471469,\n",
      "      \"learning_rate\": 6.476370510396976e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 138200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.709537162481489,\n",
      "      \"grad_norm\": 0.004022454842925072,\n",
      "      \"learning_rate\": 6.470069313169502e-06,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 138220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.710797441633321,\n",
      "      \"grad_norm\": 0.0001175201396108605,\n",
      "      \"learning_rate\": 6.463768115942029e-06,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 138240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.712057720785154,\n",
      "      \"grad_norm\": 0.025745999068021774,\n",
      "      \"learning_rate\": 6.457466918714557e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 138260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.713317999936987,\n",
      "      \"grad_norm\": 0.0001770342787494883,\n",
      "      \"learning_rate\": 6.4511657214870836e-06,\n",
      "      \"loss\": 0.029,\n",
      "      \"step\": 138280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.714578279088819,\n",
      "      \"grad_norm\": 0.0004363475018180907,\n",
      "      \"learning_rate\": 6.44486452425961e-06,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 138300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.71583855824065,\n",
      "      \"grad_norm\": 0.0006910222582519054,\n",
      "      \"learning_rate\": 6.4385633270321366e-06,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 138320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.717098837392482,\n",
      "      \"grad_norm\": 0.00043181079672649503,\n",
      "      \"learning_rate\": 6.432262129804663e-06,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 138340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.718359116544315,\n",
      "      \"grad_norm\": 0.0026517792139202356,\n",
      "      \"learning_rate\": 6.4259609325771896e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 138360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.719619395696146,\n",
      "      \"grad_norm\": 0.00046936559374444187,\n",
      "      \"learning_rate\": 6.419659735349717e-06,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 138380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.720879674847978,\n",
      "      \"grad_norm\": 0.0001300569565501064,\n",
      "      \"learning_rate\": 6.413358538122244e-06,\n",
      "      \"loss\": 0.006,\n",
      "      \"step\": 138400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.722139953999811,\n",
      "      \"grad_norm\": 0.00025887502124533057,\n",
      "      \"learning_rate\": 6.407057340894771e-06,\n",
      "      \"loss\": 0.058,\n",
      "      \"step\": 138420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.723400233151644,\n",
      "      \"grad_norm\": 0.007100861519575119,\n",
      "      \"learning_rate\": 6.400756143667297e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 138440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.724660512303474,\n",
      "      \"grad_norm\": 0.000423813791712746,\n",
      "      \"learning_rate\": 6.394454946439824e-06,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 138460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.725920791455307,\n",
      "      \"grad_norm\": 0.005650707986205816,\n",
      "      \"learning_rate\": 6.38815374921235e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 138480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.72718107060714,\n",
      "      \"grad_norm\": 0.0007014223956502974,\n",
      "      \"learning_rate\": 6.381852551984878e-06,\n",
      "      \"loss\": 0.0436,\n",
      "      \"step\": 138500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.728441349758972,\n",
      "      \"grad_norm\": 0.0010716011747717857,\n",
      "      \"learning_rate\": 6.375551354757404e-06,\n",
      "      \"loss\": 0.0073,\n",
      "      \"step\": 138520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.729701628910803,\n",
      "      \"grad_norm\": 0.0001200093756779097,\n",
      "      \"learning_rate\": 6.369250157529931e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 138540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.730961908062636,\n",
      "      \"grad_norm\": 0.0003260214871261269,\n",
      "      \"learning_rate\": 6.362948960302458e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 138560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.732222187214468,\n",
      "      \"grad_norm\": 0.001346773118712008,\n",
      "      \"learning_rate\": 6.3566477630749844e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 138580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.7334824663663,\n",
      "      \"grad_norm\": 0.00012814736692234874,\n",
      "      \"learning_rate\": 6.350346565847512e-06,\n",
      "      \"loss\": 0.0124,\n",
      "      \"step\": 138600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.734742745518131,\n",
      "      \"grad_norm\": 0.001229780144058168,\n",
      "      \"learning_rate\": 6.344045368620038e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 138620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.736003024669964,\n",
      "      \"grad_norm\": 0.0012934425612911582,\n",
      "      \"learning_rate\": 6.337744171392565e-06,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 138640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.737263303821797,\n",
      "      \"grad_norm\": 0.0007953945896588266,\n",
      "      \"learning_rate\": 6.331442974165091e-06,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 138660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.73852358297363,\n",
      "      \"grad_norm\": 0.04216970130801201,\n",
      "      \"learning_rate\": 6.325141776937618e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 138680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.73978386212546,\n",
      "      \"grad_norm\": 0.0036082991864532232,\n",
      "      \"learning_rate\": 6.318840579710144e-06,\n",
      "      \"loss\": 0.0169,\n",
      "      \"step\": 138700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.741044141277293,\n",
      "      \"grad_norm\": 0.00010761620796984062,\n",
      "      \"learning_rate\": 6.3125393824826725e-06,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 138720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.742304420429125,\n",
      "      \"grad_norm\": 0.0008854392799548805,\n",
      "      \"learning_rate\": 6.306238185255199e-06,\n",
      "      \"loss\": 0.0318,\n",
      "      \"step\": 138740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.743564699580958,\n",
      "      \"grad_norm\": 0.01975417695939541,\n",
      "      \"learning_rate\": 6.2999369880277254e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 138760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.744824978732789,\n",
      "      \"grad_norm\": 0.0004054825403727591,\n",
      "      \"learning_rate\": 6.293635790800252e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 138780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.746085257884621,\n",
      "      \"grad_norm\": 0.000952119764406234,\n",
      "      \"learning_rate\": 6.2873345935727784e-06,\n",
      "      \"loss\": 0.0349,\n",
      "      \"step\": 138800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.747345537036454,\n",
      "      \"grad_norm\": 0.0001598582894075662,\n",
      "      \"learning_rate\": 6.281033396345305e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 138820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.748605816188286,\n",
      "      \"grad_norm\": 0.014567124657332897,\n",
      "      \"learning_rate\": 6.274732199117833e-06,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 138840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.749866095340117,\n",
      "      \"grad_norm\": 0.04521028324961662,\n",
      "      \"learning_rate\": 6.26843100189036e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 138860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.75112637449195,\n",
      "      \"grad_norm\": 0.24901320040225983,\n",
      "      \"learning_rate\": 6.262129804662886e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 138880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.752386653643782,\n",
      "      \"grad_norm\": 0.009502563625574112,\n",
      "      \"learning_rate\": 6.255828607435413e-06,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 138900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.753646932795615,\n",
      "      \"grad_norm\": 0.0388292632997036,\n",
      "      \"learning_rate\": 6.24952741020794e-06,\n",
      "      \"loss\": 0.0206,\n",
      "      \"step\": 138920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.754907211947446,\n",
      "      \"grad_norm\": 0.0032009845599532127,\n",
      "      \"learning_rate\": 6.2432262129804665e-06,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 138940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.756167491099278,\n",
      "      \"grad_norm\": 0.04448215663433075,\n",
      "      \"learning_rate\": 6.236925015752993e-06,\n",
      "      \"loss\": 0.0506,\n",
      "      \"step\": 138960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.75742777025111,\n",
      "      \"grad_norm\": 0.0029505109414458275,\n",
      "      \"learning_rate\": 6.23062381852552e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 138980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.758688049402943,\n",
      "      \"grad_norm\": 0.003452860051766038,\n",
      "      \"learning_rate\": 6.224322621298047e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 139000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.759948328554774,\n",
      "      \"grad_norm\": 0.0009029260254465044,\n",
      "      \"learning_rate\": 6.218021424070573e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 139020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.761208607706607,\n",
      "      \"grad_norm\": 0.000714163645170629,\n",
      "      \"learning_rate\": 6.211720226843101e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 139040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.76246888685844,\n",
      "      \"grad_norm\": 0.034319519996643066,\n",
      "      \"learning_rate\": 6.205419029615627e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 139060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.763729166010272,\n",
      "      \"grad_norm\": 0.21168005466461182,\n",
      "      \"learning_rate\": 6.1991178323881545e-06,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 139080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.764989445162103,\n",
      "      \"grad_norm\": 0.002971465466544032,\n",
      "      \"learning_rate\": 6.192816635160681e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 139100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.766249724313935,\n",
      "      \"grad_norm\": 0.009024177677929401,\n",
      "      \"learning_rate\": 6.1865154379332075e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 139120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.767510003465768,\n",
      "      \"grad_norm\": 0.0009896198753267527,\n",
      "      \"learning_rate\": 6.180214240705735e-06,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 139140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.7687702826176,\n",
      "      \"grad_norm\": 0.06426413357257843,\n",
      "      \"learning_rate\": 6.174228103339635e-06,\n",
      "      \"loss\": 0.0321,\n",
      "      \"step\": 139160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.770030561769431,\n",
      "      \"grad_norm\": 0.0015515984268859029,\n",
      "      \"learning_rate\": 6.167926906112162e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 139180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.771290840921264,\n",
      "      \"grad_norm\": 0.00015660564531572163,\n",
      "      \"learning_rate\": 6.161625708884689e-06,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 139200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.772551120073096,\n",
      "      \"grad_norm\": 0.006236466113477945,\n",
      "      \"learning_rate\": 6.155324511657215e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 139220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.773811399224929,\n",
      "      \"grad_norm\": 0.46436136960983276,\n",
      "      \"learning_rate\": 6.149023314429743e-06,\n",
      "      \"loss\": 0.032,\n",
      "      \"step\": 139240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.77507167837676,\n",
      "      \"grad_norm\": 0.0005952866049483418,\n",
      "      \"learning_rate\": 6.142722117202269e-06,\n",
      "      \"loss\": 0.0178,\n",
      "      \"step\": 139260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.776331957528592,\n",
      "      \"grad_norm\": 0.02225656434893608,\n",
      "      \"learning_rate\": 6.136420919974796e-06,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 139280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.777592236680425,\n",
      "      \"grad_norm\": 0.024183249101042747,\n",
      "      \"learning_rate\": 6.130119722747323e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 139300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.778852515832257,\n",
      "      \"grad_norm\": 0.017676791176199913,\n",
      "      \"learning_rate\": 6.123818525519849e-06,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 139320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.780112794984088,\n",
      "      \"grad_norm\": 0.00023389808484353125,\n",
      "      \"learning_rate\": 6.117517328292376e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 139340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.78137307413592,\n",
      "      \"grad_norm\": 0.0038835667073726654,\n",
      "      \"learning_rate\": 6.111216131064903e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 139360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.782633353287753,\n",
      "      \"grad_norm\": 0.024090616032481194,\n",
      "      \"learning_rate\": 6.10491493383743e-06,\n",
      "      \"loss\": 0.037,\n",
      "      \"step\": 139380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.783893632439586,\n",
      "      \"grad_norm\": 0.00037732403143309057,\n",
      "      \"learning_rate\": 6.098613736609956e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 139400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.785153911591417,\n",
      "      \"grad_norm\": 0.03245899826288223,\n",
      "      \"learning_rate\": 6.092312539382483e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 139420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.78641419074325,\n",
      "      \"grad_norm\": 0.004243344068527222,\n",
      "      \"learning_rate\": 6.086011342155009e-06,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 139440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.787674469895082,\n",
      "      \"grad_norm\": 0.0022532327566295862,\n",
      "      \"learning_rate\": 6.079710144927537e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 139460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.788934749046915,\n",
      "      \"grad_norm\": 0.0007340426091104746,\n",
      "      \"learning_rate\": 6.073408947700063e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 139480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.790195028198745,\n",
      "      \"grad_norm\": 0.02363618090748787,\n",
      "      \"learning_rate\": 6.06710775047259e-06,\n",
      "      \"loss\": 0.0229,\n",
      "      \"step\": 139500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.791455307350578,\n",
      "      \"grad_norm\": 0.0011174113024026155,\n",
      "      \"learning_rate\": 6.060806553245117e-06,\n",
      "      \"loss\": 0.0156,\n",
      "      \"step\": 139520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.79271558650241,\n",
      "      \"grad_norm\": 0.0005655298009514809,\n",
      "      \"learning_rate\": 6.0545053560176434e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 139540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.793975865654243,\n",
      "      \"grad_norm\": 0.020948532968759537,\n",
      "      \"learning_rate\": 6.04820415879017e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 139560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.795236144806074,\n",
      "      \"grad_norm\": 0.0009052734239958227,\n",
      "      \"learning_rate\": 6.041902961562697e-06,\n",
      "      \"loss\": 0.0196,\n",
      "      \"step\": 139580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.796496423957906,\n",
      "      \"grad_norm\": 0.0003666110278572887,\n",
      "      \"learning_rate\": 6.035601764335224e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 139600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.797756703109739,\n",
      "      \"grad_norm\": 0.006491020787507296,\n",
      "      \"learning_rate\": 6.02930056710775e-06,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 139620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.799016982261572,\n",
      "      \"grad_norm\": 0.0023824095260351896,\n",
      "      \"learning_rate\": 6.022999369880278e-06,\n",
      "      \"loss\": 0.0135,\n",
      "      \"step\": 139640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.800277261413402,\n",
      "      \"grad_norm\": 0.18546496331691742,\n",
      "      \"learning_rate\": 6.016698172652804e-06,\n",
      "      \"loss\": 0.0129,\n",
      "      \"step\": 139660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.801537540565235,\n",
      "      \"grad_norm\": 0.00011711665138136595,\n",
      "      \"learning_rate\": 6.010396975425331e-06,\n",
      "      \"loss\": 0.0069,\n",
      "      \"step\": 139680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.802797819717068,\n",
      "      \"grad_norm\": 0.0023520933464169502,\n",
      "      \"learning_rate\": 6.004095778197858e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 139700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.8040580988689,\n",
      "      \"grad_norm\": 0.0007058741175569594,\n",
      "      \"learning_rate\": 5.9977945809703845e-06,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 139720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.805318378020731,\n",
      "      \"grad_norm\": 0.0001596753136254847,\n",
      "      \"learning_rate\": 5.991493383742911e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 139740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.806578657172563,\n",
      "      \"grad_norm\": 0.00015183414507191628,\n",
      "      \"learning_rate\": 5.985192186515438e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 139760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.807838936324396,\n",
      "      \"grad_norm\": 0.0005315984599292278,\n",
      "      \"learning_rate\": 5.978890989287965e-06,\n",
      "      \"loss\": 0.0184,\n",
      "      \"step\": 139780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.809099215476229,\n",
      "      \"grad_norm\": 0.00011288225505268201,\n",
      "      \"learning_rate\": 5.972904851921866e-06,\n",
      "      \"loss\": 0.0053,\n",
      "      \"step\": 139800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.81035949462806,\n",
      "      \"grad_norm\": 0.00013480240886565298,\n",
      "      \"learning_rate\": 5.966603654694392e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 139820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.811619773779892,\n",
      "      \"grad_norm\": 0.00013050626148469746,\n",
      "      \"learning_rate\": 5.960302457466919e-06,\n",
      "      \"loss\": 0.017,\n",
      "      \"step\": 139840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.812880052931725,\n",
      "      \"grad_norm\": 0.00012819482071790844,\n",
      "      \"learning_rate\": 5.954001260239446e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 139860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.814140332083557,\n",
      "      \"grad_norm\": 0.901671290397644,\n",
      "      \"learning_rate\": 5.9477000630119725e-06,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 139880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.815400611235388,\n",
      "      \"grad_norm\": 0.0065263365395367146,\n",
      "      \"learning_rate\": 5.941398865784499e-06,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 139900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.81666089038722,\n",
      "      \"grad_norm\": 0.020956024527549744,\n",
      "      \"learning_rate\": 5.935097668557026e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 139920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.817921169539053,\n",
      "      \"grad_norm\": 0.07282280176877975,\n",
      "      \"learning_rate\": 5.928796471329553e-06,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 139940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.819181448690886,\n",
      "      \"grad_norm\": 0.004615887068212032,\n",
      "      \"learning_rate\": 5.922495274102079e-06,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 139960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.820441727842717,\n",
      "      \"grad_norm\": 0.0004356000863481313,\n",
      "      \"learning_rate\": 5.916194076874607e-06,\n",
      "      \"loss\": 0.0181,\n",
      "      \"step\": 139980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.821702006994549,\n",
      "      \"grad_norm\": 0.012316010892391205,\n",
      "      \"learning_rate\": 5.909892879647133e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 140000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.822962286146382,\n",
      "      \"grad_norm\": 0.0054663317278027534,\n",
      "      \"learning_rate\": 5.9035916824196606e-06,\n",
      "      \"loss\": 0.0068,\n",
      "      \"step\": 140020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.824222565298214,\n",
      "      \"grad_norm\": 0.023687994107604027,\n",
      "      \"learning_rate\": 5.897290485192187e-06,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 140040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.825482844450045,\n",
      "      \"grad_norm\": 17.381427764892578,\n",
      "      \"learning_rate\": 5.8909892879647136e-06,\n",
      "      \"loss\": 0.0038,\n",
      "      \"step\": 140060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.826743123601878,\n",
      "      \"grad_norm\": 0.007776135113090277,\n",
      "      \"learning_rate\": 5.884688090737241e-06,\n",
      "      \"loss\": 0.0049,\n",
      "      \"step\": 140080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.82800340275371,\n",
      "      \"grad_norm\": 0.04702669754624367,\n",
      "      \"learning_rate\": 5.878386893509767e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 140100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.829263681905543,\n",
      "      \"grad_norm\": 0.000951218418776989,\n",
      "      \"learning_rate\": 5.872085696282294e-06,\n",
      "      \"loss\": 0.0268,\n",
      "      \"step\": 140120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.830523961057374,\n",
      "      \"grad_norm\": 0.00678650289773941,\n",
      "      \"learning_rate\": 5.865784499054821e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 140140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.831784240209206,\n",
      "      \"grad_norm\": 0.0002068400353891775,\n",
      "      \"learning_rate\": 5.859483301827348e-06,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 140160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.833044519361039,\n",
      "      \"grad_norm\": 0.001973359612748027,\n",
      "      \"learning_rate\": 5.853182104599874e-06,\n",
      "      \"loss\": 0.0243,\n",
      "      \"step\": 140180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.834304798512871,\n",
      "      \"grad_norm\": 0.0002717557945288718,\n",
      "      \"learning_rate\": 5.846880907372402e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 140200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.835565077664702,\n",
      "      \"grad_norm\": 0.0017455032793805003,\n",
      "      \"learning_rate\": 5.840579710144928e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 140220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.836825356816535,\n",
      "      \"grad_norm\": 0.0003121590125374496,\n",
      "      \"learning_rate\": 5.834278512917455e-06,\n",
      "      \"loss\": 0.0045,\n",
      "      \"step\": 140240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.838085635968367,\n",
      "      \"grad_norm\": 58.858062744140625,\n",
      "      \"learning_rate\": 5.827977315689982e-06,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 140260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.8393459151202,\n",
      "      \"grad_norm\": 0.0012985580833628774,\n",
      "      \"learning_rate\": 5.8216761184625084e-06,\n",
      "      \"loss\": 0.0245,\n",
      "      \"step\": 140280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.84060619427203,\n",
      "      \"grad_norm\": 0.0026495540514588356,\n",
      "      \"learning_rate\": 5.815374921235035e-06,\n",
      "      \"loss\": 0.022,\n",
      "      \"step\": 140300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.841866473423863,\n",
      "      \"grad_norm\": 0.00555851124227047,\n",
      "      \"learning_rate\": 5.8090737240075614e-06,\n",
      "      \"loss\": 0.0161,\n",
      "      \"step\": 140320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.843126752575696,\n",
      "      \"grad_norm\": 0.0005277899326756597,\n",
      "      \"learning_rate\": 5.802772526780089e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 140340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.844387031727528,\n",
      "      \"grad_norm\": 0.00042164811748079956,\n",
      "      \"learning_rate\": 5.796471329552615e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 140360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.84564731087936,\n",
      "      \"grad_norm\": 0.04423747956752777,\n",
      "      \"learning_rate\": 5.790170132325142e-06,\n",
      "      \"loss\": 0.0248,\n",
      "      \"step\": 140380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.846907590031192,\n",
      "      \"grad_norm\": 0.00011881591490237042,\n",
      "      \"learning_rate\": 5.783868935097668e-06,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 140400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.848167869183024,\n",
      "      \"grad_norm\": 0.001069810357876122,\n",
      "      \"learning_rate\": 5.777567737870196e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 140420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.849428148334857,\n",
      "      \"grad_norm\": 0.3007791340351105,\n",
      "      \"learning_rate\": 5.771266540642722e-06,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 140440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.850688427486688,\n",
      "      \"grad_norm\": 0.42112424969673157,\n",
      "      \"learning_rate\": 5.764965343415249e-06,\n",
      "      \"loss\": 0.0148,\n",
      "      \"step\": 140460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.85194870663852,\n",
      "      \"grad_norm\": 0.017568575218319893,\n",
      "      \"learning_rate\": 5.758664146187776e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 140480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.853208985790353,\n",
      "      \"grad_norm\": 0.012839856557548046,\n",
      "      \"learning_rate\": 5.7523629489603025e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 140500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.854469264942185,\n",
      "      \"grad_norm\": 0.00021975550043862313,\n",
      "      \"learning_rate\": 5.746061751732829e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 140520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.855729544094016,\n",
      "      \"grad_norm\": 0.0002500223636161536,\n",
      "      \"learning_rate\": 5.739760554505356e-06,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 140540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.856989823245849,\n",
      "      \"grad_norm\": 0.0002061863779090345,\n",
      "      \"learning_rate\": 5.733459357277883e-06,\n",
      "      \"loss\": 0.0267,\n",
      "      \"step\": 140560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.858250102397681,\n",
      "      \"grad_norm\": 0.0001996408391278237,\n",
      "      \"learning_rate\": 5.727158160050409e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 140580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.859510381549514,\n",
      "      \"grad_norm\": 0.000284750247374177,\n",
      "      \"learning_rate\": 5.720856962822937e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 140600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.860770660701345,\n",
      "      \"grad_norm\": 0.00011077349336119369,\n",
      "      \"learning_rate\": 5.714555765595463e-06,\n",
      "      \"loss\": 0.0017,\n",
      "      \"step\": 140620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.862030939853177,\n",
      "      \"grad_norm\": 0.0009036004776135087,\n",
      "      \"learning_rate\": 5.70825456836799e-06,\n",
      "      \"loss\": 0.0051,\n",
      "      \"step\": 140640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.86329121900501,\n",
      "      \"grad_norm\": 0.00041272054659202695,\n",
      "      \"learning_rate\": 5.701953371140517e-06,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 140660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.864551498156843,\n",
      "      \"grad_norm\": 0.011889828369021416,\n",
      "      \"learning_rate\": 5.6956521739130435e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 140680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.865811777308673,\n",
      "      \"grad_norm\": 9.65781364357099e-05,\n",
      "      \"learning_rate\": 5.689350976685571e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 140700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.867072056460506,\n",
      "      \"grad_norm\": 0.002715814160183072,\n",
      "      \"learning_rate\": 5.683049779458097e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 140720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.868332335612338,\n",
      "      \"grad_norm\": 0.033844586461782455,\n",
      "      \"learning_rate\": 5.676748582230624e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 140740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.869592614764171,\n",
      "      \"grad_norm\": 0.00010365655907662585,\n",
      "      \"learning_rate\": 5.670447385003151e-06,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 140760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.870852893916002,\n",
      "      \"grad_norm\": 0.4631544053554535,\n",
      "      \"learning_rate\": 5.664146187775678e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 140780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.872113173067834,\n",
      "      \"grad_norm\": 0.011743942275643349,\n",
      "      \"learning_rate\": 5.657844990548204e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 140800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.873373452219667,\n",
      "      \"grad_norm\": 0.00031490245601162314,\n",
      "      \"learning_rate\": 5.6515437933207315e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 140820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.8746337313715,\n",
      "      \"grad_norm\": 0.002246541902422905,\n",
      "      \"learning_rate\": 5.645242596093258e-06,\n",
      "      \"loss\": 0.0163,\n",
      "      \"step\": 140840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.87589401052333,\n",
      "      \"grad_norm\": 0.0003304517304059118,\n",
      "      \"learning_rate\": 5.6389413988657845e-06,\n",
      "      \"loss\": 0.0229,\n",
      "      \"step\": 140860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.877154289675163,\n",
      "      \"grad_norm\": 0.00014927926531527191,\n",
      "      \"learning_rate\": 5.632640201638312e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 140880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.878414568826996,\n",
      "      \"grad_norm\": 0.020769312977790833,\n",
      "      \"learning_rate\": 5.626339004410838e-06,\n",
      "      \"loss\": 0.0066,\n",
      "      \"step\": 140900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.879674847978828,\n",
      "      \"grad_norm\": 9.406519529875368e-05,\n",
      "      \"learning_rate\": 5.620037807183365e-06,\n",
      "      \"loss\": 0.0059,\n",
      "      \"step\": 140920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.880935127130659,\n",
      "      \"grad_norm\": 0.16178053617477417,\n",
      "      \"learning_rate\": 5.613736609955892e-06,\n",
      "      \"loss\": 0.0268,\n",
      "      \"step\": 140940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.882195406282491,\n",
      "      \"grad_norm\": 0.00029770020046271384,\n",
      "      \"learning_rate\": 5.607435412728419e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 140960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.883455685434324,\n",
      "      \"grad_norm\": 0.00010058442421723157,\n",
      "      \"learning_rate\": 5.601134215500945e-06,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 140980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.884715964586157,\n",
      "      \"grad_norm\": 1.9395235776901245,\n",
      "      \"learning_rate\": 5.5948330182734725e-06,\n",
      "      \"loss\": 0.0133,\n",
      "      \"step\": 141000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.885976243737987,\n",
      "      \"grad_norm\": 0.003069365629926324,\n",
      "      \"learning_rate\": 5.588531821045999e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 141020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.88723652288982,\n",
      "      \"grad_norm\": 0.0001297113485634327,\n",
      "      \"learning_rate\": 5.582230623818526e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 141040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.888496802041653,\n",
      "      \"grad_norm\": 9.951490937964991e-05,\n",
      "      \"learning_rate\": 5.575929426591053e-06,\n",
      "      \"loss\": 0.005,\n",
      "      \"step\": 141060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.889757081193485,\n",
      "      \"grad_norm\": 0.00011528679169714451,\n",
      "      \"learning_rate\": 5.569628229363579e-06,\n",
      "      \"loss\": 0.0282,\n",
      "      \"step\": 141080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.891017360345316,\n",
      "      \"grad_norm\": 0.11772975325584412,\n",
      "      \"learning_rate\": 5.563327032136107e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 141100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.892277639497149,\n",
      "      \"grad_norm\": 0.089220330119133,\n",
      "      \"learning_rate\": 5.557025834908633e-06,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 141120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.893537918648981,\n",
      "      \"grad_norm\": 0.005413088947534561,\n",
      "      \"learning_rate\": 5.55072463768116e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 141140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.894798197800814,\n",
      "      \"grad_norm\": 0.00043186251423321664,\n",
      "      \"learning_rate\": 5.544423440453687e-06,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 141160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.896058476952645,\n",
      "      \"grad_norm\": 0.0001749855582602322,\n",
      "      \"learning_rate\": 5.5381222432262136e-06,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 141180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.897318756104477,\n",
      "      \"grad_norm\": 0.00014433353499043733,\n",
      "      \"learning_rate\": 5.53182104599874e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 141200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.89857903525631,\n",
      "      \"grad_norm\": 0.00011834791075671092,\n",
      "      \"learning_rate\": 5.525519848771267e-06,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 141220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.89983931440814,\n",
      "      \"grad_norm\": 0.03716322034597397,\n",
      "      \"learning_rate\": 5.519218651543794e-06,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 141240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.901099593559973,\n",
      "      \"grad_norm\": 0.007478475105017424,\n",
      "      \"learning_rate\": 5.51291745431632e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 141260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.902359872711806,\n",
      "      \"grad_norm\": 0.0003931953979190439,\n",
      "      \"learning_rate\": 5.506616257088847e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 141280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.903620151863638,\n",
      "      \"grad_norm\": 0.18931998312473297,\n",
      "      \"learning_rate\": 5.500315059861373e-06,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 141300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.90488043101547,\n",
      "      \"grad_norm\": 23.97159194946289,\n",
      "      \"learning_rate\": 5.494013862633901e-06,\n",
      "      \"loss\": 0.0218,\n",
      "      \"step\": 141320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.906140710167302,\n",
      "      \"grad_norm\": 0.0017510884208604693,\n",
      "      \"learning_rate\": 5.487712665406427e-06,\n",
      "      \"loss\": 0.0028,\n",
      "      \"step\": 141340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.907400989319134,\n",
      "      \"grad_norm\": 0.00010786044003907591,\n",
      "      \"learning_rate\": 5.481411468178954e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 141360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.908661268470967,\n",
      "      \"grad_norm\": 0.010601463727653027,\n",
      "      \"learning_rate\": 5.475110270951481e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 141380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.909921547622798,\n",
      "      \"grad_norm\": 31.687942504882812,\n",
      "      \"learning_rate\": 5.468809073724008e-06,\n",
      "      \"loss\": 0.0021,\n",
      "      \"step\": 141400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.91118182677463,\n",
      "      \"grad_norm\": 0.00010012089478550479,\n",
      "      \"learning_rate\": 5.462507876496534e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 141420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.912442105926463,\n",
      "      \"grad_norm\": 0.00010469833796378225,\n",
      "      \"learning_rate\": 5.4562066792690614e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 141440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.913702385078295,\n",
      "      \"grad_norm\": 0.004656060598790646,\n",
      "      \"learning_rate\": 5.449905482041588e-06,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 141460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.914962664230128,\n",
      "      \"grad_norm\": 0.0475655235350132,\n",
      "      \"learning_rate\": 5.4436042848141144e-06,\n",
      "      \"loss\": 0.0269,\n",
      "      \"step\": 141480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.916222943381959,\n",
      "      \"grad_norm\": 0.00011898983211722225,\n",
      "      \"learning_rate\": 5.437303087586642e-06,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 141500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.917483222533791,\n",
      "      \"grad_norm\": 0.0032517111394554377,\n",
      "      \"learning_rate\": 5.431001890359168e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 141520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.918743501685624,\n",
      "      \"grad_norm\": 0.0004646815068554133,\n",
      "      \"learning_rate\": 5.424700693131695e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 141540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.920003780837455,\n",
      "      \"grad_norm\": 0.07818889617919922,\n",
      "      \"learning_rate\": 5.418399495904222e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 141560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.921264059989287,\n",
      "      \"grad_norm\": 0.004121960140764713,\n",
      "      \"learning_rate\": 5.412098298676749e-06,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 141580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.92252433914112,\n",
      "      \"grad_norm\": 0.0001617980597075075,\n",
      "      \"learning_rate\": 5.405797101449275e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 141600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.923784618292952,\n",
      "      \"grad_norm\": 0.0009169434197247028,\n",
      "      \"learning_rate\": 5.3994959042218025e-06,\n",
      "      \"loss\": 0.0132,\n",
      "      \"step\": 141620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.925044897444785,\n",
      "      \"grad_norm\": 0.0006560547626577318,\n",
      "      \"learning_rate\": 5.393194706994329e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 141640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.926305176596616,\n",
      "      \"grad_norm\": 0.0036740547511726618,\n",
      "      \"learning_rate\": 5.386893509766856e-06,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 141660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.927565455748448,\n",
      "      \"grad_norm\": 0.11783137917518616,\n",
      "      \"learning_rate\": 5.380592312539383e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 141680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.92882573490028,\n",
      "      \"grad_norm\": 0.0001052461884682998,\n",
      "      \"learning_rate\": 5.374291115311909e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 141700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.930086014052112,\n",
      "      \"grad_norm\": 0.011611297726631165,\n",
      "      \"learning_rate\": 5.367989918084437e-06,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 141720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.931346293203944,\n",
      "      \"grad_norm\": 9.992363629862666e-05,\n",
      "      \"learning_rate\": 5.361688720856963e-06,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 141740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.932606572355777,\n",
      "      \"grad_norm\": 0.00011002394603565335,\n",
      "      \"learning_rate\": 5.35538752362949e-06,\n",
      "      \"loss\": 0.0231,\n",
      "      \"step\": 141760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.93386685150761,\n",
      "      \"grad_norm\": 9.193779987981543e-05,\n",
      "      \"learning_rate\": 5.349086326402017e-06,\n",
      "      \"loss\": 0.0068,\n",
      "      \"step\": 141780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.935127130659442,\n",
      "      \"grad_norm\": 0.003002881770953536,\n",
      "      \"learning_rate\": 5.3427851291745435e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 141800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.936387409811273,\n",
      "      \"grad_norm\": 0.0002572560333646834,\n",
      "      \"learning_rate\": 5.33648393194707e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 141820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.937647688963105,\n",
      "      \"grad_norm\": 0.00041706624324433506,\n",
      "      \"learning_rate\": 5.330182734719597e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 141840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.938907968114938,\n",
      "      \"grad_norm\": 0.00011077350791310892,\n",
      "      \"learning_rate\": 5.323881537492124e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 141860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.940168247266769,\n",
      "      \"grad_norm\": 0.008304771967232227,\n",
      "      \"learning_rate\": 5.31758034026465e-06,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 141880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.941428526418601,\n",
      "      \"grad_norm\": 9.652901644585654e-05,\n",
      "      \"learning_rate\": 5.311279143037178e-06,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 141900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.942688805570434,\n",
      "      \"grad_norm\": 0.00017894124903250486,\n",
      "      \"learning_rate\": 5.304977945809704e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 141920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.943949084722266,\n",
      "      \"grad_norm\": 0.028532294556498528,\n",
      "      \"learning_rate\": 5.298676748582231e-06,\n",
      "      \"loss\": 0.0364,\n",
      "      \"step\": 141940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.945209363874099,\n",
      "      \"grad_norm\": 8.817696652840823e-05,\n",
      "      \"learning_rate\": 5.292375551354758e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 141960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.94646964302593,\n",
      "      \"grad_norm\": 0.00010338873107684776,\n",
      "      \"learning_rate\": 5.2860743541272845e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 141980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.947729922177762,\n",
      "      \"grad_norm\": 0.0011054164497181773,\n",
      "      \"learning_rate\": 5.279773156899812e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 142000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.948990201329595,\n",
      "      \"grad_norm\": 0.0023366098757833242,\n",
      "      \"learning_rate\": 5.273471959672338e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 142020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.950250480481426,\n",
      "      \"grad_norm\": 0.00015926704509183764,\n",
      "      \"learning_rate\": 5.267170762444865e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 142040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.951510759633258,\n",
      "      \"grad_norm\": 9.742463589645922e-05,\n",
      "      \"learning_rate\": 5.260869565217392e-06,\n",
      "      \"loss\": 0.0159,\n",
      "      \"step\": 142060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.952771038785091,\n",
      "      \"grad_norm\": 0.06304515898227692,\n",
      "      \"learning_rate\": 5.254568367989919e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 142080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.954031317936924,\n",
      "      \"grad_norm\": 0.00010681348067009822,\n",
      "      \"learning_rate\": 5.248267170762445e-06,\n",
      "      \"loss\": 0.017,\n",
      "      \"step\": 142100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.955291597088754,\n",
      "      \"grad_norm\": 0.008458222262561321,\n",
      "      \"learning_rate\": 5.2419659735349725e-06,\n",
      "      \"loss\": 0.0143,\n",
      "      \"step\": 142120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.956551876240587,\n",
      "      \"grad_norm\": 0.0021940022706985474,\n",
      "      \"learning_rate\": 5.235664776307499e-06,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 142140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.95781215539242,\n",
      "      \"grad_norm\": 0.013203538954257965,\n",
      "      \"learning_rate\": 5.2293635790800255e-06,\n",
      "      \"loss\": 0.0066,\n",
      "      \"step\": 142160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.959072434544252,\n",
      "      \"grad_norm\": 0.0010666924063116312,\n",
      "      \"learning_rate\": 5.223062381852552e-06,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 142180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.960332713696083,\n",
      "      \"grad_norm\": 1.7165590524673462,\n",
      "      \"learning_rate\": 5.216761184625079e-06,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 142200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.961592992847915,\n",
      "      \"grad_norm\": 0.02600845880806446,\n",
      "      \"learning_rate\": 5.210459987397606e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 142220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.962853271999748,\n",
      "      \"grad_norm\": 0.028075437992811203,\n",
      "      \"learning_rate\": 5.204158790170132e-06,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 142240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.96411355115158,\n",
      "      \"grad_norm\": 0.029528163373470306,\n",
      "      \"learning_rate\": 5.197857592942659e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 142260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.965373830303411,\n",
      "      \"grad_norm\": 0.03304939344525337,\n",
      "      \"learning_rate\": 5.191556395715186e-06,\n",
      "      \"loss\": 0.005,\n",
      "      \"step\": 142280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.966634109455244,\n",
      "      \"grad_norm\": 9.784354188013822e-05,\n",
      "      \"learning_rate\": 5.185255198487713e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 142300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.967894388607077,\n",
      "      \"grad_norm\": 0.0026423330418765545,\n",
      "      \"learning_rate\": 5.178954001260239e-06,\n",
      "      \"loss\": 0.0048,\n",
      "      \"step\": 142320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.96915466775891,\n",
      "      \"grad_norm\": 0.001989344833418727,\n",
      "      \"learning_rate\": 5.1726528040327666e-06,\n",
      "      \"loss\": 0.0059,\n",
      "      \"step\": 142340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.97041494691074,\n",
      "      \"grad_norm\": 0.00010156475036637858,\n",
      "      \"learning_rate\": 5.166351606805293e-06,\n",
      "      \"loss\": 0.0152,\n",
      "      \"step\": 142360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.971675226062573,\n",
      "      \"grad_norm\": 0.00010598805965855718,\n",
      "      \"learning_rate\": 5.1600504095778196e-06,\n",
      "      \"loss\": 0.0051,\n",
      "      \"step\": 142380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.972935505214405,\n",
      "      \"grad_norm\": 0.003145094495266676,\n",
      "      \"learning_rate\": 5.153749212350347e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 142400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.974195784366238,\n",
      "      \"grad_norm\": 0.0012818013783544302,\n",
      "      \"learning_rate\": 5.147448015122873e-06,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 142420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.975456063518068,\n",
      "      \"grad_norm\": 0.00994106288999319,\n",
      "      \"learning_rate\": 5.1411468178954e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 142440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.976716342669901,\n",
      "      \"grad_norm\": 0.00011091048509115353,\n",
      "      \"learning_rate\": 5.134845620667927e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 142460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.977976621821734,\n",
      "      \"grad_norm\": 0.0019674308132380247,\n",
      "      \"learning_rate\": 5.128544423440454e-06,\n",
      "      \"loss\": 0.0223,\n",
      "      \"step\": 142480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.979236900973566,\n",
      "      \"grad_norm\": 0.00013400170428212732,\n",
      "      \"learning_rate\": 5.12224322621298e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 142500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.980497180125397,\n",
      "      \"grad_norm\": 0.0017121633281931281,\n",
      "      \"learning_rate\": 5.115942028985508e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 142520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.98175745927723,\n",
      "      \"grad_norm\": 0.00012241014337632805,\n",
      "      \"learning_rate\": 5.109640831758034e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 142540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.983017738429062,\n",
      "      \"grad_norm\": 0.00019492997671477497,\n",
      "      \"learning_rate\": 5.103339634530561e-06,\n",
      "      \"loss\": 0.0275,\n",
      "      \"step\": 142560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.984278017580895,\n",
      "      \"grad_norm\": 35.01461410522461,\n",
      "      \"learning_rate\": 5.097038437303088e-06,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 142580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.985538296732726,\n",
      "      \"grad_norm\": 0.011021094396710396,\n",
      "      \"learning_rate\": 5.0907372400756144e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 142600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.986798575884558,\n",
      "      \"grad_norm\": 7.279234409332275,\n",
      "      \"learning_rate\": 5.084436042848141e-06,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 142620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.98805885503639,\n",
      "      \"grad_norm\": 0.046973817050457,\n",
      "      \"learning_rate\": 5.078134845620668e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 142640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.989319134188223,\n",
      "      \"grad_norm\": 0.01542310044169426,\n",
      "      \"learning_rate\": 5.071833648393195e-06,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 142660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.990579413340054,\n",
      "      \"grad_norm\": 0.024824731051921844,\n",
      "      \"learning_rate\": 5.065532451165722e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 142680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.991839692491887,\n",
      "      \"grad_norm\": 0.0014380370266735554,\n",
      "      \"learning_rate\": 5.059231253938249e-06,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 142700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.99309997164372,\n",
      "      \"grad_norm\": 0.00021355942590162158,\n",
      "      \"learning_rate\": 5.052930056710775e-06,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 142720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.994360250795552,\n",
      "      \"grad_norm\": 9.806274465518072e-05,\n",
      "      \"learning_rate\": 5.0466288594833025e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 142740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.995620529947383,\n",
      "      \"grad_norm\": 2.401671886444092,\n",
      "      \"learning_rate\": 5.040327662255829e-06,\n",
      "      \"loss\": 0.0028,\n",
      "      \"step\": 142760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.996880809099215,\n",
      "      \"grad_norm\": 0.0005207605427131057,\n",
      "      \"learning_rate\": 5.0340264650283555e-06,\n",
      "      \"loss\": 0.0433,\n",
      "      \"step\": 142780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.998141088251048,\n",
      "      \"grad_norm\": 0.7810210585594177,\n",
      "      \"learning_rate\": 5.027725267800883e-06,\n",
      "      \"loss\": 0.0183,\n",
      "      \"step\": 142800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.99940136740288,\n",
      "      \"grad_norm\": 0.0021866115275770426,\n",
      "      \"learning_rate\": 5.021424070573409e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 142820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.0,\n",
      "      \"eval_accuracy\": 0.9997794719929431,\n",
      "      \"eval_f1\": 0.9997794789402388,\n",
      "      \"eval_loss\": 0.0013297521509230137,\n",
      "      \"eval_precision\": 0.9997479838709677,\n",
      "      \"eval_recall\": 0.9998109759939512,\n",
      "      \"eval_runtime\": 676.3308,\n",
      "      \"eval_samples_per_second\": 46.933,\n",
      "      \"eval_steps_per_second\": 5.867,\n",
      "      \"step\": 142830\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.000630139575916,\n",
      "      \"grad_norm\": 0.010942514054477215,\n",
      "      \"learning_rate\": 5.015122873345936e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 142840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.001890418727749,\n",
      "      \"grad_norm\": 0.15387819707393646,\n",
      "      \"learning_rate\": 5.008821676118463e-06,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 142860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.00315069787958,\n",
      "      \"grad_norm\": 0.20997509360313416,\n",
      "      \"learning_rate\": 5.00252047889099e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 142880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.004410977031412,\n",
      "      \"grad_norm\": 0.0018009048653766513,\n",
      "      \"learning_rate\": 4.996219281663516e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 142900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.005671256183245,\n",
      "      \"grad_norm\": 0.0192536860704422,\n",
      "      \"learning_rate\": 4.9899180844360435e-06,\n",
      "      \"loss\": 0.0352,\n",
      "      \"step\": 142920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.006931535335077,\n",
      "      \"grad_norm\": 0.00160763890016824,\n",
      "      \"learning_rate\": 4.98361688720857e-06,\n",
      "      \"loss\": 0.0168,\n",
      "      \"step\": 142940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.008191814486908,\n",
      "      \"grad_norm\": 0.0001963656977750361,\n",
      "      \"learning_rate\": 4.9773156899810965e-06,\n",
      "      \"loss\": 0.0051,\n",
      "      \"step\": 142960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.00945209363874,\n",
      "      \"grad_norm\": 0.0024024471640586853,\n",
      "      \"learning_rate\": 4.971014492753624e-06,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 142980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.010712372790573,\n",
      "      \"grad_norm\": 0.002838494023308158,\n",
      "      \"learning_rate\": 4.96471329552615e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 143000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.011972651942406,\n",
      "      \"grad_norm\": 1.393959879875183,\n",
      "      \"learning_rate\": 4.958412098298678e-06,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 143020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.013232931094237,\n",
      "      \"grad_norm\": 0.0013574528275057673,\n",
      "      \"learning_rate\": 4.952110901071204e-06,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 143040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.01449321024607,\n",
      "      \"grad_norm\": 0.0024921444710344076,\n",
      "      \"learning_rate\": 4.945809703843731e-06,\n",
      "      \"loss\": 0.0254,\n",
      "      \"step\": 143060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.015753489397902,\n",
      "      \"grad_norm\": 0.001045550568960607,\n",
      "      \"learning_rate\": 4.939508506616258e-06,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 143080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.017013768549734,\n",
      "      \"grad_norm\": 0.00023225400946103036,\n",
      "      \"learning_rate\": 4.9332073093887845e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 143100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.018274047701565,\n",
      "      \"grad_norm\": 0.0005496683879755437,\n",
      "      \"learning_rate\": 4.926906112161311e-06,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 143120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.019534326853398,\n",
      "      \"grad_norm\": 0.013333107344806194,\n",
      "      \"learning_rate\": 4.9206049149338375e-06,\n",
      "      \"loss\": 0.0354,\n",
      "      \"step\": 143140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.02079460600523,\n",
      "      \"grad_norm\": 0.0013557126512750983,\n",
      "      \"learning_rate\": 4.914303717706365e-06,\n",
      "      \"loss\": 0.0057,\n",
      "      \"step\": 143160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.022054885157063,\n",
      "      \"grad_norm\": 0.004705911036580801,\n",
      "      \"learning_rate\": 4.908002520478891e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 143180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.023315164308894,\n",
      "      \"grad_norm\": 0.0009210477001033723,\n",
      "      \"learning_rate\": 4.901701323251418e-06,\n",
      "      \"loss\": 0.0197,\n",
      "      \"step\": 143200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.024575443460726,\n",
      "      \"grad_norm\": 0.0008431829628534615,\n",
      "      \"learning_rate\": 4.895400126023944e-06,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 143220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.025835722612559,\n",
      "      \"grad_norm\": 0.0009128061938099563,\n",
      "      \"learning_rate\": 4.889098928796472e-06,\n",
      "      \"loss\": 0.0132,\n",
      "      \"step\": 143240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.027096001764392,\n",
      "      \"grad_norm\": 0.0027601993642747402,\n",
      "      \"learning_rate\": 4.882797731568998e-06,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 143260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.028356280916222,\n",
      "      \"grad_norm\": 0.001388802076689899,\n",
      "      \"learning_rate\": 4.876496534341525e-06,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 143280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.029616560068055,\n",
      "      \"grad_norm\": 0.00115015113260597,\n",
      "      \"learning_rate\": 4.870195337114051e-06,\n",
      "      \"loss\": 0.0068,\n",
      "      \"step\": 143300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.030876839219887,\n",
      "      \"grad_norm\": 0.00018073635874316096,\n",
      "      \"learning_rate\": 4.8638941398865785e-06,\n",
      "      \"loss\": 0.0286,\n",
      "      \"step\": 143320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.03213711837172,\n",
      "      \"grad_norm\": 0.004881005734205246,\n",
      "      \"learning_rate\": 4.857592942659105e-06,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 143340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.03339739752355,\n",
      "      \"grad_norm\": 0.008759972639381886,\n",
      "      \"learning_rate\": 4.851291745431632e-06,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 143360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.034657676675383,\n",
      "      \"grad_norm\": 0.00090051005827263,\n",
      "      \"learning_rate\": 4.844990548204159e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 143380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.035917955827216,\n",
      "      \"grad_norm\": 0.012709246017038822,\n",
      "      \"learning_rate\": 4.838689350976685e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 143400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.037178234979049,\n",
      "      \"grad_norm\": 0.0025767663028091192,\n",
      "      \"learning_rate\": 4.832388153749213e-06,\n",
      "      \"loss\": 0.0349,\n",
      "      \"step\": 143420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.03843851413088,\n",
      "      \"grad_norm\": 0.03337782248854637,\n",
      "      \"learning_rate\": 4.826086956521739e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 143440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.039698793282712,\n",
      "      \"grad_norm\": 0.0012495535193011165,\n",
      "      \"learning_rate\": 4.819785759294266e-06,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 143460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.040959072434545,\n",
      "      \"grad_norm\": 0.0003327481390442699,\n",
      "      \"learning_rate\": 4.813484562066793e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 143480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.042219351586377,\n",
      "      \"grad_norm\": 0.00013010372640565038,\n",
      "      \"learning_rate\": 4.8071833648393196e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 143500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.043479630738208,\n",
      "      \"grad_norm\": 0.019301386550068855,\n",
      "      \"learning_rate\": 4.800882167611846e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 143520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.04473990989004,\n",
      "      \"grad_norm\": 0.0015149351675063372,\n",
      "      \"learning_rate\": 4.794580970384373e-06,\n",
      "      \"loss\": 0.0015,\n",
      "      \"step\": 143540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.046000189041873,\n",
      "      \"grad_norm\": 0.00025803272728808224,\n",
      "      \"learning_rate\": 4.7882797731569e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 143560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.047260468193706,\n",
      "      \"grad_norm\": 0.00036353623727336526,\n",
      "      \"learning_rate\": 4.781978575929426e-06,\n",
      "      \"loss\": 0.0156,\n",
      "      \"step\": 143580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.048520747345536,\n",
      "      \"grad_norm\": 0.008982845582067966,\n",
      "      \"learning_rate\": 4.775677378701954e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 143600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.049781026497369,\n",
      "      \"grad_norm\": 0.00015456830442417413,\n",
      "      \"learning_rate\": 4.76937618147448e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 143620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.051041305649202,\n",
      "      \"grad_norm\": 0.0010331375524401665,\n",
      "      \"learning_rate\": 4.763074984247007e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 143640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.052301584801034,\n",
      "      \"grad_norm\": 0.006322089117020369,\n",
      "      \"learning_rate\": 4.756773787019534e-06,\n",
      "      \"loss\": 0.023,\n",
      "      \"step\": 143660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.053561863952865,\n",
      "      \"grad_norm\": 0.001590945292264223,\n",
      "      \"learning_rate\": 4.750472589792061e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 143680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.054822143104698,\n",
      "      \"grad_norm\": 0.00022648433514405042,\n",
      "      \"learning_rate\": 4.744171392564588e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 143700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.05608242225653,\n",
      "      \"grad_norm\": 0.0007220604456961155,\n",
      "      \"learning_rate\": 4.7378701953371144e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 143720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.057342701408363,\n",
      "      \"grad_norm\": 0.00035918818321079016,\n",
      "      \"learning_rate\": 4.731568998109641e-06,\n",
      "      \"loss\": 0.007,\n",
      "      \"step\": 143740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.058602980560194,\n",
      "      \"grad_norm\": 0.10965977609157562,\n",
      "      \"learning_rate\": 4.725267800882168e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 143760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.059863259712026,\n",
      "      \"grad_norm\": 0.002041793428361416,\n",
      "      \"learning_rate\": 4.718966603654695e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 143780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.061123538863859,\n",
      "      \"grad_norm\": 0.007410102523863316,\n",
      "      \"learning_rate\": 4.712665406427221e-06,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 143800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.062383818015691,\n",
      "      \"grad_norm\": 0.003972816281020641,\n",
      "      \"learning_rate\": 4.706364209199749e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 143820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.063644097167522,\n",
      "      \"grad_norm\": 0.0038127691950649023,\n",
      "      \"learning_rate\": 4.700063011972275e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 143840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.064904376319355,\n",
      "      \"grad_norm\": 0.0009346434380859137,\n",
      "      \"learning_rate\": 4.693761814744802e-06,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 143860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.066164655471187,\n",
      "      \"grad_norm\": 0.006389194168150425,\n",
      "      \"learning_rate\": 4.687460617517329e-06,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 143880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.06742493462302,\n",
      "      \"grad_norm\": 0.001633489620871842,\n",
      "      \"learning_rate\": 4.6811594202898555e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 143900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.06868521377485,\n",
      "      \"grad_norm\": 0.0002438283117953688,\n",
      "      \"learning_rate\": 4.674858223062382e-06,\n",
      "      \"loss\": 0.0242,\n",
      "      \"step\": 143920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.069945492926683,\n",
      "      \"grad_norm\": 0.017476975917816162,\n",
      "      \"learning_rate\": 4.668557025834909e-06,\n",
      "      \"loss\": 0.0099,\n",
      "      \"step\": 143940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.071205772078516,\n",
      "      \"grad_norm\": 0.03368065878748894,\n",
      "      \"learning_rate\": 4.662255828607436e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 143960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.072466051230348,\n",
      "      \"grad_norm\": 0.08934590220451355,\n",
      "      \"learning_rate\": 4.655954631379962e-06,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 143980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.07372633038218,\n",
      "      \"grad_norm\": 0.03011198528110981,\n",
      "      \"learning_rate\": 4.64965343415249e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 144000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.074986609534012,\n",
      "      \"grad_norm\": 0.0007521957159042358,\n",
      "      \"learning_rate\": 4.643352236925016e-06,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 144020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.076246888685844,\n",
      "      \"grad_norm\": 0.004417688585817814,\n",
      "      \"learning_rate\": 4.6370510396975435e-06,\n",
      "      \"loss\": 0.0163,\n",
      "      \"step\": 144040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.077507167837677,\n",
      "      \"grad_norm\": 0.0003918222209904343,\n",
      "      \"learning_rate\": 4.63074984247007e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 144060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.078767446989508,\n",
      "      \"grad_norm\": 0.0008301414200104773,\n",
      "      \"learning_rate\": 4.62476370510397e-06,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 144080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.08002772614134,\n",
      "      \"grad_norm\": 0.012154054827988148,\n",
      "      \"learning_rate\": 4.6184625078764965e-06,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 144100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.081288005293173,\n",
      "      \"grad_norm\": 0.00034654312185011804,\n",
      "      \"learning_rate\": 4.612161310649023e-06,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 144120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.082548284445005,\n",
      "      \"grad_norm\": 0.00033863523276522756,\n",
      "      \"learning_rate\": 4.60586011342155e-06,\n",
      "      \"loss\": 0.0219,\n",
      "      \"step\": 144140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.083808563596836,\n",
      "      \"grad_norm\": 0.0014880780363455415,\n",
      "      \"learning_rate\": 4.599558916194077e-06,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 144160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.085068842748669,\n",
      "      \"grad_norm\": 0.000626457913313061,\n",
      "      \"learning_rate\": 4.593257718966603e-06,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 144180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.086329121900501,\n",
      "      \"grad_norm\": 0.0014572452055290341,\n",
      "      \"learning_rate\": 4.586956521739131e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 144200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.087589401052334,\n",
      "      \"grad_norm\": 0.001414815429598093,\n",
      "      \"learning_rate\": 4.580655324511657e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 144220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.088849680204165,\n",
      "      \"grad_norm\": 0.17238909006118774,\n",
      "      \"learning_rate\": 4.574354127284184e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 144240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.090109959355997,\n",
      "      \"grad_norm\": 0.001034731394611299,\n",
      "      \"learning_rate\": 4.568052930056711e-06,\n",
      "      \"loss\": 0.0029,\n",
      "      \"step\": 144260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.09137023850783,\n",
      "      \"grad_norm\": 0.050548575818538666,\n",
      "      \"learning_rate\": 4.5617517328292376e-06,\n",
      "      \"loss\": 0.0114,\n",
      "      \"step\": 144280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.092630517659662,\n",
      "      \"grad_norm\": 0.0015331236645579338,\n",
      "      \"learning_rate\": 4.555450535601764e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 144300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.093890796811493,\n",
      "      \"grad_norm\": 0.0003401256399229169,\n",
      "      \"learning_rate\": 4.549149338374291e-06,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 144320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.095151075963326,\n",
      "      \"grad_norm\": 0.003207010682672262,\n",
      "      \"learning_rate\": 4.542848141146818e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 144340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.096411355115158,\n",
      "      \"grad_norm\": 0.000692200381308794,\n",
      "      \"learning_rate\": 4.536546943919344e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 144360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.097671634266991,\n",
      "      \"grad_norm\": 0.006916014011949301,\n",
      "      \"learning_rate\": 4.530245746691872e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 144380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.098931913418822,\n",
      "      \"grad_norm\": 0.0003877922135870904,\n",
      "      \"learning_rate\": 4.523944549464398e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 144400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.100192192570654,\n",
      "      \"grad_norm\": 0.002629480790346861,\n",
      "      \"learning_rate\": 4.517643352236925e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 144420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.101452471722487,\n",
      "      \"grad_norm\": 0.016363095492124557,\n",
      "      \"learning_rate\": 4.511342155009452e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 144440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.10271275087432,\n",
      "      \"grad_norm\": 0.00025786118931137025,\n",
      "      \"learning_rate\": 4.505040957781979e-06,\n",
      "      \"loss\": 0.0077,\n",
      "      \"step\": 144460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.10397303002615,\n",
      "      \"grad_norm\": 0.0001150634852820076,\n",
      "      \"learning_rate\": 4.498739760554506e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 144480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.105233309177983,\n",
      "      \"grad_norm\": 0.5973681211471558,\n",
      "      \"learning_rate\": 4.492438563327032e-06,\n",
      "      \"loss\": 0.0274,\n",
      "      \"step\": 144500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.106493588329815,\n",
      "      \"grad_norm\": 0.002301529748365283,\n",
      "      \"learning_rate\": 4.486137366099559e-06,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 144520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.107753867481648,\n",
      "      \"grad_norm\": 0.00028936975286342204,\n",
      "      \"learning_rate\": 4.479836168872086e-06,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 144540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.109014146633479,\n",
      "      \"grad_norm\": 0.003254376584663987,\n",
      "      \"learning_rate\": 4.473534971644613e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 144560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.110274425785311,\n",
      "      \"grad_norm\": 0.00041230145143345,\n",
      "      \"learning_rate\": 4.467233774417139e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 144580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.111534704937144,\n",
      "      \"grad_norm\": 0.01616567000746727,\n",
      "      \"learning_rate\": 4.460932577189667e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 144600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.112794984088975,\n",
      "      \"grad_norm\": 0.00013571078306995332,\n",
      "      \"learning_rate\": 4.454631379962193e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 144620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.114055263240807,\n",
      "      \"grad_norm\": 0.027050673961639404,\n",
      "      \"learning_rate\": 4.44833018273472e-06,\n",
      "      \"loss\": 0.0134,\n",
      "      \"step\": 144640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.11531554239264,\n",
      "      \"grad_norm\": 0.0007090197177603841,\n",
      "      \"learning_rate\": 4.442028985507247e-06,\n",
      "      \"loss\": 0.0295,\n",
      "      \"step\": 144660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.116575821544473,\n",
      "      \"grad_norm\": 0.005125631578266621,\n",
      "      \"learning_rate\": 4.4357277882797734e-06,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 144680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.117836100696305,\n",
      "      \"grad_norm\": 0.00028764878516085446,\n",
      "      \"learning_rate\": 4.4294265910523e-06,\n",
      "      \"loss\": 0.032,\n",
      "      \"step\": 144700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.119096379848136,\n",
      "      \"grad_norm\": 0.00012328218144830316,\n",
      "      \"learning_rate\": 4.423125393824827e-06,\n",
      "      \"loss\": 0.0227,\n",
      "      \"step\": 144720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.120356658999969,\n",
      "      \"grad_norm\": 0.0385453961789608,\n",
      "      \"learning_rate\": 4.416824196597354e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 144740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.121616938151801,\n",
      "      \"grad_norm\": 0.00011012408504029736,\n",
      "      \"learning_rate\": 4.41052299936988e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 144760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.122877217303632,\n",
      "      \"grad_norm\": 0.007323266007006168,\n",
      "      \"learning_rate\": 4.404221802142408e-06,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 144780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.124137496455464,\n",
      "      \"grad_norm\": 0.005374591331928968,\n",
      "      \"learning_rate\": 4.397920604914934e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 144800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.125397775607297,\n",
      "      \"grad_norm\": 0.004569161217659712,\n",
      "      \"learning_rate\": 4.3916194076874615e-06,\n",
      "      \"loss\": 0.0249,\n",
      "      \"step\": 144820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.12665805475913,\n",
      "      \"grad_norm\": 0.027876025065779686,\n",
      "      \"learning_rate\": 4.385318210459988e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 144840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.12791833391096,\n",
      "      \"grad_norm\": 0.0003446679038461298,\n",
      "      \"learning_rate\": 4.3790170132325145e-06,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 144860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.129178613062793,\n",
      "      \"grad_norm\": 0.00040165294194594026,\n",
      "      \"learning_rate\": 4.372715816005042e-06,\n",
      "      \"loss\": 0.0073,\n",
      "      \"step\": 144880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.130438892214626,\n",
      "      \"grad_norm\": 0.00011577825353015214,\n",
      "      \"learning_rate\": 4.366414618777568e-06,\n",
      "      \"loss\": 0.0212,\n",
      "      \"step\": 144900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.131699171366458,\n",
      "      \"grad_norm\": 0.000495235261041671,\n",
      "      \"learning_rate\": 4.360113421550095e-06,\n",
      "      \"loss\": 0.0422,\n",
      "      \"step\": 144920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.132959450518289,\n",
      "      \"grad_norm\": 0.0009817915270105004,\n",
      "      \"learning_rate\": 4.353812224322622e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 144940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.134219729670122,\n",
      "      \"grad_norm\": 0.0005185716436244547,\n",
      "      \"learning_rate\": 4.347511027095149e-06,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 144960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.135480008821954,\n",
      "      \"grad_norm\": 0.003439994528889656,\n",
      "      \"learning_rate\": 4.341209829867675e-06,\n",
      "      \"loss\": 0.0145,\n",
      "      \"step\": 144980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.136740287973787,\n",
      "      \"grad_norm\": 0.0003735786594916135,\n",
      "      \"learning_rate\": 4.334908632640202e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 145000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.138000567125617,\n",
      "      \"grad_norm\": 0.0018115990096703172,\n",
      "      \"learning_rate\": 4.328607435412729e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 145020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.13926084627745,\n",
      "      \"grad_norm\": 0.019349267706274986,\n",
      "      \"learning_rate\": 4.3223062381852555e-06,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 145040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.140521125429283,\n",
      "      \"grad_norm\": 0.13878300786018372,\n",
      "      \"learning_rate\": 4.316005040957782e-06,\n",
      "      \"loss\": 0.0115,\n",
      "      \"step\": 145060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.141781404581115,\n",
      "      \"grad_norm\": 0.00021303354878909886,\n",
      "      \"learning_rate\": 4.3097038437303085e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 145080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.143041683732946,\n",
      "      \"grad_norm\": 0.0013715699315071106,\n",
      "      \"learning_rate\": 4.303402646502836e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 145100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.144301962884779,\n",
      "      \"grad_norm\": 0.0021333799231797457,\n",
      "      \"learning_rate\": 4.297101449275362e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 145120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.145562242036611,\n",
      "      \"grad_norm\": 0.00039635651046410203,\n",
      "      \"learning_rate\": 4.290800252047889e-06,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 145140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.146822521188444,\n",
      "      \"grad_norm\": 0.0005405283882282674,\n",
      "      \"learning_rate\": 4.284499054820416e-06,\n",
      "      \"loss\": 0.0258,\n",
      "      \"step\": 145160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.148082800340275,\n",
      "      \"grad_norm\": 0.023997459560632706,\n",
      "      \"learning_rate\": 4.278197857592943e-06,\n",
      "      \"loss\": 0.0198,\n",
      "      \"step\": 145180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.149343079492107,\n",
      "      \"grad_norm\": 0.00208526523783803,\n",
      "      \"learning_rate\": 4.271896660365469e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 145200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.15060335864394,\n",
      "      \"grad_norm\": 0.03301544860005379,\n",
      "      \"learning_rate\": 4.2655954631379965e-06,\n",
      "      \"loss\": 0.0344,\n",
      "      \"step\": 145220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.151863637795772,\n",
      "      \"grad_norm\": 0.0008190188673324883,\n",
      "      \"learning_rate\": 4.259294265910523e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 145240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.153123916947603,\n",
      "      \"grad_norm\": 0.09152476489543915,\n",
      "      \"learning_rate\": 4.2529930686830495e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 145260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.154384196099436,\n",
      "      \"grad_norm\": 0.0013776863925158978,\n",
      "      \"learning_rate\": 4.246691871455577e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 145280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.155644475251268,\n",
      "      \"grad_norm\": 0.08424504101276398,\n",
      "      \"learning_rate\": 4.240390674228103e-06,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 145300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.1569047544031,\n",
      "      \"grad_norm\": 0.017952531576156616,\n",
      "      \"learning_rate\": 4.23408947700063e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 145320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.158165033554932,\n",
      "      \"grad_norm\": 0.0023550442419946194,\n",
      "      \"learning_rate\": 4.227788279773157e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 145340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.159425312706764,\n",
      "      \"grad_norm\": 0.0003330458712298423,\n",
      "      \"learning_rate\": 4.221487082545684e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 145360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.160685591858597,\n",
      "      \"grad_norm\": 0.000813731923699379,\n",
      "      \"learning_rate\": 4.21518588531821e-06,\n",
      "      \"loss\": 0.0279,\n",
      "      \"step\": 145380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.16194587101043,\n",
      "      \"grad_norm\": 0.004607961978763342,\n",
      "      \"learning_rate\": 4.2088846880907375e-06,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 145400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.16320615016226,\n",
      "      \"grad_norm\": 0.0003273116599302739,\n",
      "      \"learning_rate\": 4.202583490863264e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 145420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.164466429314093,\n",
      "      \"grad_norm\": 0.0022346710320562124,\n",
      "      \"learning_rate\": 4.1962822936357905e-06,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 145440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.165726708465925,\n",
      "      \"grad_norm\": 0.00046256795758381486,\n",
      "      \"learning_rate\": 4.189981096408318e-06,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 145460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.166986987617758,\n",
      "      \"grad_norm\": 0.0051185074262320995,\n",
      "      \"learning_rate\": 4.183679899180844e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 145480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.168247266769589,\n",
      "      \"grad_norm\": 0.000973350543063134,\n",
      "      \"learning_rate\": 4.177378701953372e-06,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 145500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.169507545921421,\n",
      "      \"grad_norm\": 0.03316449373960495,\n",
      "      \"learning_rate\": 4.171077504725898e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 145520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.170767825073254,\n",
      "      \"grad_norm\": 0.0001193308926303871,\n",
      "      \"learning_rate\": 4.164776307498425e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 145540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.172028104225086,\n",
      "      \"grad_norm\": 0.00024229787231888622,\n",
      "      \"learning_rate\": 4.158475110270952e-06,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 145560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.173288383376917,\n",
      "      \"grad_norm\": 0.001075849635526538,\n",
      "      \"learning_rate\": 4.1521739130434786e-06,\n",
      "      \"loss\": 0.0055,\n",
      "      \"step\": 145580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.17454866252875,\n",
      "      \"grad_norm\": 0.0014032818144187331,\n",
      "      \"learning_rate\": 4.145872715816005e-06,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 145600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.175808941680582,\n",
      "      \"grad_norm\": 10.3612699508667,\n",
      "      \"learning_rate\": 4.139571518588532e-06,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 145620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.177069220832415,\n",
      "      \"grad_norm\": 0.002892945194616914,\n",
      "      \"learning_rate\": 4.133270321361059e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 145640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.178329499984246,\n",
      "      \"grad_norm\": 0.20238859951496124,\n",
      "      \"learning_rate\": 4.126969124133585e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 145660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.179589779136078,\n",
      "      \"grad_norm\": 0.010823807679116726,\n",
      "      \"learning_rate\": 4.120667926906113e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 145680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.180850058287911,\n",
      "      \"grad_norm\": 0.0009338927920907736,\n",
      "      \"learning_rate\": 4.114366729678639e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 145700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.182110337439743,\n",
      "      \"grad_norm\": 0.009229188784956932,\n",
      "      \"learning_rate\": 4.108065532451166e-06,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 145720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.183370616591574,\n",
      "      \"grad_norm\": 0.007709229364991188,\n",
      "      \"learning_rate\": 4.101764335223693e-06,\n",
      "      \"loss\": 0.0187,\n",
      "      \"step\": 145740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.184630895743407,\n",
      "      \"grad_norm\": 0.014913946390151978,\n",
      "      \"learning_rate\": 4.09546313799622e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 145760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.18589117489524,\n",
      "      \"grad_norm\": 0.00019146087288390845,\n",
      "      \"learning_rate\": 4.089161940768746e-06,\n",
      "      \"loss\": 0.0083,\n",
      "      \"step\": 145780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.187151454047072,\n",
      "      \"grad_norm\": 0.000308970978949219,\n",
      "      \"learning_rate\": 4.0828607435412734e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 145800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.188411733198903,\n",
      "      \"grad_norm\": 0.03015710972249508,\n",
      "      \"learning_rate\": 4.0765595463138e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 145820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.189672012350735,\n",
      "      \"grad_norm\": 0.00042469354229979217,\n",
      "      \"learning_rate\": 4.070258349086327e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 145840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.190932291502568,\n",
      "      \"grad_norm\": 0.00040822854498401284,\n",
      "      \"learning_rate\": 4.063957151858854e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 145860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.1921925706544,\n",
      "      \"grad_norm\": 0.0004109421861357987,\n",
      "      \"learning_rate\": 4.05765595463138e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 145880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.193452849806231,\n",
      "      \"grad_norm\": 0.012671969830989838,\n",
      "      \"learning_rate\": 4.051354757403908e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 145900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.194713128958064,\n",
      "      \"grad_norm\": 0.00019883147615473717,\n",
      "      \"learning_rate\": 4.045053560176434e-06,\n",
      "      \"loss\": 0.0122,\n",
      "      \"step\": 145920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.195973408109896,\n",
      "      \"grad_norm\": 0.0006730844033882022,\n",
      "      \"learning_rate\": 4.038752362948961e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 145940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.197233687261729,\n",
      "      \"grad_norm\": 0.0002590286312624812,\n",
      "      \"learning_rate\": 4.032451165721487e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 145960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.19849396641356,\n",
      "      \"grad_norm\": 0.00043720557005144656,\n",
      "      \"learning_rate\": 4.026149968494014e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 145980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.199754245565392,\n",
      "      \"grad_norm\": 0.008655569516122341,\n",
      "      \"learning_rate\": 4.019848771266541e-06,\n",
      "      \"loss\": 0.005,\n",
      "      \"step\": 146000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.201014524717225,\n",
      "      \"grad_norm\": 0.0025612821336835623,\n",
      "      \"learning_rate\": 4.0135475740390675e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 146020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.202274803869058,\n",
      "      \"grad_norm\": 0.00029484392143785954,\n",
      "      \"learning_rate\": 4.007246376811594e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 146040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.203535083020888,\n",
      "      \"grad_norm\": 0.00022301849094219506,\n",
      "      \"learning_rate\": 4.0009451795841205e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 146060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.204795362172721,\n",
      "      \"grad_norm\": 0.00015826091112103313,\n",
      "      \"learning_rate\": 3.994643982356648e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 146080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.206055641324554,\n",
      "      \"grad_norm\": 0.0008592449594289064,\n",
      "      \"learning_rate\": 3.988342785129174e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 146100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.207315920476386,\n",
      "      \"grad_norm\": 0.06345081329345703,\n",
      "      \"learning_rate\": 3.982041587901701e-06,\n",
      "      \"loss\": 0.0184,\n",
      "      \"step\": 146120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.208576199628217,\n",
      "      \"grad_norm\": 0.018625272437930107,\n",
      "      \"learning_rate\": 3.975740390674228e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 146140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.20983647878005,\n",
      "      \"grad_norm\": 0.00047041571815498173,\n",
      "      \"learning_rate\": 3.969439193446755e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 146160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.211096757931882,\n",
      "      \"grad_norm\": 0.008510679006576538,\n",
      "      \"learning_rate\": 3.963137996219282e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 146180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.212357037083715,\n",
      "      \"grad_norm\": 0.006864272989332676,\n",
      "      \"learning_rate\": 3.9568367989918085e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 146200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.213617316235545,\n",
      "      \"grad_norm\": 0.00032908227876760066,\n",
      "      \"learning_rate\": 3.950535601764335e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 146220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.214877595387378,\n",
      "      \"grad_norm\": 0.005863700527697802,\n",
      "      \"learning_rate\": 3.944234404536862e-06,\n",
      "      \"loss\": 0.0136,\n",
      "      \"step\": 146240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.21613787453921,\n",
      "      \"grad_norm\": 0.00016986610717140138,\n",
      "      \"learning_rate\": 3.937933207309389e-06,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 146260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.217398153691043,\n",
      "      \"grad_norm\": 0.00024891417706385255,\n",
      "      \"learning_rate\": 3.931632010081915e-06,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 146280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.218658432842874,\n",
      "      \"grad_norm\": 0.00016549948486499488,\n",
      "      \"learning_rate\": 3.925330812854443e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 146300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.219918711994707,\n",
      "      \"grad_norm\": 0.004068875685334206,\n",
      "      \"learning_rate\": 3.919029615626969e-06,\n",
      "      \"loss\": 0.0142,\n",
      "      \"step\": 146320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.22117899114654,\n",
      "      \"grad_norm\": 0.00018683192320168018,\n",
      "      \"learning_rate\": 3.912728418399496e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 146340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.222439270298372,\n",
      "      \"grad_norm\": 0.0006396091775968671,\n",
      "      \"learning_rate\": 3.906427221172023e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 146360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.223699549450203,\n",
      "      \"grad_norm\": 0.0002042973501374945,\n",
      "      \"learning_rate\": 3.9001260239445495e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 146380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.224959828602035,\n",
      "      \"grad_norm\": 0.0012817700626328588,\n",
      "      \"learning_rate\": 3.893824826717076e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 146400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.226220107753868,\n",
      "      \"grad_norm\": 0.01010492630302906,\n",
      "      \"learning_rate\": 3.887523629489603e-06,\n",
      "      \"loss\": 0.0196,\n",
      "      \"step\": 146420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.2274803869057,\n",
      "      \"grad_norm\": 0.001034354092553258,\n",
      "      \"learning_rate\": 3.88122243226213e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 146440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.228740666057531,\n",
      "      \"grad_norm\": 0.003048468381166458,\n",
      "      \"learning_rate\": 3.874921235034656e-06,\n",
      "      \"loss\": 0.0026,\n",
      "      \"step\": 146460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.230000945209364,\n",
      "      \"grad_norm\": 0.0001579096569912508,\n",
      "      \"learning_rate\": 3.868620037807184e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 146480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.231261224361196,\n",
      "      \"grad_norm\": 0.0025594497565180063,\n",
      "      \"learning_rate\": 3.86231884057971e-06,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 146500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.232521503513029,\n",
      "      \"grad_norm\": 0.0017043465049937367,\n",
      "      \"learning_rate\": 3.8560176433522375e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 146520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.23378178266486,\n",
      "      \"grad_norm\": 0.00012599947513081133,\n",
      "      \"learning_rate\": 3.849716446124764e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 146540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.235042061816692,\n",
      "      \"grad_norm\": 0.00027094222605228424,\n",
      "      \"learning_rate\": 3.8434152488972905e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 146560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.236302340968525,\n",
      "      \"grad_norm\": 0.0009355377987958491,\n",
      "      \"learning_rate\": 3.837114051669818e-06,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 146580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.237562620120357,\n",
      "      \"grad_norm\": 0.0031690075993537903,\n",
      "      \"learning_rate\": 3.830812854442344e-06,\n",
      "      \"loss\": 0.0221,\n",
      "      \"step\": 146600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.238822899272188,\n",
      "      \"grad_norm\": 0.0017812331207096577,\n",
      "      \"learning_rate\": 3.824511657214871e-06,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 146620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.24008317842402,\n",
      "      \"grad_norm\": 0.0017237773863598704,\n",
      "      \"learning_rate\": 3.818210459987398e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 146640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.241343457575853,\n",
      "      \"grad_norm\": 0.0011288290843367577,\n",
      "      \"learning_rate\": 3.8119092627599247e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 146660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.242603736727686,\n",
      "      \"grad_norm\": 0.00048264735960401595,\n",
      "      \"learning_rate\": 3.8056080655324512e-06,\n",
      "      \"loss\": 0.0286,\n",
      "      \"step\": 146680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.243864015879517,\n",
      "      \"grad_norm\": 0.016684582456946373,\n",
      "      \"learning_rate\": 3.799306868304978e-06,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 146700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.24512429503135,\n",
      "      \"grad_norm\": 0.0004992267931811512,\n",
      "      \"learning_rate\": 3.7930056710775046e-06,\n",
      "      \"loss\": 0.0076,\n",
      "      \"step\": 146720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.246384574183182,\n",
      "      \"grad_norm\": 0.01838517002761364,\n",
      "      \"learning_rate\": 3.7867044738500316e-06,\n",
      "      \"loss\": 0.0212,\n",
      "      \"step\": 146740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.247644853335014,\n",
      "      \"grad_norm\": 0.053223270922899246,\n",
      "      \"learning_rate\": 3.7804032766225585e-06,\n",
      "      \"loss\": 0.0166,\n",
      "      \"step\": 146760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.248905132486845,\n",
      "      \"grad_norm\": 0.00135571020655334,\n",
      "      \"learning_rate\": 3.774102079395085e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 146780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.250165411638678,\n",
      "      \"grad_norm\": 0.0006524328491650522,\n",
      "      \"learning_rate\": 3.7678008821676123e-06,\n",
      "      \"loss\": 0.0159,\n",
      "      \"step\": 146800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.25142569079051,\n",
      "      \"grad_norm\": 0.016452614217996597,\n",
      "      \"learning_rate\": 3.761499684940139e-06,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 146820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.252685969942343,\n",
      "      \"grad_norm\": 2.1924216747283936,\n",
      "      \"learning_rate\": 3.7551984877126653e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 146840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.253946249094174,\n",
      "      \"grad_norm\": 0.001036411733366549,\n",
      "      \"learning_rate\": 3.7488972904851927e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 146860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.255206528246006,\n",
      "      \"grad_norm\": 0.0032603961881250143,\n",
      "      \"learning_rate\": 3.742596093257719e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 146880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.256466807397839,\n",
      "      \"grad_norm\": 0.000143254961585626,\n",
      "      \"learning_rate\": 3.7362948960302457e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 146900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.257727086549671,\n",
      "      \"grad_norm\": 0.01658603921532631,\n",
      "      \"learning_rate\": 3.729993698802773e-06,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 146920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.258987365701502,\n",
      "      \"grad_norm\": 0.16850629448890686,\n",
      "      \"learning_rate\": 3.7236925015752995e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 146940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.260247644853335,\n",
      "      \"grad_norm\": 0.20270974934101105,\n",
      "      \"learning_rate\": 3.717391304347826e-06,\n",
      "      \"loss\": 0.0161,\n",
      "      \"step\": 146960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.261507924005167,\n",
      "      \"grad_norm\": 0.0006676739430986345,\n",
      "      \"learning_rate\": 3.7110901071203534e-06,\n",
      "      \"loss\": 0.0178,\n",
      "      \"step\": 146980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.262768203157,\n",
      "      \"grad_norm\": 0.0005340123898349702,\n",
      "      \"learning_rate\": 3.70478890989288e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 147000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.26402848230883,\n",
      "      \"grad_norm\": 0.00018621538765728474,\n",
      "      \"learning_rate\": 3.6984877126654064e-06,\n",
      "      \"loss\": 0.0347,\n",
      "      \"step\": 147020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.265288761460663,\n",
      "      \"grad_norm\": 0.00024251251306850463,\n",
      "      \"learning_rate\": 3.6921865154379337e-06,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 147040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.266549040612496,\n",
      "      \"grad_norm\": 0.0011971737258136272,\n",
      "      \"learning_rate\": 3.68588531821046e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 147060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.267809319764329,\n",
      "      \"grad_norm\": 0.0008279532776214182,\n",
      "      \"learning_rate\": 3.6795841209829867e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 147080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.26906959891616,\n",
      "      \"grad_norm\": 0.003964997362345457,\n",
      "      \"learning_rate\": 3.673282923755514e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 147100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.270329878067992,\n",
      "      \"grad_norm\": 0.0009288115543313324,\n",
      "      \"learning_rate\": 3.6669817265280405e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 147120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.271590157219824,\n",
      "      \"grad_norm\": 0.005369808059185743,\n",
      "      \"learning_rate\": 3.6606805293005675e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 147140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.272850436371657,\n",
      "      \"grad_norm\": 0.0001318752474617213,\n",
      "      \"learning_rate\": 3.654379332073094e-06,\n",
      "      \"loss\": 0.0353,\n",
      "      \"step\": 147160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.274110715523488,\n",
      "      \"grad_norm\": 0.0020398092456161976,\n",
      "      \"learning_rate\": 3.648078134845621e-06,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 147180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.27537099467532,\n",
      "      \"grad_norm\": 0.2798119783401489,\n",
      "      \"learning_rate\": 3.641776937618148e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 147200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.276631273827153,\n",
      "      \"grad_norm\": 0.00047405072837136686,\n",
      "      \"learning_rate\": 3.6354757403906743e-06,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 147220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.277891552978986,\n",
      "      \"grad_norm\": 0.0005244376952759922,\n",
      "      \"learning_rate\": 3.629174543163201e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 147240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.279151832130816,\n",
      "      \"grad_norm\": 0.001404207549057901,\n",
      "      \"learning_rate\": 3.622873345935728e-06,\n",
      "      \"loss\": 0.0364,\n",
      "      \"step\": 147260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.280412111282649,\n",
      "      \"grad_norm\": 0.003602708224207163,\n",
      "      \"learning_rate\": 3.6165721487082546e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 147280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.281672390434482,\n",
      "      \"grad_norm\": 0.00029163717408664525,\n",
      "      \"learning_rate\": 3.610270951480781e-06,\n",
      "      \"loss\": 0.0157,\n",
      "      \"step\": 147300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.282932669586314,\n",
      "      \"grad_norm\": 0.002575516700744629,\n",
      "      \"learning_rate\": 3.6039697542533085e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 147320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.284192948738145,\n",
      "      \"grad_norm\": 0.0019080258207395673,\n",
      "      \"learning_rate\": 3.597668557025835e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 147340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.285453227889978,\n",
      "      \"grad_norm\": 0.02705039642751217,\n",
      "      \"learning_rate\": 3.5913673597983615e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 147360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.28671350704181,\n",
      "      \"grad_norm\": 0.054065022617578506,\n",
      "      \"learning_rate\": 3.585066162570889e-06,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 147380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.287973786193643,\n",
      "      \"grad_norm\": 0.028780899941921234,\n",
      "      \"learning_rate\": 3.5787649653434153e-06,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 147400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.289234065345473,\n",
      "      \"grad_norm\": 0.00010771500819828361,\n",
      "      \"learning_rate\": 3.572463768115942e-06,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 147420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.290494344497306,\n",
      "      \"grad_norm\": 0.002262526424601674,\n",
      "      \"learning_rate\": 3.566162570888469e-06,\n",
      "      \"loss\": 0.0357,\n",
      "      \"step\": 147440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.291754623649139,\n",
      "      \"grad_norm\": 0.00018776279466692358,\n",
      "      \"learning_rate\": 3.5598613736609957e-06,\n",
      "      \"loss\": 0.027,\n",
      "      \"step\": 147460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.293014902800971,\n",
      "      \"grad_norm\": 0.0003530195099301636,\n",
      "      \"learning_rate\": 3.553560176433523e-06,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 147480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.294275181952802,\n",
      "      \"grad_norm\": 0.0038938354700803757,\n",
      "      \"learning_rate\": 3.5472589792060495e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 147500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.295535461104635,\n",
      "      \"grad_norm\": 0.0002908225287683308,\n",
      "      \"learning_rate\": 3.540957781978576e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 147520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.296795740256467,\n",
      "      \"grad_norm\": 12.497896194458008,\n",
      "      \"learning_rate\": 3.5346565847511034e-06,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 147540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.2980560194083,\n",
      "      \"grad_norm\": 0.000152257111039944,\n",
      "      \"learning_rate\": 3.52835538752363e-06,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 147560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.29931629856013,\n",
      "      \"grad_norm\": 0.002277470426633954,\n",
      "      \"learning_rate\": 3.5220541902961564e-06,\n",
      "      \"loss\": 0.016,\n",
      "      \"step\": 147580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.300576577711963,\n",
      "      \"grad_norm\": 0.0006128954119049013,\n",
      "      \"learning_rate\": 3.5157529930686833e-06,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 147600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.301836856863796,\n",
      "      \"grad_norm\": 0.0001845079386839643,\n",
      "      \"learning_rate\": 3.50945179584121e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 147620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.303097136015628,\n",
      "      \"grad_norm\": 0.8966405391693115,\n",
      "      \"learning_rate\": 3.5031505986137367e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 147640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.304357415167459,\n",
      "      \"grad_norm\": 0.0004975830670446157,\n",
      "      \"learning_rate\": 3.4968494013862636e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 147660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.305617694319292,\n",
      "      \"grad_norm\": 0.006166352424770594,\n",
      "      \"learning_rate\": 3.49054820415879e-06,\n",
      "      \"loss\": 0.0129,\n",
      "      \"step\": 147680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.306877973471124,\n",
      "      \"grad_norm\": 0.0952511727809906,\n",
      "      \"learning_rate\": 3.484247006931317e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 147700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.308138252622957,\n",
      "      \"grad_norm\": 6.045716762542725,\n",
      "      \"learning_rate\": 3.477945809703844e-06,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 147720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.309398531774788,\n",
      "      \"grad_norm\": 5.083076477050781,\n",
      "      \"learning_rate\": 3.4716446124763705e-06,\n",
      "      \"loss\": 0.0217,\n",
      "      \"step\": 147740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.31065881092662,\n",
      "      \"grad_norm\": 0.002050040289759636,\n",
      "      \"learning_rate\": 3.465343415248897e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 147760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.311919090078453,\n",
      "      \"grad_norm\": 0.014240933582186699,\n",
      "      \"learning_rate\": 3.4590422180214243e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 147780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.313179369230285,\n",
      "      \"grad_norm\": 0.0002645348140504211,\n",
      "      \"learning_rate\": 3.452741020793951e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 147800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.314439648382116,\n",
      "      \"grad_norm\": 0.003563872305676341,\n",
      "      \"learning_rate\": 3.446439823566478e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 147820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.315699927533949,\n",
      "      \"grad_norm\": 0.10688121616840363,\n",
      "      \"learning_rate\": 3.4401386263390046e-06,\n",
      "      \"loss\": 0.0023,\n",
      "      \"step\": 147840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.316960206685781,\n",
      "      \"grad_norm\": 0.027896465733647346,\n",
      "      \"learning_rate\": 3.433837429111531e-06,\n",
      "      \"loss\": 0.0213,\n",
      "      \"step\": 147860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.318220485837614,\n",
      "      \"grad_norm\": 0.00022179244842845947,\n",
      "      \"learning_rate\": 3.4275362318840585e-06,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 147880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.319480764989445,\n",
      "      \"grad_norm\": 0.0008088352042250335,\n",
      "      \"learning_rate\": 3.421235034656585e-06,\n",
      "      \"loss\": 0.0029,\n",
      "      \"step\": 147900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.320741044141277,\n",
      "      \"grad_norm\": 0.00011928572348551825,\n",
      "      \"learning_rate\": 3.4149338374291115e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 147920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.32200132329311,\n",
      "      \"grad_norm\": 0.0006549322279170156,\n",
      "      \"learning_rate\": 3.408632640201639e-06,\n",
      "      \"loss\": 0.0122,\n",
      "      \"step\": 147940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.323261602444942,\n",
      "      \"grad_norm\": 0.0026211468502879143,\n",
      "      \"learning_rate\": 3.4023314429741653e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 147960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.324521881596773,\n",
      "      \"grad_norm\": 0.006064413581043482,\n",
      "      \"learning_rate\": 3.396030245746692e-06,\n",
      "      \"loss\": 0.0057,\n",
      "      \"step\": 147980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.325782160748606,\n",
      "      \"grad_norm\": 0.10146597772836685,\n",
      "      \"learning_rate\": 3.389729048519219e-06,\n",
      "      \"loss\": 0.0115,\n",
      "      \"step\": 148000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.327042439900438,\n",
      "      \"grad_norm\": 0.021671274676918983,\n",
      "      \"learning_rate\": 3.3834278512917457e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 148020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.328302719052271,\n",
      "      \"grad_norm\": 0.001474084798246622,\n",
      "      \"learning_rate\": 3.377126654064272e-06,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 148040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.329562998204102,\n",
      "      \"grad_norm\": 0.0018298993818461895,\n",
      "      \"learning_rate\": 3.3708254568367995e-06,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 148060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.330823277355934,\n",
      "      \"grad_norm\": 0.016117073595523834,\n",
      "      \"learning_rate\": 3.364524259609326e-06,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 148080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.332083556507767,\n",
      "      \"grad_norm\": 0.0011381497606635094,\n",
      "      \"learning_rate\": 3.3582230623818525e-06,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 148100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.333343835659598,\n",
      "      \"grad_norm\": 0.002583495806902647,\n",
      "      \"learning_rate\": 3.352236925015753e-06,\n",
      "      \"loss\": 0.0032,\n",
      "      \"step\": 148120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.33460411481143,\n",
      "      \"grad_norm\": 0.0009165699011646211,\n",
      "      \"learning_rate\": 3.3459357277882795e-06,\n",
      "      \"loss\": 0.0308,\n",
      "      \"step\": 148140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.335864393963263,\n",
      "      \"grad_norm\": 0.9565048813819885,\n",
      "      \"learning_rate\": 3.339634530560807e-06,\n",
      "      \"loss\": 0.0294,\n",
      "      \"step\": 148160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.337124673115095,\n",
      "      \"grad_norm\": 0.003901909803971648,\n",
      "      \"learning_rate\": 3.3333333333333333e-06,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 148180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.338384952266928,\n",
      "      \"grad_norm\": 0.0008779754280112684,\n",
      "      \"learning_rate\": 3.32703213610586e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 148200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.339645231418759,\n",
      "      \"grad_norm\": 0.0036286693066358566,\n",
      "      \"learning_rate\": 3.320730938878387e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 148220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.340905510570591,\n",
      "      \"grad_norm\": 0.044197265058755875,\n",
      "      \"learning_rate\": 3.3144297416509137e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 148240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.342165789722424,\n",
      "      \"grad_norm\": 6.928829669952393,\n",
      "      \"learning_rate\": 3.30812854442344e-06,\n",
      "      \"loss\": 0.0186,\n",
      "      \"step\": 148260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.343426068874255,\n",
      "      \"grad_norm\": 0.05219082534313202,\n",
      "      \"learning_rate\": 3.3018273471959675e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 148280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.344686348026087,\n",
      "      \"grad_norm\": 0.0025591705925762653,\n",
      "      \"learning_rate\": 3.295526149968494e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 148300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.34594662717792,\n",
      "      \"grad_norm\": 0.00018420761625748128,\n",
      "      \"learning_rate\": 3.2892249527410214e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 148320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.347206906329752,\n",
      "      \"grad_norm\": 0.03195479139685631,\n",
      "      \"learning_rate\": 3.282923755513548e-06,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 148340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.348467185481585,\n",
      "      \"grad_norm\": 0.004133396781980991,\n",
      "      \"learning_rate\": 3.2766225582860743e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 148360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.349727464633416,\n",
      "      \"grad_norm\": 0.0002452691551297903,\n",
      "      \"learning_rate\": 3.2703213610586017e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 148380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.350987743785248,\n",
      "      \"grad_norm\": 0.00040019061998464167,\n",
      "      \"learning_rate\": 3.264020163831128e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 148400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.352248022937081,\n",
      "      \"grad_norm\": 0.014973949640989304,\n",
      "      \"learning_rate\": 3.2577189666036547e-06,\n",
      "      \"loss\": 0.0056,\n",
      "      \"step\": 148420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.353508302088912,\n",
      "      \"grad_norm\": 0.000373876333469525,\n",
      "      \"learning_rate\": 3.251417769376182e-06,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 148440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.354768581240744,\n",
      "      \"grad_norm\": 0.0013595538912340999,\n",
      "      \"learning_rate\": 3.2451165721487085e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 148460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.356028860392577,\n",
      "      \"grad_norm\": 0.00015223943046294153,\n",
      "      \"learning_rate\": 3.238815374921235e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 148480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.35728913954441,\n",
      "      \"grad_norm\": 0.0003753244527615607,\n",
      "      \"learning_rate\": 3.232514177693762e-06,\n",
      "      \"loss\": 0.0087,\n",
      "      \"step\": 148500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.358549418696242,\n",
      "      \"grad_norm\": 0.29667457938194275,\n",
      "      \"learning_rate\": 3.226212980466289e-06,\n",
      "      \"loss\": 0.0019,\n",
      "      \"step\": 148520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.359809697848073,\n",
      "      \"grad_norm\": 0.0008111581555567682,\n",
      "      \"learning_rate\": 3.2199117832388154e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 148540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.361069976999906,\n",
      "      \"grad_norm\": 0.0006576519226655364,\n",
      "      \"learning_rate\": 3.2136105860113423e-06,\n",
      "      \"loss\": 0.0458,\n",
      "      \"step\": 148560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.362330256151738,\n",
      "      \"grad_norm\": 0.1481061428785324,\n",
      "      \"learning_rate\": 3.207309388783869e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 148580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.363590535303569,\n",
      "      \"grad_norm\": 0.0001645168085815385,\n",
      "      \"learning_rate\": 3.2010081915563957e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 148600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.364850814455401,\n",
      "      \"grad_norm\": 0.0008170880610123277,\n",
      "      \"learning_rate\": 3.1947069943289226e-06,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 148620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.366111093607234,\n",
      "      \"grad_norm\": 0.0008309173281304538,\n",
      "      \"learning_rate\": 3.1887208569628227e-06,\n",
      "      \"loss\": 0.0295,\n",
      "      \"step\": 148640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.367371372759067,\n",
      "      \"grad_norm\": 0.00015158744645304978,\n",
      "      \"learning_rate\": 3.18241965973535e-06,\n",
      "      \"loss\": 0.0351,\n",
      "      \"step\": 148660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.368631651910897,\n",
      "      \"grad_norm\": 0.0013196825748309493,\n",
      "      \"learning_rate\": 3.1761184625078765e-06,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 148680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.36989193106273,\n",
      "      \"grad_norm\": 0.004742206074297428,\n",
      "      \"learning_rate\": 3.169817265280403e-06,\n",
      "      \"loss\": 0.0163,\n",
      "      \"step\": 148700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.371152210214563,\n",
      "      \"grad_norm\": 0.020076341927051544,\n",
      "      \"learning_rate\": 3.1635160680529304e-06,\n",
      "      \"loss\": 0.0053,\n",
      "      \"step\": 148720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.372412489366395,\n",
      "      \"grad_norm\": 0.004891891032457352,\n",
      "      \"learning_rate\": 3.157214870825457e-06,\n",
      "      \"loss\": 0.0371,\n",
      "      \"step\": 148740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.373672768518226,\n",
      "      \"grad_norm\": 0.00043412382365204394,\n",
      "      \"learning_rate\": 3.1509136735979834e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 148760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.374933047670059,\n",
      "      \"grad_norm\": 0.026655612513422966,\n",
      "      \"learning_rate\": 3.1446124763705107e-06,\n",
      "      \"loss\": 0.0383,\n",
      "      \"step\": 148780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.376193326821891,\n",
      "      \"grad_norm\": 0.0047335331328213215,\n",
      "      \"learning_rate\": 3.1383112791430372e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 148800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.377453605973724,\n",
      "      \"grad_norm\": 0.0004446202947292477,\n",
      "      \"learning_rate\": 3.1320100819155646e-06,\n",
      "      \"loss\": 0.007,\n",
      "      \"step\": 148820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.378713885125554,\n",
      "      \"grad_norm\": 0.00011615719267865643,\n",
      "      \"learning_rate\": 3.125708884688091e-06,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 148840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.379974164277387,\n",
      "      \"grad_norm\": 0.0001221096026711166,\n",
      "      \"learning_rate\": 3.119407687460618e-06,\n",
      "      \"loss\": 0.0159,\n",
      "      \"step\": 148860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.38123444342922,\n",
      "      \"grad_norm\": 0.008074806071817875,\n",
      "      \"learning_rate\": 3.1131064902331445e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 148880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.382494722581052,\n",
      "      \"grad_norm\": 0.007947870530188084,\n",
      "      \"learning_rate\": 3.1068052930056714e-06,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 148900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.383755001732883,\n",
      "      \"grad_norm\": 0.005429795943200588,\n",
      "      \"learning_rate\": 3.1005040957781983e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 148920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.385015280884716,\n",
      "      \"grad_norm\": 0.00012509923544712365,\n",
      "      \"learning_rate\": 3.094202898550725e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 148940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.386275560036548,\n",
      "      \"grad_norm\": 0.0002007841394515708,\n",
      "      \"learning_rate\": 3.0879017013232517e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 148960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.38753583918838,\n",
      "      \"grad_norm\": 0.0002330063725821674,\n",
      "      \"learning_rate\": 3.0816005040957782e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 148980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.388796118340212,\n",
      "      \"grad_norm\": 0.0002442000259179622,\n",
      "      \"learning_rate\": 3.075299306868305e-06,\n",
      "      \"loss\": 0.0029,\n",
      "      \"step\": 149000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.390056397492044,\n",
      "      \"grad_norm\": 0.0007938735652714968,\n",
      "      \"learning_rate\": 3.0689981096408317e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 149020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.391316676643877,\n",
      "      \"grad_norm\": 0.002603822620585561,\n",
      "      \"learning_rate\": 3.0626969124133586e-06,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 149040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.39257695579571,\n",
      "      \"grad_norm\": 0.0002855948405340314,\n",
      "      \"learning_rate\": 3.0563957151858855e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 149060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.39383723494754,\n",
      "      \"grad_norm\": 0.00047337112482637167,\n",
      "      \"learning_rate\": 3.050094517958412e-06,\n",
      "      \"loss\": 0.0046,\n",
      "      \"step\": 149080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.395097514099373,\n",
      "      \"grad_norm\": 0.009706757962703705,\n",
      "      \"learning_rate\": 3.043793320730939e-06,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 149100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.396357793251205,\n",
      "      \"grad_norm\": 0.006015756633132696,\n",
      "      \"learning_rate\": 3.037492123503466e-06,\n",
      "      \"loss\": 0.007,\n",
      "      \"step\": 149120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.397618072403038,\n",
      "      \"grad_norm\": 0.01220989041030407,\n",
      "      \"learning_rate\": 3.0311909262759923e-06,\n",
      "      \"loss\": 0.0129,\n",
      "      \"step\": 149140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.398878351554869,\n",
      "      \"grad_norm\": 0.00070104596670717,\n",
      "      \"learning_rate\": 3.0248897290485193e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 149160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.400138630706701,\n",
      "      \"grad_norm\": 0.015089803375303745,\n",
      "      \"learning_rate\": 3.018588531821046e-06,\n",
      "      \"loss\": 0.0218,\n",
      "      \"step\": 149180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.401398909858534,\n",
      "      \"grad_norm\": 0.0010942989028990269,\n",
      "      \"learning_rate\": 3.012287334593573e-06,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 149200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.402659189010366,\n",
      "      \"grad_norm\": 0.00012578537280205637,\n",
      "      \"learning_rate\": 3.0059861373660996e-06,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 149220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.403919468162197,\n",
      "      \"grad_norm\": 0.015224861912429333,\n",
      "      \"learning_rate\": 2.9996849401386265e-06,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 149240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.40517974731403,\n",
      "      \"grad_norm\": 0.07802480459213257,\n",
      "      \"learning_rate\": 2.9933837429111534e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 149260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.406440026465862,\n",
      "      \"grad_norm\": 0.0014147028559818864,\n",
      "      \"learning_rate\": 2.98708254568368e-06,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 149280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.407700305617695,\n",
      "      \"grad_norm\": 0.025782568380236626,\n",
      "      \"learning_rate\": 2.980781348456207e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 149300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.408960584769526,\n",
      "      \"grad_norm\": 0.001238569850102067,\n",
      "      \"learning_rate\": 2.974480151228734e-06,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 149320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.410220863921358,\n",
      "      \"grad_norm\": 0.084542416036129,\n",
      "      \"learning_rate\": 2.9681789540012603e-06,\n",
      "      \"loss\": 0.0053,\n",
      "      \"step\": 149340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.41148114307319,\n",
      "      \"grad_norm\": 0.0010070573771372437,\n",
      "      \"learning_rate\": 2.9618777567737872e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 149360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.412741422225023,\n",
      "      \"grad_norm\": 0.0008641835884191096,\n",
      "      \"learning_rate\": 2.955576559546314e-06,\n",
      "      \"loss\": 0.007,\n",
      "      \"step\": 149380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.414001701376854,\n",
      "      \"grad_norm\": 0.028655150905251503,\n",
      "      \"learning_rate\": 2.949275362318841e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 149400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.415261980528687,\n",
      "      \"grad_norm\": 0.00015880823775660247,\n",
      "      \"learning_rate\": 2.9429741650913676e-06,\n",
      "      \"loss\": 0.0132,\n",
      "      \"step\": 149420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.41652225968052,\n",
      "      \"grad_norm\": 0.0003681764646898955,\n",
      "      \"learning_rate\": 2.9366729678638945e-06,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 149440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.417782538832352,\n",
      "      \"grad_norm\": 0.0034801962319761515,\n",
      "      \"learning_rate\": 2.930371770636421e-06,\n",
      "      \"loss\": 0.0196,\n",
      "      \"step\": 149460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.419042817984183,\n",
      "      \"grad_norm\": 0.0002265985676785931,\n",
      "      \"learning_rate\": 2.924070573408948e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 149480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.420303097136015,\n",
      "      \"grad_norm\": 0.00035736695281229913,\n",
      "      \"learning_rate\": 2.9177693761814744e-06,\n",
      "      \"loss\": 0.0132,\n",
      "      \"step\": 149500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.421563376287848,\n",
      "      \"grad_norm\": 0.0001517994824098423,\n",
      "      \"learning_rate\": 2.9114681789540013e-06,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 149520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.42282365543968,\n",
      "      \"grad_norm\": 0.0001136269565904513,\n",
      "      \"learning_rate\": 2.9051669817265282e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 149540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.424083934591511,\n",
      "      \"grad_norm\": 0.08072986453771591,\n",
      "      \"learning_rate\": 2.8988657844990547e-06,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 149560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.425344213743344,\n",
      "      \"grad_norm\": 0.0020687589421868324,\n",
      "      \"learning_rate\": 2.8925645872715817e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 149580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.426604492895176,\n",
      "      \"grad_norm\": 0.0029791926499456167,\n",
      "      \"learning_rate\": 2.8862633900441086e-06,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 149600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.427864772047009,\n",
      "      \"grad_norm\": 0.00016926095122471452,\n",
      "      \"learning_rate\": 2.879962192816635e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 149620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.42912505119884,\n",
      "      \"grad_norm\": 0.0035139413084834814,\n",
      "      \"learning_rate\": 2.873660995589162e-06,\n",
      "      \"loss\": 0.0315,\n",
      "      \"step\": 149640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.430385330350672,\n",
      "      \"grad_norm\": 0.00015188533870968968,\n",
      "      \"learning_rate\": 2.867359798361689e-06,\n",
      "      \"loss\": 0.0025,\n",
      "      \"step\": 149660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.431645609502505,\n",
      "      \"grad_norm\": 0.004550582263618708,\n",
      "      \"learning_rate\": 2.861058601134216e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 149680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.432905888654338,\n",
      "      \"grad_norm\": 0.001593884313479066,\n",
      "      \"learning_rate\": 2.8547574039067423e-06,\n",
      "      \"loss\": 0.0089,\n",
      "      \"step\": 149700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.434166167806168,\n",
      "      \"grad_norm\": 0.0020681042224168777,\n",
      "      \"learning_rate\": 2.8484562066792693e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 149720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.435426446958001,\n",
      "      \"grad_norm\": 0.0005323607474565506,\n",
      "      \"learning_rate\": 2.842155009451796e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 149740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.436686726109834,\n",
      "      \"grad_norm\": 0.00637428555637598,\n",
      "      \"learning_rate\": 2.8358538122243227e-06,\n",
      "      \"loss\": 0.018,\n",
      "      \"step\": 149760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.437947005261666,\n",
      "      \"grad_norm\": 0.07918894290924072,\n",
      "      \"learning_rate\": 2.8295526149968496e-06,\n",
      "      \"loss\": 0.0116,\n",
      "      \"step\": 149780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.439207284413497,\n",
      "      \"grad_norm\": 0.00015209615230560303,\n",
      "      \"learning_rate\": 2.8232514177693765e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 149800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.44046756356533,\n",
      "      \"grad_norm\": 0.00012265279656276107,\n",
      "      \"learning_rate\": 2.816950220541903e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 149820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.441727842717162,\n",
      "      \"grad_norm\": 0.0007503307424485683,\n",
      "      \"learning_rate\": 2.81064902331443e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 149840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.442988121868995,\n",
      "      \"grad_norm\": 0.006319998297840357,\n",
      "      \"learning_rate\": 2.804347826086957e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 149860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.444248401020825,\n",
      "      \"grad_norm\": 0.002167527563869953,\n",
      "      \"learning_rate\": 2.798046628859484e-06,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 149880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.445508680172658,\n",
      "      \"grad_norm\": 0.5851114392280579,\n",
      "      \"learning_rate\": 2.7917454316320103e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 149900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.44676895932449,\n",
      "      \"grad_norm\": 0.000507313059642911,\n",
      "      \"learning_rate\": 2.785444234404537e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 149920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.448029238476323,\n",
      "      \"grad_norm\": 0.00022175397316459566,\n",
      "      \"learning_rate\": 2.7791430371770637e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 149940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.449289517628154,\n",
      "      \"grad_norm\": 0.04295961186289787,\n",
      "      \"learning_rate\": 2.77284183994959e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 149960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.450549796779987,\n",
      "      \"grad_norm\": 0.004492626525461674,\n",
      "      \"learning_rate\": 2.766540642722117e-06,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 149980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.451810075931819,\n",
      "      \"grad_norm\": 0.0348055325448513,\n",
      "      \"learning_rate\": 2.760239445494644e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 150000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.453070355083652,\n",
      "      \"grad_norm\": 0.009295432828366756,\n",
      "      \"learning_rate\": 2.753938248267171e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 150020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.454330634235482,\n",
      "      \"grad_norm\": 0.00025323760928586125,\n",
      "      \"learning_rate\": 2.7476370510396975e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 150040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.455590913387315,\n",
      "      \"grad_norm\": 0.0002861606772057712,\n",
      "      \"learning_rate\": 2.7413358538122244e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 150060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.456851192539148,\n",
      "      \"grad_norm\": 0.009792134165763855,\n",
      "      \"learning_rate\": 2.7350346565847513e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 150080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.45811147169098,\n",
      "      \"grad_norm\": 0.00013284692249726504,\n",
      "      \"learning_rate\": 2.728733459357278e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 150100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.459371750842811,\n",
      "      \"grad_norm\": 0.009554465301334858,\n",
      "      \"learning_rate\": 2.7224322621298047e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 150120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.460632029994644,\n",
      "      \"grad_norm\": 0.00047976779751479626,\n",
      "      \"learning_rate\": 2.7161310649023317e-06,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 150140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.461892309146476,\n",
      "      \"grad_norm\": 0.01444715354591608,\n",
      "      \"learning_rate\": 2.709829867674858e-06,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 150160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.463152588298309,\n",
      "      \"grad_norm\": 0.00027246991521678865,\n",
      "      \"learning_rate\": 2.703528670447385e-06,\n",
      "      \"loss\": 0.019,\n",
      "      \"step\": 150180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.46441286745014,\n",
      "      \"grad_norm\": 0.009617958217859268,\n",
      "      \"learning_rate\": 2.697227473219912e-06,\n",
      "      \"loss\": 0.0082,\n",
      "      \"step\": 150200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.465673146601972,\n",
      "      \"grad_norm\": 0.0012457073898985982,\n",
      "      \"learning_rate\": 2.690926275992439e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 150220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.466933425753805,\n",
      "      \"grad_norm\": 0.00014209348591975868,\n",
      "      \"learning_rate\": 2.6846250787649654e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 150240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.468193704905637,\n",
      "      \"grad_norm\": 0.007816970348358154,\n",
      "      \"learning_rate\": 2.6783238815374923e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 150260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.469453984057468,\n",
      "      \"grad_norm\": 0.0001639850961510092,\n",
      "      \"learning_rate\": 2.6720226843100193e-06,\n",
      "      \"loss\": 0.0076,\n",
      "      \"step\": 150280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.4707142632093,\n",
      "      \"grad_norm\": 0.028414659202098846,\n",
      "      \"learning_rate\": 2.6657214870825458e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 150300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.471974542361133,\n",
      "      \"grad_norm\": 0.004461617674678564,\n",
      "      \"learning_rate\": 2.6594202898550727e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 150320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.473234821512966,\n",
      "      \"grad_norm\": 0.0002053334901574999,\n",
      "      \"learning_rate\": 2.6531190926275996e-06,\n",
      "      \"loss\": 0.0135,\n",
      "      \"step\": 150340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.474495100664797,\n",
      "      \"grad_norm\": 0.0005643446347676218,\n",
      "      \"learning_rate\": 2.646817895400126e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 150360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.47575537981663,\n",
      "      \"grad_norm\": 0.02863677218556404,\n",
      "      \"learning_rate\": 2.640516698172653e-06,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 150380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.477015658968462,\n",
      "      \"grad_norm\": 0.00018604898650664836,\n",
      "      \"learning_rate\": 2.6342155009451795e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 150400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.478275938120294,\n",
      "      \"grad_norm\": 0.04163030534982681,\n",
      "      \"learning_rate\": 2.6279143037177064e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 150420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.479536217272125,\n",
      "      \"grad_norm\": 0.00014912788174115121,\n",
      "      \"learning_rate\": 2.621613106490233e-06,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 150440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.480796496423958,\n",
      "      \"grad_norm\": 0.00011579419515328482,\n",
      "      \"learning_rate\": 2.61531190926276e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 150460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.48205677557579,\n",
      "      \"grad_norm\": 0.00013907425454817712,\n",
      "      \"learning_rate\": 2.6090107120352868e-06,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 150480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.483317054727623,\n",
      "      \"grad_norm\": 0.0001248181943083182,\n",
      "      \"learning_rate\": 2.6027095148078133e-06,\n",
      "      \"loss\": 0.0479,\n",
      "      \"step\": 150500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.484577333879454,\n",
      "      \"grad_norm\": 0.011982209049165249,\n",
      "      \"learning_rate\": 2.59640831758034e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 150520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.485837613031286,\n",
      "      \"grad_norm\": 0.0038828901015222073,\n",
      "      \"learning_rate\": 2.590107120352867e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 150540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.487097892183119,\n",
      "      \"grad_norm\": 0.00011469878518255427,\n",
      "      \"learning_rate\": 2.583805923125394e-06,\n",
      "      \"loss\": 0.0124,\n",
      "      \"step\": 150560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.488358171334951,\n",
      "      \"grad_norm\": 0.000695477647241205,\n",
      "      \"learning_rate\": 2.5775047258979205e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 150580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.489618450486782,\n",
      "      \"grad_norm\": 0.004629869479686022,\n",
      "      \"learning_rate\": 2.5712035286704475e-06,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 150600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.490878729638615,\n",
      "      \"grad_norm\": 0.0002438880765112117,\n",
      "      \"learning_rate\": 2.5649023314429744e-06,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 150620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.492139008790447,\n",
      "      \"grad_norm\": 0.0012004048330709338,\n",
      "      \"learning_rate\": 2.558601134215501e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 150640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.49339928794228,\n",
      "      \"grad_norm\": 0.007954271510243416,\n",
      "      \"learning_rate\": 2.552299936988028e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 150660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.49465956709411,\n",
      "      \"grad_norm\": 0.00014011160237714648,\n",
      "      \"learning_rate\": 2.5459987397605547e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 150680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.495919846245943,\n",
      "      \"grad_norm\": 0.0033041937276721,\n",
      "      \"learning_rate\": 2.5396975425330817e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 150700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.497180125397776,\n",
      "      \"grad_norm\": 0.0023128369357436895,\n",
      "      \"learning_rate\": 2.533396345305608e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 150720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.498440404549608,\n",
      "      \"grad_norm\": 0.00010579481750028208,\n",
      "      \"learning_rate\": 2.527095148078135e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 150740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.49970068370144,\n",
      "      \"grad_norm\": 0.0009443031740374863,\n",
      "      \"learning_rate\": 2.520793950850662e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 150760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.500960962853272,\n",
      "      \"grad_norm\": 0.00013250864867586643,\n",
      "      \"learning_rate\": 2.5144927536231885e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 150780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.502221242005104,\n",
      "      \"grad_norm\": 0.0008763880468904972,\n",
      "      \"learning_rate\": 2.5081915563957154e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 150800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.503481521156937,\n",
      "      \"grad_norm\": 0.00261337379924953,\n",
      "      \"learning_rate\": 2.5018903591682423e-06,\n",
      "      \"loss\": 0.0125,\n",
      "      \"step\": 150820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.504741800308768,\n",
      "      \"grad_norm\": 0.0002712510176934302,\n",
      "      \"learning_rate\": 2.495589161940769e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 150840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.5060020794606,\n",
      "      \"grad_norm\": 0.00013007796951569617,\n",
      "      \"learning_rate\": 2.4892879647132958e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 150860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.507262358612433,\n",
      "      \"grad_norm\": 0.048868581652641296,\n",
      "      \"learning_rate\": 2.4829867674858223e-06,\n",
      "      \"loss\": 0.0113,\n",
      "      \"step\": 150880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.508522637764266,\n",
      "      \"grad_norm\": 0.00011892158363480121,\n",
      "      \"learning_rate\": 2.476685570258349e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 150900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.509782916916096,\n",
      "      \"grad_norm\": 0.01927991770207882,\n",
      "      \"learning_rate\": 2.4703843730308757e-06,\n",
      "      \"loss\": 0.0231,\n",
      "      \"step\": 150920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.511043196067929,\n",
      "      \"grad_norm\": 0.0002506682649254799,\n",
      "      \"learning_rate\": 2.4640831758034026e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 150940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.512303475219761,\n",
      "      \"grad_norm\": 0.012898659333586693,\n",
      "      \"learning_rate\": 2.4577819785759295e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 150960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.513563754371594,\n",
      "      \"grad_norm\": 0.07408253103494644,\n",
      "      \"learning_rate\": 2.451480781348456e-06,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 150980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.514824033523425,\n",
      "      \"grad_norm\": 0.0042374772019684315,\n",
      "      \"learning_rate\": 2.445179584120983e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 151000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.516084312675257,\n",
      "      \"grad_norm\": 11.868978500366211,\n",
      "      \"learning_rate\": 2.43887838689351e-06,\n",
      "      \"loss\": 0.0227,\n",
      "      \"step\": 151020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.51734459182709,\n",
      "      \"grad_norm\": 0.0003824680461548269,\n",
      "      \"learning_rate\": 2.4325771896660368e-06,\n",
      "      \"loss\": 0.0131,\n",
      "      \"step\": 151040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.518604870978923,\n",
      "      \"grad_norm\": 0.00029600010020658374,\n",
      "      \"learning_rate\": 2.4262759924385633e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 151060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.519865150130753,\n",
      "      \"grad_norm\": 0.00010417726298328489,\n",
      "      \"learning_rate\": 2.41997479521109e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 151080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.521125429282586,\n",
      "      \"grad_norm\": 0.0001057621484505944,\n",
      "      \"learning_rate\": 2.413673597983617e-06,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 151100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.522385708434419,\n",
      "      \"grad_norm\": 0.000865813868585974,\n",
      "      \"learning_rate\": 2.4073724007561436e-06,\n",
      "      \"loss\": 0.0271,\n",
      "      \"step\": 151120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.523645987586251,\n",
      "      \"grad_norm\": 0.00024926409241743386,\n",
      "      \"learning_rate\": 2.4010712035286705e-06,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 151140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.524906266738082,\n",
      "      \"grad_norm\": 0.0005538348923437297,\n",
      "      \"learning_rate\": 2.3947700063011975e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 151160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.526166545889915,\n",
      "      \"grad_norm\": 0.0193719994276762,\n",
      "      \"learning_rate\": 2.3884688090737244e-06,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 151180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.527426825041747,\n",
      "      \"grad_norm\": 18.201982498168945,\n",
      "      \"learning_rate\": 2.382167611846251e-06,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 151200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.52868710419358,\n",
      "      \"grad_norm\": 0.0022818013094365597,\n",
      "      \"learning_rate\": 2.375866414618778e-06,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 151220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.52994738334541,\n",
      "      \"grad_norm\": 0.0003285131824668497,\n",
      "      \"learning_rate\": 2.3695652173913047e-06,\n",
      "      \"loss\": 0.023,\n",
      "      \"step\": 151240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.531207662497243,\n",
      "      \"grad_norm\": 0.0004129915323574096,\n",
      "      \"learning_rate\": 2.3632640201638312e-06,\n",
      "      \"loss\": 0.0039,\n",
      "      \"step\": 151260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.532467941649076,\n",
      "      \"grad_norm\": 0.003136051818728447,\n",
      "      \"learning_rate\": 2.356962822936358e-06,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 151280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.533728220800908,\n",
      "      \"grad_norm\": 0.00011412300227675587,\n",
      "      \"learning_rate\": 2.350661625708885e-06,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 151300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.534988499952739,\n",
      "      \"grad_norm\": 0.0003563922946341336,\n",
      "      \"learning_rate\": 2.3443604284814116e-06,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 151320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.536248779104572,\n",
      "      \"grad_norm\": 0.004212755244225264,\n",
      "      \"learning_rate\": 2.3380592312539385e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 151340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.537509058256404,\n",
      "      \"grad_norm\": 0.010220571421086788,\n",
      "      \"learning_rate\": 2.331758034026465e-06,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 151360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.538769337408237,\n",
      "      \"grad_norm\": 0.0004357362340670079,\n",
      "      \"learning_rate\": 2.325456836798992e-06,\n",
      "      \"loss\": 0.0168,\n",
      "      \"step\": 151380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.540029616560068,\n",
      "      \"grad_norm\": 0.019986746832728386,\n",
      "      \"learning_rate\": 2.3191556395715184e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 151400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.5412898957119,\n",
      "      \"grad_norm\": 0.00021476652182172984,\n",
      "      \"learning_rate\": 2.3128544423440453e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 151420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.542550174863733,\n",
      "      \"grad_norm\": 0.001963619375601411,\n",
      "      \"learning_rate\": 2.3065532451165723e-06,\n",
      "      \"loss\": 0.0233,\n",
      "      \"step\": 151440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.543810454015565,\n",
      "      \"grad_norm\": 0.00040335868834517896,\n",
      "      \"learning_rate\": 2.3002520478890988e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 151460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.545070733167396,\n",
      "      \"grad_norm\": 0.02604101598262787,\n",
      "      \"learning_rate\": 2.2939508506616257e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 151480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.546331012319229,\n",
      "      \"grad_norm\": 0.015176431275904179,\n",
      "      \"learning_rate\": 2.2876496534341526e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 151500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.547591291471061,\n",
      "      \"grad_norm\": 0.005536885932087898,\n",
      "      \"learning_rate\": 2.2813484562066795e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 151520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.548851570622894,\n",
      "      \"grad_norm\": 0.00047078190254978836,\n",
      "      \"learning_rate\": 2.275047258979206e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 151540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.550111849774725,\n",
      "      \"grad_norm\": 0.0017369823763146996,\n",
      "      \"learning_rate\": 2.268746061751733e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 151560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.551372128926557,\n",
      "      \"grad_norm\": 0.0004443409270606935,\n",
      "      \"learning_rate\": 2.26244486452426e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 151580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.55263240807839,\n",
      "      \"grad_norm\": 0.01573377661406994,\n",
      "      \"learning_rate\": 2.2561436672967864e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 151600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.55389268723022,\n",
      "      \"grad_norm\": 0.004883304238319397,\n",
      "      \"learning_rate\": 2.2498424700693133e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 151620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.555152966382053,\n",
      "      \"grad_norm\": 0.007511892821639776,\n",
      "      \"learning_rate\": 2.24354127284184e-06,\n",
      "      \"loss\": 0.0344,\n",
      "      \"step\": 151640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.556413245533886,\n",
      "      \"grad_norm\": 0.00011482057743705809,\n",
      "      \"learning_rate\": 2.2372400756143667e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 151660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.557673524685718,\n",
      "      \"grad_norm\": 0.005878590978682041,\n",
      "      \"learning_rate\": 2.2309388783868936e-06,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 151680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.55893380383755,\n",
      "      \"grad_norm\": 0.0002771650906652212,\n",
      "      \"learning_rate\": 2.2246376811594205e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 151700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.560194082989382,\n",
      "      \"grad_norm\": 0.0034444998018443584,\n",
      "      \"learning_rate\": 2.2183364839319475e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 151720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.561454362141214,\n",
      "      \"grad_norm\": 0.01038062758743763,\n",
      "      \"learning_rate\": 2.212035286704474e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 151740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.562714641293047,\n",
      "      \"grad_norm\": 0.00260835699737072,\n",
      "      \"learning_rate\": 2.205734089477001e-06,\n",
      "      \"loss\": 0.0124,\n",
      "      \"step\": 151760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.563974920444878,\n",
      "      \"grad_norm\": 0.0023504234850406647,\n",
      "      \"learning_rate\": 2.199432892249528e-06,\n",
      "      \"loss\": 0.0049,\n",
      "      \"step\": 151780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.56523519959671,\n",
      "      \"grad_norm\": 8.214435577392578,\n",
      "      \"learning_rate\": 2.1931316950220543e-06,\n",
      "      \"loss\": 0.0218,\n",
      "      \"step\": 151800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.566495478748543,\n",
      "      \"grad_norm\": 0.00037170472205616534,\n",
      "      \"learning_rate\": 2.1868304977945812e-06,\n",
      "      \"loss\": 0.0352,\n",
      "      \"step\": 151820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.567755757900375,\n",
      "      \"grad_norm\": 0.0010015126317739487,\n",
      "      \"learning_rate\": 2.1805293005671077e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 151840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.569016037052208,\n",
      "      \"grad_norm\": 0.000149496816447936,\n",
      "      \"learning_rate\": 2.1742281033396347e-06,\n",
      "      \"loss\": 0.0163,\n",
      "      \"step\": 151860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.570276316204039,\n",
      "      \"grad_norm\": 0.00047088495921343565,\n",
      "      \"learning_rate\": 2.167926906112161e-06,\n",
      "      \"loss\": 0.0054,\n",
      "      \"step\": 151880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.571536595355871,\n",
      "      \"grad_norm\": 0.004641932900995016,\n",
      "      \"learning_rate\": 2.161625708884688e-06,\n",
      "      \"loss\": 0.0272,\n",
      "      \"step\": 151900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.572796874507704,\n",
      "      \"grad_norm\": 0.00013363400648813695,\n",
      "      \"learning_rate\": 2.155324511657215e-06,\n",
      "      \"loss\": 0.0156,\n",
      "      \"step\": 151920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.574057153659535,\n",
      "      \"grad_norm\": 0.00011651494423858821,\n",
      "      \"learning_rate\": 2.1490233144297415e-06,\n",
      "      \"loss\": 0.0131,\n",
      "      \"step\": 151940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.575317432811367,\n",
      "      \"grad_norm\": 0.06053625047206879,\n",
      "      \"learning_rate\": 2.1427221172022684e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 151960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.5765777119632,\n",
      "      \"grad_norm\": 0.0002772664884105325,\n",
      "      \"learning_rate\": 2.1364209199747953e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 151980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.577837991115032,\n",
      "      \"grad_norm\": 0.00012661410437431186,\n",
      "      \"learning_rate\": 2.130119722747322e-06,\n",
      "      \"loss\": 0.0018,\n",
      "      \"step\": 152000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.579098270266865,\n",
      "      \"grad_norm\": 0.028322240337729454,\n",
      "      \"learning_rate\": 2.1238185255198488e-06,\n",
      "      \"loss\": 0.0162,\n",
      "      \"step\": 152020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.580358549418696,\n",
      "      \"grad_norm\": 0.020709460601210594,\n",
      "      \"learning_rate\": 2.1175173282923757e-06,\n",
      "      \"loss\": 0.0067,\n",
      "      \"step\": 152040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.581618828570528,\n",
      "      \"grad_norm\": 0.00023350775882136077,\n",
      "      \"learning_rate\": 2.1112161310649026e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 152060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.582879107722361,\n",
      "      \"grad_norm\": 0.00019085426174569875,\n",
      "      \"learning_rate\": 2.104914933837429e-06,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 152080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.584139386874192,\n",
      "      \"grad_norm\": 0.0635935440659523,\n",
      "      \"learning_rate\": 2.098613736609956e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 152100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.585399666026024,\n",
      "      \"grad_norm\": 0.00023961751139722764,\n",
      "      \"learning_rate\": 2.092312539382483e-06,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 152120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.586659945177857,\n",
      "      \"grad_norm\": 0.22491423785686493,\n",
      "      \"learning_rate\": 2.0860113421550094e-06,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 152140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.58792022432969,\n",
      "      \"grad_norm\": 0.09842614084482193,\n",
      "      \"learning_rate\": 2.0797101449275364e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 152160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.589180503481522,\n",
      "      \"grad_norm\": 4.966667652130127,\n",
      "      \"learning_rate\": 2.0734089477000633e-06,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 152180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.590440782633353,\n",
      "      \"grad_norm\": 0.054959941655397415,\n",
      "      \"learning_rate\": 2.06710775047259e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 152200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.591701061785185,\n",
      "      \"grad_norm\": 0.0035925894044339657,\n",
      "      \"learning_rate\": 2.0608065532451167e-06,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 152220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.592961340937018,\n",
      "      \"grad_norm\": 0.00019842834444716573,\n",
      "      \"learning_rate\": 2.0545053560176436e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 152240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.594221620088849,\n",
      "      \"grad_norm\": 0.00010427569213788956,\n",
      "      \"learning_rate\": 2.0482041587901705e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 152260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.595481899240681,\n",
      "      \"grad_norm\": 0.0024811257608234882,\n",
      "      \"learning_rate\": 2.041902961562697e-06,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 152280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.596742178392514,\n",
      "      \"grad_norm\": 0.00015530861855950207,\n",
      "      \"learning_rate\": 2.035601764335224e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 152300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.598002457544347,\n",
      "      \"grad_norm\": 0.00039563101017847657,\n",
      "      \"learning_rate\": 2.0293005671077505e-06,\n",
      "      \"loss\": 0.0037,\n",
      "      \"step\": 152320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.59926273669618,\n",
      "      \"grad_norm\": 0.0006417549448087811,\n",
      "      \"learning_rate\": 2.0229993698802774e-06,\n",
      "      \"loss\": 0.0155,\n",
      "      \"step\": 152340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.60052301584801,\n",
      "      \"grad_norm\": 9.676750050857663e-05,\n",
      "      \"learning_rate\": 2.016698172652804e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 152360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.601783294999843,\n",
      "      \"grad_norm\": 0.0004191021143924445,\n",
      "      \"learning_rate\": 2.010396975425331e-06,\n",
      "      \"loss\": 0.0513,\n",
      "      \"step\": 152380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.603043574151675,\n",
      "      \"grad_norm\": 0.00012972929107490927,\n",
      "      \"learning_rate\": 2.0040957781978577e-06,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 152400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.604303853303506,\n",
      "      \"grad_norm\": 0.006624047644436359,\n",
      "      \"learning_rate\": 1.9977945809703842e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 152420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.605564132455338,\n",
      "      \"grad_norm\": 0.09537369757890701,\n",
      "      \"learning_rate\": 1.991493383742911e-06,\n",
      "      \"loss\": 0.0095,\n",
      "      \"step\": 152440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.606824411607171,\n",
      "      \"grad_norm\": 0.01053590141236782,\n",
      "      \"learning_rate\": 1.985192186515438e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 152460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.608084690759004,\n",
      "      \"grad_norm\": 0.01700511761009693,\n",
      "      \"learning_rate\": 1.9788909892879646e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 152480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.609344969910834,\n",
      "      \"grad_norm\": 0.0008877869113348424,\n",
      "      \"learning_rate\": 1.9725897920604915e-06,\n",
      "      \"loss\": 0.0325,\n",
      "      \"step\": 152500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.610605249062667,\n",
      "      \"grad_norm\": 0.0017607110785320401,\n",
      "      \"learning_rate\": 1.9662885948330184e-06,\n",
      "      \"loss\": 0.0266,\n",
      "      \"step\": 152520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.6118655282145,\n",
      "      \"grad_norm\": 0.3083209991455078,\n",
      "      \"learning_rate\": 1.9599873976055453e-06,\n",
      "      \"loss\": 0.0136,\n",
      "      \"step\": 152540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.613125807366332,\n",
      "      \"grad_norm\": 0.002146698534488678,\n",
      "      \"learning_rate\": 1.953686200378072e-06,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 152560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.614386086518163,\n",
      "      \"grad_norm\": 0.0008892763871699572,\n",
      "      \"learning_rate\": 1.9473850031505988e-06,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 152580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.615646365669996,\n",
      "      \"grad_norm\": 0.007243680767714977,\n",
      "      \"learning_rate\": 1.9410838059231257e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 152600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.616906644821828,\n",
      "      \"grad_norm\": 0.0006245124968700111,\n",
      "      \"learning_rate\": 1.934782608695652e-06,\n",
      "      \"loss\": 0.0147,\n",
      "      \"step\": 152620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.61816692397366,\n",
      "      \"grad_norm\": 0.04801302030682564,\n",
      "      \"learning_rate\": 1.928481411468179e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 152640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.619427203125491,\n",
      "      \"grad_norm\": 0.00042484368896111846,\n",
      "      \"learning_rate\": 1.922180214240706e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 152660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.620687482277324,\n",
      "      \"grad_norm\": 0.0005388316349126399,\n",
      "      \"learning_rate\": 1.915879017013233e-06,\n",
      "      \"loss\": 0.0037,\n",
      "      \"step\": 152680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.621947761429157,\n",
      "      \"grad_norm\": 0.0004121182137168944,\n",
      "      \"learning_rate\": 1.9095778197857594e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 152700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.62320804058099,\n",
      "      \"grad_norm\": 0.00039868452586233616,\n",
      "      \"learning_rate\": 1.9032766225582861e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 152720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.62446831973282,\n",
      "      \"grad_norm\": 0.0031803825404495,\n",
      "      \"learning_rate\": 1.8972904851921866e-06,\n",
      "      \"loss\": 0.0045,\n",
      "      \"step\": 152740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.625728598884653,\n",
      "      \"grad_norm\": 0.006319631822407246,\n",
      "      \"learning_rate\": 1.8909892879647135e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 152760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.626988878036485,\n",
      "      \"grad_norm\": 0.0009488878422416747,\n",
      "      \"learning_rate\": 1.88468809073724e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 152780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.628249157188318,\n",
      "      \"grad_norm\": 0.00012306793360039592,\n",
      "      \"learning_rate\": 1.878386893509767e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 152800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.629509436340149,\n",
      "      \"grad_norm\": 0.0003694880288094282,\n",
      "      \"learning_rate\": 1.8720856962822939e-06,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 152820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.630769715491981,\n",
      "      \"grad_norm\": 0.01648390106856823,\n",
      "      \"learning_rate\": 1.8657844990548206e-06,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 152840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.632029994643814,\n",
      "      \"grad_norm\": 0.0037292272318154573,\n",
      "      \"learning_rate\": 1.8594833018273473e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 152860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.633290273795646,\n",
      "      \"grad_norm\": 0.001098486827686429,\n",
      "      \"learning_rate\": 1.853182104599874e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 152880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.634550552947477,\n",
      "      \"grad_norm\": 0.0002049152972176671,\n",
      "      \"learning_rate\": 1.846880907372401e-06,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 152900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.63581083209931,\n",
      "      \"grad_norm\": 0.005494378972798586,\n",
      "      \"learning_rate\": 1.8405797101449274e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 152920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.637071111251142,\n",
      "      \"grad_norm\": 0.00011007225111825392,\n",
      "      \"learning_rate\": 1.8342785129174544e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 152940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.638331390402975,\n",
      "      \"grad_norm\": 0.00015445667668245733,\n",
      "      \"learning_rate\": 1.8279773156899813e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 152960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.639591669554806,\n",
      "      \"grad_norm\": 0.004970130044966936,\n",
      "      \"learning_rate\": 1.8216761184625078e-06,\n",
      "      \"loss\": 0.0142,\n",
      "      \"step\": 152980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.640851948706638,\n",
      "      \"grad_norm\": 0.0008330749114975333,\n",
      "      \"learning_rate\": 1.8153749212350347e-06,\n",
      "      \"loss\": 0.0244,\n",
      "      \"step\": 153000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.64211222785847,\n",
      "      \"grad_norm\": 0.002172998618334532,\n",
      "      \"learning_rate\": 1.8090737240075616e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 153020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.643372507010303,\n",
      "      \"grad_norm\": 0.0010242400458082557,\n",
      "      \"learning_rate\": 1.8027725267800885e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 153040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.644632786162134,\n",
      "      \"grad_norm\": 0.0003011949302162975,\n",
      "      \"learning_rate\": 1.796471329552615e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 153060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.645893065313967,\n",
      "      \"grad_norm\": 0.017972655594348907,\n",
      "      \"learning_rate\": 1.790170132325142e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 153080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.6471533444658,\n",
      "      \"grad_norm\": 0.0006891782977618277,\n",
      "      \"learning_rate\": 1.7838689350976687e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 153100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.648413623617632,\n",
      "      \"grad_norm\": 0.00036250861012376845,\n",
      "      \"learning_rate\": 1.7775677378701954e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 153120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.649673902769463,\n",
      "      \"grad_norm\": 0.01955946534872055,\n",
      "      \"learning_rate\": 1.771266540642722e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 153140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.650934181921295,\n",
      "      \"grad_norm\": 0.0054433923214674,\n",
      "      \"learning_rate\": 1.764965343415249e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 153160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.652194461073128,\n",
      "      \"grad_norm\": 0.002370499772951007,\n",
      "      \"learning_rate\": 1.758664146187776e-06,\n",
      "      \"loss\": 0.022,\n",
      "      \"step\": 153180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.65345474022496,\n",
      "      \"grad_norm\": 0.0014204982435330749,\n",
      "      \"learning_rate\": 1.7523629489603024e-06,\n",
      "      \"loss\": 0.0136,\n",
      "      \"step\": 153200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.654715019376791,\n",
      "      \"grad_norm\": 0.01643872633576393,\n",
      "      \"learning_rate\": 1.7460617517328294e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 153220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.655975298528624,\n",
      "      \"grad_norm\": 0.0015612883726134896,\n",
      "      \"learning_rate\": 1.7397605545053563e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 153240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.657235577680456,\n",
      "      \"grad_norm\": 0.02603749744594097,\n",
      "      \"learning_rate\": 1.7334593572778828e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 153260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.658495856832289,\n",
      "      \"grad_norm\": 0.0005939538823440671,\n",
      "      \"learning_rate\": 1.7271581600504097e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 153280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.65975613598412,\n",
      "      \"grad_norm\": 13.663744926452637,\n",
      "      \"learning_rate\": 1.7208569628229366e-06,\n",
      "      \"loss\": 0.0316,\n",
      "      \"step\": 153300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.661016415135952,\n",
      "      \"grad_norm\": 0.03912213444709778,\n",
      "      \"learning_rate\": 1.7145557655954631e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 153320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.662276694287785,\n",
      "      \"grad_norm\": 0.0018562081968411803,\n",
      "      \"learning_rate\": 1.7082545683679898e-06,\n",
      "      \"loss\": 0.0026,\n",
      "      \"step\": 153340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.663536973439617,\n",
      "      \"grad_norm\": 0.0007178927189670503,\n",
      "      \"learning_rate\": 1.7019533711405167e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 153360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.664797252591448,\n",
      "      \"grad_norm\": 0.00033405248541384935,\n",
      "      \"learning_rate\": 1.6956521739130437e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 153380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.66605753174328,\n",
      "      \"grad_norm\": 1.6795485019683838,\n",
      "      \"learning_rate\": 1.6893509766855702e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 153400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.667317810895113,\n",
      "      \"grad_norm\": 0.0008942750282585621,\n",
      "      \"learning_rate\": 1.683049779458097e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 153420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.668578090046946,\n",
      "      \"grad_norm\": 0.000514867075253278,\n",
      "      \"learning_rate\": 1.676748582230624e-06,\n",
      "      \"loss\": 0.0038,\n",
      "      \"step\": 153440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.669838369198777,\n",
      "      \"grad_norm\": 0.014939476735889912,\n",
      "      \"learning_rate\": 1.6704473850031505e-06,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 153460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.67109864835061,\n",
      "      \"grad_norm\": 4.446093559265137,\n",
      "      \"learning_rate\": 1.6641461877756774e-06,\n",
      "      \"loss\": 0.0284,\n",
      "      \"step\": 153480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.672358927502442,\n",
      "      \"grad_norm\": 0.0006556169828400016,\n",
      "      \"learning_rate\": 1.6578449905482044e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 153500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.673619206654275,\n",
      "      \"grad_norm\": 0.01724526286125183,\n",
      "      \"learning_rate\": 1.6515437933207313e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 153520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.674879485806105,\n",
      "      \"grad_norm\": 9.810441406443715e-05,\n",
      "      \"learning_rate\": 1.6452425960932578e-06,\n",
      "      \"loss\": 0.0139,\n",
      "      \"step\": 153540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.676139764957938,\n",
      "      \"grad_norm\": 0.0004441158380359411,\n",
      "      \"learning_rate\": 1.6389413988657845e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 153560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.67740004410977,\n",
      "      \"grad_norm\": 0.003615086665377021,\n",
      "      \"learning_rate\": 1.6326402016383114e-06,\n",
      "      \"loss\": 0.0558,\n",
      "      \"step\": 153580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.678660323261603,\n",
      "      \"grad_norm\": 0.11344396322965622,\n",
      "      \"learning_rate\": 1.626339004410838e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 153600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.679920602413434,\n",
      "      \"grad_norm\": 0.00012305582640692592,\n",
      "      \"learning_rate\": 1.6200378071833648e-06,\n",
      "      \"loss\": 0.0128,\n",
      "      \"step\": 153620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.681180881565266,\n",
      "      \"grad_norm\": 0.00016233399219345301,\n",
      "      \"learning_rate\": 1.6137366099558917e-06,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 153640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.682441160717099,\n",
      "      \"grad_norm\": 0.03089081682264805,\n",
      "      \"learning_rate\": 1.6074354127284187e-06,\n",
      "      \"loss\": 0.0118,\n",
      "      \"step\": 153660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.683701439868932,\n",
      "      \"grad_norm\": 0.0007551811868324876,\n",
      "      \"learning_rate\": 1.6011342155009452e-06,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 153680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.684961719020762,\n",
      "      \"grad_norm\": 0.004329188261181116,\n",
      "      \"learning_rate\": 1.594833018273472e-06,\n",
      "      \"loss\": 0.0203,\n",
      "      \"step\": 153700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.686221998172595,\n",
      "      \"grad_norm\": 0.0008110462804324925,\n",
      "      \"learning_rate\": 1.588531821045999e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 153720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.687482277324428,\n",
      "      \"grad_norm\": 0.04463334381580353,\n",
      "      \"learning_rate\": 1.5822306238185255e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 153740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.68874255647626,\n",
      "      \"grad_norm\": 0.00016321225848514587,\n",
      "      \"learning_rate\": 1.5759294265910524e-06,\n",
      "      \"loss\": 0.0105,\n",
      "      \"step\": 153760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.690002835628091,\n",
      "      \"grad_norm\": 0.006568070501089096,\n",
      "      \"learning_rate\": 1.5699432892249527e-06,\n",
      "      \"loss\": 0.0256,\n",
      "      \"step\": 153780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.691263114779924,\n",
      "      \"grad_norm\": 0.00130385160446167,\n",
      "      \"learning_rate\": 1.5636420919974796e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 153800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.692523393931756,\n",
      "      \"grad_norm\": 0.06053170561790466,\n",
      "      \"learning_rate\": 1.5573408947700063e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 153820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.693783673083589,\n",
      "      \"grad_norm\": 0.005175221245735884,\n",
      "      \"learning_rate\": 1.551039697542533e-06,\n",
      "      \"loss\": 0.0102,\n",
      "      \"step\": 153840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.69504395223542,\n",
      "      \"grad_norm\": 0.0010772832902148366,\n",
      "      \"learning_rate\": 1.54473850031506e-06,\n",
      "      \"loss\": 0.0127,\n",
      "      \"step\": 153860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.696304231387252,\n",
      "      \"grad_norm\": 0.00010943564120680094,\n",
      "      \"learning_rate\": 1.5384373030875867e-06,\n",
      "      \"loss\": 0.0023,\n",
      "      \"step\": 153880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.697564510539085,\n",
      "      \"grad_norm\": 0.00014345819363370538,\n",
      "      \"learning_rate\": 1.5321361058601136e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 153900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.698824789690917,\n",
      "      \"grad_norm\": 0.0002718952891882509,\n",
      "      \"learning_rate\": 1.5258349086326403e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 153920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.700085068842748,\n",
      "      \"grad_norm\": 0.0006213283631950617,\n",
      "      \"learning_rate\": 1.5195337114051672e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 153940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.70134534799458,\n",
      "      \"grad_norm\": 0.006990919355303049,\n",
      "      \"learning_rate\": 1.513232514177694e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 153960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.702605627146413,\n",
      "      \"grad_norm\": 0.0013271179050207138,\n",
      "      \"learning_rate\": 1.5069313169502206e-06,\n",
      "      \"loss\": 0.0211,\n",
      "      \"step\": 153980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.703865906298246,\n",
      "      \"grad_norm\": 0.00013653430505655706,\n",
      "      \"learning_rate\": 1.5006301197227473e-06,\n",
      "      \"loss\": 0.0016,\n",
      "      \"step\": 154000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.705126185450077,\n",
      "      \"grad_norm\": 0.03165176510810852,\n",
      "      \"learning_rate\": 1.494328922495274e-06,\n",
      "      \"loss\": 0.0061,\n",
      "      \"step\": 154020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.70638646460191,\n",
      "      \"grad_norm\": 0.0006916455458849669,\n",
      "      \"learning_rate\": 1.488027725267801e-06,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 154040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.707646743753742,\n",
      "      \"grad_norm\": 0.00020735926227644086,\n",
      "      \"learning_rate\": 1.4817265280403277e-06,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 154060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.708907022905574,\n",
      "      \"grad_norm\": 0.00011814333265647292,\n",
      "      \"learning_rate\": 1.4754253308128544e-06,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 154080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.710167302057405,\n",
      "      \"grad_norm\": 6.809028625488281,\n",
      "      \"learning_rate\": 1.4691241335853813e-06,\n",
      "      \"loss\": 0.0278,\n",
      "      \"step\": 154100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.711427581209238,\n",
      "      \"grad_norm\": 0.00022292819630820304,\n",
      "      \"learning_rate\": 1.462822936357908e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 154120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.71268786036107,\n",
      "      \"grad_norm\": 0.04065302759408951,\n",
      "      \"learning_rate\": 1.456521739130435e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 154140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.713948139512903,\n",
      "      \"grad_norm\": 0.0007584532140754163,\n",
      "      \"learning_rate\": 1.4502205419029617e-06,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 154160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.715208418664734,\n",
      "      \"grad_norm\": 0.0005303198704496026,\n",
      "      \"learning_rate\": 1.4439193446754886e-06,\n",
      "      \"loss\": 0.029,\n",
      "      \"step\": 154180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.716468697816566,\n",
      "      \"grad_norm\": 0.00017675694834906608,\n",
      "      \"learning_rate\": 1.4376181474480153e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 154200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.717728976968399,\n",
      "      \"grad_norm\": 0.0001431940181646496,\n",
      "      \"learning_rate\": 1.431316950220542e-06,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 154220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.718989256120231,\n",
      "      \"grad_norm\": 0.006163676269352436,\n",
      "      \"learning_rate\": 1.4250157529930687e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 154240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.720249535272062,\n",
      "      \"grad_norm\": 0.0009534049895592034,\n",
      "      \"learning_rate\": 1.4187145557655954e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 154260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.721509814423895,\n",
      "      \"grad_norm\": 0.0018394935177639127,\n",
      "      \"learning_rate\": 1.4124133585381223e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 154280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.722770093575727,\n",
      "      \"grad_norm\": 0.000530688208527863,\n",
      "      \"learning_rate\": 1.406112161310649e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 154300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.72403037272756,\n",
      "      \"grad_norm\": 9.71439266204834,\n",
      "      \"learning_rate\": 1.3998109640831758e-06,\n",
      "      \"loss\": 0.016,\n",
      "      \"step\": 154320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.72529065187939,\n",
      "      \"grad_norm\": 0.008638736791908741,\n",
      "      \"learning_rate\": 1.3935097668557027e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 154340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.726550931031223,\n",
      "      \"grad_norm\": 0.00013650604523718357,\n",
      "      \"learning_rate\": 1.3872085696282294e-06,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 154360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.727811210183056,\n",
      "      \"grad_norm\": 0.0004911724245175719,\n",
      "      \"learning_rate\": 1.3809073724007563e-06,\n",
      "      \"loss\": 0.0183,\n",
      "      \"step\": 154380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.729071489334888,\n",
      "      \"grad_norm\": 0.062116190791130066,\n",
      "      \"learning_rate\": 1.374606175173283e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 154400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.73033176848672,\n",
      "      \"grad_norm\": 0.000274142250418663,\n",
      "      \"learning_rate\": 1.3683049779458097e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 154420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.731592047638552,\n",
      "      \"grad_norm\": 0.0019002374028787017,\n",
      "      \"learning_rate\": 1.3620037807183367e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 154440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.732852326790384,\n",
      "      \"grad_norm\": 0.0001222372957272455,\n",
      "      \"learning_rate\": 1.3557025834908634e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 154460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.734112605942217,\n",
      "      \"grad_norm\": 0.0013029464753344655,\n",
      "      \"learning_rate\": 1.34940138626339e-06,\n",
      "      \"loss\": 0.024,\n",
      "      \"step\": 154480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.735372885094048,\n",
      "      \"grad_norm\": 0.012016369961202145,\n",
      "      \"learning_rate\": 1.3431001890359168e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 154500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.73663316424588,\n",
      "      \"grad_norm\": 0.00011334812006680295,\n",
      "      \"learning_rate\": 1.3367989918084437e-06,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 154520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.737893443397713,\n",
      "      \"grad_norm\": 0.0002542297006584704,\n",
      "      \"learning_rate\": 1.3304977945809704e-06,\n",
      "      \"loss\": 0.0092,\n",
      "      \"step\": 154540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.739153722549545,\n",
      "      \"grad_norm\": 0.003570129629224539,\n",
      "      \"learning_rate\": 1.3241965973534971e-06,\n",
      "      \"loss\": 0.0046,\n",
      "      \"step\": 154560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.740414001701376,\n",
      "      \"grad_norm\": 0.05121997743844986,\n",
      "      \"learning_rate\": 1.317895400126024e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 154580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.741674280853209,\n",
      "      \"grad_norm\": 0.0002701567718759179,\n",
      "      \"learning_rate\": 1.3115942028985508e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 154600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.742934560005041,\n",
      "      \"grad_norm\": 0.0007962745148688555,\n",
      "      \"learning_rate\": 1.3052930056710777e-06,\n",
      "      \"loss\": 0.014,\n",
      "      \"step\": 154620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.744194839156874,\n",
      "      \"grad_norm\": 0.020500069484114647,\n",
      "      \"learning_rate\": 1.2989918084436044e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 154640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.745455118308705,\n",
      "      \"grad_norm\": 0.003616137895733118,\n",
      "      \"learning_rate\": 1.2926906112161311e-06,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 154660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.746715397460537,\n",
      "      \"grad_norm\": 0.0026079860981553793,\n",
      "      \"learning_rate\": 1.286389413988658e-06,\n",
      "      \"loss\": 0.0032,\n",
      "      \"step\": 154680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.74797567661237,\n",
      "      \"grad_norm\": 0.00019763804448302835,\n",
      "      \"learning_rate\": 1.2800882167611847e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 154700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.749235955764203,\n",
      "      \"grad_norm\": 0.0066465819254517555,\n",
      "      \"learning_rate\": 1.2737870195337114e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 154720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.750496234916033,\n",
      "      \"grad_norm\": 0.13476470112800598,\n",
      "      \"learning_rate\": 1.2674858223062382e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 154740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.751756514067866,\n",
      "      \"grad_norm\": 0.0005153025267645717,\n",
      "      \"learning_rate\": 1.2611846250787649e-06,\n",
      "      \"loss\": 0.002,\n",
      "      \"step\": 154760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.753016793219698,\n",
      "      \"grad_norm\": 0.0011038316879421473,\n",
      "      \"learning_rate\": 1.2548834278512918e-06,\n",
      "      \"loss\": 0.0093,\n",
      "      \"step\": 154780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.754277072371531,\n",
      "      \"grad_norm\": 0.0001613388303667307,\n",
      "      \"learning_rate\": 1.2485822306238185e-06,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 154800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.755537351523362,\n",
      "      \"grad_norm\": 0.00276030576787889,\n",
      "      \"learning_rate\": 1.2422810333963454e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 154820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.756797630675194,\n",
      "      \"grad_norm\": 5.507505416870117,\n",
      "      \"learning_rate\": 1.2359798361688721e-06,\n",
      "      \"loss\": 0.009,\n",
      "      \"step\": 154840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.758057909827027,\n",
      "      \"grad_norm\": 0.002144957659766078,\n",
      "      \"learning_rate\": 1.229678638941399e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 154860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.75931818897886,\n",
      "      \"grad_norm\": 0.002678781282156706,\n",
      "      \"learning_rate\": 1.2233774417139258e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 154880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.76057846813069,\n",
      "      \"grad_norm\": 0.00036915336386300623,\n",
      "      \"learning_rate\": 1.2170762444864525e-06,\n",
      "      \"loss\": 0.0141,\n",
      "      \"step\": 154900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.761838747282523,\n",
      "      \"grad_norm\": 0.011855514720082283,\n",
      "      \"learning_rate\": 1.2107750472589794e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 154920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.763099026434356,\n",
      "      \"grad_norm\": 0.6356455683708191,\n",
      "      \"learning_rate\": 1.2044738500315061e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 154940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.764359305586188,\n",
      "      \"grad_norm\": 0.0026991069316864014,\n",
      "      \"learning_rate\": 1.1981726528040328e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 154960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.765619584738019,\n",
      "      \"grad_norm\": 0.0005717474850825965,\n",
      "      \"learning_rate\": 1.1918714555765595e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 154980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.766879863889852,\n",
      "      \"grad_norm\": 0.06171642243862152,\n",
      "      \"learning_rate\": 1.1855702583490862e-06,\n",
      "      \"loss\": 0.0105,\n",
      "      \"step\": 155000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.768140143041684,\n",
      "      \"grad_norm\": 0.0007715266547165811,\n",
      "      \"learning_rate\": 1.1792690611216132e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 155020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.769400422193517,\n",
      "      \"grad_norm\": 0.010483215562999249,\n",
      "      \"learning_rate\": 1.1729678638941399e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 155040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.770660701345347,\n",
      "      \"grad_norm\": 0.00018255511531606317,\n",
      "      \"learning_rate\": 1.1666666666666668e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 155060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.77192098049718,\n",
      "      \"grad_norm\": 0.0054906923323869705,\n",
      "      \"learning_rate\": 1.1603654694391935e-06,\n",
      "      \"loss\": 0.0191,\n",
      "      \"step\": 155080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.773181259649013,\n",
      "      \"grad_norm\": 0.6750081181526184,\n",
      "      \"learning_rate\": 1.154379332073094e-06,\n",
      "      \"loss\": 0.0064,\n",
      "      \"step\": 155100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.774441538800845,\n",
      "      \"grad_norm\": 0.00039278113399632275,\n",
      "      \"learning_rate\": 1.1480781348456207e-06,\n",
      "      \"loss\": 0.0142,\n",
      "      \"step\": 155120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.775701817952676,\n",
      "      \"grad_norm\": 0.0002188834478147328,\n",
      "      \"learning_rate\": 1.1417769376181474e-06,\n",
      "      \"loss\": 0.0117,\n",
      "      \"step\": 155140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.776962097104509,\n",
      "      \"grad_norm\": 0.0011665363563224673,\n",
      "      \"learning_rate\": 1.135475740390674e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 155160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.778222376256341,\n",
      "      \"grad_norm\": 0.0009592375135980546,\n",
      "      \"learning_rate\": 1.129174543163201e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 155180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.779482655408174,\n",
      "      \"grad_norm\": 0.001827073167078197,\n",
      "      \"learning_rate\": 1.1228733459357277e-06,\n",
      "      \"loss\": 0.0028,\n",
      "      \"step\": 155200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.780742934560005,\n",
      "      \"grad_norm\": 0.00030723054078407586,\n",
      "      \"learning_rate\": 1.1165721487082547e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 155220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.782003213711837,\n",
      "      \"grad_norm\": 0.125353142619133,\n",
      "      \"learning_rate\": 1.1102709514807814e-06,\n",
      "      \"loss\": 0.0045,\n",
      "      \"step\": 155240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.78326349286367,\n",
      "      \"grad_norm\": 0.04343677684664726,\n",
      "      \"learning_rate\": 1.1039697542533083e-06,\n",
      "      \"loss\": 0.0237,\n",
      "      \"step\": 155260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.7845237720155,\n",
      "      \"grad_norm\": 0.0008279501344077289,\n",
      "      \"learning_rate\": 1.097668557025835e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 155280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.785784051167333,\n",
      "      \"grad_norm\": 0.0009849087800830603,\n",
      "      \"learning_rate\": 1.0913673597983617e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 155300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.787044330319166,\n",
      "      \"grad_norm\": 0.00012115287245251238,\n",
      "      \"learning_rate\": 1.0850661625708886e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 155320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.788304609470998,\n",
      "      \"grad_norm\": 0.0001329439110122621,\n",
      "      \"learning_rate\": 1.0787649653434153e-06,\n",
      "      \"loss\": 0.0137,\n",
      "      \"step\": 155340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.78956488862283,\n",
      "      \"grad_norm\": 0.00012315248022787273,\n",
      "      \"learning_rate\": 1.072463768115942e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 155360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.790825167774662,\n",
      "      \"grad_norm\": 0.0030143309850245714,\n",
      "      \"learning_rate\": 1.0661625708884688e-06,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 155380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.792085446926494,\n",
      "      \"grad_norm\": 0.05106062442064285,\n",
      "      \"learning_rate\": 1.0598613736609955e-06,\n",
      "      \"loss\": 0.011,\n",
      "      \"step\": 155400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.793345726078327,\n",
      "      \"grad_norm\": 0.00011903542326763272,\n",
      "      \"learning_rate\": 1.0535601764335224e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 155420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.794606005230158,\n",
      "      \"grad_norm\": 0.00025001485482789576,\n",
      "      \"learning_rate\": 1.047258979206049e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 155440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.79586628438199,\n",
      "      \"grad_norm\": 0.00044952737516723573,\n",
      "      \"learning_rate\": 1.040957781978576e-06,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 155460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.797126563533823,\n",
      "      \"grad_norm\": 0.000572573917452246,\n",
      "      \"learning_rate\": 1.0346565847511027e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 155480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.798386842685655,\n",
      "      \"grad_norm\": 0.0005494626821018755,\n",
      "      \"learning_rate\": 1.0283553875236297e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 155500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.799647121837488,\n",
      "      \"grad_norm\": 0.0013122850796207786,\n",
      "      \"learning_rate\": 1.0220541902961564e-06,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 155520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.800907400989319,\n",
      "      \"grad_norm\": 0.0002575824037194252,\n",
      "      \"learning_rate\": 1.015752993068683e-06,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 155540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.802167680141151,\n",
      "      \"grad_norm\": 0.0002134649403160438,\n",
      "      \"learning_rate\": 1.00945179584121e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 155560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.803427959292984,\n",
      "      \"grad_norm\": 0.008037432096898556,\n",
      "      \"learning_rate\": 1.0031505986137367e-06,\n",
      "      \"loss\": 0.0131,\n",
      "      \"step\": 155580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.804688238444815,\n",
      "      \"grad_norm\": 0.0019488160032778978,\n",
      "      \"learning_rate\": 9.968494013862634e-07,\n",
      "      \"loss\": 0.0154,\n",
      "      \"step\": 155600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.805948517596647,\n",
      "      \"grad_norm\": 0.00018701553926803172,\n",
      "      \"learning_rate\": 9.905482041587901e-07,\n",
      "      \"loss\": 0.0158,\n",
      "      \"step\": 155620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.80720879674848,\n",
      "      \"grad_norm\": 0.00028444547206163406,\n",
      "      \"learning_rate\": 9.842470069313168e-07,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 155640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.808469075900312,\n",
      "      \"grad_norm\": 0.0032170163467526436,\n",
      "      \"learning_rate\": 9.779458097038438e-07,\n",
      "      \"loss\": 0.0078,\n",
      "      \"step\": 155660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.809729355052145,\n",
      "      \"grad_norm\": 6.347883224487305,\n",
      "      \"learning_rate\": 9.716446124763705e-07,\n",
      "      \"loss\": 0.0283,\n",
      "      \"step\": 155680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.810989634203976,\n",
      "      \"grad_norm\": 0.00013031582057010382,\n",
      "      \"learning_rate\": 9.653434152488974e-07,\n",
      "      \"loss\": 0.0093,\n",
      "      \"step\": 155700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.812249913355808,\n",
      "      \"grad_norm\": 0.00019041750056203455,\n",
      "      \"learning_rate\": 9.59042218021424e-07,\n",
      "      \"loss\": 0.0064,\n",
      "      \"step\": 155720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.813510192507641,\n",
      "      \"grad_norm\": 0.00012009157944703475,\n",
      "      \"learning_rate\": 9.527410207939508e-07,\n",
      "      \"loss\": 0.0322,\n",
      "      \"step\": 155740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.814770471659472,\n",
      "      \"grad_norm\": 0.002032057149335742,\n",
      "      \"learning_rate\": 9.464398235664777e-07,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 155760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.816030750811304,\n",
      "      \"grad_norm\": 0.000827545765787363,\n",
      "      \"learning_rate\": 9.401386263390044e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 155780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.817291029963137,\n",
      "      \"grad_norm\": 0.000695685506798327,\n",
      "      \"learning_rate\": 9.338374291115313e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 155800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.81855130911497,\n",
      "      \"grad_norm\": 0.0010275385575369,\n",
      "      \"learning_rate\": 9.27536231884058e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 155820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.819811588266802,\n",
      "      \"grad_norm\": 0.005300955381244421,\n",
      "      \"learning_rate\": 9.212350346565849e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 155840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.821071867418633,\n",
      "      \"grad_norm\": 0.002820906462147832,\n",
      "      \"learning_rate\": 9.149338374291116e-07,\n",
      "      \"loss\": 0.0209,\n",
      "      \"step\": 155860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.822332146570465,\n",
      "      \"grad_norm\": 12.18957805633545,\n",
      "      \"learning_rate\": 9.086326402016383e-07,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 155880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.823592425722298,\n",
      "      \"grad_norm\": 0.00011442159302532673,\n",
      "      \"learning_rate\": 9.023314429741651e-07,\n",
      "      \"loss\": 0.0072,\n",
      "      \"step\": 155900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.824852704874129,\n",
      "      \"grad_norm\": 0.00010821744217537344,\n",
      "      \"learning_rate\": 8.960302457466918e-07,\n",
      "      \"loss\": 0.0012,\n",
      "      \"step\": 155920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.826112984025961,\n",
      "      \"grad_norm\": 0.00035077688517048955,\n",
      "      \"learning_rate\": 8.897290485192188e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 155940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.827373263177794,\n",
      "      \"grad_norm\": 0.03682345524430275,\n",
      "      \"learning_rate\": 8.834278512917455e-07,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 155960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.828633542329626,\n",
      "      \"grad_norm\": 0.006871176417917013,\n",
      "      \"learning_rate\": 8.771266540642722e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 155980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.829893821481459,\n",
      "      \"grad_norm\": 0.002973980037495494,\n",
      "      \"learning_rate\": 8.708254568367991e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 156000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.83115410063329,\n",
      "      \"grad_norm\": 0.0012577564921230078,\n",
      "      \"learning_rate\": 8.645242596093258e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 156020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.832414379785122,\n",
      "      \"grad_norm\": 0.000118689400551375,\n",
      "      \"learning_rate\": 8.582230623818526e-07,\n",
      "      \"loss\": 0.0018,\n",
      "      \"step\": 156040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.833674658936955,\n",
      "      \"grad_norm\": 0.0025346511974930763,\n",
      "      \"learning_rate\": 8.519218651543793e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 156060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.834934938088786,\n",
      "      \"grad_norm\": 0.0010818112641572952,\n",
      "      \"learning_rate\": 8.456206679269063e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 156080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.836195217240618,\n",
      "      \"grad_norm\": 0.0011871164897456765,\n",
      "      \"learning_rate\": 8.39319470699433e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 156100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.837455496392451,\n",
      "      \"grad_norm\": 0.0001060845170286484,\n",
      "      \"learning_rate\": 8.330182734719597e-07,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 156120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.838715775544284,\n",
      "      \"grad_norm\": 0.031571004539728165,\n",
      "      \"learning_rate\": 8.267170762444865e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 156140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.839976054696114,\n",
      "      \"grad_norm\": 0.055510010570287704,\n",
      "      \"learning_rate\": 8.204158790170132e-07,\n",
      "      \"loss\": 0.0136,\n",
      "      \"step\": 156160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.841236333847947,\n",
      "      \"grad_norm\": 0.01467104908078909,\n",
      "      \"learning_rate\": 8.141146817895401e-07,\n",
      "      \"loss\": 0.0201,\n",
      "      \"step\": 156180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.84249661299978,\n",
      "      \"grad_norm\": 0.00011218805593671277,\n",
      "      \"learning_rate\": 8.078134845620668e-07,\n",
      "      \"loss\": 0.0165,\n",
      "      \"step\": 156200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.843756892151612,\n",
      "      \"grad_norm\": 0.00048513541696593165,\n",
      "      \"learning_rate\": 8.015122873345935e-07,\n",
      "      \"loss\": 0.0205,\n",
      "      \"step\": 156220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.845017171303443,\n",
      "      \"grad_norm\": 0.0011314384173601866,\n",
      "      \"learning_rate\": 7.952110901071205e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 156240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.846277450455275,\n",
      "      \"grad_norm\": 0.00011961333075305447,\n",
      "      \"learning_rate\": 7.889098928796472e-07,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 156260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.847537729607108,\n",
      "      \"grad_norm\": 0.00031462812330573797,\n",
      "      \"learning_rate\": 7.82608695652174e-07,\n",
      "      \"loss\": 0.0026,\n",
      "      \"step\": 156280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.84879800875894,\n",
      "      \"grad_norm\": 0.007640186231583357,\n",
      "      \"learning_rate\": 7.763074984247007e-07,\n",
      "      \"loss\": 0.0226,\n",
      "      \"step\": 156300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.850058287910771,\n",
      "      \"grad_norm\": 0.00995580479502678,\n",
      "      \"learning_rate\": 7.700063011972275e-07,\n",
      "      \"loss\": 0.0108,\n",
      "      \"step\": 156320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.851318567062604,\n",
      "      \"grad_norm\": 0.020435510203242302,\n",
      "      \"learning_rate\": 7.637051039697543e-07,\n",
      "      \"loss\": 0.0147,\n",
      "      \"step\": 156340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.852578846214437,\n",
      "      \"grad_norm\": 0.005940633825957775,\n",
      "      \"learning_rate\": 7.574039067422812e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 156360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.85383912536627,\n",
      "      \"grad_norm\": 0.008281528949737549,\n",
      "      \"learning_rate\": 7.511027095148079e-07,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 156380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.8550994045181,\n",
      "      \"grad_norm\": 0.0004968898138031363,\n",
      "      \"learning_rate\": 7.448015122873346e-07,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 156400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.856359683669933,\n",
      "      \"grad_norm\": 0.0005161391454748809,\n",
      "      \"learning_rate\": 7.385003150598614e-07,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 156420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.857619962821765,\n",
      "      \"grad_norm\": 0.0047034225426614285,\n",
      "      \"learning_rate\": 7.321991178323882e-07,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 156440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.858880241973598,\n",
      "      \"grad_norm\": 0.0006702049868181348,\n",
      "      \"learning_rate\": 7.25897920604915e-07,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 156460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.860140521125428,\n",
      "      \"grad_norm\": 0.0001699069980531931,\n",
      "      \"learning_rate\": 7.195967233774418e-07,\n",
      "      \"loss\": 0.0085,\n",
      "      \"step\": 156480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.861400800277261,\n",
      "      \"grad_norm\": 0.005874432157725096,\n",
      "      \"learning_rate\": 7.132955261499685e-07,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 156500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.862661079429094,\n",
      "      \"grad_norm\": 0.000266790360910818,\n",
      "      \"learning_rate\": 7.069943289224953e-07,\n",
      "      \"loss\": 0.0212,\n",
      "      \"step\": 156520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.863921358580926,\n",
      "      \"grad_norm\": 0.0039030907209962606,\n",
      "      \"learning_rate\": 7.006931316950221e-07,\n",
      "      \"loss\": 0.0109,\n",
      "      \"step\": 156540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.865181637732757,\n",
      "      \"grad_norm\": 0.0007194601930677891,\n",
      "      \"learning_rate\": 6.943919344675489e-07,\n",
      "      \"loss\": 0.0294,\n",
      "      \"step\": 156560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.86644191688459,\n",
      "      \"grad_norm\": 0.0014839633367955685,\n",
      "      \"learning_rate\": 6.880907372400757e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 156580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.867702196036422,\n",
      "      \"grad_norm\": 0.00015242579684127122,\n",
      "      \"learning_rate\": 6.817895400126025e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 156600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.868962475188255,\n",
      "      \"grad_norm\": 0.0011096170637756586,\n",
      "      \"learning_rate\": 6.754883427851292e-07,\n",
      "      \"loss\": 0.041,\n",
      "      \"step\": 156620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.870222754340086,\n",
      "      \"grad_norm\": 0.0008329693810082972,\n",
      "      \"learning_rate\": 6.691871455576559e-07,\n",
      "      \"loss\": 0.055,\n",
      "      \"step\": 156640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.871483033491918,\n",
      "      \"grad_norm\": 0.0030265911482274532,\n",
      "      \"learning_rate\": 6.628859483301828e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 156660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.87274331264375,\n",
      "      \"grad_norm\": 0.00014591433864552528,\n",
      "      \"learning_rate\": 6.565847511027096e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 156680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.874003591795583,\n",
      "      \"grad_norm\": 0.0004922538646496832,\n",
      "      \"learning_rate\": 6.502835538752364e-07,\n",
      "      \"loss\": 0.0111,\n",
      "      \"step\": 156700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.875263870947414,\n",
      "      \"grad_norm\": 0.005713973194360733,\n",
      "      \"learning_rate\": 6.439823566477631e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 156720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.876524150099247,\n",
      "      \"grad_norm\": 0.0026151975616812706,\n",
      "      \"learning_rate\": 6.376811594202898e-07,\n",
      "      \"loss\": 0.0119,\n",
      "      \"step\": 156740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.87778442925108,\n",
      "      \"grad_norm\": 0.00012514519039541483,\n",
      "      \"learning_rate\": 6.313799621928166e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 156760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.879044708402912,\n",
      "      \"grad_norm\": 0.00020648092322517186,\n",
      "      \"learning_rate\": 6.250787649653434e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 156780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.880304987554743,\n",
      "      \"grad_norm\": 0.001184588996693492,\n",
      "      \"learning_rate\": 6.187775677378703e-07,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 156800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.881565266706575,\n",
      "      \"grad_norm\": 0.0001550415763631463,\n",
      "      \"learning_rate\": 6.124763705103971e-07,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 156820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.882825545858408,\n",
      "      \"grad_norm\": 0.0005887510487809777,\n",
      "      \"learning_rate\": 6.061751732829238e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 156840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.88408582501024,\n",
      "      \"grad_norm\": 0.4051019549369812,\n",
      "      \"learning_rate\": 5.998739760554505e-07,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 156860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.885346104162071,\n",
      "      \"grad_norm\": 0.0005067931488156319,\n",
      "      \"learning_rate\": 5.935727788279773e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 156880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.886606383313904,\n",
      "      \"grad_norm\": 8.016511917114258,\n",
      "      \"learning_rate\": 5.872715816005041e-07,\n",
      "      \"loss\": 0.0309,\n",
      "      \"step\": 156900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.887866662465736,\n",
      "      \"grad_norm\": 0.01496957615017891,\n",
      "      \"learning_rate\": 5.809703843730309e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 156920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.889126941617569,\n",
      "      \"grad_norm\": 0.0014230638043954968,\n",
      "      \"learning_rate\": 5.746691871455578e-07,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 156940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.8903872207694,\n",
      "      \"grad_norm\": 0.0007206453592516482,\n",
      "      \"learning_rate\": 5.683679899180845e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 156960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.891647499921232,\n",
      "      \"grad_norm\": 0.6522142887115479,\n",
      "      \"learning_rate\": 5.620667926906112e-07,\n",
      "      \"loss\": 0.003,\n",
      "      \"step\": 156980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.892907779073065,\n",
      "      \"grad_norm\": 0.0025953177828341722,\n",
      "      \"learning_rate\": 5.55765595463138e-07,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 157000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.894168058224897,\n",
      "      \"grad_norm\": 0.00013433564163278788,\n",
      "      \"learning_rate\": 5.494643982356648e-07,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 157020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.895428337376728,\n",
      "      \"grad_norm\": 0.00016642193077132106,\n",
      "      \"learning_rate\": 5.431632010081916e-07,\n",
      "      \"loss\": 0.0039,\n",
      "      \"step\": 157040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.89668861652856,\n",
      "      \"grad_norm\": 0.00010961968655465171,\n",
      "      \"learning_rate\": 5.368620037807184e-07,\n",
      "      \"loss\": 0.0383,\n",
      "      \"step\": 157060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.897948895680393,\n",
      "      \"grad_norm\": 0.011890122666954994,\n",
      "      \"learning_rate\": 5.305608065532451e-07,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 157080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.899209174832226,\n",
      "      \"grad_norm\": 0.0008738990873098373,\n",
      "      \"learning_rate\": 5.242596093257719e-07,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 157100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.900469453984057,\n",
      "      \"grad_norm\": 0.0003173015429638326,\n",
      "      \"learning_rate\": 5.179584120982987e-07,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 157120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.90172973313589,\n",
      "      \"grad_norm\": 0.00128204096108675,\n",
      "      \"learning_rate\": 5.116572148708255e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 157140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.902990012287722,\n",
      "      \"grad_norm\": 0.00020717921142932028,\n",
      "      \"learning_rate\": 5.053560176433523e-07,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 157160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.904250291439554,\n",
      "      \"grad_norm\": 0.00013874786964152008,\n",
      "      \"learning_rate\": 4.990548204158791e-07,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 157180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.905510570591385,\n",
      "      \"grad_norm\": 0.005035486537963152,\n",
      "      \"learning_rate\": 4.927536231884058e-07,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 157200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.906770849743218,\n",
      "      \"grad_norm\": 0.001074145082384348,\n",
      "      \"learning_rate\": 4.864524259609325e-07,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 157220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.90803112889505,\n",
      "      \"grad_norm\": 15.177498817443848,\n",
      "      \"learning_rate\": 4.801512287334594e-07,\n",
      "      \"loss\": 0.019,\n",
      "      \"step\": 157240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.909291408046883,\n",
      "      \"grad_norm\": 0.6630449295043945,\n",
      "      \"learning_rate\": 4.7385003150598617e-07,\n",
      "      \"loss\": 0.0171,\n",
      "      \"step\": 157260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.910551687198714,\n",
      "      \"grad_norm\": 0.0001374439598293975,\n",
      "      \"learning_rate\": 4.67548834278513e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 157280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.911811966350546,\n",
      "      \"grad_norm\": 0.0009315409115515649,\n",
      "      \"learning_rate\": 4.612476370510397e-07,\n",
      "      \"loss\": 0.015,\n",
      "      \"step\": 157300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.913072245502379,\n",
      "      \"grad_norm\": 0.09868688136339188,\n",
      "      \"learning_rate\": 4.5494643982356646e-07,\n",
      "      \"loss\": 0.0379,\n",
      "      \"step\": 157320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.914332524654212,\n",
      "      \"grad_norm\": 0.008800010196864605,\n",
      "      \"learning_rate\": 4.486452425960933e-07,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 157340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.915592803806042,\n",
      "      \"grad_norm\": 0.0005666000070050359,\n",
      "      \"learning_rate\": 4.4234404536862004e-07,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 157360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.916853082957875,\n",
      "      \"grad_norm\": 0.0005985410534776747,\n",
      "      \"learning_rate\": 4.3604284814114686e-07,\n",
      "      \"loss\": 0.0175,\n",
      "      \"step\": 157380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.918113362109708,\n",
      "      \"grad_norm\": 0.00014299536996986717,\n",
      "      \"learning_rate\": 4.2974165091367367e-07,\n",
      "      \"loss\": 0.0063,\n",
      "      \"step\": 157400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.91937364126154,\n",
      "      \"grad_norm\": 0.00028230997850187123,\n",
      "      \"learning_rate\": 4.234404536862004e-07,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 157420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.92063392041337,\n",
      "      \"grad_norm\": 0.016590218991041183,\n",
      "      \"learning_rate\": 4.1713925645872715e-07,\n",
      "      \"loss\": 0.0031,\n",
      "      \"step\": 157440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.921894199565203,\n",
      "      \"grad_norm\": 0.000987179926596582,\n",
      "      \"learning_rate\": 4.1083805923125396e-07,\n",
      "      \"loss\": 0.0209,\n",
      "      \"step\": 157460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.923154478717036,\n",
      "      \"grad_norm\": 0.0009537137812003493,\n",
      "      \"learning_rate\": 4.045368620037807e-07,\n",
      "      \"loss\": 0.0223,\n",
      "      \"step\": 157480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.924414757868869,\n",
      "      \"grad_norm\": 0.0003376642125658691,\n",
      "      \"learning_rate\": 3.9823566477630754e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 157500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.9256750370207,\n",
      "      \"grad_norm\": 0.0007220174884423614,\n",
      "      \"learning_rate\": 3.9193446754883436e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 157520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.926935316172532,\n",
      "      \"grad_norm\": 0.00014957666280679405,\n",
      "      \"learning_rate\": 3.8563327032136107e-07,\n",
      "      \"loss\": 0.0094,\n",
      "      \"step\": 157540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.928195595324365,\n",
      "      \"grad_norm\": 0.0006684277323074639,\n",
      "      \"learning_rate\": 3.7933207309388783e-07,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 157560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.929455874476197,\n",
      "      \"grad_norm\": 0.0005055043147876859,\n",
      "      \"learning_rate\": 3.7303087586641465e-07,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 157580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.930716153628028,\n",
      "      \"grad_norm\": 0.031183239072561264,\n",
      "      \"learning_rate\": 3.667296786389414e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 157600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.93197643277986,\n",
      "      \"grad_norm\": 0.0006471541710197926,\n",
      "      \"learning_rate\": 3.6042848141146817e-07,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 157620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.933236711931693,\n",
      "      \"grad_norm\": 0.0002492721541784704,\n",
      "      \"learning_rate\": 3.54127284183995e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 157640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.934496991083526,\n",
      "      \"grad_norm\": 0.0003928910882677883,\n",
      "      \"learning_rate\": 3.4782608695652175e-07,\n",
      "      \"loss\": 0.0066,\n",
      "      \"step\": 157660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.935757270235356,\n",
      "      \"grad_norm\": 0.001709595788270235,\n",
      "      \"learning_rate\": 3.415248897290485e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 157680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.937017549387189,\n",
      "      \"grad_norm\": 0.08805067092180252,\n",
      "      \"learning_rate\": 3.3522369250157533e-07,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 157700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.938277828539022,\n",
      "      \"grad_norm\": 0.002520255045965314,\n",
      "      \"learning_rate\": 3.289224952741021e-07,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 157720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.939538107690854,\n",
      "      \"grad_norm\": 0.011104363948106766,\n",
      "      \"learning_rate\": 3.2262129804662886e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 157740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.940798386842685,\n",
      "      \"grad_norm\": 0.00013698454131372273,\n",
      "      \"learning_rate\": 3.1632010081915567e-07,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 157760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.942058665994518,\n",
      "      \"grad_norm\": 0.0006902054301463068,\n",
      "      \"learning_rate\": 3.1001890359168243e-07,\n",
      "      \"loss\": 0.021,\n",
      "      \"step\": 157780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.94331894514635,\n",
      "      \"grad_norm\": 0.00014062982518225908,\n",
      "      \"learning_rate\": 3.037177063642092e-07,\n",
      "      \"loss\": 0.0053,\n",
      "      \"step\": 157800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.944579224298183,\n",
      "      \"grad_norm\": 0.017910366877913475,\n",
      "      \"learning_rate\": 2.97416509136736e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 157820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.945839503450014,\n",
      "      \"grad_norm\": 0.05324394628405571,\n",
      "      \"learning_rate\": 2.911153119092628e-07,\n",
      "      \"loss\": 0.0071,\n",
      "      \"step\": 157840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.947099782601846,\n",
      "      \"grad_norm\": 0.036767974495887756,\n",
      "      \"learning_rate\": 2.8481411468178954e-07,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 157860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.948360061753679,\n",
      "      \"grad_norm\": 0.0002187312493333593,\n",
      "      \"learning_rate\": 2.7851291745431636e-07,\n",
      "      \"loss\": 0.0097,\n",
      "      \"step\": 157880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.949620340905511,\n",
      "      \"grad_norm\": 0.152391254901886,\n",
      "      \"learning_rate\": 2.722117202268431e-07,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 157900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.950880620057342,\n",
      "      \"grad_norm\": 0.019190259277820587,\n",
      "      \"learning_rate\": 2.659105229993699e-07,\n",
      "      \"loss\": 0.0215,\n",
      "      \"step\": 157920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.952140899209175,\n",
      "      \"grad_norm\": 0.00015786569565534592,\n",
      "      \"learning_rate\": 2.596093257718967e-07,\n",
      "      \"loss\": 0.0079,\n",
      "      \"step\": 157940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.953401178361007,\n",
      "      \"grad_norm\": 0.000266326213022694,\n",
      "      \"learning_rate\": 2.5330812854442346e-07,\n",
      "      \"loss\": 0.0096,\n",
      "      \"step\": 157960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.95466145751284,\n",
      "      \"grad_norm\": 0.07516319304704666,\n",
      "      \"learning_rate\": 2.470069313169502e-07,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 157980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.95592173666467,\n",
      "      \"grad_norm\": 0.01493501290678978,\n",
      "      \"learning_rate\": 2.4070573408947704e-07,\n",
      "      \"loss\": 0.0173,\n",
      "      \"step\": 158000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.957182015816503,\n",
      "      \"grad_norm\": 0.0008979990379884839,\n",
      "      \"learning_rate\": 2.3440453686200378e-07,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 158020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.958442294968336,\n",
      "      \"grad_norm\": 0.00015693395107518882,\n",
      "      \"learning_rate\": 2.2810333963453057e-07,\n",
      "      \"loss\": 0.0195,\n",
      "      \"step\": 158040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.959702574120168,\n",
      "      \"grad_norm\": 0.00025654942146502435,\n",
      "      \"learning_rate\": 2.2180214240705735e-07,\n",
      "      \"loss\": 0.0065,\n",
      "      \"step\": 158060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.960962853272,\n",
      "      \"grad_norm\": 0.22646990418434143,\n",
      "      \"learning_rate\": 2.1550094517958412e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 158080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.962223132423832,\n",
      "      \"grad_norm\": 0.000500781461596489,\n",
      "      \"learning_rate\": 2.091997479521109e-07,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 158100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.963483411575664,\n",
      "      \"grad_norm\": 0.00046740073594264686,\n",
      "      \"learning_rate\": 2.028985507246377e-07,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 158120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.964743690727497,\n",
      "      \"grad_norm\": 0.06152218580245972,\n",
      "      \"learning_rate\": 1.9659735349716446e-07,\n",
      "      \"loss\": 0.0231,\n",
      "      \"step\": 158140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.966003969879328,\n",
      "      \"grad_norm\": 0.004510209895670414,\n",
      "      \"learning_rate\": 1.9029615626969125e-07,\n",
      "      \"loss\": 0.0104,\n",
      "      \"step\": 158160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.96726424903116,\n",
      "      \"grad_norm\": 0.00029165600426495075,\n",
      "      \"learning_rate\": 1.83994959042218e-07,\n",
      "      \"loss\": 0.0064,\n",
      "      \"step\": 158180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.968524528182993,\n",
      "      \"grad_norm\": 0.0034493543207645416,\n",
      "      \"learning_rate\": 1.7769376181474483e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 158200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.969784807334825,\n",
      "      \"grad_norm\": 0.010027713142335415,\n",
      "      \"learning_rate\": 1.713925645872716e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 158220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.971045086486656,\n",
      "      \"grad_norm\": 0.17007867991924286,\n",
      "      \"learning_rate\": 1.6509136735979835e-07,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 158240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.972305365638489,\n",
      "      \"grad_norm\": 0.0011111469939351082,\n",
      "      \"learning_rate\": 1.5879017013232517e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 158260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.973565644790321,\n",
      "      \"grad_norm\": 0.00014565064338967204,\n",
      "      \"learning_rate\": 1.5248897290485193e-07,\n",
      "      \"loss\": 0.0135,\n",
      "      \"step\": 158280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.974825923942154,\n",
      "      \"grad_norm\": 0.002258319640532136,\n",
      "      \"learning_rate\": 1.461877756773787e-07,\n",
      "      \"loss\": 0.0049,\n",
      "      \"step\": 158300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.976086203093985,\n",
      "      \"grad_norm\": 0.30497658252716064,\n",
      "      \"learning_rate\": 1.3988657844990549e-07,\n",
      "      \"loss\": 0.0083,\n",
      "      \"step\": 158320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.977346482245817,\n",
      "      \"grad_norm\": 0.0009103618795052171,\n",
      "      \"learning_rate\": 1.3358538122243228e-07,\n",
      "      \"loss\": 0.0103,\n",
      "      \"step\": 158340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.97860676139765,\n",
      "      \"grad_norm\": 0.05550891160964966,\n",
      "      \"learning_rate\": 1.2728418399495904e-07,\n",
      "      \"loss\": 0.0271,\n",
      "      \"step\": 158360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.979867040549482,\n",
      "      \"grad_norm\": 0.0001335768320132047,\n",
      "      \"learning_rate\": 1.2098298676748583e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 158380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.981127319701313,\n",
      "      \"grad_norm\": 0.0001487507688580081,\n",
      "      \"learning_rate\": 1.1468178954001262e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 158400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.982387598853146,\n",
      "      \"grad_norm\": 0.00035553795169107616,\n",
      "      \"learning_rate\": 1.0838059231253938e-07,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 158420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.983647878004978,\n",
      "      \"grad_norm\": 0.0012297852663323283,\n",
      "      \"learning_rate\": 1.0207939508506616e-07,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 158440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.984908157156811,\n",
      "      \"grad_norm\": 0.0037942843046039343,\n",
      "      \"learning_rate\": 9.577819785759295e-08,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 158460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.986168436308642,\n",
      "      \"grad_norm\": 0.00034284230787307024,\n",
      "      \"learning_rate\": 8.947700063011972e-08,\n",
      "      \"loss\": 0.0143,\n",
      "      \"step\": 158480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.987428715460474,\n",
      "      \"grad_norm\": 0.0010448538232594728,\n",
      "      \"learning_rate\": 8.317580340264651e-08,\n",
      "      \"loss\": 0.0065,\n",
      "      \"step\": 158500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.988688994612307,\n",
      "      \"grad_norm\": 0.0008965049055404961,\n",
      "      \"learning_rate\": 7.687460617517329e-08,\n",
      "      \"loss\": 0.018,\n",
      "      \"step\": 158520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.98994927376414,\n",
      "      \"grad_norm\": 0.0005554458475671709,\n",
      "      \"learning_rate\": 7.057340894770006e-08,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 158540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.99120955291597,\n",
      "      \"grad_norm\": 0.002632287098094821,\n",
      "      \"learning_rate\": 6.427221172022685e-08,\n",
      "      \"loss\": 0.0419,\n",
      "      \"step\": 158560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.992469832067803,\n",
      "      \"grad_norm\": 0.0002138690324500203,\n",
      "      \"learning_rate\": 5.797101449275362e-08,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 158580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.993730111219636,\n",
      "      \"grad_norm\": 6.9726881980896,\n",
      "      \"learning_rate\": 5.1669817265280406e-08,\n",
      "      \"loss\": 0.0199,\n",
      "      \"step\": 158600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.994990390371468,\n",
      "      \"grad_norm\": 0.00020496231445576996,\n",
      "      \"learning_rate\": 4.536862003780719e-08,\n",
      "      \"loss\": 0.0,\n",
      "      \"step\": 158620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.996250669523299,\n",
      "      \"grad_norm\": 0.00043653754983097315,\n",
      "      \"learning_rate\": 3.9067422810333965e-08,\n",
      "      \"loss\": 0.0235,\n",
      "      \"step\": 158640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.997510948675131,\n",
      "      \"grad_norm\": 0.003000198397785425,\n",
      "      \"learning_rate\": 3.276622558286074e-08,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 158660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.998771227826964,\n",
      "      \"grad_norm\": 0.00798595231026411,\n",
      "      \"learning_rate\": 2.6465028355387524e-08,\n",
      "      \"loss\": 0.0269,\n",
      "      \"step\": 158680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 10.0,\n",
      "      \"grad_norm\": 4.799490125151351e-05,\n",
      "      \"learning_rate\": 2.0163831127914303e-08,\n",
      "      \"loss\": 0.0001,\n",
      "      \"step\": 158700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 10.0,\n",
      "      \"eval_accuracy\": 0.9998739839959675,\n",
      "      \"eval_f1\": 0.9998739919354839,\n",
      "      \"eval_loss\": 0.0010025477968156338,\n",
      "      \"eval_precision\": 0.9998109998109999,\n",
      "      \"eval_recall\": 0.9999369919979837,\n",
      "      \"eval_runtime\": 594.7864,\n",
      "      \"eval_samples_per_second\": 53.367,\n",
      "      \"eval_steps_per_second\": 6.671,\n",
      "      \"step\": 158700\n",
      "    }\n",
      "  ],\n",
      "  \"logging_steps\": 20,\n",
      "  \"max_steps\": 158700,\n",
      "  \"num_input_tokens_seen\": 0,\n",
      "  \"num_train_epochs\": 10,\n",
      "  \"save_steps\": 500,\n",
      "  \"stateful_callbacks\": {\n",
      "    \"TrainerControl\": {\n",
      "      \"args\": {\n",
      "        \"should_epoch_stop\": false,\n",
      "        \"should_evaluate\": false,\n",
      "        \"should_log\": false,\n",
      "        \"should_save\": true,\n",
      "        \"should_training_stop\": true\n",
      "      },\n",
      "      \"attributes\": {}\n",
      "    }\n",
      "  },\n",
      "  \"total_flos\": 4.2240782021685215e+19,\n",
      "  \"train_batch_size\": 8,\n",
      "  \"trial_name\": null,\n",
      "  \"trial_params\": null\n",
      "}\n",
      "Model loaded on cuda\n",
      "Raw logits: tensor([[ 6.3640, -6.6944]], device='cuda:0')\n",
      "Raw probabilities: tensor([[1.0000e+00, 2.1321e-06]], device='cuda:0')\n",
      "P(fake): 1.0000\n",
      "P(real): 0.0000\n",
      "Prediction: fake\n",
      "Confidence: 1.0000\n",
      "Probabilities - Fake: 1.0000, Real: 0.0000\n"
     ]
    }
   ],
   "source": [
    "##### MODEL EVALUATION AND PREDICTION\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define the checkpoint path\n",
    "checkpoint_path = \"runs/ast_classifier/checkpoint-158700\"\n",
    "\n",
    "# Load the model and feature extractor\n",
    "model = ASTForAudioClassification.from_pretrained(checkpoint_path)\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Look for training history\n",
    "trainer_state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "if os.path.exists(trainer_state_path):\n",
    "    with open(trainer_state_path, \"r\") as f:\n",
    "        trainer_state = json.load(f)\n",
    "    print(\"\\nTraining metrics from trainer_state.json:\")\n",
    "    print(json.dumps(trainer_state, indent=2))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "import librosa\n",
    "def predict_audio(file_path, model, feature_extractor, device=\"cuda\"):\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(file_path, sr=feature_extractor.sampling_rate)\n",
    "\n",
    "    # Preprocess the audio\n",
    "    inputs = feature_extractor(\n",
    "        audio,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    print(f\"Raw logits: {logits}\")\n",
    "    print(f\"Raw probabilities: {probabilities}\")\n",
    "\n",
    "    # Get predicted class (0 for fake, 1 for real)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "    print(f\"P(fake): {probabilities[0][0].item():.4f}\")\n",
    "    print(f\"P(real): {probabilities[0][1].item():.4f}\")\n",
    "\n",
    "    # Map class index to label\n",
    "    label = \"fake\" if predicted_class == 0 else \"real\"\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"confidence\": confidence,\n",
    "        \"probabilities\": {\n",
    "            \"fake\": probabilities[0][0].item(),\n",
    "            \"real\": probabilities[0][1].item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "audio_file_path = \"./cut/HitlerAI(10).wav\" # Adjust the path as needed\n",
    "result = predict_audio(audio_file_path, model, feature_extractor, device)\n",
    "\n",
    "print(f\"Prediction: {result['label']}\")\n",
    "print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "print(f\"Probabilities - Fake: {result['probabilities']['fake']:.4f}, Real: {result['probabilities']['real']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "835cc84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./cut\\HitlerAI(1).wav\n",
      "Saved: ./cut\\HitlerAI(2).wav\n",
      "Saved: ./cut\\HitlerAI(3).wav\n",
      "Saved: ./cut\\HitlerAI(4).wav\n",
      "Saved: ./cut\\HitlerAI(5).wav\n",
      "Saved: ./cut\\HitlerAI(6).wav\n",
      "Saved: ./cut\\HitlerAI(7).wav\n",
      "Saved: ./cut\\HitlerAI(8).wav\n",
      "Saved: ./cut\\HitlerAI(9).wav\n",
      "Saved: ./cut\\HitlerAI(10).wav\n",
      "Saved: ./cut\\HitlerAI(11).wav\n",
      "Saved: ./cut\\HitlerAI(12).wav\n",
      "Saved: ./cut\\HitlerAI(13).wav\n",
      "Saved: ./cut\\HitlerAI(14).wav\n",
      "Saved: ./cut\\HitlerAI(15).wav\n",
      "Saved: ./cut\\HitlerAI(16).wav\n",
      "Saved: ./cut\\HitlerAI(17).wav\n",
      "Saved: ./cut\\HitlerAI(18).wav\n",
      "Saved: ./cut\\HitlerAI(19).wav\n",
      "Saved: ./cut\\HitlerAI(20).wav\n",
      "Saved: ./cut\\HitlerAI(21).wav\n",
      "Saved: ./cut\\HitlerAI(22).wav\n",
      "Saved: ./cut\\HitlerAI(23).wav\n",
      "Saved: ./cut\\HitlerAI(24).wav\n",
      "Saved: ./cut\\HitlerAI(25).wav\n",
      "Saved: ./cut\\HitlerAI(26).wav\n"
     ]
    }
   ],
   "source": [
    "#### AUDIO CUTTING SCRIPT\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def cut_audio(input_path, output_dir=\"./cut\", clip_duration=5):\n",
    "     # Ensure output directory exists\n",
    "     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "     # Get file name and extension\n",
    "     base_name = os.path.basename(input_path)\n",
    "     name, ext = os.path.splitext(base_name)\n",
    "\n",
    "     # Load audio\n",
    "     audio, sr = librosa.load(input_path, sr=None)\n",
    "     total_duration = librosa.get_duration(y=audio, sr=sr)\n",
    "     clip_samples = int(clip_duration * sr)\n",
    "     num_clips = int(total_duration // clip_duration) + (1 if total_duration % clip_duration > 0 else 0)\n",
    "\n",
    "     for i in range(num_clips):\n",
    "          start_sample = i * clip_samples\n",
    "          end_sample = min((i + 1) * clip_samples, len(audio))\n",
    "          clip_audio = audio[start_sample:end_sample]\n",
    "          out_path = os.path.join(output_dir, f\"{name}({i+1}){ext}\")\n",
    "          sf.write(out_path, clip_audio, sr)\n",
    "          print(f\"Saved: {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     if len(sys.argv) < 2:\n",
    "          print(\"Usage: python cut_audio.py <path_to_audio_file>\")\n",
    "          sys.exit(1)\n",
    "     audio_path = \"./dataset/HitlerAI.wav\"  # Adjust the path as needed\n",
    "     if not os.path.exists(audio_path):\n",
    "          print(f\"File not found: {audio_path}\")\n",
    "          sys.exit(1)\n",
    "     cut_audio(audio_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
