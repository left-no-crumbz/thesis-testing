{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27471c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import ASTFeatureExtractor\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.profiler\n",
    "import numpy as np\n",
    "from datasets import Audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3821aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.0\n",
      "System: Windows 10\n",
      "PyTorch version: 2.7.1+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"System: {platform.system()} {platform.release()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ac471d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb13f3103514affbad3577d141d823b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/13956 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441d6b52beef4ebca85be8fe371a0bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2826 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f59fcb91d5241aaafaaa9213c5001ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1088 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"audiofolder\", data_dir=\"./for-2seconds\")\n",
    "\n",
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model, num_mel_bins=64, max_length=507)\n",
    "\n",
    "\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "SAMPLING_RATE = feature_extractor.sampling_rate\n",
    "num_labels = len(np.unique(dataset[\"train\"][\"label\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f317f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 13956\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 2826\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 1088\n",
      "    })\n",
      "})\n",
      "dataset train features: {'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "dataset test features: {'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "dataset validation features: {'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'label': ClassLabel(names=['fake', 'real'], id=None)}\n",
      "model_input_name: input_values\n",
      "SAMPLING_RATE: 16000\n",
      "dataset[\"train\"][0]: {'audio': {'path': 'C:\\\\Users\\\\Crumbz\\\\Desktop\\\\4TH-YEAR-1ST-SEM\\\\THESIS\\\\thesis-testing\\\\for-2seconds\\\\training\\\\fake\\\\file10005.mp3.wav_16k.wav_norm.wav_mono.wav_silence.wav_2sec.wav', 'array': array([ 0.10552979,  0.11013794,  0.00952148, ..., -0.20837402,\n",
      "       -0.25552368, -0.24255371]), 'sampling_rate': 16000}, 'label': 0}\n",
      "num_labels: 2\n",
      "dataset columns: ['audio', 'label']\n"
     ]
    }
   ],
   "source": [
    "print('dataset:', dataset)\n",
    "print('dataset train features:', dataset['train'].features)\n",
    "print('dataset test features:', dataset['test'].features)\n",
    "print('dataset validation features:', dataset['validation'].features)\n",
    "print('model_input_name:', model_input_name)\n",
    "print('SAMPLING_RATE:', SAMPLING_RATE)\n",
    "print('dataset[\"train\"][0]:', dataset['train'][0])\n",
    "print('num_labels:', num_labels)\n",
    "print('dataset columns:', dataset['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18162b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  -1.6385198831558228\n",
      "std:  3.336308717727661\n"
     ]
    }
   ],
   "source": [
    "# calculate values for normalization\n",
    "feature_extractor.do_normalize = False\n",
    "\n",
    "# Initialize running statistics\n",
    "n = 0\n",
    "mean = 0.0\n",
    "M2 = 0.0  # For running variance calculation\n",
    "\n",
    "def preprocess_audio(batch):\n",
    "    wavs = [audio[\"array\"] for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\", return_attention_mask=True, max_length=507)\n",
    "    return {\n",
    "        model_input_name: inputs.get(model_input_name),\n",
    "        \"labels\": torch.tensor(batch[\"label\"])\n",
    "    }\n",
    "\n",
    "dataset = dataset.rename_column(\"audio\", \"input_values\")\n",
    "dataset[\"train\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "\n",
    "# Process in batches to save memory\n",
    "for batch in dataset[\"train\"]:\n",
    "    \n",
    "    audio_input = batch[model_input_name]\n",
    "    batch_size = audio_input.shape[0]\n",
    "\n",
    "    # Calculate batch statistics\n",
    "    batch_mean = torch.mean(audio_input)\n",
    "    batch_variance = torch.var(audio_input, unbiased=False)  # Use N instead of N-1 for population variance\n",
    "    \n",
    "    # Update running statistics using Welford's online algorithm\n",
    "    delta = batch_mean - mean\n",
    "    mean += delta * batch_size / (n + batch_size)\n",
    "    M2 += batch_variance * batch_size + delta ** 2 * n * batch_size / (n + batch_size)\n",
    "    n += batch_size\n",
    "\n",
    "# Calculate final statistics\n",
    "feature_extractor.mean = mean.item()\n",
    "feature_extractor.std = torch.sqrt(M2 / n).item()  # Population standard deviation\n",
    "feature_extractor.do_normalize = True\n",
    "\n",
    "print('mean: ', feature_extractor.mean)\n",
    "print('std: ', feature_extractor.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af852b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianSNR, GainTransition, Gain, ClippingDistortion, TimeStretch, PitchShift\n",
    "\n",
    "audio_augmentations = Compose([\n",
    "    AddGaussianSNR(min_snr_db=10, max_snr_db=20),\n",
    "    Gain(min_gain_db=-6, max_gain_db=6),\n",
    "    GainTransition(min_gain_db=-6, max_gain_db=6, min_duration=0.01, max_duration=0.3, duration_unit=\"fraction\"),\n",
    "    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=30, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4),\n",
    "], p=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd7f4b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_values', 'label'],\n",
      "        num_rows: 13956\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_values', 'label'],\n",
      "        num_rows: 2826\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_values', 'label'],\n",
      "        num_rows: 1088\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def preprocess_audio_with_transforms(batch):\n",
    "    # we apply augmentations on each waveform\n",
    "    wavs = [audio_augmentations(audio[\"array\"], sample_rate=SAMPLING_RATE) for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\", return_attention_mask=True, max_length=507)\n",
    "    return {\n",
    "        model_input_name: inputs.get(model_input_name),\n",
    "        \"labels\": torch.tensor(batch[\"label\"])\n",
    "    }\n",
    "\n",
    "print(dataset)\n",
    "# Cast the audio column to the appropriate feature type and rename it\n",
    "dataset = dataset.cast_column(\"input_values\", Audio(sampling_rate=feature_extractor.sampling_rate))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88736686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with augmentations on the training set\n",
    "dataset[\"train\"].set_transform(preprocess_audio_with_transforms, output_all_columns=False)\n",
    "# w/o augmentations on the test set\n",
    "dataset[\"test\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "dataset[\"validation\"].set_transform(preprocess_audio, output_all_columns=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dd71c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_values': {'path': 'C:\\\\Users\\\\Crumbz\\\\Desktop\\\\4TH-YEAR-1ST-SEM\\\\THESIS\\\\thesis-testing\\\\for-2seconds\\\\training\\\\fake\\\\file10005.mp3.wav_16k.wav_norm.wav_mono.wav_silence.wav_2sec.wav', 'array': array([ 0.10552979,  0.11013794,  0.00952148, ..., -0.20837402,\n",
      "       -0.25552368, -0.24255371]), 'sampling_rate': 16000}, 'label': 0}\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "temp_ds = dataset['train'].with_format(None)  # This removes any applied transforms\n",
    "temp_ds_test = dataset['test'].with_format(None)  # This removes any applied transforms\n",
    "\n",
    "print(temp_ds[0])\n",
    "unique_labels = sorted(set(temp_ds[\"label\"] + temp_ds_test[\"label\"]))\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feb51aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- audio_spectrogram_transformer.embeddings.position_embeddings: found shape torch.Size([1, 1214, 768]) in the checkpoint and torch.Size([1, 252, 768]) in the model instantiated\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ASTConfig, ASTForAudioClassification\n",
    "\n",
    "# Load configuration from the pretrained model\n",
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "\n",
    "# Update configuration with the number of labels in our dataset\n",
    "config.num_mel_bins = 64  # Make sure this matches your feature extractor\n",
    "config.max_length = 507   # Or whatever your sequence length is\n",
    "config.num_labels = num_labels\n",
    "config.label2id = {\"fake\": 0, \"real\": 1}\n",
    "config.id2label = {0: \"fake\", 1: \"real\"}\n",
    "\n",
    "# Initialize the model with the updated configuration\n",
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    pretrained_model,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59a04121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Crumbz\\Desktop\\4TH-YEAR-1ST-SEM\\THESIS\\thesis-testing\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1604: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Configure training run with TrainingArguments class\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./runs/ast_classifier\",\n",
    "    logging_dir=\"./logs/ast_classifier\",\n",
    "    report_to=\"tensorboard\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,  # Start with 8, can increase if memory allows\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    dataloader_num_workers=0,  # Changed from 4 to 0 for Windows\n",
    "    dataloader_pin_memory=False,  # Disable pin_memory on Windows\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_steps=20,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d070a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "AVERAGE = \"macro\" if config.num_labels > 2 else \"binary\"\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    metrics = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "    metrics.update(precision.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a8de353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Crumbz\\AppData\\Local\\Temp\\ipykernel_9992\\3091343712.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, DataCollatorWithPadding\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize the data collator\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=feature_extractor,  # Your feature extractor acts as the tokenizer\n",
    "    padding=True,\n",
    "    max_length=config.max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aac110b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Crumbz\\Desktop\\4TH-YEAR-1ST-SEM\\THESIS\\thesis-testing\\.venv\\Lib\\site-packages\\audiomentations\\core\\transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input shape: torch.Size([507, 64])\n",
      "Forward pass successful!\n"
     ]
    }
   ],
   "source": [
    "# Add this before training to verify shapes\n",
    "sample = next(iter(dataset[\"train\"]))\n",
    "print(f\"Sample input shape: {sample['input_values'].shape}\")\n",
    "\n",
    "# Forward pass to check for errors\n",
    "with torch.no_grad():\n",
    "    output = model(sample[\"input_values\"].unsqueeze(0))\n",
    "print(\"Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "300ab2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config: ASTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ASTForAudioClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"frequency_stride\": 10,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"fake\",\n",
      "    \"1\": \"real\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"fake\": 0,\n",
      "    \"real\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_length\": 507,\n",
      "  \"model_type\": \"audio-spectrogram-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_mel_bins\": 64,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"time_stride\": 10,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.1\"\n",
      "}\n",
      "\n",
      "Sample input shape: torch.Size([507, 64])\n",
      "Model's expected input shape: 507\n",
      "Feature extractor config: ASTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"ASTFeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"max_length\": 507,\n",
      "  \"mean\": -1.6385198831558228,\n",
      "  \"num_mel_bins\": 64,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000,\n",
      "  \"std\": 3.336308717727661\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model config: {model.config}\")\n",
    "print(f\"Sample input shape: {sample['input_values'].shape}\")\n",
    "print(f\"Model's expected input shape: {model.config.max_length}\")\n",
    "print(f\"Feature extractor config: {feature_extractor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0103e3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "print(f\"Using device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "     print(f\"GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b32f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train(resume_from_checkpoint=False) #Set to False when no checkpoint is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03b94859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "     print(f\"GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "     print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48022aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training metrics from trainer_state.json:\n",
      "{\n",
      "  \"best_global_step\": 6111,\n",
      "  \"best_metric\": 0.9384191176470589,\n",
      "  \"best_model_checkpoint\": \"./runs/ast_classifier\\\\checkpoint-6111\",\n",
      "  \"epoch\": 10.0,\n",
      "  \"eval_steps\": 500,\n",
      "  \"global_step\": 8730,\n",
      "  \"is_hyper_param_search\": false,\n",
      "  \"is_local_process_zero\": true,\n",
      "  \"is_world_process_zero\": true,\n",
      "  \"log_history\": [\n",
      "    {\n",
      "      \"epoch\": 0.022922636103151862,\n",
      "      \"grad_norm\": 21.214120864868164,\n",
      "      \"learning_rate\": 4.990263459335625e-05,\n",
      "      \"loss\": 0.7607,\n",
      "      \"step\": 20\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.045845272206303724,\n",
      "      \"grad_norm\": 15.751541137695312,\n",
      "      \"learning_rate\": 4.9788087056128296e-05,\n",
      "      \"loss\": 0.4327,\n",
      "      \"step\": 40\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.06876790830945559,\n",
      "      \"grad_norm\": 6.153901100158691,\n",
      "      \"learning_rate\": 4.967353951890035e-05,\n",
      "      \"loss\": 0.4689,\n",
      "      \"step\": 60\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.09169054441260745,\n",
      "      \"grad_norm\": 4.759942054748535,\n",
      "      \"learning_rate\": 4.95589919816724e-05,\n",
      "      \"loss\": 0.3048,\n",
      "      \"step\": 80\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.11461318051575932,\n",
      "      \"grad_norm\": 8.46361255645752,\n",
      "      \"learning_rate\": 4.9444444444444446e-05,\n",
      "      \"loss\": 0.2614,\n",
      "      \"step\": 100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.13753581661891118,\n",
      "      \"grad_norm\": 1.7147080898284912,\n",
      "      \"learning_rate\": 4.93298969072165e-05,\n",
      "      \"loss\": 0.239,\n",
      "      \"step\": 120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.16045845272206305,\n",
      "      \"grad_norm\": 7.539628982543945,\n",
      "      \"learning_rate\": 4.921534936998854e-05,\n",
      "      \"loss\": 0.33,\n",
      "      \"step\": 140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.1833810888252149,\n",
      "      \"grad_norm\": 6.304424285888672,\n",
      "      \"learning_rate\": 4.9100801832760597e-05,\n",
      "      \"loss\": 0.2196,\n",
      "      \"step\": 160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.20630372492836677,\n",
      "      \"grad_norm\": 22.633914947509766,\n",
      "      \"learning_rate\": 4.8986254295532644e-05,\n",
      "      \"loss\": 0.2246,\n",
      "      \"step\": 180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.22922636103151864,\n",
      "      \"grad_norm\": 7.912237167358398,\n",
      "      \"learning_rate\": 4.88717067583047e-05,\n",
      "      \"loss\": 0.2117,\n",
      "      \"step\": 200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2521489971346705,\n",
      "      \"grad_norm\": 14.367006301879883,\n",
      "      \"learning_rate\": 4.875715922107675e-05,\n",
      "      \"loss\": 0.3521,\n",
      "      \"step\": 220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.27507163323782235,\n",
      "      \"grad_norm\": 1.2276074886322021,\n",
      "      \"learning_rate\": 4.86426116838488e-05,\n",
      "      \"loss\": 0.2351,\n",
      "      \"step\": 240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.2979942693409742,\n",
      "      \"grad_norm\": 5.735454082489014,\n",
      "      \"learning_rate\": 4.852806414662085e-05,\n",
      "      \"loss\": 0.2474,\n",
      "      \"step\": 260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3209169054441261,\n",
      "      \"grad_norm\": 2.9812912940979004,\n",
      "      \"learning_rate\": 4.84135166093929e-05,\n",
      "      \"loss\": 0.2566,\n",
      "      \"step\": 280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3438395415472779,\n",
      "      \"grad_norm\": 8.149778366088867,\n",
      "      \"learning_rate\": 4.829896907216495e-05,\n",
      "      \"loss\": 0.4507,\n",
      "      \"step\": 300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.3667621776504298,\n",
      "      \"grad_norm\": 18.92896270751953,\n",
      "      \"learning_rate\": 4.8184421534937e-05,\n",
      "      \"loss\": 0.226,\n",
      "      \"step\": 320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.38968481375358166,\n",
      "      \"grad_norm\": 3.7301480770111084,\n",
      "      \"learning_rate\": 4.8069873997709055e-05,\n",
      "      \"loss\": 0.2847,\n",
      "      \"step\": 340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.41260744985673353,\n",
      "      \"grad_norm\": 6.449732780456543,\n",
      "      \"learning_rate\": 4.79553264604811e-05,\n",
      "      \"loss\": 0.2075,\n",
      "      \"step\": 360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4355300859598854,\n",
      "      \"grad_norm\": 5.394011974334717,\n",
      "      \"learning_rate\": 4.784077892325316e-05,\n",
      "      \"loss\": 0.1943,\n",
      "      \"step\": 380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4584527220630373,\n",
      "      \"grad_norm\": 13.62792682647705,\n",
      "      \"learning_rate\": 4.7726231386025205e-05,\n",
      "      \"loss\": 0.2234,\n",
      "      \"step\": 400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.4813753581661891,\n",
      "      \"grad_norm\": 18.550609588623047,\n",
      "      \"learning_rate\": 4.761168384879725e-05,\n",
      "      \"loss\": 0.1511,\n",
      "      \"step\": 420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.504297994269341,\n",
      "      \"grad_norm\": 19.077356338500977,\n",
      "      \"learning_rate\": 4.74971363115693e-05,\n",
      "      \"loss\": 0.1773,\n",
      "      \"step\": 440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5272206303724928,\n",
      "      \"grad_norm\": 0.5022916793823242,\n",
      "      \"learning_rate\": 4.7382588774341355e-05,\n",
      "      \"loss\": 0.1221,\n",
      "      \"step\": 460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5501432664756447,\n",
      "      \"grad_norm\": 11.169661521911621,\n",
      "      \"learning_rate\": 4.72680412371134e-05,\n",
      "      \"loss\": 0.2707,\n",
      "      \"step\": 480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5730659025787965,\n",
      "      \"grad_norm\": 5.993165493011475,\n",
      "      \"learning_rate\": 4.715349369988545e-05,\n",
      "      \"loss\": 0.2281,\n",
      "      \"step\": 500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.5959885386819485,\n",
      "      \"grad_norm\": 5.472165107727051,\n",
      "      \"learning_rate\": 4.7038946162657506e-05,\n",
      "      \"loss\": 0.1333,\n",
      "      \"step\": 520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6189111747851003,\n",
      "      \"grad_norm\": 19.630828857421875,\n",
      "      \"learning_rate\": 4.6924398625429554e-05,\n",
      "      \"loss\": 0.1646,\n",
      "      \"step\": 540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6418338108882522,\n",
      "      \"grad_norm\": 10.943011283874512,\n",
      "      \"learning_rate\": 4.680985108820161e-05,\n",
      "      \"loss\": 0.1697,\n",
      "      \"step\": 560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.664756446991404,\n",
      "      \"grad_norm\": 1.4663958549499512,\n",
      "      \"learning_rate\": 4.6695303550973656e-05,\n",
      "      \"loss\": 0.1587,\n",
      "      \"step\": 580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.6876790830945558,\n",
      "      \"grad_norm\": 4.3210248947143555,\n",
      "      \"learning_rate\": 4.658075601374571e-05,\n",
      "      \"loss\": 0.1555,\n",
      "      \"step\": 600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7106017191977078,\n",
      "      \"grad_norm\": 3.836017608642578,\n",
      "      \"learning_rate\": 4.646620847651776e-05,\n",
      "      \"loss\": 0.2784,\n",
      "      \"step\": 620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7335243553008596,\n",
      "      \"grad_norm\": 12.164702415466309,\n",
      "      \"learning_rate\": 4.635166093928981e-05,\n",
      "      \"loss\": 0.1195,\n",
      "      \"step\": 640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7564469914040115,\n",
      "      \"grad_norm\": 26.954496383666992,\n",
      "      \"learning_rate\": 4.623711340206186e-05,\n",
      "      \"loss\": 0.1929,\n",
      "      \"step\": 660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.7793696275071633,\n",
      "      \"grad_norm\": 0.08518917113542557,\n",
      "      \"learning_rate\": 4.612256586483391e-05,\n",
      "      \"loss\": 0.1727,\n",
      "      \"step\": 680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8022922636103151,\n",
      "      \"grad_norm\": 15.867758750915527,\n",
      "      \"learning_rate\": 4.600801832760596e-05,\n",
      "      \"loss\": 0.2791,\n",
      "      \"step\": 700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8252148997134671,\n",
      "      \"grad_norm\": 0.30856558680534363,\n",
      "      \"learning_rate\": 4.5893470790378005e-05,\n",
      "      \"loss\": 0.1269,\n",
      "      \"step\": 720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8481375358166189,\n",
      "      \"grad_norm\": 2.056345224380493,\n",
      "      \"learning_rate\": 4.577892325315006e-05,\n",
      "      \"loss\": 0.1313,\n",
      "      \"step\": 740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8710601719197708,\n",
      "      \"grad_norm\": 3.601426362991333,\n",
      "      \"learning_rate\": 4.566437571592211e-05,\n",
      "      \"loss\": 0.244,\n",
      "      \"step\": 760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.8939828080229226,\n",
      "      \"grad_norm\": 5.313527584075928,\n",
      "      \"learning_rate\": 4.554982817869416e-05,\n",
      "      \"loss\": 0.1647,\n",
      "      \"step\": 780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9169054441260746,\n",
      "      \"grad_norm\": 1.9218894243240356,\n",
      "      \"learning_rate\": 4.543528064146621e-05,\n",
      "      \"loss\": 0.0721,\n",
      "      \"step\": 800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9398280802292264,\n",
      "      \"grad_norm\": 0.045638129115104675,\n",
      "      \"learning_rate\": 4.532073310423826e-05,\n",
      "      \"loss\": 0.1774,\n",
      "      \"step\": 820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9627507163323782,\n",
      "      \"grad_norm\": 7.011731147766113,\n",
      "      \"learning_rate\": 4.520618556701031e-05,\n",
      "      \"loss\": 0.2193,\n",
      "      \"step\": 840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 0.9856733524355301,\n",
      "      \"grad_norm\": 10.767053604125977,\n",
      "      \"learning_rate\": 4.509163802978236e-05,\n",
      "      \"loss\": 0.2053,\n",
      "      \"step\": 860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0,\n",
      "      \"eval_accuracy\": 0.7941176470588235,\n",
      "      \"eval_f1\": 0.746031746031746,\n",
      "      \"eval_loss\": 0.5705437660217285,\n",
      "      \"eval_precision\": 0.9733727810650887,\n",
      "      \"eval_recall\": 0.6047794117647058,\n",
      "      \"eval_runtime\": 6.3239,\n",
      "      \"eval_samples_per_second\": 172.047,\n",
      "      \"eval_steps_per_second\": 21.506,\n",
      "      \"step\": 873\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.008022922636103,\n",
      "      \"grad_norm\": 12.148658752441406,\n",
      "      \"learning_rate\": 4.4977090492554415e-05,\n",
      "      \"loss\": 0.134,\n",
      "      \"step\": 880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.030945558739255,\n",
      "      \"grad_norm\": 7.404831409454346,\n",
      "      \"learning_rate\": 4.486254295532646e-05,\n",
      "      \"loss\": 0.1568,\n",
      "      \"step\": 900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.053868194842407,\n",
      "      \"grad_norm\": 24.122270584106445,\n",
      "      \"learning_rate\": 4.474799541809852e-05,\n",
      "      \"loss\": 0.1253,\n",
      "      \"step\": 920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0767908309455587,\n",
      "      \"grad_norm\": 12.87827205657959,\n",
      "      \"learning_rate\": 4.4633447880870566e-05,\n",
      "      \"loss\": 0.2173,\n",
      "      \"step\": 940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.0997134670487105,\n",
      "      \"grad_norm\": 0.8668273687362671,\n",
      "      \"learning_rate\": 4.4518900343642613e-05,\n",
      "      \"loss\": 0.0678,\n",
      "      \"step\": 960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1226361031518626,\n",
      "      \"grad_norm\": 28.182552337646484,\n",
      "      \"learning_rate\": 4.440435280641466e-05,\n",
      "      \"loss\": 0.0975,\n",
      "      \"step\": 980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1455587392550144,\n",
      "      \"grad_norm\": 2.135047674179077,\n",
      "      \"learning_rate\": 4.4289805269186716e-05,\n",
      "      \"loss\": 0.2378,\n",
      "      \"step\": 1000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.1684813753581662,\n",
      "      \"grad_norm\": 7.991982936859131,\n",
      "      \"learning_rate\": 4.4175257731958764e-05,\n",
      "      \"loss\": 0.168,\n",
      "      \"step\": 1020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.191404011461318,\n",
      "      \"grad_norm\": 8.475685119628906,\n",
      "      \"learning_rate\": 4.406071019473081e-05,\n",
      "      \"loss\": 0.113,\n",
      "      \"step\": 1040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2143266475644698,\n",
      "      \"grad_norm\": 9.167315483093262,\n",
      "      \"learning_rate\": 4.3946162657502866e-05,\n",
      "      \"loss\": 0.1641,\n",
      "      \"step\": 1060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2372492836676217,\n",
      "      \"grad_norm\": 1.0061261653900146,\n",
      "      \"learning_rate\": 4.3831615120274914e-05,\n",
      "      \"loss\": 0.1216,\n",
      "      \"step\": 1080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2601719197707737,\n",
      "      \"grad_norm\": 4.479187965393066,\n",
      "      \"learning_rate\": 4.371706758304697e-05,\n",
      "      \"loss\": 0.1277,\n",
      "      \"step\": 1100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.2830945558739255,\n",
      "      \"grad_norm\": 0.457811564207077,\n",
      "      \"learning_rate\": 4.360252004581902e-05,\n",
      "      \"loss\": 0.1154,\n",
      "      \"step\": 1120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3060171919770773,\n",
      "      \"grad_norm\": 15.195171356201172,\n",
      "      \"learning_rate\": 4.348797250859107e-05,\n",
      "      \"loss\": 0.094,\n",
      "      \"step\": 1140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3289398280802291,\n",
      "      \"grad_norm\": 6.175048828125,\n",
      "      \"learning_rate\": 4.337342497136312e-05,\n",
      "      \"loss\": 0.1518,\n",
      "      \"step\": 1160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3518624641833812,\n",
      "      \"grad_norm\": 1.2738949060440063,\n",
      "      \"learning_rate\": 4.325887743413517e-05,\n",
      "      \"loss\": 0.1957,\n",
      "      \"step\": 1180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.374785100286533,\n",
      "      \"grad_norm\": 13.223384857177734,\n",
      "      \"learning_rate\": 4.314432989690722e-05,\n",
      "      \"loss\": 0.1232,\n",
      "      \"step\": 1200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.3977077363896848,\n",
      "      \"grad_norm\": 10.977272033691406,\n",
      "      \"learning_rate\": 4.302978235967927e-05,\n",
      "      \"loss\": 0.1145,\n",
      "      \"step\": 1220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4206303724928366,\n",
      "      \"grad_norm\": 0.10785717517137527,\n",
      "      \"learning_rate\": 4.291523482245132e-05,\n",
      "      \"loss\": 0.0985,\n",
      "      \"step\": 1240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4435530085959885,\n",
      "      \"grad_norm\": 7.453568458557129,\n",
      "      \"learning_rate\": 4.2800687285223366e-05,\n",
      "      \"loss\": 0.1165,\n",
      "      \"step\": 1260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4664756446991403,\n",
      "      \"grad_norm\": 0.021318521350622177,\n",
      "      \"learning_rate\": 4.268613974799542e-05,\n",
      "      \"loss\": 0.125,\n",
      "      \"step\": 1280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.4893982808022923,\n",
      "      \"grad_norm\": 24.0463809967041,\n",
      "      \"learning_rate\": 4.257159221076747e-05,\n",
      "      \"loss\": 0.1729,\n",
      "      \"step\": 1300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5123209169054441,\n",
      "      \"grad_norm\": 0.11226465553045273,\n",
      "      \"learning_rate\": 4.245704467353952e-05,\n",
      "      \"loss\": 0.113,\n",
      "      \"step\": 1320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.535243553008596,\n",
      "      \"grad_norm\": 26.140771865844727,\n",
      "      \"learning_rate\": 4.234249713631157e-05,\n",
      "      \"loss\": 0.0785,\n",
      "      \"step\": 1340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.558166189111748,\n",
      "      \"grad_norm\": 2.2360401153564453,\n",
      "      \"learning_rate\": 4.222794959908362e-05,\n",
      "      \"loss\": 0.1534,\n",
      "      \"step\": 1360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.5810888252148998,\n",
      "      \"grad_norm\": 0.31663164496421814,\n",
      "      \"learning_rate\": 4.211340206185567e-05,\n",
      "      \"loss\": 0.1245,\n",
      "      \"step\": 1380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6040114613180516,\n",
      "      \"grad_norm\": 10.919252395629883,\n",
      "      \"learning_rate\": 4.199885452462772e-05,\n",
      "      \"loss\": 0.066,\n",
      "      \"step\": 1400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6269340974212034,\n",
      "      \"grad_norm\": 5.095860958099365,\n",
      "      \"learning_rate\": 4.1884306987399776e-05,\n",
      "      \"loss\": 0.1358,\n",
      "      \"step\": 1420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6498567335243552,\n",
      "      \"grad_norm\": 2.8114752769470215,\n",
      "      \"learning_rate\": 4.1769759450171824e-05,\n",
      "      \"loss\": 0.1861,\n",
      "      \"step\": 1440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.672779369627507,\n",
      "      \"grad_norm\": 5.384004592895508,\n",
      "      \"learning_rate\": 4.165521191294388e-05,\n",
      "      \"loss\": 0.1172,\n",
      "      \"step\": 1460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.6957020057306589,\n",
      "      \"grad_norm\": 9.045029640197754,\n",
      "      \"learning_rate\": 4.1540664375715926e-05,\n",
      "      \"loss\": 0.0768,\n",
      "      \"step\": 1480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.718624641833811,\n",
      "      \"grad_norm\": 0.1181814894080162,\n",
      "      \"learning_rate\": 4.1426116838487974e-05,\n",
      "      \"loss\": 0.093,\n",
      "      \"step\": 1500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7415472779369627,\n",
      "      \"grad_norm\": 10.75899600982666,\n",
      "      \"learning_rate\": 4.131156930126002e-05,\n",
      "      \"loss\": 0.1138,\n",
      "      \"step\": 1520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7644699140401148,\n",
      "      \"grad_norm\": 9.980586051940918,\n",
      "      \"learning_rate\": 4.1197021764032076e-05,\n",
      "      \"loss\": 0.1063,\n",
      "      \"step\": 1540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.7873925501432666,\n",
      "      \"grad_norm\": 0.37948471307754517,\n",
      "      \"learning_rate\": 4.1082474226804124e-05,\n",
      "      \"loss\": 0.1565,\n",
      "      \"step\": 1560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8103151862464184,\n",
      "      \"grad_norm\": 9.694762229919434,\n",
      "      \"learning_rate\": 4.096792668957617e-05,\n",
      "      \"loss\": 0.0726,\n",
      "      \"step\": 1580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8332378223495702,\n",
      "      \"grad_norm\": 20.431459426879883,\n",
      "      \"learning_rate\": 4.085337915234823e-05,\n",
      "      \"loss\": 0.1078,\n",
      "      \"step\": 1600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.856160458452722,\n",
      "      \"grad_norm\": 0.11045525968074799,\n",
      "      \"learning_rate\": 4.0738831615120275e-05,\n",
      "      \"loss\": 0.1105,\n",
      "      \"step\": 1620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.8790830945558739,\n",
      "      \"grad_norm\": 0.016390910372138023,\n",
      "      \"learning_rate\": 4.062428407789233e-05,\n",
      "      \"loss\": 0.0458,\n",
      "      \"step\": 1640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9020057306590257,\n",
      "      \"grad_norm\": 14.51551342010498,\n",
      "      \"learning_rate\": 4.050973654066438e-05,\n",
      "      \"loss\": 0.0849,\n",
      "      \"step\": 1660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9249283667621775,\n",
      "      \"grad_norm\": 31.725502014160156,\n",
      "      \"learning_rate\": 4.039518900343643e-05,\n",
      "      \"loss\": 0.0465,\n",
      "      \"step\": 1680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9478510028653295,\n",
      "      \"grad_norm\": 19.685359954833984,\n",
      "      \"learning_rate\": 4.028064146620848e-05,\n",
      "      \"loss\": 0.0961,\n",
      "      \"step\": 1700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9707736389684813,\n",
      "      \"grad_norm\": 25.982078552246094,\n",
      "      \"learning_rate\": 4.016609392898053e-05,\n",
      "      \"loss\": 0.0377,\n",
      "      \"step\": 1720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 1.9936962750716334,\n",
      "      \"grad_norm\": 8.746831893920898,\n",
      "      \"learning_rate\": 4.005154639175258e-05,\n",
      "      \"loss\": 0.17,\n",
      "      \"step\": 1740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0,\n",
      "      \"eval_accuracy\": 0.8768382352941176,\n",
      "      \"eval_f1\": 0.8716475095785441,\n",
      "      \"eval_loss\": 0.407251238822937,\n",
      "      \"eval_precision\": 0.91,\n",
      "      \"eval_recall\": 0.8363970588235294,\n",
      "      \"eval_runtime\": 5.9674,\n",
      "      \"eval_samples_per_second\": 182.324,\n",
      "      \"eval_steps_per_second\": 22.791,\n",
      "      \"step\": 1746\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.016045845272206,\n",
      "      \"grad_norm\": 5.795254707336426,\n",
      "      \"learning_rate\": 3.993699885452463e-05,\n",
      "      \"loss\": 0.1163,\n",
      "      \"step\": 1760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.0389684813753584,\n",
      "      \"grad_norm\": 0.0936555415391922,\n",
      "      \"learning_rate\": 3.982245131729668e-05,\n",
      "      \"loss\": 0.0651,\n",
      "      \"step\": 1780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.06189111747851,\n",
      "      \"grad_norm\": 0.8103609681129456,\n",
      "      \"learning_rate\": 3.9707903780068726e-05,\n",
      "      \"loss\": 0.0614,\n",
      "      \"step\": 1800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.084813753581662,\n",
      "      \"grad_norm\": 16.212160110473633,\n",
      "      \"learning_rate\": 3.959335624284078e-05,\n",
      "      \"loss\": 0.1548,\n",
      "      \"step\": 1820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.107736389684814,\n",
      "      \"grad_norm\": 12.498269081115723,\n",
      "      \"learning_rate\": 3.947880870561283e-05,\n",
      "      \"loss\": 0.0828,\n",
      "      \"step\": 1840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1306590257879656,\n",
      "      \"grad_norm\": 0.4901289641857147,\n",
      "      \"learning_rate\": 3.936426116838488e-05,\n",
      "      \"loss\": 0.0661,\n",
      "      \"step\": 1860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1535816618911174,\n",
      "      \"grad_norm\": 12.523852348327637,\n",
      "      \"learning_rate\": 3.924971363115693e-05,\n",
      "      \"loss\": 0.1171,\n",
      "      \"step\": 1880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.1765042979942693,\n",
      "      \"grad_norm\": 8.722021102905273,\n",
      "      \"learning_rate\": 3.913516609392898e-05,\n",
      "      \"loss\": 0.1253,\n",
      "      \"step\": 1900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.199426934097421,\n",
      "      \"grad_norm\": 2.3035295009613037,\n",
      "      \"learning_rate\": 3.9020618556701034e-05,\n",
      "      \"loss\": 0.1392,\n",
      "      \"step\": 1920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.222349570200573,\n",
      "      \"grad_norm\": 14.798346519470215,\n",
      "      \"learning_rate\": 3.890607101947308e-05,\n",
      "      \"loss\": 0.0856,\n",
      "      \"step\": 1940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.245272206303725,\n",
      "      \"grad_norm\": 0.034406229853630066,\n",
      "      \"learning_rate\": 3.8791523482245136e-05,\n",
      "      \"loss\": 0.0868,\n",
      "      \"step\": 1960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.268194842406877,\n",
      "      \"grad_norm\": 0.0778827965259552,\n",
      "      \"learning_rate\": 3.8676975945017184e-05,\n",
      "      \"loss\": 0.1429,\n",
      "      \"step\": 1980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.291117478510029,\n",
      "      \"grad_norm\": 23.242685317993164,\n",
      "      \"learning_rate\": 3.856242840778924e-05,\n",
      "      \"loss\": 0.1294,\n",
      "      \"step\": 2000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3140401146131806,\n",
      "      \"grad_norm\": 9.119553565979004,\n",
      "      \"learning_rate\": 3.844788087056129e-05,\n",
      "      \"loss\": 0.1013,\n",
      "      \"step\": 2020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3369627507163324,\n",
      "      \"grad_norm\": 0.5608826875686646,\n",
      "      \"learning_rate\": 3.8333333333333334e-05,\n",
      "      \"loss\": 0.0872,\n",
      "      \"step\": 2040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.3598853868194842,\n",
      "      \"grad_norm\": 0.27309468388557434,\n",
      "      \"learning_rate\": 3.821878579610538e-05,\n",
      "      \"loss\": 0.1395,\n",
      "      \"step\": 2060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.382808022922636,\n",
      "      \"grad_norm\": 7.5405449867248535,\n",
      "      \"learning_rate\": 3.810423825887744e-05,\n",
      "      \"loss\": 0.0994,\n",
      "      \"step\": 2080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.405730659025788,\n",
      "      \"grad_norm\": 0.3477562665939331,\n",
      "      \"learning_rate\": 3.7989690721649485e-05,\n",
      "      \"loss\": 0.0703,\n",
      "      \"step\": 2100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4286532951289397,\n",
      "      \"grad_norm\": 4.408957004547119,\n",
      "      \"learning_rate\": 3.787514318442153e-05,\n",
      "      \"loss\": 0.2243,\n",
      "      \"step\": 2120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4515759312320915,\n",
      "      \"grad_norm\": 0.2656366527080536,\n",
      "      \"learning_rate\": 3.776059564719359e-05,\n",
      "      \"loss\": 0.0399,\n",
      "      \"step\": 2140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4744985673352433,\n",
      "      \"grad_norm\": 0.0661749541759491,\n",
      "      \"learning_rate\": 3.7646048109965635e-05,\n",
      "      \"loss\": 0.093,\n",
      "      \"step\": 2160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.4974212034383956,\n",
      "      \"grad_norm\": 0.07147428393363953,\n",
      "      \"learning_rate\": 3.753150057273769e-05,\n",
      "      \"loss\": 0.1332,\n",
      "      \"step\": 2180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.5203438395415474,\n",
      "      \"grad_norm\": 0.04808759316802025,\n",
      "      \"learning_rate\": 3.741695303550974e-05,\n",
      "      \"loss\": 0.0314,\n",
      "      \"step\": 2200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.543266475644699,\n",
      "      \"grad_norm\": 0.3698189854621887,\n",
      "      \"learning_rate\": 3.730813287514319e-05,\n",
      "      \"loss\": 0.1384,\n",
      "      \"step\": 2220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.566189111747851,\n",
      "      \"grad_norm\": 12.613994598388672,\n",
      "      \"learning_rate\": 3.719358533791524e-05,\n",
      "      \"loss\": 0.0964,\n",
      "      \"step\": 2240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.589111747851003,\n",
      "      \"grad_norm\": 0.15159544348716736,\n",
      "      \"learning_rate\": 3.7079037800687285e-05,\n",
      "      \"loss\": 0.0937,\n",
      "      \"step\": 2260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6120343839541547,\n",
      "      \"grad_norm\": 0.07510057836771011,\n",
      "      \"learning_rate\": 3.696449026345934e-05,\n",
      "      \"loss\": 0.058,\n",
      "      \"step\": 2280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6349570200573065,\n",
      "      \"grad_norm\": 11.187105178833008,\n",
      "      \"learning_rate\": 3.684994272623139e-05,\n",
      "      \"loss\": 0.0876,\n",
      "      \"step\": 2300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6578796561604583,\n",
      "      \"grad_norm\": 0.14438368380069733,\n",
      "      \"learning_rate\": 3.673539518900344e-05,\n",
      "      \"loss\": 0.0758,\n",
      "      \"step\": 2320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.6808022922636106,\n",
      "      \"grad_norm\": 0.2490651160478592,\n",
      "      \"learning_rate\": 3.6620847651775484e-05,\n",
      "      \"loss\": 0.0875,\n",
      "      \"step\": 2340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7037249283667624,\n",
      "      \"grad_norm\": 0.01903894543647766,\n",
      "      \"learning_rate\": 3.650630011454754e-05,\n",
      "      \"loss\": 0.0513,\n",
      "      \"step\": 2360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.726647564469914,\n",
      "      \"grad_norm\": 19.278841018676758,\n",
      "      \"learning_rate\": 3.6391752577319586e-05,\n",
      "      \"loss\": 0.0812,\n",
      "      \"step\": 2380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.749570200573066,\n",
      "      \"grad_norm\": 0.824958086013794,\n",
      "      \"learning_rate\": 3.627720504009164e-05,\n",
      "      \"loss\": 0.1159,\n",
      "      \"step\": 2400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.772492836676218,\n",
      "      \"grad_norm\": 16.655494689941406,\n",
      "      \"learning_rate\": 3.616265750286369e-05,\n",
      "      \"loss\": 0.0577,\n",
      "      \"step\": 2420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.7954154727793696,\n",
      "      \"grad_norm\": 0.11453112959861755,\n",
      "      \"learning_rate\": 3.604810996563574e-05,\n",
      "      \"loss\": 0.0883,\n",
      "      \"step\": 2440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8183381088825215,\n",
      "      \"grad_norm\": 0.12225945293903351,\n",
      "      \"learning_rate\": 3.593356242840779e-05,\n",
      "      \"loss\": 0.0867,\n",
      "      \"step\": 2460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.8412607449856733,\n",
      "      \"grad_norm\": 0.18973803520202637,\n",
      "      \"learning_rate\": 3.581901489117984e-05,\n",
      "      \"loss\": 0.0721,\n",
      "      \"step\": 2480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.864183381088825,\n",
      "      \"grad_norm\": 0.1310456395149231,\n",
      "      \"learning_rate\": 3.5704467353951894e-05,\n",
      "      \"loss\": 0.0797,\n",
      "      \"step\": 2500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.887106017191977,\n",
      "      \"grad_norm\": 0.23435160517692566,\n",
      "      \"learning_rate\": 3.558991981672394e-05,\n",
      "      \"loss\": 0.1287,\n",
      "      \"step\": 2520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9100286532951287,\n",
      "      \"grad_norm\": 7.872849941253662,\n",
      "      \"learning_rate\": 3.5475372279495996e-05,\n",
      "      \"loss\": 0.0673,\n",
      "      \"step\": 2540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9329512893982805,\n",
      "      \"grad_norm\": 10.166168212890625,\n",
      "      \"learning_rate\": 3.5360824742268044e-05,\n",
      "      \"loss\": 0.074,\n",
      "      \"step\": 2560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.955873925501433,\n",
      "      \"grad_norm\": 0.427828311920166,\n",
      "      \"learning_rate\": 3.52462772050401e-05,\n",
      "      \"loss\": 0.0146,\n",
      "      \"step\": 2580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 2.9787965616045846,\n",
      "      \"grad_norm\": 1.5753976106643677,\n",
      "      \"learning_rate\": 3.513172966781214e-05,\n",
      "      \"loss\": 0.0471,\n",
      "      \"step\": 2600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0,\n",
      "      \"eval_accuracy\": 0.8428308823529411,\n",
      "      \"eval_f1\": 0.8635275339185954,\n",
      "      \"eval_loss\": 0.491738885641098,\n",
      "      \"eval_precision\": 0.763046544428773,\n",
      "      \"eval_recall\": 0.9944852941176471,\n",
      "      \"eval_runtime\": 6.1035,\n",
      "      \"eval_samples_per_second\": 178.258,\n",
      "      \"eval_steps_per_second\": 22.282,\n",
      "      \"step\": 2619\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0011461318051578,\n",
      "      \"grad_norm\": 0.34866055846214294,\n",
      "      \"learning_rate\": 3.5017182130584194e-05,\n",
      "      \"loss\": 0.1687,\n",
      "      \"step\": 2620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0240687679083096,\n",
      "      \"grad_norm\": 0.2547266483306885,\n",
      "      \"learning_rate\": 3.490263459335624e-05,\n",
      "      \"loss\": 0.0901,\n",
      "      \"step\": 2640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.0469914040114614,\n",
      "      \"grad_norm\": 0.6665408611297607,\n",
      "      \"learning_rate\": 3.478808705612829e-05,\n",
      "      \"loss\": 0.0278,\n",
      "      \"step\": 2660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.069914040114613,\n",
      "      \"grad_norm\": 0.19821003079414368,\n",
      "      \"learning_rate\": 3.4673539518900345e-05,\n",
      "      \"loss\": 0.0869,\n",
      "      \"step\": 2680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.092836676217765,\n",
      "      \"grad_norm\": 22.415559768676758,\n",
      "      \"learning_rate\": 3.455899198167239e-05,\n",
      "      \"loss\": 0.0871,\n",
      "      \"step\": 2700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.115759312320917,\n",
      "      \"grad_norm\": 19.855466842651367,\n",
      "      \"learning_rate\": 3.444444444444445e-05,\n",
      "      \"loss\": 0.0489,\n",
      "      \"step\": 2720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1386819484240687,\n",
      "      \"grad_norm\": 0.4302346408367157,\n",
      "      \"learning_rate\": 3.4329896907216495e-05,\n",
      "      \"loss\": 0.0497,\n",
      "      \"step\": 2740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1616045845272205,\n",
      "      \"grad_norm\": 0.3605550229549408,\n",
      "      \"learning_rate\": 3.421534936998855e-05,\n",
      "      \"loss\": 0.0782,\n",
      "      \"step\": 2760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.1845272206303723,\n",
      "      \"grad_norm\": 0.17818626761436462,\n",
      "      \"learning_rate\": 3.41008018327606e-05,\n",
      "      \"loss\": 0.0752,\n",
      "      \"step\": 2780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2074498567335246,\n",
      "      \"grad_norm\": 0.4531317949295044,\n",
      "      \"learning_rate\": 3.3986254295532646e-05,\n",
      "      \"loss\": 0.0803,\n",
      "      \"step\": 2800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.2303724928366764,\n",
      "      \"grad_norm\": 0.10437062382698059,\n",
      "      \"learning_rate\": 3.38717067583047e-05,\n",
      "      \"loss\": 0.0624,\n",
      "      \"step\": 2820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.253295128939828,\n",
      "      \"grad_norm\": 59.2630729675293,\n",
      "      \"learning_rate\": 3.375715922107675e-05,\n",
      "      \"loss\": 0.0684,\n",
      "      \"step\": 2840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.27621776504298,\n",
      "      \"grad_norm\": 0.12324246764183044,\n",
      "      \"learning_rate\": 3.36426116838488e-05,\n",
      "      \"loss\": 0.1016,\n",
      "      \"step\": 2860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.299140401146132,\n",
      "      \"grad_norm\": 0.38446909189224243,\n",
      "      \"learning_rate\": 3.3528064146620844e-05,\n",
      "      \"loss\": 0.0839,\n",
      "      \"step\": 2880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3220630372492836,\n",
      "      \"grad_norm\": 13.179975509643555,\n",
      "      \"learning_rate\": 3.34135166093929e-05,\n",
      "      \"loss\": 0.0937,\n",
      "      \"step\": 2900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3449856733524355,\n",
      "      \"grad_norm\": 0.13565261662006378,\n",
      "      \"learning_rate\": 3.3298969072164947e-05,\n",
      "      \"loss\": 0.0787,\n",
      "      \"step\": 2920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.3679083094555873,\n",
      "      \"grad_norm\": 0.17267444729804993,\n",
      "      \"learning_rate\": 3.3184421534937e-05,\n",
      "      \"loss\": 0.0521,\n",
      "      \"step\": 2940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.390830945558739,\n",
      "      \"grad_norm\": 0.19395919144153595,\n",
      "      \"learning_rate\": 3.306987399770905e-05,\n",
      "      \"loss\": 0.059,\n",
      "      \"step\": 2960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.413753581661891,\n",
      "      \"grad_norm\": 0.39965543150901794,\n",
      "      \"learning_rate\": 3.2955326460481104e-05,\n",
      "      \"loss\": 0.1699,\n",
      "      \"step\": 2980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.436676217765043,\n",
      "      \"grad_norm\": 0.39167913794517517,\n",
      "      \"learning_rate\": 3.284077892325315e-05,\n",
      "      \"loss\": 0.1228,\n",
      "      \"step\": 3000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.459598853868195,\n",
      "      \"grad_norm\": 15.13223934173584,\n",
      "      \"learning_rate\": 3.27262313860252e-05,\n",
      "      \"loss\": 0.0571,\n",
      "      \"step\": 3020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.482521489971347,\n",
      "      \"grad_norm\": 0.027908921241760254,\n",
      "      \"learning_rate\": 3.2611683848797254e-05,\n",
      "      \"loss\": 0.0827,\n",
      "      \"step\": 3040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5054441260744986,\n",
      "      \"grad_norm\": 0.031907450407743454,\n",
      "      \"learning_rate\": 3.24971363115693e-05,\n",
      "      \"loss\": 0.0145,\n",
      "      \"step\": 3060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5283667621776504,\n",
      "      \"grad_norm\": 14.374930381774902,\n",
      "      \"learning_rate\": 3.238258877434136e-05,\n",
      "      \"loss\": 0.096,\n",
      "      \"step\": 3080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.5512893982808023,\n",
      "      \"grad_norm\": 5.919641971588135,\n",
      "      \"learning_rate\": 3.2268041237113405e-05,\n",
      "      \"loss\": 0.0728,\n",
      "      \"step\": 3100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.574212034383954,\n",
      "      \"grad_norm\": 0.6738690137863159,\n",
      "      \"learning_rate\": 3.215349369988546e-05,\n",
      "      \"loss\": 0.0223,\n",
      "      \"step\": 3120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.597134670487106,\n",
      "      \"grad_norm\": 0.043391767889261246,\n",
      "      \"learning_rate\": 3.203894616265751e-05,\n",
      "      \"loss\": 0.0691,\n",
      "      \"step\": 3140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.620057306590258,\n",
      "      \"grad_norm\": 12.476292610168457,\n",
      "      \"learning_rate\": 3.1924398625429555e-05,\n",
      "      \"loss\": 0.096,\n",
      "      \"step\": 3160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.64297994269341,\n",
      "      \"grad_norm\": 0.06938707828521729,\n",
      "      \"learning_rate\": 3.18098510882016e-05,\n",
      "      \"loss\": 0.1139,\n",
      "      \"step\": 3180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.665902578796562,\n",
      "      \"grad_norm\": 1.745160460472107,\n",
      "      \"learning_rate\": 3.169530355097365e-05,\n",
      "      \"loss\": 0.0752,\n",
      "      \"step\": 3200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.6888252148997136,\n",
      "      \"grad_norm\": 13.777776718139648,\n",
      "      \"learning_rate\": 3.1580756013745705e-05,\n",
      "      \"loss\": 0.0713,\n",
      "      \"step\": 3220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7117478510028654,\n",
      "      \"grad_norm\": 0.3910318613052368,\n",
      "      \"learning_rate\": 3.146620847651775e-05,\n",
      "      \"loss\": 0.0081,\n",
      "      \"step\": 3240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.7346704871060172,\n",
      "      \"grad_norm\": 0.04552454128861427,\n",
      "      \"learning_rate\": 3.135166093928981e-05,\n",
      "      \"loss\": 0.075,\n",
      "      \"step\": 3260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.757593123209169,\n",
      "      \"grad_norm\": 26.156747817993164,\n",
      "      \"learning_rate\": 3.1237113402061856e-05,\n",
      "      \"loss\": 0.1175,\n",
      "      \"step\": 3280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.780515759312321,\n",
      "      \"grad_norm\": 0.3711104393005371,\n",
      "      \"learning_rate\": 3.112256586483391e-05,\n",
      "      \"loss\": 0.0956,\n",
      "      \"step\": 3300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8034383954154727,\n",
      "      \"grad_norm\": 26.355268478393555,\n",
      "      \"learning_rate\": 3.100801832760596e-05,\n",
      "      \"loss\": 0.0436,\n",
      "      \"step\": 3320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8263610315186245,\n",
      "      \"grad_norm\": 0.2875008285045624,\n",
      "      \"learning_rate\": 3.0893470790378006e-05,\n",
      "      \"loss\": 0.0828,\n",
      "      \"step\": 3340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8492836676217763,\n",
      "      \"grad_norm\": 18.585973739624023,\n",
      "      \"learning_rate\": 3.077892325315006e-05,\n",
      "      \"loss\": 0.0977,\n",
      "      \"step\": 3360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.872206303724928,\n",
      "      \"grad_norm\": 0.3537771701812744,\n",
      "      \"learning_rate\": 3.066437571592211e-05,\n",
      "      \"loss\": 0.0456,\n",
      "      \"step\": 3380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.8951289398280804,\n",
      "      \"grad_norm\": 4.1503586769104,\n",
      "      \"learning_rate\": 3.0549828178694163e-05,\n",
      "      \"loss\": 0.094,\n",
      "      \"step\": 3400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.918051575931232,\n",
      "      \"grad_norm\": 0.08368301391601562,\n",
      "      \"learning_rate\": 3.0435280641466208e-05,\n",
      "      \"loss\": 0.0681,\n",
      "      \"step\": 3420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.940974212034384,\n",
      "      \"grad_norm\": 0.26489096879959106,\n",
      "      \"learning_rate\": 3.032073310423826e-05,\n",
      "      \"loss\": 0.0559,\n",
      "      \"step\": 3440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.963896848137536,\n",
      "      \"grad_norm\": 5.762895584106445,\n",
      "      \"learning_rate\": 3.020618556701031e-05,\n",
      "      \"loss\": 0.0736,\n",
      "      \"step\": 3460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 3.9868194842406877,\n",
      "      \"grad_norm\": 7.119098663330078,\n",
      "      \"learning_rate\": 3.009163802978236e-05,\n",
      "      \"loss\": 0.0641,\n",
      "      \"step\": 3480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.0,\n",
      "      \"eval_accuracy\": 0.9117647058823529,\n",
      "      \"eval_f1\": 0.9157894736842105,\n",
      "      \"eval_loss\": 0.2997386157512665,\n",
      "      \"eval_precision\": 0.8758389261744967,\n",
      "      \"eval_recall\": 0.9595588235294118,\n",
      "      \"eval_runtime\": 6.2284,\n",
      "      \"eval_samples_per_second\": 174.684,\n",
      "      \"eval_steps_per_second\": 21.836,\n",
      "      \"step\": 3492\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.00916905444126,\n",
      "      \"grad_norm\": 0.025205688551068306,\n",
      "      \"learning_rate\": 2.997709049255441e-05,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 3500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.032091690544412,\n",
      "      \"grad_norm\": 0.05400218442082405,\n",
      "      \"learning_rate\": 2.986254295532646e-05,\n",
      "      \"loss\": 0.0454,\n",
      "      \"step\": 3520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.055014326647565,\n",
      "      \"grad_norm\": 0.043758317828178406,\n",
      "      \"learning_rate\": 2.9747995418098512e-05,\n",
      "      \"loss\": 0.0683,\n",
      "      \"step\": 3540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.077936962750717,\n",
      "      \"grad_norm\": 0.9882086515426636,\n",
      "      \"learning_rate\": 2.9633447880870563e-05,\n",
      "      \"loss\": 0.0474,\n",
      "      \"step\": 3560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.1008595988538685,\n",
      "      \"grad_norm\": 0.695027232170105,\n",
      "      \"learning_rate\": 2.9518900343642615e-05,\n",
      "      \"loss\": 0.0208,\n",
      "      \"step\": 3580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.12378223495702,\n",
      "      \"grad_norm\": 24.256450653076172,\n",
      "      \"learning_rate\": 2.9404352806414666e-05,\n",
      "      \"loss\": 0.0705,\n",
      "      \"step\": 3600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.146704871060172,\n",
      "      \"grad_norm\": 0.05782554671168327,\n",
      "      \"learning_rate\": 2.9289805269186714e-05,\n",
      "      \"loss\": 0.0713,\n",
      "      \"step\": 3620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.169627507163324,\n",
      "      \"grad_norm\": 0.11687364429235458,\n",
      "      \"learning_rate\": 2.9175257731958765e-05,\n",
      "      \"loss\": 0.0232,\n",
      "      \"step\": 3640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.192550143266476,\n",
      "      \"grad_norm\": 0.0012601783964782953,\n",
      "      \"learning_rate\": 2.9060710194730816e-05,\n",
      "      \"loss\": 0.0844,\n",
      "      \"step\": 3660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.215472779369628,\n",
      "      \"grad_norm\": 2.849111795425415,\n",
      "      \"learning_rate\": 2.8946162657502868e-05,\n",
      "      \"loss\": 0.0799,\n",
      "      \"step\": 3680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.238395415472779,\n",
      "      \"grad_norm\": 0.25755688548088074,\n",
      "      \"learning_rate\": 2.8831615120274912e-05,\n",
      "      \"loss\": 0.0681,\n",
      "      \"step\": 3700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.261318051575931,\n",
      "      \"grad_norm\": 13.324786186218262,\n",
      "      \"learning_rate\": 2.8717067583046963e-05,\n",
      "      \"loss\": 0.0323,\n",
      "      \"step\": 3720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.284240687679083,\n",
      "      \"grad_norm\": 0.002821204951032996,\n",
      "      \"learning_rate\": 2.8602520045819015e-05,\n",
      "      \"loss\": 0.0753,\n",
      "      \"step\": 3740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.307163323782235,\n",
      "      \"grad_norm\": 0.10430637001991272,\n",
      "      \"learning_rate\": 2.8487972508591066e-05,\n",
      "      \"loss\": 0.0502,\n",
      "      \"step\": 3760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.330085959885387,\n",
      "      \"grad_norm\": 13.600162506103516,\n",
      "      \"learning_rate\": 2.8373424971363117e-05,\n",
      "      \"loss\": 0.0591,\n",
      "      \"step\": 3780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.3530085959885385,\n",
      "      \"grad_norm\": 0.29043781757354736,\n",
      "      \"learning_rate\": 2.825887743413517e-05,\n",
      "      \"loss\": 0.108,\n",
      "      \"step\": 3800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.37593123209169,\n",
      "      \"grad_norm\": 34.025238037109375,\n",
      "      \"learning_rate\": 2.8144329896907216e-05,\n",
      "      \"loss\": 0.0214,\n",
      "      \"step\": 3820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.398853868194842,\n",
      "      \"grad_norm\": 0.013031949289143085,\n",
      "      \"learning_rate\": 2.8029782359679268e-05,\n",
      "      \"loss\": 0.0379,\n",
      "      \"step\": 3840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.421776504297994,\n",
      "      \"grad_norm\": 18.419246673583984,\n",
      "      \"learning_rate\": 2.791523482245132e-05,\n",
      "      \"loss\": 0.0521,\n",
      "      \"step\": 3860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.444699140401146,\n",
      "      \"grad_norm\": 0.08108682185411453,\n",
      "      \"learning_rate\": 2.780068728522337e-05,\n",
      "      \"loss\": 0.0578,\n",
      "      \"step\": 3880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.467621776504298,\n",
      "      \"grad_norm\": 4.1178975105285645,\n",
      "      \"learning_rate\": 2.768613974799542e-05,\n",
      "      \"loss\": 0.0715,\n",
      "      \"step\": 3900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.49054441260745,\n",
      "      \"grad_norm\": 12.269513130187988,\n",
      "      \"learning_rate\": 2.7571592210767473e-05,\n",
      "      \"loss\": 0.0692,\n",
      "      \"step\": 3920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.513467048710602,\n",
      "      \"grad_norm\": 0.08036638796329498,\n",
      "      \"learning_rate\": 2.7457044673539524e-05,\n",
      "      \"loss\": 0.0502,\n",
      "      \"step\": 3940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.536389684813754,\n",
      "      \"grad_norm\": 0.0055150785483419895,\n",
      "      \"learning_rate\": 2.734249713631157e-05,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 3960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.559312320916906,\n",
      "      \"grad_norm\": 0.03642969951033592,\n",
      "      \"learning_rate\": 2.722794959908362e-05,\n",
      "      \"loss\": 0.0565,\n",
      "      \"step\": 3980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.582234957020058,\n",
      "      \"grad_norm\": 2.4480388164520264,\n",
      "      \"learning_rate\": 2.711340206185567e-05,\n",
      "      \"loss\": 0.0481,\n",
      "      \"step\": 4000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.605157593123209,\n",
      "      \"grad_norm\": 0.007671267259865999,\n",
      "      \"learning_rate\": 2.699885452462772e-05,\n",
      "      \"loss\": 0.0656,\n",
      "      \"step\": 4020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.628080229226361,\n",
      "      \"grad_norm\": 1.3466531038284302,\n",
      "      \"learning_rate\": 2.688430698739977e-05,\n",
      "      \"loss\": 0.0708,\n",
      "      \"step\": 4040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.651002865329513,\n",
      "      \"grad_norm\": 0.19433555006980896,\n",
      "      \"learning_rate\": 2.676975945017182e-05,\n",
      "      \"loss\": 0.0465,\n",
      "      \"step\": 4060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.673925501432665,\n",
      "      \"grad_norm\": 0.41744178533554077,\n",
      "      \"learning_rate\": 2.6655211912943873e-05,\n",
      "      \"loss\": 0.0091,\n",
      "      \"step\": 4080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.696848137535817,\n",
      "      \"grad_norm\": 7.014568328857422,\n",
      "      \"learning_rate\": 2.6540664375715924e-05,\n",
      "      \"loss\": 0.0888,\n",
      "      \"step\": 4100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.7197707736389685,\n",
      "      \"grad_norm\": 14.408547401428223,\n",
      "      \"learning_rate\": 2.6426116838487975e-05,\n",
      "      \"loss\": 0.0554,\n",
      "      \"step\": 4120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.74269340974212,\n",
      "      \"grad_norm\": 18.478971481323242,\n",
      "      \"learning_rate\": 2.6311569301260027e-05,\n",
      "      \"loss\": 0.0386,\n",
      "      \"step\": 4140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.765616045845272,\n",
      "      \"grad_norm\": 0.34290173649787903,\n",
      "      \"learning_rate\": 2.6197021764032074e-05,\n",
      "      \"loss\": 0.103,\n",
      "      \"step\": 4160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.788538681948424,\n",
      "      \"grad_norm\": 10.97011661529541,\n",
      "      \"learning_rate\": 2.6082474226804126e-05,\n",
      "      \"loss\": 0.0429,\n",
      "      \"step\": 4180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.811461318051576,\n",
      "      \"grad_norm\": 20.69083595275879,\n",
      "      \"learning_rate\": 2.5967926689576177e-05,\n",
      "      \"loss\": 0.0545,\n",
      "      \"step\": 4200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.8343839541547275,\n",
      "      \"grad_norm\": 20.174392700195312,\n",
      "      \"learning_rate\": 2.5853379152348228e-05,\n",
      "      \"loss\": 0.0842,\n",
      "      \"step\": 4220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.857306590257879,\n",
      "      \"grad_norm\": 8.058281898498535,\n",
      "      \"learning_rate\": 2.574455899198167e-05,\n",
      "      \"loss\": 0.0776,\n",
      "      \"step\": 4240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.880229226361031,\n",
      "      \"grad_norm\": 0.3186200261116028,\n",
      "      \"learning_rate\": 2.563001145475372e-05,\n",
      "      \"loss\": 0.0584,\n",
      "      \"step\": 4260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.903151862464183,\n",
      "      \"grad_norm\": 0.3132939338684082,\n",
      "      \"learning_rate\": 2.5515463917525772e-05,\n",
      "      \"loss\": 0.0473,\n",
      "      \"step\": 4280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.926074498567335,\n",
      "      \"grad_norm\": 0.0252685546875,\n",
      "      \"learning_rate\": 2.5400916380297823e-05,\n",
      "      \"loss\": 0.0564,\n",
      "      \"step\": 4300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.948997134670487,\n",
      "      \"grad_norm\": 20.775678634643555,\n",
      "      \"learning_rate\": 2.5286368843069875e-05,\n",
      "      \"loss\": 0.0738,\n",
      "      \"step\": 4320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.971919770773639,\n",
      "      \"grad_norm\": 12.16853141784668,\n",
      "      \"learning_rate\": 2.5171821305841926e-05,\n",
      "      \"loss\": 0.0553,\n",
      "      \"step\": 4340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 4.994842406876791,\n",
      "      \"grad_norm\": 6.755401611328125,\n",
      "      \"learning_rate\": 2.5057273768613977e-05,\n",
      "      \"loss\": 0.0978,\n",
      "      \"step\": 4360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.0,\n",
      "      \"eval_accuracy\": 0.7665441176470589,\n",
      "      \"eval_f1\": 0.8098802395209581,\n",
      "      \"eval_loss\": 0.9610410332679749,\n",
      "      \"eval_precision\": 0.6830808080808081,\n",
      "      \"eval_recall\": 0.9944852941176471,\n",
      "      \"eval_runtime\": 6.1295,\n",
      "      \"eval_samples_per_second\": 177.502,\n",
      "      \"eval_steps_per_second\": 22.188,\n",
      "      \"step\": 4365\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.017191977077364,\n",
      "      \"grad_norm\": 8.89062213897705,\n",
      "      \"learning_rate\": 2.4942726231386025e-05,\n",
      "      \"loss\": 0.0306,\n",
      "      \"step\": 4380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.040114613180516,\n",
      "      \"grad_norm\": 0.07689054310321808,\n",
      "      \"learning_rate\": 2.4828178694158076e-05,\n",
      "      \"loss\": 0.036,\n",
      "      \"step\": 4400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.063037249283668,\n",
      "      \"grad_norm\": 8.546205520629883,\n",
      "      \"learning_rate\": 2.4713631156930128e-05,\n",
      "      \"loss\": 0.0601,\n",
      "      \"step\": 4420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.08595988538682,\n",
      "      \"grad_norm\": 0.0068421754986047745,\n",
      "      \"learning_rate\": 2.4599083619702176e-05,\n",
      "      \"loss\": 0.081,\n",
      "      \"step\": 4440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.108882521489972,\n",
      "      \"grad_norm\": 0.01663929410278797,\n",
      "      \"learning_rate\": 2.4484536082474227e-05,\n",
      "      \"loss\": 0.081,\n",
      "      \"step\": 4460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.131805157593123,\n",
      "      \"grad_norm\": 0.033093977719545364,\n",
      "      \"learning_rate\": 2.4369988545246278e-05,\n",
      "      \"loss\": 0.0779,\n",
      "      \"step\": 4480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.154727793696275,\n",
      "      \"grad_norm\": 1.9761340618133545,\n",
      "      \"learning_rate\": 2.425544100801833e-05,\n",
      "      \"loss\": 0.0774,\n",
      "      \"step\": 4500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.177650429799427,\n",
      "      \"grad_norm\": 0.034796249121427536,\n",
      "      \"learning_rate\": 2.414089347079038e-05,\n",
      "      \"loss\": 0.0511,\n",
      "      \"step\": 4520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.200573065902579,\n",
      "      \"grad_norm\": 0.2989315986633301,\n",
      "      \"learning_rate\": 2.4026345933562432e-05,\n",
      "      \"loss\": 0.0516,\n",
      "      \"step\": 4540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.223495702005731,\n",
      "      \"grad_norm\": 0.03295838087797165,\n",
      "      \"learning_rate\": 2.391179839633448e-05,\n",
      "      \"loss\": 0.0563,\n",
      "      \"step\": 4560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.2464183381088825,\n",
      "      \"grad_norm\": 0.035239629447460175,\n",
      "      \"learning_rate\": 2.3797250859106528e-05,\n",
      "      \"loss\": 0.0466,\n",
      "      \"step\": 4580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.269340974212034,\n",
      "      \"grad_norm\": 0.07905202358961105,\n",
      "      \"learning_rate\": 2.368270332187858e-05,\n",
      "      \"loss\": 0.0667,\n",
      "      \"step\": 4600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.292263610315186,\n",
      "      \"grad_norm\": 0.22853323817253113,\n",
      "      \"learning_rate\": 2.356815578465063e-05,\n",
      "      \"loss\": 0.0868,\n",
      "      \"step\": 4620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.315186246418338,\n",
      "      \"grad_norm\": 0.1387978196144104,\n",
      "      \"learning_rate\": 2.345360824742268e-05,\n",
      "      \"loss\": 0.0354,\n",
      "      \"step\": 4640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.33810888252149,\n",
      "      \"grad_norm\": 0.11069244891405106,\n",
      "      \"learning_rate\": 2.3339060710194733e-05,\n",
      "      \"loss\": 0.0328,\n",
      "      \"step\": 4660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.361031518624642,\n",
      "      \"grad_norm\": 0.14926032721996307,\n",
      "      \"learning_rate\": 2.3224513172966784e-05,\n",
      "      \"loss\": 0.04,\n",
      "      \"step\": 4680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.383954154727793,\n",
      "      \"grad_norm\": 0.08371987193822861,\n",
      "      \"learning_rate\": 2.3109965635738835e-05,\n",
      "      \"loss\": 0.0395,\n",
      "      \"step\": 4700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.406876790830945,\n",
      "      \"grad_norm\": 0.09728603065013885,\n",
      "      \"learning_rate\": 2.2995418098510883e-05,\n",
      "      \"loss\": 0.04,\n",
      "      \"step\": 4720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.429799426934097,\n",
      "      \"grad_norm\": 0.05892961844801903,\n",
      "      \"learning_rate\": 2.288087056128293e-05,\n",
      "      \"loss\": 0.0902,\n",
      "      \"step\": 4740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.45272206303725,\n",
      "      \"grad_norm\": 0.005607706494629383,\n",
      "      \"learning_rate\": 2.2766323024054982e-05,\n",
      "      \"loss\": 0.0818,\n",
      "      \"step\": 4760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.4756446991404015,\n",
      "      \"grad_norm\": 5.732883453369141,\n",
      "      \"learning_rate\": 2.2651775486827034e-05,\n",
      "      \"loss\": 0.0428,\n",
      "      \"step\": 4780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.498567335243553,\n",
      "      \"grad_norm\": 18.41051483154297,\n",
      "      \"learning_rate\": 2.2537227949599085e-05,\n",
      "      \"loss\": 0.1247,\n",
      "      \"step\": 4800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.521489971346705,\n",
      "      \"grad_norm\": 0.12373650074005127,\n",
      "      \"learning_rate\": 2.2422680412371136e-05,\n",
      "      \"loss\": 0.0329,\n",
      "      \"step\": 4820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.544412607449857,\n",
      "      \"grad_norm\": 9.154227256774902,\n",
      "      \"learning_rate\": 2.2308132875143187e-05,\n",
      "      \"loss\": 0.0707,\n",
      "      \"step\": 4840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.567335243553009,\n",
      "      \"grad_norm\": 0.04256933182477951,\n",
      "      \"learning_rate\": 2.2193585337915235e-05,\n",
      "      \"loss\": 0.052,\n",
      "      \"step\": 4860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.590257879656161,\n",
      "      \"grad_norm\": 0.008846736513078213,\n",
      "      \"learning_rate\": 2.2079037800687287e-05,\n",
      "      \"loss\": 0.0662,\n",
      "      \"step\": 4880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.613180515759312,\n",
      "      \"grad_norm\": 0.39225348830223083,\n",
      "      \"learning_rate\": 2.1964490263459338e-05,\n",
      "      \"loss\": 0.0023,\n",
      "      \"step\": 4900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.636103151862464,\n",
      "      \"grad_norm\": 0.3318259119987488,\n",
      "      \"learning_rate\": 2.1849942726231386e-05,\n",
      "      \"loss\": 0.0014,\n",
      "      \"step\": 4920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.659025787965616,\n",
      "      \"grad_norm\": 0.0013391509419307113,\n",
      "      \"learning_rate\": 2.1735395189003437e-05,\n",
      "      \"loss\": 0.052,\n",
      "      \"step\": 4940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.681948424068768,\n",
      "      \"grad_norm\": 0.08637948334217072,\n",
      "      \"learning_rate\": 2.1620847651775488e-05,\n",
      "      \"loss\": 0.0164,\n",
      "      \"step\": 4960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.70487106017192,\n",
      "      \"grad_norm\": 0.027709277346730232,\n",
      "      \"learning_rate\": 2.150630011454754e-05,\n",
      "      \"loss\": 0.045,\n",
      "      \"step\": 4980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.7277936962750715,\n",
      "      \"grad_norm\": 8.22111988067627,\n",
      "      \"learning_rate\": 2.1391752577319587e-05,\n",
      "      \"loss\": 0.1235,\n",
      "      \"step\": 5000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.750716332378223,\n",
      "      \"grad_norm\": 0.014280080795288086,\n",
      "      \"learning_rate\": 2.127720504009164e-05,\n",
      "      \"loss\": 0.0404,\n",
      "      \"step\": 5020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.773638968481375,\n",
      "      \"grad_norm\": 5.807032585144043,\n",
      "      \"learning_rate\": 2.116265750286369e-05,\n",
      "      \"loss\": 0.0667,\n",
      "      \"step\": 5040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.796561604584527,\n",
      "      \"grad_norm\": 1.3020652532577515,\n",
      "      \"learning_rate\": 2.104810996563574e-05,\n",
      "      \"loss\": 0.0334,\n",
      "      \"step\": 5060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.819484240687679,\n",
      "      \"grad_norm\": 7.757806777954102,\n",
      "      \"learning_rate\": 2.0933562428407792e-05,\n",
      "      \"loss\": 0.0517,\n",
      "      \"step\": 5080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.842406876790831,\n",
      "      \"grad_norm\": 0.04348793253302574,\n",
      "      \"learning_rate\": 2.081901489117984e-05,\n",
      "      \"loss\": 0.0654,\n",
      "      \"step\": 5100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.865329512893982,\n",
      "      \"grad_norm\": 8.876002311706543,\n",
      "      \"learning_rate\": 2.070446735395189e-05,\n",
      "      \"loss\": 0.0191,\n",
      "      \"step\": 5120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.888252148997134,\n",
      "      \"grad_norm\": 1.0207222700119019,\n",
      "      \"learning_rate\": 2.058991981672394e-05,\n",
      "      \"loss\": 0.0577,\n",
      "      \"step\": 5140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.911174785100286,\n",
      "      \"grad_norm\": 7.682071208953857,\n",
      "      \"learning_rate\": 2.047537227949599e-05,\n",
      "      \"loss\": 0.0693,\n",
      "      \"step\": 5160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.934097421203439,\n",
      "      \"grad_norm\": 0.19991929829120636,\n",
      "      \"learning_rate\": 2.0360824742268042e-05,\n",
      "      \"loss\": 0.0332,\n",
      "      \"step\": 5180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.957020057306591,\n",
      "      \"grad_norm\": 0.5058720707893372,\n",
      "      \"learning_rate\": 2.0246277205040093e-05,\n",
      "      \"loss\": 0.0631,\n",
      "      \"step\": 5200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 5.979942693409742,\n",
      "      \"grad_norm\": 8.80553913116455,\n",
      "      \"learning_rate\": 2.0131729667812145e-05,\n",
      "      \"loss\": 0.0843,\n",
      "      \"step\": 5220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.0,\n",
      "      \"eval_accuracy\": 0.921875,\n",
      "      \"eval_f1\": 0.916256157635468,\n",
      "      \"eval_loss\": 0.2925114035606384,\n",
      "      \"eval_precision\": 0.9872611464968153,\n",
      "      \"eval_recall\": 0.8547794117647058,\n",
      "      \"eval_runtime\": 6.1715,\n",
      "      \"eval_samples_per_second\": 176.295,\n",
      "      \"eval_steps_per_second\": 22.037,\n",
      "      \"step\": 5238\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.0022922636103155,\n",
      "      \"grad_norm\": 0.17223480343818665,\n",
      "      \"learning_rate\": 2.0017182130584196e-05,\n",
      "      \"loss\": 0.0225,\n",
      "      \"step\": 5240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.025214899713467,\n",
      "      \"grad_norm\": 0.06902205944061279,\n",
      "      \"learning_rate\": 1.9902634593356244e-05,\n",
      "      \"loss\": 0.0528,\n",
      "      \"step\": 5260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.048137535816619,\n",
      "      \"grad_norm\": 0.08092392235994339,\n",
      "      \"learning_rate\": 1.9788087056128295e-05,\n",
      "      \"loss\": 0.0408,\n",
      "      \"step\": 5280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.071060171919771,\n",
      "      \"grad_norm\": 46.438148498535156,\n",
      "      \"learning_rate\": 1.9673539518900343e-05,\n",
      "      \"loss\": 0.0609,\n",
      "      \"step\": 5300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.093982808022923,\n",
      "      \"grad_norm\": 0.07102359086275101,\n",
      "      \"learning_rate\": 1.9558991981672394e-05,\n",
      "      \"loss\": 0.0282,\n",
      "      \"step\": 5320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.116905444126075,\n",
      "      \"grad_norm\": 0.0270327590405941,\n",
      "      \"learning_rate\": 1.9444444444444445e-05,\n",
      "      \"loss\": 0.0471,\n",
      "      \"step\": 5340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.139828080229226,\n",
      "      \"grad_norm\": 0.01055362168699503,\n",
      "      \"learning_rate\": 1.9329896907216497e-05,\n",
      "      \"loss\": 0.0388,\n",
      "      \"step\": 5360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.162750716332378,\n",
      "      \"grad_norm\": 15.026063919067383,\n",
      "      \"learning_rate\": 1.9215349369988548e-05,\n",
      "      \"loss\": 0.0844,\n",
      "      \"step\": 5380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.18567335243553,\n",
      "      \"grad_norm\": 13.226699829101562,\n",
      "      \"learning_rate\": 1.9100801832760596e-05,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 5400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.208595988538682,\n",
      "      \"grad_norm\": 0.06645267456769943,\n",
      "      \"learning_rate\": 1.8986254295532647e-05,\n",
      "      \"loss\": 0.0432,\n",
      "      \"step\": 5420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.231518624641834,\n",
      "      \"grad_norm\": 0.7470769882202148,\n",
      "      \"learning_rate\": 1.88717067583047e-05,\n",
      "      \"loss\": 0.0444,\n",
      "      \"step\": 5440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.2544412607449855,\n",
      "      \"grad_norm\": 0.01211561169475317,\n",
      "      \"learning_rate\": 1.8757159221076746e-05,\n",
      "      \"loss\": 0.0498,\n",
      "      \"step\": 5460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.277363896848137,\n",
      "      \"grad_norm\": 0.01033823937177658,\n",
      "      \"learning_rate\": 1.8642611683848797e-05,\n",
      "      \"loss\": 0.0894,\n",
      "      \"step\": 5480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.300286532951289,\n",
      "      \"grad_norm\": 11.154529571533203,\n",
      "      \"learning_rate\": 1.852806414662085e-05,\n",
      "      \"loss\": 0.047,\n",
      "      \"step\": 5500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.323209169054441,\n",
      "      \"grad_norm\": 0.07244076579809189,\n",
      "      \"learning_rate\": 1.84135166093929e-05,\n",
      "      \"loss\": 0.0492,\n",
      "      \"step\": 5520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.346131805157593,\n",
      "      \"grad_norm\": 22.514183044433594,\n",
      "      \"learning_rate\": 1.8298969072164948e-05,\n",
      "      \"loss\": 0.0098,\n",
      "      \"step\": 5540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.369054441260745,\n",
      "      \"grad_norm\": 0.002562997629866004,\n",
      "      \"learning_rate\": 1.8184421534937e-05,\n",
      "      \"loss\": 0.0326,\n",
      "      \"step\": 5560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.391977077363897,\n",
      "      \"grad_norm\": 17.121946334838867,\n",
      "      \"learning_rate\": 1.806987399770905e-05,\n",
      "      \"loss\": 0.0762,\n",
      "      \"step\": 5580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.414899713467049,\n",
      "      \"grad_norm\": 0.014191544614732265,\n",
      "      \"learning_rate\": 1.7955326460481102e-05,\n",
      "      \"loss\": 0.0256,\n",
      "      \"step\": 5600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.437822349570201,\n",
      "      \"grad_norm\": 5.465099334716797,\n",
      "      \"learning_rate\": 1.7840778923253153e-05,\n",
      "      \"loss\": 0.052,\n",
      "      \"step\": 5620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.460744985673353,\n",
      "      \"grad_norm\": 0.06655122339725494,\n",
      "      \"learning_rate\": 1.77262313860252e-05,\n",
      "      \"loss\": 0.0051,\n",
      "      \"step\": 5640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.483667621776505,\n",
      "      \"grad_norm\": 0.03247886523604393,\n",
      "      \"learning_rate\": 1.7611683848797252e-05,\n",
      "      \"loss\": 0.047,\n",
      "      \"step\": 5660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.506590257879656,\n",
      "      \"grad_norm\": 0.07830391824245453,\n",
      "      \"learning_rate\": 1.74971363115693e-05,\n",
      "      \"loss\": 0.0045,\n",
      "      \"step\": 5680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.529512893982808,\n",
      "      \"grad_norm\": 0.002184557495638728,\n",
      "      \"learning_rate\": 1.738258877434135e-05,\n",
      "      \"loss\": 0.0107,\n",
      "      \"step\": 5700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.55243553008596,\n",
      "      \"grad_norm\": 0.00012805458391085267,\n",
      "      \"learning_rate\": 1.7268041237113403e-05,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 5720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.575358166189112,\n",
      "      \"grad_norm\": 0.07765253633260727,\n",
      "      \"learning_rate\": 1.7153493699885454e-05,\n",
      "      \"loss\": 0.0051,\n",
      "      \"step\": 5740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.598280802292264,\n",
      "      \"grad_norm\": 0.013388313353061676,\n",
      "      \"learning_rate\": 1.7038946162657505e-05,\n",
      "      \"loss\": 0.0374,\n",
      "      \"step\": 5760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.6212034383954155,\n",
      "      \"grad_norm\": 0.01150006614625454,\n",
      "      \"learning_rate\": 1.6924398625429556e-05,\n",
      "      \"loss\": 0.0671,\n",
      "      \"step\": 5780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.644126074498567,\n",
      "      \"grad_norm\": 3.4650022983551025,\n",
      "      \"learning_rate\": 1.6809851088201604e-05,\n",
      "      \"loss\": 0.0296,\n",
      "      \"step\": 5800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.667048710601719,\n",
      "      \"grad_norm\": 0.013263324275612831,\n",
      "      \"learning_rate\": 1.6695303550973655e-05,\n",
      "      \"loss\": 0.0276,\n",
      "      \"step\": 5820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.689971346704871,\n",
      "      \"grad_norm\": 0.41411855816841125,\n",
      "      \"learning_rate\": 1.6580756013745703e-05,\n",
      "      \"loss\": 0.0221,\n",
      "      \"step\": 5840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.712893982808023,\n",
      "      \"grad_norm\": 0.003158753039315343,\n",
      "      \"learning_rate\": 1.6466208476517755e-05,\n",
      "      \"loss\": 0.0747,\n",
      "      \"step\": 5860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.735816618911175,\n",
      "      \"grad_norm\": 0.00066919851815328,\n",
      "      \"learning_rate\": 1.6351660939289806e-05,\n",
      "      \"loss\": 0.0844,\n",
      "      \"step\": 5880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.758739255014326,\n",
      "      \"grad_norm\": 0.02683885209262371,\n",
      "      \"learning_rate\": 1.6237113402061857e-05,\n",
      "      \"loss\": 0.0413,\n",
      "      \"step\": 5900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.781661891117478,\n",
      "      \"grad_norm\": 0.008513164706528187,\n",
      "      \"learning_rate\": 1.612256586483391e-05,\n",
      "      \"loss\": 0.0258,\n",
      "      \"step\": 5920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.80458452722063,\n",
      "      \"grad_norm\": 0.1779230237007141,\n",
      "      \"learning_rate\": 1.6008018327605956e-05,\n",
      "      \"loss\": 0.0188,\n",
      "      \"step\": 5940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.827507163323782,\n",
      "      \"grad_norm\": 0.003918647300451994,\n",
      "      \"learning_rate\": 1.5893470790378008e-05,\n",
      "      \"loss\": 0.0748,\n",
      "      \"step\": 5960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.850429799426934,\n",
      "      \"grad_norm\": 30.474905014038086,\n",
      "      \"learning_rate\": 1.577892325315006e-05,\n",
      "      \"loss\": 0.0256,\n",
      "      \"step\": 5980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.873352435530086,\n",
      "      \"grad_norm\": 0.0218906719237566,\n",
      "      \"learning_rate\": 1.5664375715922107e-05,\n",
      "      \"loss\": 0.0424,\n",
      "      \"step\": 6000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.896275071633238,\n",
      "      \"grad_norm\": 0.008453361690044403,\n",
      "      \"learning_rate\": 1.5549828178694158e-05,\n",
      "      \"loss\": 0.0544,\n",
      "      \"step\": 6020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.91919770773639,\n",
      "      \"grad_norm\": 0.003237735712900758,\n",
      "      \"learning_rate\": 1.543528064146621e-05,\n",
      "      \"loss\": 0.0232,\n",
      "      \"step\": 6040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.942120343839542,\n",
      "      \"grad_norm\": 0.11230571568012238,\n",
      "      \"learning_rate\": 1.532073310423826e-05,\n",
      "      \"loss\": 0.0334,\n",
      "      \"step\": 6060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.965042979942694,\n",
      "      \"grad_norm\": 0.0034027535002678633,\n",
      "      \"learning_rate\": 1.5206185567010308e-05,\n",
      "      \"loss\": 0.0528,\n",
      "      \"step\": 6080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 6.987965616045845,\n",
      "      \"grad_norm\": 0.12057509273290634,\n",
      "      \"learning_rate\": 1.509163802978236e-05,\n",
      "      \"loss\": 0.0656,\n",
      "      \"step\": 6100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.0,\n",
      "      \"eval_accuracy\": 0.9384191176470589,\n",
      "      \"eval_f1\": 0.940550133096717,\n",
      "      \"eval_loss\": 0.2628655433654785,\n",
      "      \"eval_precision\": 0.9090909090909091,\n",
      "      \"eval_recall\": 0.9742647058823529,\n",
      "      \"eval_runtime\": 6.0354,\n",
      "      \"eval_samples_per_second\": 180.269,\n",
      "      \"eval_steps_per_second\": 22.534,\n",
      "      \"step\": 6111\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.010315186246419,\n",
      "      \"grad_norm\": 0.17651057243347168,\n",
      "      \"learning_rate\": 1.4977090492554411e-05,\n",
      "      \"loss\": 0.0417,\n",
      "      \"step\": 6120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.03323782234957,\n",
      "      \"grad_norm\": 0.04348354786634445,\n",
      "      \"learning_rate\": 1.486254295532646e-05,\n",
      "      \"loss\": 0.0299,\n",
      "      \"step\": 6140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.056160458452722,\n",
      "      \"grad_norm\": 0.007399214431643486,\n",
      "      \"learning_rate\": 1.4747995418098512e-05,\n",
      "      \"loss\": 0.0468,\n",
      "      \"step\": 6160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.079083094555874,\n",
      "      \"grad_norm\": 1.4909002780914307,\n",
      "      \"learning_rate\": 1.4633447880870563e-05,\n",
      "      \"loss\": 0.0714,\n",
      "      \"step\": 6180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.102005730659026,\n",
      "      \"grad_norm\": 8.204842567443848,\n",
      "      \"learning_rate\": 1.4518900343642614e-05,\n",
      "      \"loss\": 0.0358,\n",
      "      \"step\": 6200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.124928366762178,\n",
      "      \"grad_norm\": 0.0006350511685013771,\n",
      "      \"learning_rate\": 1.4404352806414662e-05,\n",
      "      \"loss\": 0.0153,\n",
      "      \"step\": 6220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.1478510028653295,\n",
      "      \"grad_norm\": 0.0005924510769546032,\n",
      "      \"learning_rate\": 1.4289805269186712e-05,\n",
      "      \"loss\": 0.0106,\n",
      "      \"step\": 6240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.170773638968481,\n",
      "      \"grad_norm\": 0.05214580520987511,\n",
      "      \"learning_rate\": 1.4175257731958763e-05,\n",
      "      \"loss\": 0.0323,\n",
      "      \"step\": 6260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.193696275071633,\n",
      "      \"grad_norm\": 1.1149020195007324,\n",
      "      \"learning_rate\": 1.4066437571592211e-05,\n",
      "      \"loss\": 0.0746,\n",
      "      \"step\": 6280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.216618911174785,\n",
      "      \"grad_norm\": 0.005622190423309803,\n",
      "      \"learning_rate\": 1.3951890034364263e-05,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 6300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.239541547277937,\n",
      "      \"grad_norm\": 0.0036448671016842127,\n",
      "      \"learning_rate\": 1.3837342497136314e-05,\n",
      "      \"loss\": 0.0216,\n",
      "      \"step\": 6320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.262464183381089,\n",
      "      \"grad_norm\": 0.07485207915306091,\n",
      "      \"learning_rate\": 1.3722794959908363e-05,\n",
      "      \"loss\": 0.0294,\n",
      "      \"step\": 6340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.28538681948424,\n",
      "      \"grad_norm\": 0.12374687939882278,\n",
      "      \"learning_rate\": 1.3608247422680411e-05,\n",
      "      \"loss\": 0.0428,\n",
      "      \"step\": 6360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.308309455587392,\n",
      "      \"grad_norm\": 0.005416178144514561,\n",
      "      \"learning_rate\": 1.3493699885452463e-05,\n",
      "      \"loss\": 0.0244,\n",
      "      \"step\": 6380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.331232091690544,\n",
      "      \"grad_norm\": 10.028615951538086,\n",
      "      \"learning_rate\": 1.3379152348224514e-05,\n",
      "      \"loss\": 0.0405,\n",
      "      \"step\": 6400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.354154727793697,\n",
      "      \"grad_norm\": 0.011460020206868649,\n",
      "      \"learning_rate\": 1.3264604810996565e-05,\n",
      "      \"loss\": 0.0385,\n",
      "      \"step\": 6420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.3770773638968485,\n",
      "      \"grad_norm\": 0.01886614039540291,\n",
      "      \"learning_rate\": 1.3150057273768615e-05,\n",
      "      \"loss\": 0.0228,\n",
      "      \"step\": 6440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.4,\n",
      "      \"grad_norm\": 0.0028715834487229586,\n",
      "      \"learning_rate\": 1.3035509736540666e-05,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 6460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.422922636103152,\n",
      "      \"grad_norm\": 0.0038181054405868053,\n",
      "      \"learning_rate\": 1.2920962199312717e-05,\n",
      "      \"loss\": 0.0246,\n",
      "      \"step\": 6480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.445845272206304,\n",
      "      \"grad_norm\": 6.073148727416992,\n",
      "      \"learning_rate\": 1.2806414662084765e-05,\n",
      "      \"loss\": 0.069,\n",
      "      \"step\": 6500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.468767908309456,\n",
      "      \"grad_norm\": 0.22703923285007477,\n",
      "      \"learning_rate\": 1.2691867124856816e-05,\n",
      "      \"loss\": 0.0173,\n",
      "      \"step\": 6520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.491690544412608,\n",
      "      \"grad_norm\": 0.5539343357086182,\n",
      "      \"learning_rate\": 1.2577319587628866e-05,\n",
      "      \"loss\": 0.0273,\n",
      "      \"step\": 6540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.514613180515759,\n",
      "      \"grad_norm\": 0.11442570388317108,\n",
      "      \"learning_rate\": 1.2462772050400917e-05,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 6560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.537535816618911,\n",
      "      \"grad_norm\": 0.884535551071167,\n",
      "      \"learning_rate\": 1.2348224513172968e-05,\n",
      "      \"loss\": 0.0385,\n",
      "      \"step\": 6580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.560458452722063,\n",
      "      \"grad_norm\": 0.13588270545005798,\n",
      "      \"learning_rate\": 1.2233676975945018e-05,\n",
      "      \"loss\": 0.0625,\n",
      "      \"step\": 6600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.583381088825215,\n",
      "      \"grad_norm\": 0.08427141606807709,\n",
      "      \"learning_rate\": 1.2119129438717068e-05,\n",
      "      \"loss\": 0.0222,\n",
      "      \"step\": 6620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.606303724928367,\n",
      "      \"grad_norm\": 0.0024845025036484003,\n",
      "      \"learning_rate\": 1.2004581901489119e-05,\n",
      "      \"loss\": 0.0086,\n",
      "      \"step\": 6640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.6292263610315185,\n",
      "      \"grad_norm\": 0.2857384979724884,\n",
      "      \"learning_rate\": 1.1890034364261168e-05,\n",
      "      \"loss\": 0.0323,\n",
      "      \"step\": 6660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.65214899713467,\n",
      "      \"grad_norm\": 0.017370248213410378,\n",
      "      \"learning_rate\": 1.177548682703322e-05,\n",
      "      \"loss\": 0.0374,\n",
      "      \"step\": 6680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.675071633237822,\n",
      "      \"grad_norm\": 0.01861732080578804,\n",
      "      \"learning_rate\": 1.166093928980527e-05,\n",
      "      \"loss\": 0.012,\n",
      "      \"step\": 6700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.697994269340974,\n",
      "      \"grad_norm\": 0.02694697678089142,\n",
      "      \"learning_rate\": 1.154639175257732e-05,\n",
      "      \"loss\": 0.0439,\n",
      "      \"step\": 6720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.720916905444126,\n",
      "      \"grad_norm\": 0.5165864825248718,\n",
      "      \"learning_rate\": 1.143184421534937e-05,\n",
      "      \"loss\": 0.0542,\n",
      "      \"step\": 6740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.743839541547278,\n",
      "      \"grad_norm\": 0.006770419422537088,\n",
      "      \"learning_rate\": 1.1317296678121421e-05,\n",
      "      \"loss\": 0.0242,\n",
      "      \"step\": 6760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.766762177650429,\n",
      "      \"grad_norm\": 0.021242739632725716,\n",
      "      \"learning_rate\": 1.1202749140893471e-05,\n",
      "      \"loss\": 0.0534,\n",
      "      \"step\": 6780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.789684813753581,\n",
      "      \"grad_norm\": 1.4012641906738281,\n",
      "      \"learning_rate\": 1.108820160366552e-05,\n",
      "      \"loss\": 0.0145,\n",
      "      \"step\": 6800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.812607449856733,\n",
      "      \"grad_norm\": 0.28481554985046387,\n",
      "      \"learning_rate\": 1.0973654066437572e-05,\n",
      "      \"loss\": 0.0007,\n",
      "      \"step\": 6820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.835530085959886,\n",
      "      \"grad_norm\": 0.0360281839966774,\n",
      "      \"learning_rate\": 1.086483390607102e-05,\n",
      "      \"loss\": 0.0783,\n",
      "      \"step\": 6840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.858452722063038,\n",
      "      \"grad_norm\": 0.007976299151778221,\n",
      "      \"learning_rate\": 1.0750286368843071e-05,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 6860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.881375358166189,\n",
      "      \"grad_norm\": 0.010671576485037804,\n",
      "      \"learning_rate\": 1.0635738831615121e-05,\n",
      "      \"loss\": 0.0351,\n",
      "      \"step\": 6880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.904297994269341,\n",
      "      \"grad_norm\": 0.022295117378234863,\n",
      "      \"learning_rate\": 1.052119129438717e-05,\n",
      "      \"loss\": 0.0168,\n",
      "      \"step\": 6900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.927220630372493,\n",
      "      \"grad_norm\": 0.011721576564013958,\n",
      "      \"learning_rate\": 1.0406643757159222e-05,\n",
      "      \"loss\": 0.0451,\n",
      "      \"step\": 6920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.950143266475645,\n",
      "      \"grad_norm\": 0.00842311792075634,\n",
      "      \"learning_rate\": 1.0292096219931271e-05,\n",
      "      \"loss\": 0.0383,\n",
      "      \"step\": 6940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.973065902578797,\n",
      "      \"grad_norm\": 0.015421513468027115,\n",
      "      \"learning_rate\": 1.0177548682703323e-05,\n",
      "      \"loss\": 0.0313,\n",
      "      \"step\": 6960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 7.9959885386819485,\n",
      "      \"grad_norm\": 38.92289733886719,\n",
      "      \"learning_rate\": 1.0063001145475374e-05,\n",
      "      \"loss\": 0.0215,\n",
      "      \"step\": 6980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.0,\n",
      "      \"eval_accuracy\": 0.8492647058823529,\n",
      "      \"eval_f1\": 0.8685897435897436,\n",
      "      \"eval_loss\": 0.7944654822349548,\n",
      "      \"eval_precision\": 0.7698863636363636,\n",
      "      \"eval_recall\": 0.9963235294117647,\n",
      "      \"eval_runtime\": 6.2343,\n",
      "      \"eval_samples_per_second\": 174.519,\n",
      "      \"eval_steps_per_second\": 21.815,\n",
      "      \"step\": 6984\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.01833810888252,\n",
      "      \"grad_norm\": 0.11353445053100586,\n",
      "      \"learning_rate\": 9.948453608247422e-06,\n",
      "      \"loss\": 0.0414,\n",
      "      \"step\": 7000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.041260744985673,\n",
      "      \"grad_norm\": 0.0024136356078088284,\n",
      "      \"learning_rate\": 9.833906071019473e-06,\n",
      "      \"loss\": 0.038,\n",
      "      \"step\": 7020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.064183381088824,\n",
      "      \"grad_norm\": 0.10332543402910233,\n",
      "      \"learning_rate\": 9.719358533791524e-06,\n",
      "      \"loss\": 0.0269,\n",
      "      \"step\": 7040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.087106017191978,\n",
      "      \"grad_norm\": 0.01303681917488575,\n",
      "      \"learning_rate\": 9.604810996563575e-06,\n",
      "      \"loss\": 0.0334,\n",
      "      \"step\": 7060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.11002865329513,\n",
      "      \"grad_norm\": 0.19195908308029175,\n",
      "      \"learning_rate\": 9.490263459335625e-06,\n",
      "      \"loss\": 0.0234,\n",
      "      \"step\": 7080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.132951289398282,\n",
      "      \"grad_norm\": 0.002110303146764636,\n",
      "      \"learning_rate\": 9.375715922107675e-06,\n",
      "      \"loss\": 0.0333,\n",
      "      \"step\": 7100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.155873925501433,\n",
      "      \"grad_norm\": 0.0003446667979005724,\n",
      "      \"learning_rate\": 9.261168384879726e-06,\n",
      "      \"loss\": 0.0013,\n",
      "      \"step\": 7120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.178796561604585,\n",
      "      \"grad_norm\": 8.82823733263649e-05,\n",
      "      \"learning_rate\": 9.146620847651775e-06,\n",
      "      \"loss\": 0.0169,\n",
      "      \"step\": 7140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.201719197707737,\n",
      "      \"grad_norm\": 0.15158572793006897,\n",
      "      \"learning_rate\": 9.032073310423827e-06,\n",
      "      \"loss\": 0.0433,\n",
      "      \"step\": 7160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.224641833810889,\n",
      "      \"grad_norm\": 0.03931021690368652,\n",
      "      \"learning_rate\": 8.917525773195876e-06,\n",
      "      \"loss\": 0.0379,\n",
      "      \"step\": 7180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.24756446991404,\n",
      "      \"grad_norm\": 0.016768086701631546,\n",
      "      \"learning_rate\": 8.802978235967928e-06,\n",
      "      \"loss\": 0.0232,\n",
      "      \"step\": 7200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.270487106017193,\n",
      "      \"grad_norm\": 0.011130846105515957,\n",
      "      \"learning_rate\": 8.688430698739977e-06,\n",
      "      \"loss\": 0.016,\n",
      "      \"step\": 7220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.293409742120344,\n",
      "      \"grad_norm\": 0.006109621375799179,\n",
      "      \"learning_rate\": 8.573883161512028e-06,\n",
      "      \"loss\": 0.0088,\n",
      "      \"step\": 7240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.316332378223496,\n",
      "      \"grad_norm\": 0.06479590386152267,\n",
      "      \"learning_rate\": 8.459335624284078e-06,\n",
      "      \"loss\": 0.0325,\n",
      "      \"step\": 7260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.339255014326648,\n",
      "      \"grad_norm\": 0.03224129229784012,\n",
      "      \"learning_rate\": 8.344788087056128e-06,\n",
      "      \"loss\": 0.0261,\n",
      "      \"step\": 7280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.3621776504298,\n",
      "      \"grad_norm\": 0.25879013538360596,\n",
      "      \"learning_rate\": 8.230240549828179e-06,\n",
      "      \"loss\": 0.0221,\n",
      "      \"step\": 7300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.385100286532952,\n",
      "      \"grad_norm\": 0.0005055117071606219,\n",
      "      \"learning_rate\": 8.11569301260023e-06,\n",
      "      \"loss\": 0.0394,\n",
      "      \"step\": 7320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.408022922636103,\n",
      "      \"grad_norm\": 0.0003365171141922474,\n",
      "      \"learning_rate\": 8.00114547537228e-06,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 7340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.430945558739255,\n",
      "      \"grad_norm\": 3.787245273590088,\n",
      "      \"learning_rate\": 7.88659793814433e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 7360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.453868194842407,\n",
      "      \"grad_norm\": 11.232473373413086,\n",
      "      \"learning_rate\": 7.77205040091638e-06,\n",
      "      \"loss\": 0.0151,\n",
      "      \"step\": 7380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.476790830945559,\n",
      "      \"grad_norm\": 0.0002549454802647233,\n",
      "      \"learning_rate\": 7.657502863688432e-06,\n",
      "      \"loss\": 0.0022,\n",
      "      \"step\": 7400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.49971346704871,\n",
      "      \"grad_norm\": 0.1487710028886795,\n",
      "      \"learning_rate\": 7.542955326460481e-06,\n",
      "      \"loss\": 0.0084,\n",
      "      \"step\": 7420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.522636103151862,\n",
      "      \"grad_norm\": 0.004401584155857563,\n",
      "      \"learning_rate\": 7.428407789232532e-06,\n",
      "      \"loss\": 0.0425,\n",
      "      \"step\": 7440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.545558739255014,\n",
      "      \"grad_norm\": 0.0008593988604843616,\n",
      "      \"learning_rate\": 7.313860252004582e-06,\n",
      "      \"loss\": 0.0235,\n",
      "      \"step\": 7460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.568481375358166,\n",
      "      \"grad_norm\": 0.019395360723137856,\n",
      "      \"learning_rate\": 7.199312714776632e-06,\n",
      "      \"loss\": 0.0519,\n",
      "      \"step\": 7480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.591404011461318,\n",
      "      \"grad_norm\": 0.13640137016773224,\n",
      "      \"learning_rate\": 7.084765177548683e-06,\n",
      "      \"loss\": 0.0343,\n",
      "      \"step\": 7500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.61432664756447,\n",
      "      \"grad_norm\": 0.10724511742591858,\n",
      "      \"learning_rate\": 6.9702176403207335e-06,\n",
      "      \"loss\": 0.0391,\n",
      "      \"step\": 7520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.637249283667622,\n",
      "      \"grad_norm\": 0.018322665244340897,\n",
      "      \"learning_rate\": 6.855670103092785e-06,\n",
      "      \"loss\": 0.0262,\n",
      "      \"step\": 7540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.660171919770773,\n",
      "      \"grad_norm\": 0.003208822337910533,\n",
      "      \"learning_rate\": 6.7411225658648335e-06,\n",
      "      \"loss\": 0.0605,\n",
      "      \"step\": 7560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.683094555873925,\n",
      "      \"grad_norm\": 0.011632462032139301,\n",
      "      \"learning_rate\": 6.626575028636885e-06,\n",
      "      \"loss\": 0.0146,\n",
      "      \"step\": 7580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.706017191977077,\n",
      "      \"grad_norm\": 0.005317831877619028,\n",
      "      \"learning_rate\": 6.512027491408935e-06,\n",
      "      \"loss\": 0.0934,\n",
      "      \"step\": 7600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.728939828080229,\n",
      "      \"grad_norm\": 0.07992788404226303,\n",
      "      \"learning_rate\": 6.397479954180985e-06,\n",
      "      \"loss\": 0.0179,\n",
      "      \"step\": 7620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.75186246418338,\n",
      "      \"grad_norm\": 0.0013258687686175108,\n",
      "      \"learning_rate\": 6.282932416953036e-06,\n",
      "      \"loss\": 0.008,\n",
      "      \"step\": 7640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.774785100286532,\n",
      "      \"grad_norm\": 0.03566892072558403,\n",
      "      \"learning_rate\": 6.1683848797250864e-06,\n",
      "      \"loss\": 0.0177,\n",
      "      \"step\": 7660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.797707736389684,\n",
      "      \"grad_norm\": 0.200466588139534,\n",
      "      \"learning_rate\": 6.053837342497137e-06,\n",
      "      \"loss\": 0.0593,\n",
      "      \"step\": 7680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.820630372492836,\n",
      "      \"grad_norm\": 0.0026684533804655075,\n",
      "      \"learning_rate\": 5.9392898052691864e-06,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 7700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.843553008595988,\n",
      "      \"grad_norm\": 0.090153269469738,\n",
      "      \"learning_rate\": 5.824742268041238e-06,\n",
      "      \"loss\": 0.0006,\n",
      "      \"step\": 7720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.86647564469914,\n",
      "      \"grad_norm\": 0.004433503374457359,\n",
      "      \"learning_rate\": 5.710194730813287e-06,\n",
      "      \"loss\": 0.0144,\n",
      "      \"step\": 7740\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.889398280802292,\n",
      "      \"grad_norm\": 0.0047609698958694935,\n",
      "      \"learning_rate\": 5.5956471935853386e-06,\n",
      "      \"loss\": 0.0188,\n",
      "      \"step\": 7760\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.912320916905443,\n",
      "      \"grad_norm\": 0.02250228449702263,\n",
      "      \"learning_rate\": 5.481099656357388e-06,\n",
      "      \"loss\": 0.0686,\n",
      "      \"step\": 7780\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.935243553008595,\n",
      "      \"grad_norm\": 0.0038538346998393536,\n",
      "      \"learning_rate\": 5.3665521191294386e-06,\n",
      "      \"loss\": 0.0372,\n",
      "      \"step\": 7800\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.958166189111747,\n",
      "      \"grad_norm\": 22.246442794799805,\n",
      "      \"learning_rate\": 5.25200458190149e-06,\n",
      "      \"loss\": 0.0126,\n",
      "      \"step\": 7820\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 8.9810888252149,\n",
      "      \"grad_norm\": 0.03928838297724724,\n",
      "      \"learning_rate\": 5.137457044673539e-06,\n",
      "      \"loss\": 0.0548,\n",
      "      \"step\": 7840\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.0,\n",
      "      \"eval_accuracy\": 0.8952205882352942,\n",
      "      \"eval_f1\": 0.9045226130653267,\n",
      "      \"eval_loss\": 0.49581921100616455,\n",
      "      \"eval_precision\": 0.8307692307692308,\n",
      "      \"eval_recall\": 0.9926470588235294,\n",
      "      \"eval_runtime\": 6.0473,\n",
      "      \"eval_samples_per_second\": 179.914,\n",
      "      \"eval_steps_per_second\": 22.489,\n",
      "      \"step\": 7857\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.003438395415472,\n",
      "      \"grad_norm\": 0.02612616866827011,\n",
      "      \"learning_rate\": 5.022909507445591e-06,\n",
      "      \"loss\": 0.0074,\n",
      "      \"step\": 7860\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.026361031518624,\n",
      "      \"grad_norm\": 0.0014328708639368415,\n",
      "      \"learning_rate\": 4.90836197021764e-06,\n",
      "      \"loss\": 0.0268,\n",
      "      \"step\": 7880\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.049283667621777,\n",
      "      \"grad_norm\": 0.1849585324525833,\n",
      "      \"learning_rate\": 4.7938144329896915e-06,\n",
      "      \"loss\": 0.0101,\n",
      "      \"step\": 7900\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.07220630372493,\n",
      "      \"grad_norm\": 0.03744812309741974,\n",
      "      \"learning_rate\": 4.679266895761741e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 7920\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.095128939828081,\n",
      "      \"grad_norm\": 0.0008724742219783366,\n",
      "      \"learning_rate\": 4.5647193585337915e-06,\n",
      "      \"loss\": 0.0224,\n",
      "      \"step\": 7940\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.118051575931233,\n",
      "      \"grad_norm\": 10.09687328338623,\n",
      "      \"learning_rate\": 4.450171821305842e-06,\n",
      "      \"loss\": 0.043,\n",
      "      \"step\": 7960\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.140974212034385,\n",
      "      \"grad_norm\": 7.2546305656433105,\n",
      "      \"learning_rate\": 4.335624284077892e-06,\n",
      "      \"loss\": 0.001,\n",
      "      \"step\": 7980\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.163896848137536,\n",
      "      \"grad_norm\": 0.22747276723384857,\n",
      "      \"learning_rate\": 4.221076746849944e-06,\n",
      "      \"loss\": 0.0568,\n",
      "      \"step\": 8000\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.186819484240688,\n",
      "      \"grad_norm\": 0.007571281399577856,\n",
      "      \"learning_rate\": 4.106529209621993e-06,\n",
      "      \"loss\": 0.0112,\n",
      "      \"step\": 8020\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.20974212034384,\n",
      "      \"grad_norm\": 0.01737595535814762,\n",
      "      \"learning_rate\": 3.991981672394044e-06,\n",
      "      \"loss\": 0.0468,\n",
      "      \"step\": 8040\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.232664756446992,\n",
      "      \"grad_norm\": 0.0016066248062998056,\n",
      "      \"learning_rate\": 3.877434135166094e-06,\n",
      "      \"loss\": 0.0205,\n",
      "      \"step\": 8060\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.255587392550144,\n",
      "      \"grad_norm\": 0.1423652619123459,\n",
      "      \"learning_rate\": 3.7628865979381445e-06,\n",
      "      \"loss\": 0.0615,\n",
      "      \"step\": 8080\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.278510028653296,\n",
      "      \"grad_norm\": 0.00033610305399633944,\n",
      "      \"learning_rate\": 3.6483390607101953e-06,\n",
      "      \"loss\": 0.0271,\n",
      "      \"step\": 8100\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.301432664756447,\n",
      "      \"grad_norm\": 5.437369346618652,\n",
      "      \"learning_rate\": 3.5337915234822453e-06,\n",
      "      \"loss\": 0.0008,\n",
      "      \"step\": 8120\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.3243553008596,\n",
      "      \"grad_norm\": 0.00047731021186336875,\n",
      "      \"learning_rate\": 3.419243986254296e-06,\n",
      "      \"loss\": 0.0121,\n",
      "      \"step\": 8140\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.347277936962751,\n",
      "      \"grad_norm\": 0.0005555422976613045,\n",
      "      \"learning_rate\": 3.304696449026346e-06,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 8160\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.370200573065903,\n",
      "      \"grad_norm\": 0.03468198701739311,\n",
      "      \"learning_rate\": 3.190148911798396e-06,\n",
      "      \"loss\": 0.0207,\n",
      "      \"step\": 8180\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.393123209169055,\n",
      "      \"grad_norm\": 0.0014455525670200586,\n",
      "      \"learning_rate\": 3.0756013745704466e-06,\n",
      "      \"loss\": 0.0028,\n",
      "      \"step\": 8200\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.416045845272206,\n",
      "      \"grad_norm\": 0.0020270671229809523,\n",
      "      \"learning_rate\": 2.961053837342497e-06,\n",
      "      \"loss\": 0.0009,\n",
      "      \"step\": 8220\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.438968481375358,\n",
      "      \"grad_norm\": 0.023917783051729202,\n",
      "      \"learning_rate\": 2.846506300114548e-06,\n",
      "      \"loss\": 0.0003,\n",
      "      \"step\": 8240\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.46189111747851,\n",
      "      \"grad_norm\": 0.0009687507408671081,\n",
      "      \"learning_rate\": 2.7319587628865982e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 8260\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.484813753581662,\n",
      "      \"grad_norm\": 0.0003205083776265383,\n",
      "      \"learning_rate\": 2.6174112256586487e-06,\n",
      "      \"loss\": 0.0324,\n",
      "      \"step\": 8280\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.507736389684814,\n",
      "      \"grad_norm\": 0.17857316136360168,\n",
      "      \"learning_rate\": 2.502863688430699e-06,\n",
      "      \"loss\": 0.0125,\n",
      "      \"step\": 8300\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.530659025787966,\n",
      "      \"grad_norm\": 0.18487177789211273,\n",
      "      \"learning_rate\": 2.388316151202749e-06,\n",
      "      \"loss\": 0.0242,\n",
      "      \"step\": 8320\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.553581661891117,\n",
      "      \"grad_norm\": 0.1617279052734375,\n",
      "      \"learning_rate\": 2.2737686139747995e-06,\n",
      "      \"loss\": 0.072,\n",
      "      \"step\": 8340\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.57650429799427,\n",
      "      \"grad_norm\": 0.052532460540533066,\n",
      "      \"learning_rate\": 2.15922107674685e-06,\n",
      "      \"loss\": 0.043,\n",
      "      \"step\": 8360\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.599426934097421,\n",
      "      \"grad_norm\": 28.674596786499023,\n",
      "      \"learning_rate\": 2.0446735395189004e-06,\n",
      "      \"loss\": 0.0306,\n",
      "      \"step\": 8380\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.622349570200573,\n",
      "      \"grad_norm\": 0.10457964986562729,\n",
      "      \"learning_rate\": 1.9301260022909508e-06,\n",
      "      \"loss\": 0.0185,\n",
      "      \"step\": 8400\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.645272206303725,\n",
      "      \"grad_norm\": 0.0011161970905959606,\n",
      "      \"learning_rate\": 1.8155784650630014e-06,\n",
      "      \"loss\": 0.0332,\n",
      "      \"step\": 8420\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.668194842406876,\n",
      "      \"grad_norm\": 0.011616433970630169,\n",
      "      \"learning_rate\": 1.7010309278350514e-06,\n",
      "      \"loss\": 0.0357,\n",
      "      \"step\": 8440\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.691117478510028,\n",
      "      \"grad_norm\": 0.0028468903619796038,\n",
      "      \"learning_rate\": 1.586483390607102e-06,\n",
      "      \"loss\": 0.0005,\n",
      "      \"step\": 8460\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.71404011461318,\n",
      "      \"grad_norm\": 0.011797486804425716,\n",
      "      \"learning_rate\": 1.4719358533791525e-06,\n",
      "      \"loss\": 0.0359,\n",
      "      \"step\": 8480\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.736962750716332,\n",
      "      \"grad_norm\": 0.025532536208629608,\n",
      "      \"learning_rate\": 1.3573883161512029e-06,\n",
      "      \"loss\": 0.0365,\n",
      "      \"step\": 8500\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.759885386819484,\n",
      "      \"grad_norm\": 0.09146895259618759,\n",
      "      \"learning_rate\": 1.242840778923253e-06,\n",
      "      \"loss\": 0.0002,\n",
      "      \"step\": 8520\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.782808022922636,\n",
      "      \"grad_norm\": 0.012477078475058079,\n",
      "      \"learning_rate\": 1.1282932416953035e-06,\n",
      "      \"loss\": 0.0216,\n",
      "      \"step\": 8540\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.805730659025787,\n",
      "      \"grad_norm\": 24.831188201904297,\n",
      "      \"learning_rate\": 1.013745704467354e-06,\n",
      "      \"loss\": 0.0289,\n",
      "      \"step\": 8560\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.82865329512894,\n",
      "      \"grad_norm\": 0.0011106588644906878,\n",
      "      \"learning_rate\": 8.991981672394044e-07,\n",
      "      \"loss\": 0.0705,\n",
      "      \"step\": 8580\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.851575931232091,\n",
      "      \"grad_norm\": 0.03501393273472786,\n",
      "      \"learning_rate\": 7.846506300114548e-07,\n",
      "      \"loss\": 0.0191,\n",
      "      \"step\": 8600\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.874498567335243,\n",
      "      \"grad_norm\": 0.004802646115422249,\n",
      "      \"learning_rate\": 6.701030927835051e-07,\n",
      "      \"loss\": 0.0156,\n",
      "      \"step\": 8620\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.897421203438395,\n",
      "      \"grad_norm\": 0.002449620747938752,\n",
      "      \"learning_rate\": 5.555555555555556e-07,\n",
      "      \"loss\": 0.0227,\n",
      "      \"step\": 8640\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.920343839541548,\n",
      "      \"grad_norm\": 0.5641228556632996,\n",
      "      \"learning_rate\": 4.41008018327606e-07,\n",
      "      \"loss\": 0.0167,\n",
      "      \"step\": 8660\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.943266475644698,\n",
      "      \"grad_norm\": 0.005499226041138172,\n",
      "      \"learning_rate\": 3.2646048109965636e-07,\n",
      "      \"loss\": 0.027,\n",
      "      \"step\": 8680\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.966189111747852,\n",
      "      \"grad_norm\": 0.0007104789256118238,\n",
      "      \"learning_rate\": 2.1191294387170676e-07,\n",
      "      \"loss\": 0.0004,\n",
      "      \"step\": 8700\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 9.989111747851004,\n",
      "      \"grad_norm\": 0.07235758751630783,\n",
      "      \"learning_rate\": 9.736540664375717e-08,\n",
      "      \"loss\": 0.0011,\n",
      "      \"step\": 8720\n",
      "    },\n",
      "    {\n",
      "      \"epoch\": 10.0,\n",
      "      \"eval_accuracy\": 0.9338235294117647,\n",
      "      \"eval_f1\": 0.9373913043478261,\n",
      "      \"eval_loss\": 0.30846619606018066,\n",
      "      \"eval_precision\": 0.8894389438943895,\n",
      "      \"eval_recall\": 0.9908088235294118,\n",
      "      \"eval_runtime\": 6.204,\n",
      "      \"eval_samples_per_second\": 175.37,\n",
      "      \"eval_steps_per_second\": 21.921,\n",
      "      \"step\": 8730\n",
      "    }\n",
      "  ],\n",
      "  \"logging_steps\": 20,\n",
      "  \"max_steps\": 8730,\n",
      "  \"num_input_tokens_seen\": 0,\n",
      "  \"num_train_epochs\": 10,\n",
      "  \"save_steps\": 500,\n",
      "  \"stateful_callbacks\": {\n",
      "    \"TrainerControl\": {\n",
      "      \"args\": {\n",
      "        \"should_epoch_stop\": false,\n",
      "        \"should_evaluate\": false,\n",
      "        \"should_log\": false,\n",
      "        \"should_save\": true,\n",
      "        \"should_training_stop\": true\n",
      "      },\n",
      "      \"attributes\": {}\n",
      "    }\n",
      "  },\n",
      "  \"total_flos\": 2.32177401831638e+18,\n",
      "  \"train_batch_size\": 8,\n",
      "  \"trial_name\": null,\n",
      "  \"trial_params\": null\n",
      "}\n",
      "Model loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the checkpoint path\n",
    "checkpoint_path = \"runs/ast_classifier/checkpoint-8730\"\n",
    "\n",
    "# Load the model and feature extractor\n",
    "model = ASTForAudioClassification.from_pretrained(checkpoint_path)\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Look for training history\n",
    "trainer_state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "if os.path.exists(trainer_state_path):\n",
    "    with open(trainer_state_path, \"r\") as f:\n",
    "        trainer_state = json.load(f)\n",
    "    print(\"\\nTraining metrics from trainer_state.json:\")\n",
    "    print(json.dumps(trainer_state, indent=2))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcf5ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "def predict_audio(file_path, model, feature_extractor, device=\"cuda\"):\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(file_path, sr=feature_extractor.sampling_rate)\n",
    "    \n",
    "    # Preprocess the audio\n",
    "    inputs = feature_extractor(\n",
    "        audio, \n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    print(f\"Raw logits: {logits}\")\n",
    "    print(f\"Raw probabilities: {probabilities}\")\n",
    "    \n",
    "    # Get predicted class (0 for fake, 1 for real)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    print(f\"P(fake): {probabilities[0][0].item():.4f}\")\n",
    "    print(f\"P(real): {probabilities[0][1].item():.4f}\")\n",
    "    \n",
    "    # Map class index to label\n",
    "    label = \"fake\" if predicted_class == 0 else \"real\"\n",
    "    \n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"confidence\": confidence,\n",
    "        \"probabilities\": {\n",
    "            \"fake\": probabilities[0][0].item(),\n",
    "            \"real\": probabilities[0][1].item()\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbbf04d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw logits: tensor([[ 4.0093, -4.5176]])\n",
      "Raw probabilities: tensor([[9.9980e-01, 1.9803e-04]])\n",
      "P(fake): 0.9998\n",
      "P(real): 0.0002\n",
      "Prediction: fake\n",
      "Confidence: 0.9998\n",
      "Probabilities - Fake: 0.9998, Real: 0.0002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "audio_file_path = \"file1186.wav_16k.wav_norm.wav_mono.wav_silence.wav_2sec.wav\"\n",
    "result = predict_audio(audio_file_path, model, feature_extractor, device)\n",
    "\n",
    "print(f\"Prediction: {result['label']}\")\n",
    "print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "print(f\"Probabilities - Fake: {result['probabilities']['fake']:.4f}, Real: {result['probabilities']['real']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
