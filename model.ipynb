{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import ASTFeatureExtractor\n",
    "from transformers.utils import is_speech_available\n",
    "from transformers.audio_utils import mel_filter_bank, spectrogram, window_function\n",
    "\n",
    "if is_speech_available():\n",
    "    import torchaudio.compliance.kaldi as ta_kaldi\n",
    "\n",
    "# based on the following literature that uses the Hamming window:\n",
    "    # https://arxiv.org/pdf/2505.15136\n",
    "    # https://arxiv.org/pdf/2409.05924\n",
    "    # https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11007653\n",
    "class ASTFeatureExtractorHamming(ASTFeatureExtractor):\n",
    "    \"\"\"\n",
    "    Custom AST Feature Extractor that uses Hamming window instead of Hann/Hanning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Override the window for numpy-based processing (when torchaudio is not available)\n",
    "        if not is_speech_available():\n",
    "            # Recalculate mel filters and window with hamming\n",
    "            mel_filters = mel_filter_bank(\n",
    "                num_frequency_bins=257,\n",
    "                num_mel_filters=self.num_mel_bins,\n",
    "                min_frequency=20,\n",
    "                max_frequency=self.sampling_rate // 2,\n",
    "                sampling_rate=self.sampling_rate,\n",
    "                norm=None,\n",
    "                mel_scale=\"kaldi\",\n",
    "                triangularize_in_mel_space=True,\n",
    "            )\n",
    "            self.mel_filters = mel_filters\n",
    "            # Use hamming window instead of hann\n",
    "            self.window = window_function(400, \"hamming\", periodic=False)\n",
    "    \n",
    "    def _extract_fbank_features(self, waveform: np.ndarray, max_length: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Override to use hamming window type in torchaudio.compliance.kaldi.fbank\n",
    "        \"\"\"\n",
    "        if is_speech_available():\n",
    "            waveform = torch.from_numpy(waveform).unsqueeze(0)\n",
    "            fbank = ta_kaldi.fbank(\n",
    "                waveform,\n",
    "                sample_frequency=self.sampling_rate,\n",
    "                window_type=\"hamming\",  # Changed from \"hanning\" to \"hamming\"\n",
    "                num_mel_bins=self.num_mel_bins,\n",
    "            )\n",
    "        else:\n",
    "            # Use numpy implementation with hamming window\n",
    "            waveform = np.squeeze(waveform)\n",
    "            fbank = spectrogram(\n",
    "                waveform,\n",
    "                self.window,  # This is now hamming window from __init__\n",
    "                frame_length=400, # this follows the 25 ms frame length used in the paper (16000mhz * 0.025 = 400)\n",
    "                hop_length=160, # this follows the hop length used in the paper (16000mhz * 0.01 = 160)\n",
    "                fft_length=512,\n",
    "                power=2.0,\n",
    "                center=False,\n",
    "                preemphasis=0.97,\n",
    "                mel_filters=self.mel_filters,\n",
    "                log_mel=\"log\",\n",
    "                mel_floor=1.192092955078125e-07,\n",
    "                remove_dc_offset=True,\n",
    "            ).T\n",
    "            fbank = torch.from_numpy(fbank)\n",
    "\n",
    "        n_frames = fbank.shape[0]\n",
    "        difference = max_length - n_frames\n",
    "\n",
    "        # pad or truncate, depending on difference\n",
    "        if difference > 0:\n",
    "            pad_module = torch.nn.ZeroPad2d((0, 0, 0, difference))\n",
    "            fbank = pad_module(fbank)\n",
    "        elif difference < 0:\n",
    "            fbank = fbank[0:max_length, :]\n",
    "\n",
    "        fbank = fbank.numpy()\n",
    "        return fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ASTForAudioClassification\n",
    "from evaluate import evaluator, combine\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import ASTFeatureExtractor\n",
    "\n",
    "NUM_MEL_BINS = 128 # based on https://arxiv.org/pdf/2409.05924\n",
    "MAX_SEQUENCE_LENGTH = 507\n",
    "\n",
    "# checkpoints = [\n",
    "#     # # 1st iteration - test run (one dataset only)\n",
    "#     # \"./runs2/ast_classifier/checkpoint-158700\",\n",
    "    \n",
    "#     # # 2nd iteration - first official run\n",
    "#     # \"./eval/ast_classifier/checkpoint-174570\", # epoch 11\n",
    "\n",
    "#     # # 3rd iteration / checking if epoch 13 is really peak (test patchout)\n",
    "#     # \"./runs2/ast_classifier/checkpoint-63480\", # epoch 9\n",
    "#     # \"./runs2/ast_classifier/checkpoint-71415\", # epoch 10\n",
    "#     # \"./runs2/ast_classifier/checkpoint-79350\", # epoch 11\n",
    "#     # \"./runs2/ast_classifier/checkpoint-87285\", # epoch 12\n",
    "#     # \"./runs2/ast_classifier/checkpoint-95220\", # epoch 13\n",
    "\n",
    "#     # 4th iteration - cleaner dataset + using librespeech. Tuned on test set so use val set for testing\n",
    "#     # \"./runs2/ast_classifier/checkpoint-31740\", # epoch 1\n",
    "#     # \"./runs2/ast_classifier/checkpoint-63480\", # epoch 2\n",
    "#     # \"./runs2/ast_classifier/checkpoint-95220\", # epoch 3\n",
    "#     # \"./runs2/ast_classifier/checkpoint-126960\", # epoch 4\n",
    "#     # \"./runs2/ast_classifier/checkpoint-158700\", # epoch 5\n",
    "#     # \"./runs2/ast_classifier/checkpoint-190440\", # epoch 6\n",
    "#     # \"./runs2/ast_classifier/checkpoint-222180\", # epoch 7\n",
    "\n",
    "#     # 5th iteration - cleaner dataset + using librespeech. Tuned on test set so use val set for testing. 15% oversampled\n",
    "#     # \"./epoch1-9/ast_classifier/checkpoint-34567\", # epoch 1\n",
    "#     # \"./epoch1-9/ast_classifier/checkpoint-69134\", # epoch 2\n",
    "#     # \"./epoch1-9/ast_classifier/checkpoint-103701\", # epoch 3\n",
    "#     # \"./epoch1-9/ast_classifier/checkpoint-138268\", # epoch 4\n",
    "#     # \"./epoch1-9/ast_classifier/checkpoint-172835\", # epoch 5\n",
    "#     # \"./epoch1-9/ast_classifier/checkpoint-207402\", # epoch 6\n",
    "#     # \"./epoch1-9/ast_classifier/checkpoint-241969\", # epoch 7    \n",
    "#     # \"./epoch1-9/ast_classifier/checkpoint-276536\", # epoch 8    \n",
    "#     # \"./epoch1-9/ast_classifier/checkpoint-311103\", # epoch 9\n",
    "# ]\n",
    "\n",
    "checkpoints = [\n",
    "    os.path.join(\"eval-checkpoints\", \"ast_classifier\", d) \n",
    "    for d in sorted(os.listdir(os.path.join(\"eval-checkpoints\", \"ast_classifier\")))\n",
    "    if os.path.isdir(os.path.join(\"eval-checkpoints\", \"ast_classifier\", d)) and \"iteration\" in d\n",
    "]\n",
    "\n",
    "print(\"Found checkpoints:\", checkpoints)\n",
    "\n",
    "def load_models(checkpoints:list):\n",
    "    models={}\n",
    "    for i, checkpoint in enumerate(checkpoints):\n",
    "        print(checkpoint)\n",
    "        model = ASTForAudioClassification.from_pretrained(checkpoint, local_files_only=True)\n",
    "        if i == len(checkpoints) - 1 or i == len(checkpoints) - 1: # last 2 model uses 1024 bin\n",
    "            feature_extractor = ASTFeatureExtractorHamming.from_pretrained(checkpoint, num_mel_bins=NUM_MEL_BINS, max_sequence_length=1024, local_files_only=True)\n",
    "            print(model)\n",
    "        else:\n",
    "            feature_extractor = ASTFeatureExtractorHamming.from_pretrained(checkpoint, num_mel_bins=NUM_MEL_BINS, max_sequence_length=MAX_SEQUENCE_LENGTH, local_files_only=True)\n",
    "        \n",
    "        print(\"do_normalize:\", feature_extractor.do_normalize)\n",
    "        print(\"mean:\", getattr(feature_extractor, \"mean\", None))\n",
    "        print(\"std:\", getattr(feature_extractor, \"std\", None))\n",
    "        feature_extractor.do_normalize = True\n",
    "        print(\"do_normalize:\", feature_extractor.do_normalize)\n",
    "        \n",
    "        models[i] = {\n",
    "            \"model\": model,\n",
    "            \"feature_extractor\": feature_extractor\n",
    "        }\n",
    "    return models\n",
    "\n",
    "task_evaluator = evaluator(task=\"audio-classification\")\n",
    "results = []\n",
    "models = load_models(checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30016af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay, classification_report,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "# List of dataset paths to evaluate on\n",
    "DATASET_PATHS = [\n",
    "    {\"name\": \"wavefake\", \"path\": r\"C:\\Users\\crumbz\\Downloads\\thesis-testing\\eval-datasets\\wavefake\"},\n",
    "    {\"name\": \"dataset-balanced-arrow\", \"path\": r\"C:\\Users\\crumbz\\Downloads\\thesis-testing\\eval-datasets\\dataset-balanced-arrow\"},\n",
    "    {\"name\": \"LibriSeVoc\", \"path\": r\"C:\\Users\\crumbz\\Downloads\\thesis-testing\\eval-datasets\\LibriSeVoc\"},\n",
    "    {\"name\": \"ASVspoof2019\", \"path\": r\"C:\\Users\\crumbz\\Downloads\\thesis-testing\\eval-datasets\\ASVSpoof2019\"},\n",
    "    {\"name\": \"release_in_the_wild\", \"path\": r\"C:\\Users\\crumbz\\Downloads\\thesis-testing\\eval-datasets\\release_in_the_wild\"},\n",
    "    {\"name\": \"SONAR_dataset\", \"path\": r\"C:\\Users\\crumbz\\Downloads\\thesis-testing\\eval-datasets\\SONAR_dataset\"},\n",
    "\n",
    "]\n",
    "\n",
    "# Evaluation settings\n",
    "OUTPUT_DIR = \"evaluation-results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def load_dataset_helper(dataset_path):\n",
    "    \"\"\"Load dataset from path, handling both arrow datasets and audio folders\"\"\"\n",
    "    try:\n",
    "        # First try loading as an arrow dataset\n",
    "        if os.path.exists(os.path.join(dataset_path, \"dataset_dict.json\")):\n",
    "            from datasets import load_from_disk\n",
    "            return load_from_disk(dataset_path)\n",
    "        # If not an arrow dataset, load as an audio folder\n",
    "        return load_dataset(\"audiofolder\", data_dir=dataset_path, num_proc=os.cpu_count() - 1, drop_labels=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset from {dataset_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def save_plots(plot_data, model_name, dataset_name, output_dir):\n",
    "    \"\"\"Save evaluation plots to disk\"\"\"\n",
    "    # Create a safe model name by taking only the last part of the path\n",
    "    safe_model_name = model_name.split('/')[-1].split('\\\\')[-1]  # Handles both / and \\ separators\n",
    "    base_path = os.path.join(output_dir, f\"{dataset_name[:10]}_{safe_model_name[:30]}\")\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save confusion matrix\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=plot_data[\"cm\"], \n",
    "                                 display_labels=[\"fake (0)\", \"real (1)\"])\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\", colorbar=False)\n",
    "    plt.title(f\"Confusion Matrix\\n{model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{base_path}_cm.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save ROC curve\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(plot_data[\"fpr\"], plot_data[\"tpr\"], \n",
    "             label=f\"AUC = {plot_data['roc_auc']:.4f}\")\n",
    "    plt.plot([0, 1], [0, 1], \"--\", color=\"gray\", alpha=0.7)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{base_path}_roc.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Add F1-score vs Threshold plot\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(plot_data[\"thresholds\"], plot_data[\"f1_scores\"], label=\"F1-score\", color=\"green\")\n",
    "    plt.axvline(x=0.5, color=\"r\", linestyle=\"--\", label=\"Default Threshold (0.5)\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"F1-Score\")\n",
    "    plt.title(\"F1-Score vs Threshold\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{base_path}_f1.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Save classification report\n",
    "    with open(f\"{base_path}_report.txt\", \"w\") as f:\n",
    "        f.write(plot_data[\"classification_report\"])\n",
    "    \n",
    "    return base_path  # Return the base path for reference\n",
    "\n",
    "# --- Main Evaluation Function ---\n",
    "def evaluate_model_on_dataset(model_info, dataset, dataset_name, output_dir):\n",
    "    \"\"\"Evaluate a single model on a single dataset\"\"\"\n",
    "    device_index = 0 if torch.cuda.is_available() else -1\n",
    "    pipe = pipeline(\n",
    "        \"audio-classification\",\n",
    "        model=model_info[\"model\"],\n",
    "        feature_extractor=model_info[\"feature_extractor\"],\n",
    "        device=device_index,\n",
    "    )\n",
    "    \n",
    "    # Prepare data\n",
    "    label_mapping = {\"fake\": 0, \"real\": 1}\n",
    "    test_ds = dataset[\"test\"]\n",
    "    references = [label_mapping[x] if x in label_mapping else int(x) \n",
    "                 for x in test_ds[\"label\"]]\n",
    "    \n",
    "    # Get predictions\n",
    "    scores_pos = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for pred in tqdm(\n",
    "        pipe(KeyDataset(test_ds, \"audio\"), top_k=None),\n",
    "        total=len(test_ds),\n",
    "        desc=f\"Evaluating on {dataset_name}\",\n",
    "    ):\n",
    "        score_map = {p[\"label\"]: p[\"score\"] for p in pred}\n",
    "        real_score = score_map.get(\"real\", 0.0)\n",
    "        fake_score = score_map.get(\"fake\", 0.0)\n",
    "        scores_pos.append(real_score)\n",
    "        y_pred.append(1 if real_score >= 0.5 else 0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": evaluate.load(\"accuracy\").compute(\n",
    "            predictions=y_pred, references=references\n",
    "        )[\"accuracy\"],\n",
    "        \"precision\": precision_score(references, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(references, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(references, y_pred, zero_division=0),\n",
    "    }\n",
    "    \n",
    "    # Calculate ROC and PR curves\n",
    "    fpr, tpr, _ = roc_curve(references, scores_pos)\n",
    "    prec, rec, _ = precision_recall_curve(references, scores_pos)\n",
    "    metrics.update({\n",
    "        \"roc_auc\": auc(fpr, tpr),\n",
    "        \"average_precision\": average_precision_score(references, scores_pos)\n",
    "    })\n",
    "    \n",
    "    # Prepare plot data\n",
    "    plot_data = {\n",
    "        \"cm\": confusion_matrix(references, y_pred, labels=[0, 1]),\n",
    "        \"fpr\": fpr,\n",
    "        \"tpr\": tpr,\n",
    "        \"prec\": prec,\n",
    "        \"rec\": rec,\n",
    "        \"roc_auc\": metrics[\"roc_auc\"],\n",
    "        \"classification_report\": classification_report(\n",
    "            references, y_pred, target_names=[\"fake\", \"real\"], digits=4\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Calculate F1-scores for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "    f1_scores = [f1_score(references, [1 if score >= t else 0 for score in scores_pos], zero_division=0) \n",
    "                for t in thresholds]\n",
    "    \n",
    "    # Update plot_data with thresholds and f1_scores\n",
    "    plot_data.update({\n",
    "        \"thresholds\": thresholds,\n",
    "        \"f1_scores\": f1_scores\n",
    "    })\n",
    "\n",
    "    return metrics, plot_data\n",
    "\n",
    "# --- Main Execution ---\n",
    "all_results = []\n",
    "\n",
    "for dataset_info in DATASET_PATHS:\n",
    "    dataset_name = dataset_info[\"name\"]\n",
    "    dataset_path = dataset_info[\"path\"]\n",
    "    \n",
    "    print(f\"\\\\n{'='*50}\")\n",
    "    print(f\"Evaluating on dataset: {dataset_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        # Load dataset\n",
    "        print(f\"Loading dataset from {dataset_path}...\")\n",
    "        dataset = load_dataset_helper(dataset_path)\n",
    "        \n",
    "        # Create output directory for this dataset\n",
    "        dataset_output_dir = os.path.join(OUTPUT_DIR, dataset_name)\n",
    "        os.makedirs(dataset_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Evaluate each model\n",
    "        for i, model_info in models.items():\n",
    "            print(f\"\\\\nEvaluating model {i+1}/{len(models)}\")\n",
    "            \n",
    "            # Run evaluation\n",
    "            metrics, plot_data = evaluate_model_on_dataset(\n",
    "                model_info=model_info,\n",
    "                dataset=dataset,\n",
    "                dataset_name=dataset_name,\n",
    "                output_dir=dataset_output_dir\n",
    "            )\n",
    "            \n",
    "            # In your main execution loop, replace the save_plots call with:\n",
    "            base_path = save_plots(\n",
    "                plot_data=plot_data,\n",
    "                model_name=checkpoints[i].split(\"/\")[-1].split(\"\\\\\")[-1],  # Get just the checkpoint name\n",
    "                dataset_name=dataset_name,\n",
    "                output_dir=dataset_output_dir\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                \"dataset\": dataset_name,\n",
    "                \"model\": checkpoints[i],\n",
    "                **metrics\n",
    "            }\n",
    "            all_results.append(result)\n",
    "            \n",
    "            # Save intermediate results\n",
    "            pd.DataFrame(all_results).to_csv(\n",
    "                os.path.join(OUTPUT_DIR, \"all_results.csv\"),\n",
    "                index=False\n",
    "            )   \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {dataset_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Display final results\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\\\nEvaluation completed! Summary of results:\")\n",
    "    display(results_df.groupby(['dataset', 'model'])[['accuracy', 'f1', 'roc_auc']].mean().round(4))\n",
    "    results_df.to_csv(os.path.join(OUTPUT_DIR, \"final_results.csv\"), index=False)\n",
    "    print(f\"\\\\nResults saved to {os.path.join(OUTPUT_DIR, 'final_results.csv')}\")\n",
    "else:\n",
    "    print(\"No results were generated. Please check for errors.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
