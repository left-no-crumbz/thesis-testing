{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc9a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODEL EVALUATION AND PREDICTION\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import ASTFeatureExtractor,ASTForAudioClassification\n",
    "\n",
    "# Define the checkpoint path\n",
    "checkpoint_path = \"runs/ast_classifier/checkpoint-95220\"\n",
    "\n",
    "# Load the model and feature extractor\n",
    "model = ASTForAudioClassification.from_pretrained(checkpoint_path)\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Look for training history\n",
    "trainer_state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "if os.path.exists(trainer_state_path):\n",
    "    with open(trainer_state_path, \"r\") as f:\n",
    "        trainer_state = json.load(f)\n",
    "    print(\"\\nTraining metrics from trainer_state.json:\")\n",
    "    print(json.dumps(trainer_state, indent=2))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "def predict_audio(file_path, model, feature_extractor, device=\"cuda\"):\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(file_path, sr=feature_extractor.sampling_rate)\n",
    "\n",
    "    spec = librosa.feature.melspectrogram(y=audio, sr=feature_extractor.sampling_rate)\n",
    "    spec_db = librosa.power_to_db(spec, ref=np.max)\n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 1)\n",
    "    img = librosa.display.specshow(spec_db, x_axis='time', y_axis='mel', ax = ax)\n",
    "    fig.colorbar(img, ax = ax, format='%+2.0f dB')\n",
    "    ax.set_title('Spectrogram')\n",
    "    fig.show()\n",
    "\n",
    "    # Preprocess the audio\n",
    "    inputs = feature_extractor(\n",
    "        audio,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    print(f\"Raw logits: {logits}\")\n",
    "    print(f\"Raw probabilities: {probabilities}\")\n",
    "\n",
    "    # Get predicted class (0 for fake, 1 for real)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "    print(f\"P(fake): {probabilities[0][0].item():.4f}\")\n",
    "    print(f\"P(real): {probabilities[0][1].item():.4f}\")\n",
    "\n",
    "    # Map class index to label\n",
    "    label = \"fake\" if predicted_class == 0 else \"real\"\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"confidence\": confidence,\n",
    "        \"probabilities\": {\n",
    "            \"fake\": probabilities[0][0].item(),\n",
    "            \"real\": probabilities[0][1].item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "audio_file_path = r\"LibriSeVoc\\train\\fake\\diffwave_26_495_000028_000000_gen.wav\" # Adjust the path as needed\n",
    "result = predict_audio(audio_file_path, model, feature_extractor, device)\n",
    "\n",
    "print(f\"Prediction: {result['label']}\")\n",
    "print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "print(f\"Probabilities - Fake: {result['probabilities']['fake']:.4f}, Real: {result['probabilities']['real']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab58d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### AUDIO CUTTING SCRIPT\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def cut_audio(input_path, output_dir=\"./cut\", clip_duration=5):\n",
    "     # Ensure output directory exists\n",
    "     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "     # Get file name and extension\n",
    "     base_name = os.path.basename(input_path)\n",
    "     name, ext = os.path.splitext(base_name)\n",
    "\n",
    "     # Load audio\n",
    "     audio, sr = librosa.load(input_path, sr=None)\n",
    "     total_duration = librosa.get_duration(y=audio, sr=sr)\n",
    "     clip_samples = int(clip_duration * sr)\n",
    "     num_clips = int(total_duration // clip_duration) + (1 if total_duration % clip_duration > 0 else 0)\n",
    "\n",
    "     for i in range(num_clips):\n",
    "          start_sample = i * clip_samples\n",
    "          end_sample = min((i + 1) * clip_samples, len(audio))\n",
    "          clip_audio = audio[start_sample:end_sample]\n",
    "          out_path = os.path.join(output_dir, f\"{name}({i+1}){ext}\")\n",
    "          sf.write(out_path, clip_audio, sr)\n",
    "          print(f\"Saved: {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     if len(sys.argv) < 2:\n",
    "          print(\"Usage: python cut_audio.py <path_to_audio_file>\")\n",
    "          sys.exit(1)\n",
    "     audio_path = \"\"  # Adjust the path as needed\n",
    "     if not os.path.exists(audio_path):\n",
    "          print(f\"File not found: {audio_path}\")\n",
    "          sys.exit(1)\n",
    "     cut_audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SEGMENTED AUDIO PREDICTION SCRIPT\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Directory containing segmented audio clips\n",
    "segmented_dir = \"./cut\"\n",
    "\n",
    "# Collect all audio files and group by base name (without segment index)\n",
    "audio_groups = defaultdict(list)\n",
    "for fname in os.listdir(segmented_dir):\n",
    "     if fname.lower().endswith(('.wav', '.mp3', '.flac', '.ogg')):\n",
    "          # Extract base name (e.g., \"audio(1).wav\" -> \"audio\")\n",
    "          base = fname.split('(')[0]\n",
    "          audio_groups[base].append(os.path.join(segmented_dir, fname))\n",
    "\n",
    "# Function to predict for a single audio file\n",
    "def predict_single(file_path):\n",
    "     return predict_audio(file_path, model, feature_extractor, device)\n",
    "\n",
    "# Aggregate predictions for each group\n",
    "results = {}\n",
    "for base, files in audio_groups.items():\n",
    "     fake_count = 0\n",
    "     real_count = 0\n",
    "     for fpath in files:\n",
    "          pred = predict_single(fpath)\n",
    "          if pred[\"label\"] == \"fake\":\n",
    "               fake_count += 1\n",
    "          else:\n",
    "               real_count += 1\n",
    "     total = fake_count + real_count\n",
    "     results[base] = {\n",
    "          \"fake_ratio\": fake_count / total if total > 0 else 0,\n",
    "          \"real_ratio\": real_count / total if total > 0 else 0,\n",
    "          \"fake_count\": fake_count,\n",
    "          \"real_count\": real_count,\n",
    "          \"total\": total\n",
    "     }\n",
    "\n",
    "# Print summary\n",
    "for base, stats in results.items():\n",
    "     print(f\"{base}: Fake {stats['fake_count']}/{stats['total']} ({stats['fake_ratio']:.2f}), \"\n",
    "            f\"Real {stats['real_count']}/{stats['total']} ({stats['real_ratio']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import ASTFeatureExtractor\n",
    "from transformers.utils import is_speech_available\n",
    "from transformers.audio_utils import mel_filter_bank, spectrogram, window_function\n",
    "\n",
    "if is_speech_available():\n",
    "    import torchaudio.compliance.kaldi as ta_kaldi\n",
    "\n",
    "# based on the following literature that uses the Hamming window:\n",
    "    # https://arxiv.org/pdf/2505.15136\n",
    "    # https://arxiv.org/pdf/2409.05924\n",
    "    # https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11007653\n",
    "class ASTFeatureExtractorHamming(ASTFeatureExtractor):\n",
    "    \"\"\"\n",
    "    Custom AST Feature Extractor that uses Hamming window instead of Hann/Hanning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Override the window for numpy-based processing (when torchaudio is not available)\n",
    "        if not is_speech_available():\n",
    "            # Recalculate mel filters and window with hamming\n",
    "            mel_filters = mel_filter_bank(\n",
    "                num_frequency_bins=257,\n",
    "                num_mel_filters=self.num_mel_bins,\n",
    "                min_frequency=20,\n",
    "                max_frequency=self.sampling_rate // 2,\n",
    "                sampling_rate=self.sampling_rate,\n",
    "                norm=None,\n",
    "                mel_scale=\"kaldi\",\n",
    "                triangularize_in_mel_space=True,\n",
    "            )\n",
    "            self.mel_filters = mel_filters\n",
    "            # Use hamming window instead of hann\n",
    "            self.window = window_function(400, \"hamming\", periodic=False)\n",
    "    \n",
    "    def _extract_fbank_features(self, waveform: np.ndarray, max_length: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Override to use hamming window type in torchaudio.compliance.kaldi.fbank\n",
    "        \"\"\"\n",
    "        if is_speech_available():\n",
    "            waveform = torch.from_numpy(waveform).unsqueeze(0)\n",
    "            fbank = ta_kaldi.fbank(\n",
    "                waveform,\n",
    "                sample_frequency=self.sampling_rate,\n",
    "                window_type=\"hamming\",  # Changed from \"hanning\" to \"hamming\"\n",
    "                num_mel_bins=self.num_mel_bins,\n",
    "            )\n",
    "        else:\n",
    "            # Use numpy implementation with hamming window\n",
    "            waveform = np.squeeze(waveform)\n",
    "            fbank = spectrogram(\n",
    "                waveform,\n",
    "                self.window,  # This is now hamming window from __init__\n",
    "                frame_length=400, # this follows the 25 ms frame length used in the paper (16000mhz * 0.025 = 400)\n",
    "                hop_length=160, # this follows the hop length used in the paper (16000mhz * 0.01 = 160)\n",
    "                fft_length=512,\n",
    "                power=2.0,\n",
    "                center=False,\n",
    "                preemphasis=0.97,\n",
    "                mel_filters=self.mel_filters,\n",
    "                log_mel=\"log\",\n",
    "                mel_floor=1.192092955078125e-07,\n",
    "                remove_dc_offset=True,\n",
    "            ).T\n",
    "            fbank = torch.from_numpy(fbank)\n",
    "\n",
    "        n_frames = fbank.shape[0]\n",
    "        difference = max_length - n_frames\n",
    "\n",
    "        # pad or truncate, depending on difference\n",
    "        if difference > 0:\n",
    "            pad_module = torch.nn.ZeroPad2d((0, 0, 0, difference))\n",
    "            fbank = pad_module(fbank)\n",
    "        elif difference < 0:\n",
    "            fbank = fbank[0:max_length, :]\n",
    "\n",
    "        fbank = fbank.numpy()\n",
    "        return fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import evaluator, combine\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import ASTFeatureExtractor,ASTForAudioClassification\n",
    "\n",
    "NUM_PROC = os.cpu_count() - 1\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=\"./LibriSeVoc\", num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print('dataset train features:', dataset['train'].features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ASTForAudioClassification\n",
    "\n",
    "NUM_MEL_BINS = 128 # based on https://arxiv.org/pdf/2409.05924\n",
    "MAX_SEQUENCE_LENGTH = 507\n",
    "\n",
    "checkpoints = [\n",
    "    # 1st iteration - test run (one dataset only)\n",
    "    \"./runs/ast_classifier/checkpoint-158700\",\n",
    "    # 2nd iteration - first official run\n",
    "    \"./runs/ast_classifier/checkpoint-174570\", # epoch 11\n",
    "    # 3rd iteration / checking if epoch 13 is really peak (test patchout)\n",
    "    \"./runs/ast_classifier/checkpoint-63480\", # epoch 9\n",
    "    \"./runs/ast_classifier/checkpoint-71415\", # epoch 10\n",
    "    \"./runs/ast_classifier/checkpoint-79350\", # epoch 11\n",
    "    \"./runs/ast_classifier/checkpoint-87285\", # epoch 12\n",
    "    \"./runs/ast_classifier/checkpoint-95220\", # epoch 13\n",
    "]\n",
    "\n",
    "mean = 0.31605425901883943\n",
    "std = 0.45787811188377187\n",
    "\n",
    "def load_models(checkpoints:list):\n",
    "    models={}\n",
    "    for i, checkpoint in enumerate(checkpoints):\n",
    "        print(checkpoint)\n",
    "        model = ASTForAudioClassification.from_pretrained(checkpoint, local_files_only=True)\n",
    "        if i == 0: # 1st iteration uses a different config\n",
    "            feature_extractor = ASTFeatureExtractor.from_pretrained(checkpoint, num_mel_bins=64, max_sequence_length=MAX_SEQUENCE_LENGTH, local_files_only=True)\n",
    "        else:\n",
    "            feature_extractor = ASTFeatureExtractor.from_pretrained(checkpoint, num_mel_bins=NUM_MEL_BINS, max_sequence_length=MAX_SEQUENCE_LENGTH, local_files_only=True)\n",
    "        \n",
    "        feature_extractor.mean = mean\n",
    "        feature_extractor.std = std\n",
    "\n",
    "        models[i] = {\n",
    "            \"model\": model,\n",
    "            \"feature_extractor\": feature_extractor\n",
    "        }\n",
    "    return models\n",
    "\n",
    "task_evaluator = evaluator(task=\"audio-classification\")\n",
    "results = []\n",
    "models = load_models(checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f226b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = (np.unique(dataset[\"train\"][\"label\"]))\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"LibriSeVoc\"  # Adjust path if needed\n",
    "\n",
    "for subdir in os.listdir(base_dir):\n",
    "    subdir_path = os.path.join(base_dir, subdir)\n",
    "    if os.path.isdir(subdir_path):\n",
    "        for filename in os.listdir(subdir_path):\n",
    "            old_path = os.path.join(subdir_path, filename)\n",
    "            if os.path.isfile(old_path):\n",
    "                new_filename = f\"{subdir}_{filename}\"\n",
    "                new_path = os.path.join(subdir_path, new_filename)\n",
    "                \n",
    "                # Avoid overwriting if file already exists\n",
    "                if not os.path.exists(new_path):\n",
    "                    os.rename(old_path, new_path)\n",
    "                else:\n",
    "                    print(f\"Skipping {old_path}: {new_path} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "real = 0\n",
    "fake = 0\n",
    "for i in range(len(dataset[\"train\"])):\n",
    "    if dataset[\"train\"][i][\"label\"] == 0:\n",
    "        real += 1\n",
    "    else:\n",
    "        fake += 1\n",
    "\n",
    "print(\"number of real datasets:\", real)\n",
    "print(\"number of fake datasets:\", fake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for i in range(len(models)):\n",
    "    results.append(\n",
    "        task_evaluator.compute(\n",
    "        model_or_pipeline=models[i][\"model\"],\n",
    "        feature_extractor=models[i][\"feature_extractor\"],\n",
    "        data=dataset[\"train\"],\n",
    "        input_column=\"audio\",\n",
    "        label_column=\"label\",\n",
    "        label_mapping={\"fake\": 0, \"real\": 1},\n",
    "        metric=combine([\"accuracy\", \"recall\", \"precision\", \"f1\"]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "df2 = pd.DataFrame(results, index=checkpoints)\n",
    "df2[[\"accuracy\", \"recall\", \"precision\", \"f1\"]].round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
