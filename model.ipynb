{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc9a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODEL EVALUATION AND PREDICTION\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import ASTFeatureExtractor,ASTForAudioClassification\n",
    "\n",
    "# Define the checkpoint path\n",
    "checkpoint_path = \"runs/ast_classifier/checkpoint-95220\"\n",
    "\n",
    "# Load the model and feature extractor\n",
    "model = ASTForAudioClassification.from_pretrained(checkpoint_path)\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Look for training history\n",
    "trainer_state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "if os.path.exists(trainer_state_path):\n",
    "    with open(trainer_state_path, \"r\") as f:\n",
    "        trainer_state = json.load(f)\n",
    "    print(\"\\nTraining metrics from trainer_state.json:\")\n",
    "    print(json.dumps(trainer_state, indent=2))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "def predict_audio(file_path, model, feature_extractor, device=\"cuda\"):\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(file_path, sr=feature_extractor.sampling_rate)\n",
    "\n",
    "    spec = librosa.feature.melspectrogram(y=audio, sr=feature_extractor.sampling_rate)\n",
    "    spec_db = librosa.power_to_db(spec, ref=np.max)\n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 1)\n",
    "    img = librosa.display.specshow(spec_db, x_axis='time', y_axis='mel', ax = ax)\n",
    "    fig.colorbar(img, ax = ax, format='%+2.0f dB')\n",
    "    ax.set_title('Spectrogram')\n",
    "    fig.show()\n",
    "\n",
    "    # Preprocess the audio\n",
    "    inputs = feature_extractor(\n",
    "        audio,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    print(f\"Raw logits: {logits}\")\n",
    "    print(f\"Raw probabilities: {probabilities}\")\n",
    "\n",
    "    # Get predicted class (0 for fake, 1 for real)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "    print(f\"P(fake): {probabilities[0][0].item():.4f}\")\n",
    "    print(f\"P(real): {probabilities[0][1].item():.4f}\")\n",
    "\n",
    "    # Map class index to label\n",
    "    label = \"fake\" if predicted_class == 0 else \"real\"\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"confidence\": confidence,\n",
    "        \"probabilities\": {\n",
    "            \"fake\": probabilities[0][0].item(),\n",
    "            \"real\": probabilities[0][1].item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "audio_file_path = r\"LibriSeVoc\\train\\fake\\diffwave_26_495_000028_000000_gen.wav\" # Adjust the path as needed\n",
    "result = predict_audio(audio_file_path, model, feature_extractor, device)\n",
    "\n",
    "print(f\"Prediction: {result['label']}\")\n",
    "print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "print(f\"Probabilities - Fake: {result['probabilities']['fake']:.4f}, Real: {result['probabilities']['real']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab58d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### AUDIO CUTTING SCRIPT\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def cut_audio(input_path, output_dir=\"./cut\", clip_duration=5):\n",
    "     # Ensure output directory exists\n",
    "     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "     # Get file name and extension\n",
    "     base_name = os.path.basename(input_path)\n",
    "     name, ext = os.path.splitext(base_name)\n",
    "\n",
    "     # Load audio\n",
    "     audio, sr = librosa.load(input_path, sr=None)\n",
    "     total_duration = librosa.get_duration(y=audio, sr=sr)\n",
    "     clip_samples = int(clip_duration * sr)\n",
    "     num_clips = int(total_duration // clip_duration) + (1 if total_duration % clip_duration > 0 else 0)\n",
    "\n",
    "     for i in range(num_clips):\n",
    "          start_sample = i * clip_samples\n",
    "          end_sample = min((i + 1) * clip_samples, len(audio))\n",
    "          clip_audio = audio[start_sample:end_sample]\n",
    "          out_path = os.path.join(output_dir, f\"{name}({i+1}){ext}\")\n",
    "          sf.write(out_path, clip_audio, sr)\n",
    "          print(f\"Saved: {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     if len(sys.argv) < 2:\n",
    "          print(\"Usage: python cut_audio.py <path_to_audio_file>\")\n",
    "          sys.exit(1)\n",
    "     audio_path = \"\"  # Adjust the path as needed\n",
    "     if not os.path.exists(audio_path):\n",
    "          print(f\"File not found: {audio_path}\")\n",
    "          sys.exit(1)\n",
    "     cut_audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SEGMENTED AUDIO PREDICTION SCRIPT\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Directory containing segmented audio clips\n",
    "segmented_dir = \"./cut\"\n",
    "\n",
    "# Collect all audio files and group by base name (without segment index)\n",
    "audio_groups = defaultdict(list)\n",
    "for fname in os.listdir(segmented_dir):\n",
    "     if fname.lower().endswith(('.wav', '.mp3', '.flac', '.ogg')):\n",
    "          # Extract base name (e.g., \"audio(1).wav\" -> \"audio\")\n",
    "          base = fname.split('(')[0]\n",
    "          audio_groups[base].append(os.path.join(segmented_dir, fname))\n",
    "\n",
    "# Function to predict for a single audio file\n",
    "def predict_single(file_path):\n",
    "     return predict_audio(file_path, model, feature_extractor, device)\n",
    "\n",
    "# Aggregate predictions for each group\n",
    "results = {}\n",
    "for base, files in audio_groups.items():\n",
    "     fake_count = 0\n",
    "     real_count = 0\n",
    "     for fpath in files:\n",
    "          pred = predict_single(fpath)\n",
    "          if pred[\"label\"] == \"fake\":\n",
    "               fake_count += 1\n",
    "          else:\n",
    "               real_count += 1\n",
    "     total = fake_count + real_count\n",
    "     results[base] = {\n",
    "          \"fake_ratio\": fake_count / total if total > 0 else 0,\n",
    "          \"real_ratio\": real_count / total if total > 0 else 0,\n",
    "          \"fake_count\": fake_count,\n",
    "          \"real_count\": real_count,\n",
    "          \"total\": total\n",
    "     }\n",
    "\n",
    "# Print summary\n",
    "for base, stats in results.items():\n",
    "     print(f\"{base}: Fake {stats['fake_count']}/{stats['total']} ({stats['fake_ratio']:.2f}), \"\n",
    "            f\"Real {stats['real_count']}/{stats['total']} ({stats['real_ratio']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3816ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import ASTFeatureExtractor\n",
    "from transformers.utils import is_speech_available\n",
    "from transformers.audio_utils import mel_filter_bank, spectrogram, window_function\n",
    "\n",
    "if is_speech_available():\n",
    "    import torchaudio.compliance.kaldi as ta_kaldi\n",
    "\n",
    "# based on the following literature that uses the Hamming window:\n",
    "    # https://arxiv.org/pdf/2505.15136\n",
    "    # https://arxiv.org/pdf/2409.05924\n",
    "    # https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11007653\n",
    "class ASTFeatureExtractorHamming(ASTFeatureExtractor):\n",
    "    \"\"\"\n",
    "    Custom AST Feature Extractor that uses Hamming window instead of Hann/Hanning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Override the window for numpy-based processing (when torchaudio is not available)\n",
    "        if not is_speech_available():\n",
    "            # Recalculate mel filters and window with hamming\n",
    "            mel_filters = mel_filter_bank(\n",
    "                num_frequency_bins=257,\n",
    "                num_mel_filters=self.num_mel_bins,\n",
    "                min_frequency=20,\n",
    "                max_frequency=self.sampling_rate // 2,\n",
    "                sampling_rate=self.sampling_rate,\n",
    "                norm=None,\n",
    "                mel_scale=\"kaldi\",\n",
    "                triangularize_in_mel_space=True,\n",
    "            )\n",
    "            self.mel_filters = mel_filters\n",
    "            # Use hamming window instead of hann\n",
    "            self.window = window_function(400, \"hamming\", periodic=False)\n",
    "    \n",
    "    def _extract_fbank_features(self, waveform: np.ndarray, max_length: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Override to use hamming window type in torchaudio.compliance.kaldi.fbank\n",
    "        \"\"\"\n",
    "        if is_speech_available():\n",
    "            waveform = torch.from_numpy(waveform).unsqueeze(0)\n",
    "            fbank = ta_kaldi.fbank(\n",
    "                waveform,\n",
    "                sample_frequency=self.sampling_rate,\n",
    "                window_type=\"hamming\",  # Changed from \"hanning\" to \"hamming\"\n",
    "                num_mel_bins=self.num_mel_bins,\n",
    "            )\n",
    "        else:\n",
    "            # Use numpy implementation with hamming window\n",
    "            waveform = np.squeeze(waveform)\n",
    "            fbank = spectrogram(\n",
    "                waveform,\n",
    "                self.window,  # This is now hamming window from __init__\n",
    "                frame_length=400, # this follows the 25 ms frame length used in the paper (16000mhz * 0.025 = 400)\n",
    "                hop_length=160, # this follows the hop length used in the paper (16000mhz * 0.01 = 160)\n",
    "                fft_length=512,\n",
    "                power=2.0,\n",
    "                center=False,\n",
    "                preemphasis=0.97,\n",
    "                mel_filters=self.mel_filters,\n",
    "                log_mel=\"log\",\n",
    "                mel_floor=1.192092955078125e-07,\n",
    "                remove_dc_offset=True,\n",
    "            ).T\n",
    "            fbank = torch.from_numpy(fbank)\n",
    "\n",
    "        n_frames = fbank.shape[0]\n",
    "        difference = max_length - n_frames\n",
    "\n",
    "        # pad or truncate, depending on difference\n",
    "        if difference > 0:\n",
    "            pad_module = torch.nn.ZeroPad2d((0, 0, 0, difference))\n",
    "            fbank = pad_module(fbank)\n",
    "        elif difference < 0:\n",
    "            fbank = fbank[0:max_length, :]\n",
    "\n",
    "        fbank = fbank.numpy()\n",
    "        return fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7340f69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5790b6a6580a4728b7d4fcf01e217282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/92407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import evaluator, combine\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import ASTFeatureExtractor,ASTForAudioClassification\n",
    "\n",
    "NUM_PROC = os.cpu_count() - 1\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=\"./LibriSeVoc\", num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print('dataset train features:', dataset['train'].features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4efb1970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./runs2/ast_classifier/checkpoint-31740\n",
      "./runs2/ast_classifier/checkpoint-63480\n",
      "./runs2/ast_classifier/checkpoint-95220\n",
      "./runs2/ast_classifier/checkpoint-126960\n",
      "./runs2/ast_classifier/checkpoint-158700\n",
      "./runs2/ast_classifier/checkpoint-190440\n",
      "./runs2/ast_classifier/checkpoint-222180\n"
     ]
    }
   ],
   "source": [
    "from transformers import ASTForAudioClassification\n",
    "\n",
    "NUM_MEL_BINS = 128 # based on https://arxiv.org/pdf/2409.05924\n",
    "MAX_SEQUENCE_LENGTH = 507\n",
    "\n",
    "checkpoints = [\n",
    "    # # 1st iteration - test run (one dataset only)\n",
    "    # \"./runs2/ast_classifier/checkpoint-158700\",\n",
    "    \n",
    "    # # 2nd iteration - first official run\n",
    "    # \"./runs2/ast_classifier/checkpoint-174570\", # epoch 11\n",
    "\n",
    "    # # 3rd iteration / checking if epoch 13 is really peak (test patchout)\n",
    "    # \"./runs2/ast_classifier/checkpoint-63480\", # epoch 9\n",
    "    # \"./runs2/ast_classifier/checkpoint-71415\", # epoch 10\n",
    "    # \"./runs2/ast_classifier/checkpoint-79350\", # epoch 11\n",
    "    # \"./runs2/ast_classifier/checkpoint-87285\", # epoch 12\n",
    "    # \"./runs2/ast_classifier/checkpoint-95220\", # epoch 13\n",
    "\n",
    "    # 4th iteration - cleaner dataset + using librespeech. Tuned on test set so use val set for testing\n",
    "    \"./runs2/ast_classifier/checkpoint-31740\", # epoch 1\n",
    "    \"./runs2/ast_classifier/checkpoint-63480\", # epoch 2\n",
    "    \"./runs2/ast_classifier/checkpoint-95220\", # epoch 3\n",
    "    \"./runs2/ast_classifier/checkpoint-126960\", # epoch 4\n",
    "    \"./runs2/ast_classifier/checkpoint-158700\", # epoch 5\n",
    "    \"./runs2/ast_classifier/checkpoint-190440\", # epoch 6\n",
    "    \"./runs2/ast_classifier/checkpoint-222180\", # epoch 7    \n",
    "]\n",
    "\n",
    "def load_models(checkpoints:list):\n",
    "    models={}\n",
    "    for i, checkpoint in enumerate(checkpoints):\n",
    "        print(checkpoint)\n",
    "        model = ASTForAudioClassification.from_pretrained(checkpoint, local_files_only=True)\n",
    "        # if i == 0: # 1st iteration uses a different config\n",
    "        #     feature_extractor = ASTFeatureExtractor.from_pretrained(checkpoint, num_mel_bins=64, max_sequence_length=MAX_SEQUENCE_LENGTH, local_files_only=True)\n",
    "        # else:\n",
    "        feature_extractor = ASTFeatureExtractor.from_pretrained(checkpoint, num_mel_bins=NUM_MEL_BINS, max_sequence_length=MAX_SEQUENCE_LENGTH, local_files_only=True)\n",
    "        feature_extractor.do_normalize = True\n",
    "        \n",
    "        models[i] = {\n",
    "            \"model\": model,\n",
    "            \"feature_extractor\": feature_extractor\n",
    "        }\n",
    "    return models\n",
    "\n",
    "task_evaluator = evaluator(task=\"audio-classification\")\n",
    "results = []\n",
    "models = load_models(checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f226b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = (np.unique(dataset[\"train\"][\"label\"]))\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"LibriSeVoc\"  # Adjust path if needed\n",
    "\n",
    "for subdir in os.listdir(base_dir):\n",
    "    subdir_path = os.path.join(base_dir, subdir)\n",
    "    if os.path.isdir(subdir_path):\n",
    "        for filename in os.listdir(subdir_path):\n",
    "            old_path = os.path.join(subdir_path, filename)\n",
    "            if os.path.isfile(old_path):\n",
    "                new_filename = f\"{subdir}_{filename}\"\n",
    "                new_path = os.path.join(subdir_path, new_filename)\n",
    "                \n",
    "                # Avoid overwriting if file already exists\n",
    "                if not os.path.exists(new_path):\n",
    "                    os.rename(old_path, new_path)\n",
    "                else:\n",
    "                    print(f\"Skipping {old_path}: {new_path} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "real = 0\n",
    "fake = 0\n",
    "for i in range(len(dataset[\"train\"])):\n",
    "    if dataset[\"train\"][i][\"label\"] == 0:\n",
    "        real += 1\n",
    "    else:\n",
    "        fake += 1\n",
    "\n",
    "print(\"number of real datasets:\", real)\n",
    "print(\"number of fake datasets:\", fake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from evaluate import combine\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configure which metrics to compute via Evaluator\n",
    "standard_metrics = [\"accuracy\", \"recall\", \"precision\", \"f1\"]\n",
    "\n",
    "# Reuse the same mapping you already had in Evaluator\n",
    "label_mapping = {\"fake\": 0, \"real\": 1}\n",
    "input_column = \"audio\"\n",
    "label_column = \"label\"\n",
    "# Identify the positive class name for ROC AUC (the class mapped to 1)\n",
    "positive_label_name = next(k for k, v in label_mapping.items() if v == 1)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(len(models)):  \n",
    "    # 1) Compute prediction-based metrics with Evaluator (unchanged)\n",
    "    eval_result = task_evaluator.compute(\n",
    "        model_or_pipeline=models[i][\"model\"],\n",
    "        feature_extractor=models[i][\"feature_extractor\"],\n",
    "        data=dataset[\"test\"],\n",
    "        input_column=input_column,\n",
    "        label_column=label_column,\n",
    "        label_mapping=label_mapping,\n",
    "        metric=combine(standard_metrics),\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "\n",
    "    # 2) Compute ROC AUC separately with batched pipeline + Dataset\n",
    "    # Resolve or build an audio-classification pipeline\n",
    "    pipe = models[i].get(\"pipeline\")\n",
    "    if pipe is None:\n",
    "        device_index = 0 if torch.cuda.is_available() else -1\n",
    "        pipe = pipeline(\n",
    "            \"audio-classification\",\n",
    "            model=models[i][\"model\"],\n",
    "            feature_extractor=models[i][\"feature_extractor\"],\n",
    "            device=\"cpu\",           # GPU=0, CPU=-1\n",
    "        )\n",
    "\n",
    "    # Collect references (mapped to ints) in dataset order\n",
    "    references = [\n",
    "        label_mapping[x] if x in label_mapping else int(x)\n",
    "        for x in dataset[\"test\"][label_column]\n",
    "    ]\n",
    "\n",
    "    # Stream audio from the Dataset and get per-sample probs for the positive class\n",
    "    scores = []\n",
    "    for pred in tqdm(\n",
    "        pipe(\n",
    "            KeyDataset(dataset[\"test\"], input_column),\n",
    "            top_k=None,                   # return all class scores\n",
    "        ),\n",
    "        total=len(dataset[\"test\"]),\n",
    "        desc=f\"ROC AUC pass (model {i})\",\n",
    "    ):\n",
    "        # pred is a list of {\"label\": str, \"score\": float}\n",
    "        real_score = next(p[\"score\"] for p in pred if p[\"label\"] == positive_label_name)\n",
    "        scores.append(real_score)\n",
    "\n",
    "    # Compute roc_auc\n",
    "    roc_auc_metric = evaluate.load(\"roc_auc\")\n",
    "    roc_auc_result = roc_auc_metric.compute(\n",
    "        prediction_scores=scores,\n",
    "        references=references,\n",
    "    )\n",
    "\n",
    "    # Merge and store\n",
    "    eval_result.update(roc_auc_result)\n",
    "    results.append(eval_result)\n",
    "\n",
    "# Final DataFrame\n",
    "df2 = pd.DataFrame(results, index=checkpoints)\n",
    "df2[standard_metrics + [\"roc_auc\"]].round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics: confusion matrix, classification report, ROC and PR curves, threshold sweep\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1) Pick the best checkpoint by ROC AUC (from df2 computed above)\n",
    "best_i = int(np.argmax(df2[\"roc_auc\"].to_numpy()))\n",
    "print(f\"Selected checkpoint for diagnostics: {checkpoints[best_i]} (ROC AUC={df2.loc[checkpoints[best_i], 'roc_auc']:.4f})\")\n",
    "\n",
    "# 2) Optional: limit to a subset of test samples for speed (set to None to use all)\n",
    "N_SAMPLES = 10_000  # e.g., 5000 for a quick pass; set None for full test set\n",
    "test_ds = dataset[\"test\"]\n",
    "if N_SAMPLES is not None and N_SAMPLES < len(test_ds):\n",
    "    rng = np.random.default_rng(42)\n",
    "    idx = rng.choice(len(test_ds), size=N_SAMPLES, replace=False)\n",
    "    idx = np.sort(idx)\n",
    "    test_ds = test_ds.select(idx)\n",
    "    print(f\"Using a subset of {len(test_ds)} samples for plotting.\")\n",
    "\n",
    "# 3) Build pipeline for the selected model\n",
    "device_index = 0 if torch.cuda.is_available() else -1\n",
    "pipe = pipeline(\n",
    "    \"audio-classification\",\n",
    "    model=models[best_i][\"model\"],\n",
    "    feature_extractor=models[best_i][\"feature_extractor\"],\n",
    "    device=device_index,\n",
    ")\n",
    "\n",
    "# 4) Prepare references (0=fake, 1=real) and run predictions\n",
    "references = [\n",
    "    label_mapping[x] if x in label_mapping else int(x)\n",
    "    for x in test_ds[\"label\"]\n",
    "]\n",
    "\n",
    "scores_pos = []  # probability/score for the positive class (real==1)\n",
    "y_pred = []      # hard predictions via argmax\n",
    "\n",
    "for pred in tqdm(\n",
    "    pipe(KeyDataset(test_ds, input_column), top_k=None),\n",
    "    total=len(test_ds),\n",
    "    desc=f\"Inference for diagnostics (model {best_i})\",\n",
    "):\n",
    "    # pred is a list of {\"label\": str, \"score\": float}\n",
    "    score_map = {p[\"label\"]: p[\"score\"] for p in pred}\n",
    "    real_score = score_map.get(\"real\", 0.0)\n",
    "    fake_score = score_map.get(\"fake\", 0.0)\n",
    "\n",
    "    scores_pos.append(real_score)\n",
    "    y_pred.append(1 if real_score >= fake_score else 0)\n",
    "\n",
    "# 5) Confusion Matrix\n",
    "cm = confusion_matrix(references, y_pred, labels=[0, 1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"fake (0)\", \"real (1)\"])\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "disp.plot(cmap=\"Blues\", ax=ax, values_format=\"d\", colorbar=False)\n",
    "ax.set_title(f\"Confusion Matrix\\n{checkpoints[best_i]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6) Classification report\n",
    "print(classification_report(references, y_pred, target_names=[\"fake\", \"real\"], digits=4))\n",
    "\n",
    "# 7) ROC Curve\n",
    "fpr, tpr, thr_roc = roc_curve(references, scores_pos)\n",
    "roc_auc_val = auc(fpr, tpr)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc_val:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], \"--\", color=\"gray\", alpha=0.7)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8) Precision–Recall Curve\n",
    "prec, rec, thr_pr = precision_recall_curve(references, scores_pos)\n",
    "ap = average_precision_score(references, scores_pos)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(rec, prec, label=f\"AP = {ap:.4f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9) Threshold sweep (optional) to see the best F1 on this test set\n",
    "thr_grid = np.linspace(0, 1, 101)\n",
    "best_f1 = -1.0\n",
    "best_thr = 0.5\n",
    "for t in thr_grid:\n",
    "    y_hat = [1 if s >= t else 0 for s in scores_pos]\n",
    "    f1 = f1_score(references, y_hat)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thr = t\n",
    "\n",
    "y_hat_best = [1 if s >= best_thr else 0 for s in scores_pos]\n",
    "p_best = precision_score(references, y_hat_best, zero_division=0)\n",
    "r_best = recall_score(references, y_hat_best, zero_division=0)\n",
    "\n",
    "print(f\"Best F1 on this set via threshold: F1={best_f1:.4f} at threshold={best_thr:.2f}\")\n",
    "print(f\"Precision={p_best:.4f}, Recall={r_best:.4f} at that threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d703cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "def plot_training_curves(log_dir, metrics=('loss',), smooth=0.9):\n",
    "    \"\"\"Plot training curves from TensorBoard logs.\n",
    "    \n",
    "    Args:\n",
    "        log_dir: Path to the directory containing TensorBoard logs\n",
    "        metrics: Tuple of metric names to plot (e.g., ('loss', 'accuracy'))\n",
    "        smooth: Smoothing factor for exponential moving average (0-1)\n",
    "    \"\"\"\n",
    "    # Find the latest run directory\n",
    "    run_dirs = [d for d in os.listdir(log_dir) if d.startswith('runs')]\n",
    "    if not run_dirs:\n",
    "        raise ValueError(f\"No run directories found in {log_dir}\")\n",
    "    \n",
    "    # Get the most recent run\n",
    "    latest_run = sorted(run_dirs)[-1]\n",
    "    log_path = os.path.join(log_dir, latest_run)\n",
    "    \n",
    "    # Load the event file\n",
    "    event_acc = EventAccumulator(log_path)\n",
    "    event_acc.Reload()\n",
    "    \n",
    "    # Plot each metric\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for metric in metrics:\n",
    "        # Get training data\n",
    "        train_metric = f'train/{metric}'\n",
    "        if train_metric in event_acc.Tags()['scalars']:\n",
    "            train_events = event_acc.Scalars(train_metric)\n",
    "            train_steps = [e.step for e in train_events]\n",
    "            train_values = [e.value for e in train_events]\n",
    "            \n",
    "            # Apply smoothing\n",
    "            if smooth > 0:\n",
    "                smooth_values = [train_values[0]]\n",
    "                for val in train_values[1:]:\n",
    "                    smooth_values.append(smooth * smooth_values[-1] + (1 - smooth) * val)\n",
    "                train_values = smooth_values\n",
    "            \n",
    "            plt.plot(train_steps, train_values, label=f'Train {metric}', alpha=0.7)\n",
    "        \n",
    "        # Get validation data if it exists\n",
    "        eval_metric = f'eval/{metric}'\n",
    "        if eval_metric in event_acc.Tags()['scalars']:\n",
    "            eval_events = event_acc.Scalars(eval_metric)\n",
    "            eval_steps = [e.step for e in eval_events]\n",
    "            eval_values = [e.value for e in eval_events]\n",
    "            \n",
    "            # For eval, we might have fewer points, so we'll plot them directly\n",
    "            plt.scatter(eval_steps, eval_values, label=f'Val {metric}', alpha=0.7, marker='o')\n",
    "    \n",
    "    plt.title('Training and Validation Metrics')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage - update the path to your logs directory\n",
    "log_dir = \"./runs/ast_classifier\"  # Update this to your actual log directory\n",
    "try:\n",
    "    plot_training_curves(log_dir, metrics=('loss',))\n",
    "except Exception as e:\n",
    "    print(f\"Error plotting: {e}\")\n",
    "    print(\"Make sure tensorboard is installed and the log directory is correct.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
