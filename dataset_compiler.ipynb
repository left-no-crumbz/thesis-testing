{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d182970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# 1. Gather file paths\n",
    "fake_dirs = [\n",
    "    r\"for-2seconds\\testing\\fake\",\n",
    "    r\"for-2seconds\\training\\fake\",\n",
    "    r\"for-2seconds\\validation\\fake\",\n",
    "    r\"release_in_the_wild\\fake\",\n",
    "    r\"generated_audio\\fake\\common_voices_prompts_from_conformer_fastspeech2_pwg_ljspeech\",\n",
    "    r\"generated_audio\\fake\\jsut_multi_band_melgan\",\n",
    "    r\"generated_audio\\fake\\jsut_parallel_wavegan\",\n",
    "    r\"generated_audio\\fake\\ljspeech_full_band_melgan\",\n",
    "    r\"generated_audio\\fake\\ljspeech_hifiGAN\",\n",
    "    r\"generated_audio\\fake\\ljspeech_melgan\",\n",
    "    r\"generated_audio\\fake\\ljspeech_melgan_large\",\n",
    "    r\"generated_audio\\fake\\ljspeech_multi_band_melgan\",\n",
    "    r\"generated_audio\\fake\\ljspeech_parallel_wavegan\",\n",
    "    r\"generated_audio\\fake\\ljspeech_waveglow\",\n",
    "]\n",
    "\n",
    "real_dirs = [\n",
    "    r\"for-2seconds\\testing\\real\",\n",
    "    r\"for-2seconds\\training\\real\",\n",
    "    r\"for-2seconds\\validation\\real\",\n",
    "    r\"common-voices-mozilla\\cv-valid-train\\wav-files\"\n",
    "]\n",
    "\n",
    "def get_files_from_directory(folder, ext=\".wav\"):\n",
    "    \"\"\"Get files from a single directory - used for parallel processing.\"\"\"\n",
    "    folder_path = Path(folder)\n",
    "    if folder_path.exists():\n",
    "        return list(folder_path.rglob(f\"*{ext}\"))\n",
    "    return []\n",
    "\n",
    "def get_all_audio_files_parallel(folder_list, ext=\".wav\", max_workers=None):\n",
    "    \"\"\"Efficiently gather all audio files using parallel processing.\"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = min(len(folder_list), mp.cpu_count())\n",
    "    \n",
    "    all_files = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_folder = {\n",
    "            executor.submit(get_files_from_directory, folder, ext): folder \n",
    "            for folder in folder_list\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(future_to_folder):\n",
    "            folder = future_to_folder[future]\n",
    "            try:\n",
    "                files = future.result()\n",
    "                all_files.extend([str(p) for p in files])\n",
    "                print(f\"Scanned {folder}: {len(files)} files\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error scanning {folder}: {e}\")\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "def generate_unique_filename(filepath, destination_dir):\n",
    "    \"\"\"Generate a unique filename to avoid collisions.\"\"\"\n",
    "    path = Path(filepath)\n",
    "    name = path.stem\n",
    "    ext = path.suffix\n",
    "    counter = 1\n",
    "    \n",
    "    new_path = destination_dir / f\"{name}{ext}\"\n",
    "    while new_path.exists():\n",
    "        new_path = destination_dir / f\"{name}_{counter}{ext}\"\n",
    "        counter += 1\n",
    "    \n",
    "    return new_path\n",
    "\n",
    "def copy_file_safe(src_path, dst_dir):\n",
    "    \"\"\"Safely copy a file with unique naming and return manifest entry.\"\"\"\n",
    "    try:\n",
    "        src = Path(src_path)\n",
    "        dst = generate_unique_filename(src, dst_dir)\n",
    "        \n",
    "        # Copy file\n",
    "        shutil.copy2(src, dst)\n",
    "        \n",
    "        return {\n",
    "            'filepath': str(dst.relative_to(dst_dir.parent.parent)),\n",
    "            'label': dst_dir.name,\n",
    "            'success': True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'filepath': src_path, # Return the original source path on failure\n",
    "            'label': dst_dir.name,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def copy_files_parallel(file_paths, destination_dir, max_workers=None):\n",
    "    \"\"\"Copy multiple files in parallel and return successes and failures.\"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = min(32, mp.cpu_count() * 2)\n",
    "    \n",
    "    destination_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(copy_file_safe, src_path, destination_dir)\n",
    "            for src_path in file_paths\n",
    "        ]\n",
    "        \n",
    "        desc = f\"Copying to {destination_dir.relative_to(Path('dataset'))}\"\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=desc):\n",
    "            results.append(future.result())\n",
    "            \n",
    "    # MODIFIED: Separate successful from failed results to be returned\n",
    "    successful_results = []\n",
    "    failed_results = []\n",
    "    \n",
    "    for r in results:\n",
    "        if r['success']:\n",
    "            successful_results.append({\n",
    "                'filepath': r['filepath'],\n",
    "                'label': r['label']\n",
    "            })\n",
    "        else:\n",
    "            failed_results.append(r)\n",
    "    \n",
    "    # MODIFIED: Return both lists\n",
    "    return successful_results, failed_results\n",
    "\n",
    "def check_existing_splits(base_dir):\n",
    "    \"\"\"Check which splits already exist and return completed ones.\"\"\"\n",
    "    completed_splits = []\n",
    "    base_path = Path(base_dir)\n",
    "    \n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        split_dir = base_path / split_name\n",
    "        manifest_file = base_path / f'{split_name}.csv'\n",
    "        \n",
    "        if split_dir.exists() and manifest_file.exists():\n",
    "            fake_count = len(list((split_dir / 'fake').glob('*.wav')))\n",
    "            real_count = len(list((split_dir / 'real').glob('*.wav')))\n",
    "            \n",
    "            if fake_count > 0 and real_count > 0:\n",
    "                completed_splits.append(split_name)\n",
    "                print(f\"Found existing {split_name} split: {fake_count} fake, {real_count} real files\")\n",
    "    \n",
    "    return completed_splits\n",
    "\n",
    "def split_balanced_dataset(fake_files, real_files, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"Split dataset ensuring equal fake/real samples in each split.\"\"\"\n",
    "    \n",
    "    random.shuffle(fake_files)\n",
    "    random.shuffle(real_files)\n",
    "    \n",
    "    min_count = min(len(fake_files), len(real_files))\n",
    "    \n",
    "    fake_files = fake_files[:min_count]\n",
    "    real_files = real_files[:min_count]\n",
    "    \n",
    "    print(f\"Using {min_count} files per class for balanced dataset\")\n",
    "    \n",
    "    train_size = int(train_ratio * min_count)\n",
    "    val_size = int(val_ratio * min_count)\n",
    "    \n",
    "    print(f\"Split sizes - Train: {train_size}, Val: {val_size}, Test: {min_count - train_size - val_size}\")\n",
    "    \n",
    "    fake_train = fake_files[:train_size]\n",
    "    fake_val = fake_files[train_size:train_size + val_size]\n",
    "    fake_test = fake_files[train_size + val_size:]\n",
    "    \n",
    "    real_train = real_files[:train_size]\n",
    "    real_val = real_files[train_size:train_size + val_size]\n",
    "    real_test = real_files[train_size + val_size:]\n",
    "    \n",
    "    return {\n",
    "        'train': {'fake': fake_train, 'real': real_train},\n",
    "        'val': {'fake': fake_val, 'real': real_val},\n",
    "        'test': {'fake': fake_test, 'real': real_test},\n",
    "    }\n",
    "\n",
    "# --- Main Execution ---\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Gathering files in parallel...\")\n",
    "fake_files = get_all_audio_files_parallel(fake_dirs)\n",
    "real_files = get_all_audio_files_parallel(real_dirs)\n",
    "\n",
    "print(f\"\\nTotal fake files found: {len(fake_files)}\")\n",
    "print(f\"Total real files found: {len(real_files)}\")\n",
    "\n",
    "print(\"\\nCreating balanced splits...\")\n",
    "splits = split_balanced_dataset(fake_files, real_files)\n",
    "\n",
    "base_dir = Path('dataset')\n",
    "log_file = Path('failed_copies.log') # MODIFIED: Define log file path\n",
    "\n",
    "# Clear the log file at the start of a new run\n",
    "if log_file.exists():\n",
    "    log_file.unlink()\n",
    "\n",
    "print(f\"\\nChecking for existing splits... (Failed filepaths will be logged to '{log_file}')\")\n",
    "completed_splits = check_existing_splits(base_dir)\n",
    "\n",
    "if completed_splits:\n",
    "    print(f\"Found existing splits: {', '.join(completed_splits)}\")\n",
    "\n",
    "print(\"\\nCreating directory structure and copying files...\")\n",
    "print(f\"Using up to {min(32, mp.cpu_count() * 2)} threads for file copying...\")\n",
    "\n",
    "for split_name, classes in splits.items():\n",
    "    if split_name in completed_splits:\n",
    "        print(f\"\\nSkipping {split_name} split (already exists)...\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nProcessing {split_name} split...\")\n",
    "    all_manifest_rows = []\n",
    "    \n",
    "    for label, files in classes.items():\n",
    "        out_dir = base_dir / split_name / label\n",
    "        \n",
    "        # MODIFIED: Capture both successful and failed copies\n",
    "        successful_copies, failed_copies = copy_files_parallel(files, out_dir)\n",
    "        all_manifest_rows.extend(successful_copies)\n",
    "\n",
    "        # MODIFIED: If there were failures, log them to the file\n",
    "        if failed_copies:\n",
    "            print(f\"  └─ ⚠️  Warning: {len(failed_copies)} files failed to copy for this batch.\")\n",
    "            with open(log_file, 'a', encoding='utf-8') as f:\n",
    "                f.write(f\"\\n--- Failures for split='{split_name}', label='{label}' ---\\n\")\n",
    "                for failure in failed_copies:\n",
    "                    # Write the source filepath and the error message\n",
    "                    f.write(f\"{failure['filepath']} | Error: {failure['error']}\\n\")\n",
    "    \n",
    "    manifest_path = base_dir / f'{split_name}.csv'\n",
    "    with open(manifest_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['filepath', 'label'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_manifest_rows)\n",
    "    \n",
    "    print(f\"  -> Created manifest: {manifest_path}\")\n",
    "    print(f\"  -> Total files in {split_name}: {len(all_manifest_rows)}\")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(\"\\nDataset creation complete!\")\n",
    "print(f\"Total time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Final verification and summary\n",
    "print(\"\\nVerification:\")\n",
    "total_files_copied = 0\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    split_dir = base_dir / split_name\n",
    "    if not split_dir.exists(): \n",
    "        continue\n",
    "    \n",
    "    fake_count = len(list((split_dir / 'fake').glob('*.wav')))\n",
    "    real_count = len(list((split_dir / 'real').glob('*.wav')))\n",
    "    total_files_copied += fake_count + real_count\n",
    "    print(f\"{split_name}: {fake_count} fake, {real_count} real (balanced: {fake_count == real_count})\")\n",
    "\n",
    "if log_file.exists():\n",
    "    print(f\"\\n❗️ A log of failed file copies was created at: {log_file.resolve()}\")\n",
    "\n",
    "if total_time > 0:\n",
    "    print(f\"\\nPerformance: {total_files_copied / total_time:.1f} files/second\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
