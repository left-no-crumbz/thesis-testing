{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41789622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering files in parallel...\n",
      "Scanned for-2seconds\\testing\\fake: 544 files\n",
      "Scanned for-2seconds\\validation\\fake: 1413 files\n",
      "Scanned release_in_the_wild\\fake: 31779 files\n",
      "Scanned generated_audio\\fake\\jsut_multi_band_melgan: 5000 files\n",
      "Scanned for-2seconds\\training\\fake: 6978 files\n",
      "Scanned generated_audio\\fake\\common_voices_prompts_from_conformer_fastspeech2_pwg_ljspeech: 16283 files\n",
      "Scanned generated_audio\\fake\\jsut_parallel_wavegan: 5000 files\n",
      "Scanned generated_audio\\fake\\ljspeech_full_band_melgan: 13100 files\n",
      "Scanned generated_audio\\fake\\ljspeech_hifiGAN: 13100 files\n",
      "Scanned generated_audio\\fake\\ljspeech_melgan: 13100 files\n",
      "Scanned generated_audio\\fake\\ljspeech_melgan_large: 13100 files\n",
      "Scanned generated_audio\\fake\\ljspeech_multi_band_melgan: 13100 files\n",
      "Scanned generated_audio\\fake\\ljspeech_parallel_wavegan: 13100 files\n",
      "Scanned generated_audio\\fake\\ljspeech_waveglow: 13100 files\n",
      "Scanned for-2seconds\\testing\\real: 544 files\n",
      "Scanned for-2seconds\\validation\\real: 1413 files\n",
      "Scanned for-2seconds\\training\\real: 6978 files\n",
      "Scanned common-voices-mozilla\\cv-valid-train\\wav-files: 149762 files\n",
      "\n",
      "Total fake files found: 158697\n",
      "Total real files found: 158697\n",
      "\n",
      "Creating balanced splits...\n",
      "Using 158697 files per class for balanced dataset\n",
      "Split sizes - Train: 126957, Val: 15869, Test: 15871\n",
      "\n",
      "Checking for existing splits... (Detailed logs will be saved to 'dataset\\logs')\n",
      "\n",
      "Creating directory structure and copying files...\n",
      "Using up to 8 threads for file copying...\n",
      "\n",
      "Processing train split...\n",
      "\n",
      "  Processing train/fake (126957 files)...\n",
      "    Expected files logged to: train_fake_EXPECTED.json\n",
      "    Starting copy of 126957 files with 8 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying to train\\fake: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126957/126957 [08:07<00:00, 260.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual files logged to: train_fake_ACTUAL.json\n",
      "    Diff analysis created: train_fake_DIFF.txt\n",
      "    ‚úÖ 84488/84488 files copied successfully\n",
      "    üö® DISK VERIFICATION MISMATCH!\n",
      "       Tracked successful: 84488\n",
      "       Actually on disk: 126957\n",
      "\n",
      "  Processing train/real (126957 files)...\n",
      "    Expected files logged to: train_real_EXPECTED.json\n",
      "    Starting copy of 126957 files with 8 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying to train\\real: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126957/126957 [08:46<00:00, 241.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual files logged to: train_real_ACTUAL.json\n",
      "    Diff analysis created: train_real_DIFF.txt\n",
      "    ‚úÖ 126875/126875 files copied successfully\n",
      "    üö® DISK VERIFICATION MISMATCH!\n",
      "       Tracked successful: 126875\n",
      "       Actually on disk: 126957\n",
      "  -> Created manifest: dataset\\train.csv\n",
      "  -> Total files in train: 253914\n",
      "\n",
      "Processing val split...\n",
      "\n",
      "  Processing val/fake (15869 files)...\n",
      "    Expected files logged to: val_fake_EXPECTED.json\n",
      "    Starting copy of 15869 files with 8 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying to val\\fake: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15869/15869 [00:53<00:00, 297.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual files logged to: val_fake_ACTUAL.json\n",
      "    Diff analysis created: val_fake_DIFF.txt\n",
      "    ‚úÖ 14674/14674 files copied successfully\n",
      "    üö® DISK VERIFICATION MISMATCH!\n",
      "       Tracked successful: 14674\n",
      "       Actually on disk: 15869\n",
      "\n",
      "  Processing val/real (15869 files)...\n",
      "    Expected files logged to: val_real_EXPECTED.json\n",
      "    Starting copy of 15869 files with 8 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying to val\\real: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15869/15869 [01:03<00:00, 249.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual files logged to: val_real_ACTUAL.json\n",
      "    Diff analysis created: val_real_DIFF.txt\n",
      "    ‚úÖ 15866/15866 files copied successfully\n",
      "    üö® DISK VERIFICATION MISMATCH!\n",
      "       Tracked successful: 15866\n",
      "       Actually on disk: 15869\n",
      "  -> Created manifest: dataset\\val.csv\n",
      "  -> Total files in val: 31738\n",
      "\n",
      "Processing test split...\n",
      "\n",
      "  Processing test/fake (15871 files)...\n",
      "    Expected files logged to: test_fake_EXPECTED.json\n",
      "    Starting copy of 15871 files with 8 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying to test\\fake: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15871/15871 [00:51<00:00, 310.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual files logged to: test_fake_ACTUAL.json\n",
      "    Diff analysis created: test_fake_DIFF.txt\n",
      "    ‚úÖ 14590/14590 files copied successfully\n",
      "    üö® DISK VERIFICATION MISMATCH!\n",
      "       Tracked successful: 14590\n",
      "       Actually on disk: 15871\n",
      "\n",
      "  Processing test/real (15871 files)...\n",
      "    Expected files logged to: test_real_EXPECTED.json\n",
      "    Starting copy of 15871 files with 8 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying to test\\real: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15871/15871 [01:01<00:00, 259.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual files logged to: test_real_ACTUAL.json\n",
      "    Diff analysis created: test_real_DIFF.txt\n",
      "    ‚úÖ 15871/15871 files copied successfully\n",
      "  -> Created manifest: dataset\\test.csv\n",
      "  -> Total files in test: 31742\n",
      "\n",
      "============================================================\n",
      "DATASET CREATION COMPLETE!\n",
      "============================================================\n",
      "Total time: 1451.96 seconds\n",
      "\n",
      "OVERALL STATISTICS:\n",
      "Expected files: 317394\n",
      "Successful copies: 317394\n",
      "Failed copies: 0\n",
      "Missing files: 0\n",
      "\n",
      "FINAL DISK VERIFICATION:\n",
      "  train: 126957 fake, 126957 real = 253914 total ‚úÖ\n",
      "  val: 15869 fake, 15869 real = 31738 total ‚úÖ\n",
      "  test: 15871 fake, 15871 real = 31742 total ‚úÖ\n",
      "  GRAND TOTAL ON DISK: 317394\n",
      "\n",
      "Performance: 218.6 files/second\n",
      "Success rate: 100.00%\n",
      "\n",
      "üìã Comprehensive per-split logs available in: C:\\Users\\Crumbz\\Desktop\\4TH-YEAR-1ST-SEM\\THESIS\\thesis-testing\\dataset\\logs\n",
      "   - *_EXPECTED.json: All files that should have been copied\n",
      "   - *_ACTUAL.json: All files that were actually processed\n",
      "   - *_DIFF.json/.txt: Diff analysis showing missing/failed files\n",
      "\n",
      "üîç Use the DIFF files to identify exactly which files went missing!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import threading\n",
    "import hashlib\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "class ThreadSafeFileTracker:\n",
    "    \"\"\"Enhanced thread-safe file tracking with comprehensive logging.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._lock = threading.RLock()  # Reentrant lock for nested calls\n",
    "        self._filename_counter_lock = threading.RLock()\n",
    "        self._processed_files = set()\n",
    "        self._failed_files = []\n",
    "        self._successful_files = []\n",
    "        self._attempts = defaultdict(int)\n",
    "        self._filename_counters = defaultdict(int)  # For unique filename generation\n",
    "        self._reserved_filenames = set()  # Track reserved filenames\n",
    "    \n",
    "    def reserve_unique_filename(self, base_path, destination_dir):\n",
    "        \"\"\"Thread-safe unique filename reservation.\"\"\"\n",
    "        with self._filename_counter_lock:\n",
    "            path = Path(base_path)\n",
    "            name = path.stem\n",
    "            ext = path.suffix\n",
    "            \n",
    "            counter = 1\n",
    "            new_name = f\"{name}{ext}\"\n",
    "            new_path = destination_dir / new_name\n",
    "            \n",
    "            # Check both filesystem and our reservation system\n",
    "            while (new_path.exists() or new_name in self._reserved_filenames):\n",
    "                new_name = f\"{name}_{counter}{ext}\"\n",
    "                new_path = destination_dir / new_name\n",
    "                counter += 1\n",
    "            \n",
    "            # Reserve this filename\n",
    "            self._reserved_filenames.add(new_name)\n",
    "            return new_path, new_name\n",
    "    \n",
    "    def release_filename_reservation(self, filename):\n",
    "        \"\"\"Release a filename reservation if copy failed.\"\"\"\n",
    "        with self._filename_counter_lock:\n",
    "            self._reserved_filenames.discard(filename)\n",
    "    \n",
    "    def mark_attempt(self, filepath):\n",
    "        with self._lock:\n",
    "            self._attempts[filepath] += 1\n",
    "            return self._attempts[filepath]\n",
    "    \n",
    "    def is_processed(self, filepath):\n",
    "        with self._lock:\n",
    "            return filepath in self._processed_files\n",
    "    \n",
    "    def mark_processed(self, filepath):\n",
    "        with self._lock:\n",
    "            self._processed_files.add(filepath)\n",
    "    \n",
    "    def add_success(self, result):\n",
    "        with self._lock:\n",
    "            self._successful_files.append(result)\n",
    "    \n",
    "    def add_failure(self, result):\n",
    "        with self._lock:\n",
    "            self._failed_files.append(result)\n",
    "    \n",
    "    def get_results(self):\n",
    "        with self._lock:\n",
    "            return self._successful_files.copy(), self._failed_files.copy()\n",
    "    \n",
    "    def get_stats(self):\n",
    "        with self._lock:\n",
    "            return {\n",
    "                'processed_count': len(self._processed_files),\n",
    "                'success_count': len(self._successful_files),\n",
    "                'failure_count': len(self._failed_files),\n",
    "                'total_attempts': sum(self._attempts.values()),\n",
    "                'reserved_filenames': len(self._reserved_filenames)\n",
    "            }\n",
    "\n",
    "class SplitLogger:\n",
    "    \"\"\"Handles comprehensive per-split logging for diff analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self._lock = threading.RLock()\n",
    "    \n",
    "    def log_expected_files(self, split_name, label, file_list):\n",
    "        \"\"\"Log all files that should be copied for a split/label.\"\"\"\n",
    "        log_file = self.log_dir / f\"{split_name}_{label}_EXPECTED.json\"\n",
    "        \n",
    "        expected_data = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'split': split_name,\n",
    "            'label': label,\n",
    "            'total_count': len(file_list),\n",
    "            'files': []\n",
    "        }\n",
    "        \n",
    "        for i, filepath in enumerate(file_list):\n",
    "            expected_data['files'].append({\n",
    "                'index': i,\n",
    "                'source_path': str(filepath),\n",
    "                'source_name': Path(filepath).name,\n",
    "                'source_size': self._safe_get_file_size(filepath)\n",
    "            })\n",
    "        \n",
    "        with open(log_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(expected_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return log_file\n",
    "    \n",
    "    def log_actual_files(self, split_name, label, successful_copies, failed_copies):\n",
    "        \"\"\"Log all files that were actually copied (or failed) for a split/label.\"\"\"\n",
    "        log_file = self.log_dir / f\"{split_name}_{label}_ACTUAL.json\"\n",
    "        \n",
    "        actual_data = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'split': split_name,\n",
    "            'label': label,\n",
    "            'successful_count': len(successful_copies),\n",
    "            'failed_count': len(failed_copies),\n",
    "            'successful_files': successful_copies,\n",
    "            'failed_files': failed_copies\n",
    "        }\n",
    "        \n",
    "        with open(log_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(actual_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return log_file\n",
    "    \n",
    "    def create_diff_analysis(self, split_name, label):\n",
    "        \"\"\"Create a diff analysis between expected and actual files.\"\"\"\n",
    "        expected_file = self.log_dir / f\"{split_name}_{label}_EXPECTED.json\"\n",
    "        actual_file = self.log_dir / f\"{split_name}_{label}_ACTUAL.json\"\n",
    "        diff_file = self.log_dir / f\"{split_name}_{label}_DIFF.json\"\n",
    "        \n",
    "        if not (expected_file.exists() and actual_file.exists()):\n",
    "            return None\n",
    "        \n",
    "        # Load data\n",
    "        with open(expected_file, 'r', encoding='utf-8') as f:\n",
    "            expected_data = json.load(f)\n",
    "        \n",
    "        with open(actual_file, 'r', encoding='utf-8') as f:\n",
    "            actual_data = json.load(f)\n",
    "        \n",
    "        # Create sets for comparison\n",
    "        expected_names = {Path(f['source_path']).name for f in expected_data['files']}\n",
    "        successful_names = set()\n",
    "        failed_names = set()\n",
    "        \n",
    "        # Extract names from successful copies\n",
    "        for success in actual_data['successful_files']:\n",
    "            # Get original filename from source_path if available\n",
    "            if 'source_path' in success:\n",
    "                successful_names.add(Path(success['source_path']).name)\n",
    "        \n",
    "        # Extract names from failed copies\n",
    "        for failure in actual_data['failed_files']:\n",
    "            if 'filepath' in failure:\n",
    "                failed_names.add(Path(failure['filepath']).name)\n",
    "        \n",
    "        # Perform diff analysis\n",
    "        attempted_names = successful_names | failed_names\n",
    "        missing_names = expected_names - attempted_names\n",
    "        \n",
    "        diff_analysis = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'split': split_name,\n",
    "            'label': label,\n",
    "            'summary': {\n",
    "                'expected_total': len(expected_names),\n",
    "                'successful_total': len(successful_names),\n",
    "                'failed_total': len(failed_names),\n",
    "                'missing_total': len(missing_names),\n",
    "                'attempted_total': len(attempted_names)\n",
    "            },\n",
    "            'missing_files': sorted(list(missing_names)),\n",
    "            'successful_files': sorted(list(successful_names)),\n",
    "            'failed_files': sorted(list(failed_names))\n",
    "        }\n",
    "        \n",
    "        # Create human-readable diff report\n",
    "        readable_diff_file = self.log_dir / f\"{split_name}_{label}_DIFF.txt\"\n",
    "        with open(readable_diff_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"DIFF ANALYSIS: {split_name}/{label}\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(\"SUMMARY:\\n\")\n",
    "            f.write(f\"  Expected files: {diff_analysis['summary']['expected_total']}\\n\")\n",
    "            f.write(f\"  Successful copies: {diff_analysis['summary']['successful_total']}\\n\")\n",
    "            f.write(f\"  Failed copies: {diff_analysis['summary']['failed_total']}\\n\")\n",
    "            f.write(f\"  Missing files: {diff_analysis['summary']['missing_total']}\\n\")\n",
    "            f.write(f\"  Files attempted: {diff_analysis['summary']['attempted_total']}\\n\\n\")\n",
    "            \n",
    "            if missing_names:\n",
    "                f.write(f\"MISSING FILES ({len(missing_names)}):\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                for name in sorted(missing_names):\n",
    "                    f.write(f\"  {name}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            if failed_names:\n",
    "                f.write(f\"FAILED FILES ({len(failed_names)}):\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                for name in sorted(failed_names):\n",
    "                    f.write(f\"  {name}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            success_rate = (len(successful_names) / len(expected_names)) * 100 if expected_names else 0\n",
    "            f.write(f\"SUCCESS RATE: {success_rate:.2f}%\\n\")\n",
    "        \n",
    "        # Save JSON diff\n",
    "        with open(diff_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(diff_analysis, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return diff_file, readable_diff_file, diff_analysis\n",
    "    \n",
    "    def _safe_get_file_size(self, filepath):\n",
    "        \"\"\"Safely get file size, return -1 if not accessible.\"\"\"\n",
    "        try:\n",
    "            return Path(filepath).stat().st_size\n",
    "        except Exception:\n",
    "            return -1\n",
    "\n",
    "def get_files_from_directory(folder, ext=\".wav\"):\n",
    "    \"\"\"Get files from a single directory - used for parallel processing.\"\"\"\n",
    "    folder_path = Path(folder)\n",
    "    if folder_path.exists():\n",
    "        return list(folder_path.rglob(f\"*{ext}\"))\n",
    "    return []\n",
    "\n",
    "def get_all_audio_files_parallel(folder_list, ext=\".wav\", max_workers=None):\n",
    "    \"\"\"Efficiently gather all audio files using parallel processing.\"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = min(len(folder_list), mp.cpu_count())\n",
    "    \n",
    "    all_files = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_folder = {\n",
    "            executor.submit(get_files_from_directory, folder, ext): folder \n",
    "            for folder in folder_list\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(future_to_folder):\n",
    "            folder = future_to_folder[future]\n",
    "            try:\n",
    "                files = future.result()\n",
    "                all_files.extend([str(p) for p in files])\n",
    "                print(f\"Scanned {folder}: {len(files)} files\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error scanning {folder}: {e}\")\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "def copy_file_atomic(src_path, dst_dir, file_tracker, max_retries=3, retry_delay=1):\n",
    "    \"\"\"\n",
    "    Atomically copy a file with race-condition-free unique naming.\n",
    "    \"\"\"\n",
    "    # Early duplicate check\n",
    "    if file_tracker.is_processed(src_path):\n",
    "        return {\n",
    "            'filepath': src_path,\n",
    "            'label': dst_dir.name,\n",
    "            'success': False,\n",
    "            'error': 'Already processed by another thread',\n",
    "            'status': 'duplicate_attempt'\n",
    "        }\n",
    "    \n",
    "    attempt_num = file_tracker.mark_attempt(src_path)\n",
    "    reserved_filename = None\n",
    "    \n",
    "    try:\n",
    "        src = Path(src_path)\n",
    "        \n",
    "        # Verify source file\n",
    "        if not src.exists():\n",
    "            return {\n",
    "                'filepath': src_path,\n",
    "                'label': dst_dir.name,\n",
    "                'success': False,\n",
    "                'error': f\"Source file does not exist: {src_path}\",\n",
    "                'status': 'source_missing',\n",
    "                'attempt': attempt_num\n",
    "            }\n",
    "        \n",
    "        # Check file size\n",
    "        try:\n",
    "            file_size = src.stat().st_size\n",
    "            if file_size == 0:\n",
    "                return {\n",
    "                    'filepath': src_path,\n",
    "                    'label': dst_dir.name,\n",
    "                    'success': False,\n",
    "                    'error': f\"Source file is empty: {src_path}\",\n",
    "                    'status': 'empty_file',\n",
    "                    'attempt': attempt_num\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'filepath': src_path,\n",
    "                'label': dst_dir.name,\n",
    "                'success': False,\n",
    "                'error': f\"Cannot access source file: {e}\",\n",
    "                'status': 'access_error',\n",
    "                'attempt': attempt_num\n",
    "            }\n",
    "        \n",
    "        # Reserve unique filename atomically\n",
    "        dst_path, reserved_filename = file_tracker.reserve_unique_filename(src, dst_dir)\n",
    "        \n",
    "        # Mark as processed BEFORE actual copy to prevent race conditions\n",
    "        file_tracker.mark_processed(src_path)\n",
    "        \n",
    "        # Attempt copy with retries\n",
    "        last_error = None\n",
    "        for retry in range(max_retries):\n",
    "            try:\n",
    "                # Use a temporary file for atomic copy\n",
    "                temp_path = dst_path.with_suffix(f'.tmp_{uuid.uuid4().hex[:8]}')\n",
    "                \n",
    "                # Copy to temporary file first\n",
    "                shutil.copy2(src, temp_path)\n",
    "                \n",
    "                # Verify temporary file\n",
    "                if not temp_path.exists():\n",
    "                    raise Exception(\"Temporary file creation failed\")\n",
    "                \n",
    "                temp_size = temp_path.stat().st_size\n",
    "                if temp_size != file_size:\n",
    "                    raise Exception(f\"Size mismatch: expected {file_size}, got {temp_size}\")\n",
    "                \n",
    "                # Atomically rename to final destination\n",
    "                temp_path.rename(dst_path)\n",
    "                \n",
    "                # Final verification\n",
    "                if not dst_path.exists():\n",
    "                    raise Exception(\"Final file does not exist after rename\")\n",
    "                \n",
    "                final_size = dst_path.stat().st_size\n",
    "                if final_size != file_size:\n",
    "                    raise Exception(f\"Final size mismatch: expected {file_size}, got {final_size}\")\n",
    "                \n",
    "                # Success!\n",
    "                result = {\n",
    "                    'filepath': str(dst_path.relative_to(dst_dir.parent.parent)),\n",
    "                    'label': dst_dir.name,\n",
    "                    'success': True,\n",
    "                    'source_path': src_path,\n",
    "                    'destination_path': str(dst_path),\n",
    "                    'source_size': file_size,\n",
    "                    'destination_size': final_size,\n",
    "                    'attempt': attempt_num,\n",
    "                    'retry': retry + 1,\n",
    "                    'verified': True\n",
    "                }\n",
    "                file_tracker.add_success(result)\n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                last_error = str(e)\n",
    "                \n",
    "                # Clean up any temporary files\n",
    "                try:\n",
    "                    if 'temp_path' in locals() and temp_path.exists():\n",
    "                        temp_path.unlink()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                if retry < max_retries - 1:\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 1.5  # Exponential backoff\n",
    "        \n",
    "        # All retries failed - release filename reservation\n",
    "        if reserved_filename:\n",
    "            file_tracker.release_filename_reservation(reserved_filename)\n",
    "        \n",
    "        result = {\n",
    "            'filepath': src_path,\n",
    "            'label': dst_dir.name,\n",
    "            'success': False,\n",
    "            'error': f\"Failed after {max_retries} attempts. Last error: {last_error}\",\n",
    "            'status': 'copy_failed',\n",
    "            'attempt': attempt_num,\n",
    "            'max_retries_reached': True\n",
    "        }\n",
    "        file_tracker.add_failure(result)\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Release reservation on unexpected error\n",
    "        if reserved_filename:\n",
    "            file_tracker.release_filename_reservation(reserved_filename)\n",
    "            \n",
    "        result = {\n",
    "            'filepath': src_path,\n",
    "            'label': dst_dir.name,\n",
    "            'success': False,\n",
    "            'error': f\"Unexpected error: {str(e)}\",\n",
    "            'status': 'unexpected_error',\n",
    "            'attempt': attempt_num\n",
    "        }\n",
    "        file_tracker.add_failure(result)\n",
    "        return result\n",
    "\n",
    "def copy_files_with_comprehensive_logging(file_paths, destination_dir, split_name, label, \n",
    "                                        split_logger, max_workers=None):\n",
    "    \"\"\"Copy files with comprehensive per-split logging and diff analysis.\"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = min(32, mp.cpu_count() * 2)\n",
    "    \n",
    "    destination_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Initialize fresh tracker for this batch\n",
    "    file_tracker = ThreadSafeFileTracker()\n",
    "    \n",
    "    # Log expected files\n",
    "    expected_log_file = split_logger.log_expected_files(split_name, label, file_paths)\n",
    "    print(f\"    Expected files logged to: {expected_log_file.name}\")\n",
    "    \n",
    "    print(f\"    Starting copy of {len(file_paths)} files with {max_workers} workers...\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(copy_file_atomic, src_path, destination_dir, file_tracker)\n",
    "            for src_path in file_paths\n",
    "        ]\n",
    "        \n",
    "        desc = f\"Copying to {destination_dir.relative_to(Path('dataset'))}\"\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=desc):\n",
    "            _ = future.result()\n",
    "    \n",
    "    # Get results\n",
    "    successful_results, failed_results = file_tracker.get_results()\n",
    "    \n",
    "    # Separate duplicates from real failures\n",
    "    duplicate_results = [r for r in failed_results if r.get('is_duplicate', False)]\n",
    "    actual_failed_results = [r for r in failed_results if not r.get('is_duplicate', False)]\n",
    "    \n",
    "    # Log actual files (excluding duplicates from failures)\n",
    "    actual_log_file = split_logger.log_actual_files(split_name, label, successful_results, actual_failed_results)\n",
    "    print(f\"    Actual files logged to: {actual_log_file.name}\")\n",
    "    \n",
    "    # Create diff analysis (using actual failures, not duplicates)\n",
    "    diff_files = split_logger.create_diff_analysis(split_name, label)\n",
    "    if diff_files:\n",
    "        diff_json, diff_txt, diff_data = diff_files\n",
    "        print(f\"    Diff analysis created: {diff_txt.name}\")\n",
    "        \n",
    "        # Print improved summary\n",
    "        summary = diff_data['summary']\n",
    "        if summary['missing_total'] > 0:\n",
    "            print(f\"    üö® {summary['missing_total']} files went MISSING (never attempted)\")\n",
    "        if summary['failed_total'] > 0:\n",
    "            print(f\"    ‚ùå {summary['failed_total']} files FAILED during copy\")\n",
    "        \n",
    "        # Show duplicate information\n",
    "        if duplicate_results:\n",
    "            print(f\"    üîÑ {len(duplicate_results)} duplicate attempts (expected in multithreading)\")\n",
    "        \n",
    "        print(f\"    ‚úÖ {summary['successful_total']}/{summary['expected_total']} files copied successfully\")\n",
    "        \n",
    "        # Verify against actual disk count\n",
    "        actual_files_on_disk = len(list(destination_dir.glob('*.wav')))\n",
    "        expected_on_disk = summary['successful_total']  # This should now match\n",
    "        \n",
    "        if actual_files_on_disk != expected_on_disk:\n",
    "            print(\"    üö® DISK VERIFICATION MISMATCH!\")\n",
    "            print(f\"       Tracked successful: {expected_on_disk}\")\n",
    "            print(f\"       Actually on disk: {actual_files_on_disk}\")\n",
    "        else:\n",
    "            print(f\"    ‚úÖ DISK VERIFICATION PASSED: {actual_files_on_disk} files on disk\")\n",
    "    \n",
    "    # Return clean results for manifest\n",
    "    clean_successful = []\n",
    "    for r in successful_results:\n",
    "        clean_successful.append({\n",
    "            'filepath': r['filepath'],\n",
    "            'label': r['label']\n",
    "        })\n",
    "    \n",
    "    # Update statistics to reflect actual situation\n",
    "    stats = file_tracker.get_stats()\n",
    "    stats['duplicate_count'] = len(duplicate_results)\n",
    "    stats['actual_failure_count'] = len(actual_failed_results)\n",
    "    \n",
    "    return clean_successful, actual_failed_results, stats\n",
    "\n",
    "def check_existing_splits(base_dir):\n",
    "    \"\"\"Check which splits already exist and return completed ones.\"\"\"\n",
    "    completed_splits = []\n",
    "    base_path = Path(base_dir)\n",
    "    \n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        split_dir = base_path / split_name\n",
    "        manifest_file = base_path / f'{split_name}.csv'\n",
    "        \n",
    "        if split_dir.exists() and manifest_file.exists():\n",
    "            fake_count = len(list((split_dir / 'fake').glob('*.wav')))\n",
    "            real_count = len(list((split_dir / 'real').glob('*.wav')))\n",
    "            \n",
    "            if fake_count > 0 and real_count > 0:\n",
    "                completed_splits.append(split_name)\n",
    "                print(f\"Found existing {split_name} split: {fake_count} fake, {real_count} real files\")\n",
    "    \n",
    "    return completed_splits\n",
    "\n",
    "def split_balanced_dataset(fake_files, real_files, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"Split dataset ensuring equal fake/real samples in each split.\"\"\"\n",
    "    random.shuffle(fake_files)\n",
    "    random.shuffle(real_files)\n",
    "    \n",
    "    min_count = min(len(fake_files), len(real_files))\n",
    "    \n",
    "    fake_files = fake_files[:min_count]\n",
    "    real_files = real_files[:min_count]\n",
    "    \n",
    "    print(f\"Using {min_count} files per class for balanced dataset\")\n",
    "    \n",
    "    train_size = int(train_ratio * min_count)\n",
    "    val_size = int(val_ratio * min_count)\n",
    "    \n",
    "    print(f\"Split sizes - Train: {train_size}, Val: {val_size}, Test: {min_count - train_size - val_size}\")\n",
    "    \n",
    "    fake_train = fake_files[:train_size]\n",
    "    fake_val = fake_files[train_size:train_size + val_size]\n",
    "    fake_test = fake_files[train_size + val_size:]\n",
    "    \n",
    "    real_train = real_files[:train_size]\n",
    "    real_val = real_files[train_size:train_size + val_size]\n",
    "    real_test = real_files[train_size + val_size:]\n",
    "    \n",
    "    return {\n",
    "        'train': {'fake': fake_train, 'real': real_train},\n",
    "        'val': {'fake': fake_val, 'real': real_val},\n",
    "        'test': {'fake': fake_test, 'real': real_test},\n",
    "    }\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Directory configuration\n",
    "    fake_dirs = [\n",
    "        r\"for-2seconds\\testing\\fake\",\n",
    "        r\"for-2seconds\\training\\fake\",\n",
    "        r\"for-2seconds\\validation\\fake\",\n",
    "        r\"release_in_the_wild\\fake\",\n",
    "        r\"generated_audio\\fake\\common_voices_prompts_from_conformer_fastspeech2_pwg_ljspeech\",\n",
    "        r\"generated_audio\\fake\\jsut_multi_band_melgan\",\n",
    "        r\"generated_audio\\fake\\jsut_parallel_wavegan\",\n",
    "        r\"generated_audio\\fake\\ljspeech_full_band_melgan\",\n",
    "        r\"generated_audio\\fake\\ljspeech_hifiGAN\",\n",
    "        r\"generated_audio\\fake\\ljspeech_melgan\",\n",
    "        r\"generated_audio\\fake\\ljspeech_melgan_large\",\n",
    "        r\"generated_audio\\fake\\ljspeech_multi_band_melgan\",\n",
    "        r\"generated_audio\\fake\\ljspeech_parallel_wavegan\",\n",
    "        r\"generated_audio\\fake\\ljspeech_waveglow\",\n",
    "    ]\n",
    "    \n",
    "    real_dirs = [\n",
    "        r\"for-2seconds\\testing\\real\",\n",
    "        r\"for-2seconds\\training\\real\",\n",
    "        r\"for-2seconds\\validation\\real\",\n",
    "        r\"common-voices-mozilla\\cv-valid-train\\wav-files\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Gathering files in parallel...\")\n",
    "    fake_files = get_all_audio_files_parallel(fake_dirs)\n",
    "    real_files = get_all_audio_files_parallel(real_dirs)\n",
    "    \n",
    "    print(f\"\\nTotal fake files found: {len(fake_files)}\")\n",
    "    print(f\"Total real files found: {len(real_files)}\")\n",
    "    \n",
    "    print(\"\\nCreating balanced splits...\")\n",
    "    splits = split_balanced_dataset(fake_files, real_files)\n",
    "    \n",
    "    base_dir = Path('dataset')\n",
    "    log_dir = base_dir / 'logs'\n",
    "    \n",
    "    # Initialize split logger\n",
    "    split_logger = SplitLogger(log_dir)\n",
    "    \n",
    "    print(f\"\\nChecking for existing splits... (Detailed logs will be saved to '{log_dir}')\")\n",
    "    completed_splits = check_existing_splits(base_dir)\n",
    "    \n",
    "    if completed_splits:\n",
    "        print(f\"Found existing splits: {', '.join(completed_splits)}\")\n",
    "    \n",
    "    print(\"\\nCreating directory structure and copying files...\")\n",
    "    print(f\"Using up to {min(32, mp.cpu_count() * 2)} threads for file copying...\")\n",
    "    \n",
    "    # Overall tracking\n",
    "    overall_stats = {\n",
    "        'total_expected': 0,\n",
    "        'total_successful': 0,\n",
    "        'total_failed': 0,\n",
    "        'total_missing': 0\n",
    "    }\n",
    "    \n",
    "    for split_name, classes in splits.items():\n",
    "        if split_name in completed_splits:\n",
    "            print(f\"\\nSkipping {split_name} split (already exists)...\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing {split_name} split...\")\n",
    "        all_manifest_rows = []\n",
    "        \n",
    "        for label, files in classes.items():\n",
    "            out_dir = base_dir / split_name / label\n",
    "            \n",
    "            print(f\"\\n  Processing {split_name}/{label} ({len(files)} files)...\")\n",
    "            \n",
    "            # Copy files with comprehensive logging\n",
    "            successful_copies, failed_copies, copy_stats = copy_files_with_comprehensive_logging(\n",
    "                files, out_dir, split_name, label, split_logger\n",
    "            )\n",
    "            \n",
    "            all_manifest_rows.extend(successful_copies)\n",
    "            \n",
    "            # Update overall statistics\n",
    "            overall_stats['total_expected'] += len(files)\n",
    "            overall_stats['total_successful'] += len(successful_copies)\n",
    "            overall_stats['total_failed'] += len(failed_copies)\n",
    "            overall_stats['total_missing'] += len(files) - len(successful_copies) - len(failed_copies)\n",
    "        \n",
    "        # Create manifest\n",
    "        manifest_path = base_dir / f'{split_name}.csv'\n",
    "        with open(manifest_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['filepath', 'label'])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_manifest_rows)\n",
    "        \n",
    "        print(f\"  -> Created manifest: {manifest_path}\")\n",
    "        print(f\"  -> Total files in {split_name}: {len(all_manifest_rows)}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATASET CREATION COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Final comprehensive summary\n",
    "    print(\"\\nOVERALL STATISTICS:\")\n",
    "    print(f\"Expected files: {overall_stats['total_expected']}\")\n",
    "    print(f\"Successful copies: {overall_stats['total_successful']}\")\n",
    "    print(f\"Failed copies: {overall_stats['total_failed']}\")\n",
    "    print(f\"Missing files: {overall_stats['total_missing']}\")\n",
    "    \n",
    "    # Verify against actual disk\n",
    "    print(\"\\nFINAL DISK VERIFICATION:\")\n",
    "    actual_total_on_disk = 0\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        split_dir = base_dir / split_name\n",
    "        if not split_dir.exists(): \n",
    "            continue\n",
    "        \n",
    "        fake_count = len(list((split_dir / 'fake').glob('*.wav')))\n",
    "        real_count = len(list((split_dir / 'real').glob('*.wav')))\n",
    "        split_total = fake_count + real_count\n",
    "        actual_total_on_disk += split_total\n",
    "        balance_status = \"‚úÖ\" if fake_count == real_count else \"‚ö†Ô∏è\"\n",
    "        print(f\"  {split_name}: {fake_count} fake, {real_count} real = {split_total} total {balance_status}\")\n",
    "    \n",
    "    print(f\"  GRAND TOTAL ON DISK: {actual_total_on_disk}\")\n",
    "    \n",
    "    if actual_total_on_disk != overall_stats['total_successful']:\n",
    "        print(f\"üö® CRITICAL DISCREPANCY!\")\n",
    "        print(f\"   Tracked successful: {overall_stats['total_successful']}\")\n",
    "        print(f\"   Actually on disk: {actual_total_on_disk}\")\n",
    "    \n",
    "    # Performance metrics\n",
    "    if total_time > 0:\n",
    "        print(f\"\\nPerformance: {overall_stats['total_successful'] / total_time:.1f} files/second\")\n",
    "    if overall_stats['total_expected'] > 0:\n",
    "        print(f\"Success rate: {(overall_stats['total_successful'] / overall_stats['total_expected'] * 100):.2f}%\")\n",
    "    \n",
    "    print(f\"\\nüìã Comprehensive per-split logs available in: {log_dir.resolve()}\")\n",
    "    print(\"   - *_EXPECTED.json: All files that should have been copied\")\n",
    "    print(\"   - *_ACTUAL.json: All files that were actually processed\")\n",
    "    print(\"   - *_DIFF.json/.txt: Diff analysis showing missing/failed files\")\n",
    "    print(\"\\nüîç Use the DIFF files to identify exactly which files went missing!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
